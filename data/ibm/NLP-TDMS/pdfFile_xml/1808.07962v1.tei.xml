<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Human-Object Interactions by Graph Parsing Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
							<email>syqi@cs.ucla.eduwenguanwang.ai@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">International Center for AI and Robot Autonomy (CARA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
							<email>baoxiongjia@ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<email>shenjianbing@bit.edu.cnsczhu@stat.ucla.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">International Center for AI and Robot Autonomy (CARA)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Human-Object Interactions by Graph Parsing Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human-Object Interaction · Message Passing · Graph Pars- ing · Neural Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the task of detecting and recognizing human-object interactions (HOI) in images and videos. We introduce the Graph Parsing Neural Network (GPNN), a framework that incorporates structural knowledge while being differentiable end-to-end. For a given scene, GPNN infers a parse graph that includes i) the HOI graph structure represented by an adjacency matrix, and ii) the node labels. Within a message passing inference framework, GPNN iteratively computes the adjacency matrices and node labels. We extensively evaluate our model on three HOI detection benchmarks on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our approach significantly outperforms state-of-art methods, verifying that GPNN is scalable to large datasets and applies to spatial-temporal settings. The code is available at https://github.com/SiyuanQi/gpnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of human-object interaction (HOI) understanding aims to infer the relationships between human and objects, such as "riding a bike" or "washing a bike". Beyond traditional visual recognition of individual instances, e.g., human pose estimation, action recognition, and object detection, recognizing HOIs requires a deeper semantic understanding of image contents. Recently, deep neural networks (DNNs) have shown impressive progress on above individual tasks of instance recognition, while relatively few methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref> were proposed for HOI recognition. This is mainly because it requires reasoning beyond perception, by integrating information from human, objects, and their complex relationships.  <ref type="figure">Fig. 1</ref>. Illustration of the proposed GPNN for learning HOI. GPNN offers a generic HOI representation that applies to (a) HOI detection in images and (b) HOI recognition in videos. With the integration of graphical model and neural network, GPNN can iteratively learn/infer the graph structures (a.v) and message passing (a.vi). The final parse graph explains a given scene with the graph structure (e.g., the link between the person and the knife) and the node labels (e.g., lick). A thicker edge corresponds to stronger information flow between nodes in the graph.</p><p>In this paper, we propose a novel model, Graph Parsing Neural Network (GPNN), for HOI recognition. The proposed GPNN offers a general framework that explicitly represents HOI structures with graphs and automatically parses the optimal graph structures in an end-to-end manner. In principle, it is an generalization of Message Passing Neural Network (MPNN) <ref type="bibr" target="#b11">[12]</ref>. An overview of GPNN is shown in <ref type="figure">Fig. 1</ref>. The following two aspects motivate our design.</p><p>First, we seek a unified framework that utilizes the learning capability of neural networks and the power of graphical representations. Recent deep learning based HOI models showed promising results, but few touched how to interpret well and explicitly leverage spatial and temporal dependencies and human-object relations in such structured task. Aiming for this, we introduce GPNN. It inherits the complementary strengths of neural networks and graphical models, for forming a coherent HOI representation with strong learning ability. Specifically, with the structured representation of an HOI graph, the rich relations are explicitly utilized, and the information from individual elements can be efficiently integrated and broadcasted over the structures. The whole model and message passing operations are well-defined and fully differentiable. Thus it can be efficiently learned from data in an end-to-end manner.</p><p>Second, based on our efficient HOI representation and learning power, GPNN applies to diverse HOI tasks in both static and dynamic scenes. Previous studies for HOI achieved good performance in their specific domains (spatial <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> or temporal <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>). However, none of them addresses a generic framework for representing and learning HOI in both images and videos. The key difficulty lies in the diverse relations between components. Given a set of human and objects candidates, there may exist an uncertain number of human-object interaction pairs (see <ref type="figure">Fig. 1</ref> (a.ii) as an example). The relations become more complex after taking temporal factors into consideration. Thus pre-fixed graph structures, as adopted by most previous graphical or structured DNN models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref>, are not an optimal choice. Seeking a better generalization ability, GPNN incorporates an essential link function for addressing the problem of graph structure learning. It learns to infer the adjacency matrix in an end-to-end manner and thus can infer a parse graph that explicitly explains the HOI relations. With such learnable graph structure, GPNN could also limit the information flow from irrelevant nodes while encouraging message to propagate between related nodes, thus improving graph parsing.</p><p>We extensively evaluate the proposed GPNN on three HOI datasets, namely HICO-DET <ref type="bibr" target="#b0">[1]</ref>, V-COCO <ref type="bibr" target="#b16">[17]</ref> and CAD-120 <ref type="bibr" target="#b21">[22]</ref>, for HOI detection from images (HICO-DET, V-COCO) and HOI recognition and anticipation in spatialtemporal settings (CAD-120). The experimental results verify the generality and scalability of our GPNN based HOI representation and show substantial improvements over state-of-the-art approaches, including pure graphical models and pure neural networks. We also demonstrate GPNN outperforms its variants and other graph neural networks with pre-fixed structures.</p><p>This paper makes three major contributions. First, we propose the GPNN that incorporates structural knowledge and DNNs for learning and inference. Second, with a set of well defined modular functions, GPNN addresses the HOI problem by jointly performing graph structure inference and message passing. Third, we empirically show that GPNN offers a scalable and generic HOI representation that applies to both static and dynamic settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Human-Object Interaction. Reasoning human actions with objects (like "playing baseball", "playing guitar"), rather than recognizing individual actions ("playing") or object instances ("baseball", "guitar"), is essential for a more comprehensive understanding of what is happening in the scene. Early work in HOI understanding studied Bayesian model <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, utilized contextual relationship between human and objects <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref>, learned structured representations with spatial interaction and context <ref type="bibr" target="#b7">[8]</ref>, exploited compositional models <ref type="bibr" target="#b8">[9]</ref>, or referred to a set of HOI exemplars <ref type="bibr" target="#b18">[19]</ref>. They were mainly based on handcrafted features (e.g., color, HOG, and SIFT) with object and human detectors. More recently, inspired by the notable success of deep learning and the availability of large-scale HOI datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, several deep learning based HOI models were then proposed. Specifically, Mallya et al. <ref type="bibr" target="#b28">[29]</ref> modified Fast RCNN model <ref type="bibr" target="#b12">[13]</ref> for HOI recognition, with the assistance of Visual Question Answering (VQA). In <ref type="bibr" target="#b37">[38]</ref>, zero-shot learning was applied for addressing the long-tail problem in HOI recognition. In <ref type="bibr" target="#b0">[1]</ref>, the human proposals, object regions, and their combinations were fed into a multi-stream network for tackling the HOI detection problem. Gkioxari et al. <ref type="bibr" target="#b13">[14]</ref> estimated an action-type specific density map for identifying the interacted object locations, with a modified Faster RCNN architecture <ref type="bibr" target="#b35">[36]</ref>.</p><p>Although promising results were achieved by above deep HOI models, we still observe two unsolved issues. First, they lack a powerful tool to represent the structures in HOI tasks explicitly and encodes them into modern network architectures efficiently. Second, despite the successes in specific tasks, a complete and generic HOI representation is missing. These approaches can not be easily extended to HOI recognition from videos. Aiming to address those issues, we introduce GPNN for imposing high-level relations into DNN, leading to a powerful HOI representation that is applicable in both static and dynamic settings. Neural Networks with Graphs/Graphical Models. In the literature, some approaches were proposed to combine graphical models and neural networks. The most intuitive approach is to build graphical models upon DNN, where the network that generates features is trained first, and its output is used to compute potential functions for the graphical predictor. Typical methods were used in human pose estimation <ref type="bibr" target="#b41">[42]</ref>, human part parsing <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45]</ref>, and semantic image segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. These methods lack a deep integration in the sense that the computation process of graphical models cannot be learned end-toend. Some attempts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51]</ref> were made to generalize neural network operations (e.g., convolutions) directly from regular grids (e.g., images) to graphs. For the HOI problem, however, a structured representation is needed to capture the high-level spatial-temporal relations between humans and objects. Some other work integrated network architectures with graphical models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref> and gained promising results on applications such as scene understanding <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref>, object detection and parsing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50]</ref>, and VQA <ref type="bibr" target="#b40">[41]</ref>. However, these methods only apply to problems that have pre-fixed graph structures. Liang et al. <ref type="bibr" target="#b25">[26]</ref> merged graph nodes using Long Short-Term Memory (LSTM) for human parsing problem, under the assumption that the nodes are mergeable.</p><p>Those methods achieved promising results in their specific tasks and well demonstrated the benefit in completing deep architectures with domain-specific structures. However, most of them are based on pre-fixed graph structures, and they have not yet been studied in HOI recognition. In this work, we extend previous graphical neural networks with learnable graph structures, which well addresses the rich and high-level relations in HOI problems. The proposed GPNN can automatically infer the graph structure and utilize that structure for enhancing information propagation and further inference. It offers a generic HOI representation for both spatial and spatial-temporal settings. To the best of our knowledge, this is a first attempt to integrate graph models with neural networks in a unified framework to achieve state-of-art results in HOI recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Parsing Neural Network for HOI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>For HOI understanding, human and objects are represented by nodes, and their relations are defined as edges. Given a complete HOI graph that includes all the possible relationships among human and objects, we want to automatically infer a parse graph by keeping the meaningful edges and labeling the nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message Function Update Function Readout Function Link Function</head><p>Object Human</p><p>Object Action Action Action Iteration s = 0  <ref type="figure">Fig. 2</ref>. Illustration of the forward pass of GPNN. GPNN takes node and edge features as input, and outputs a parse graph in a message passing fasion. The structure of the parse graph is given by a soft adjacency matrix. It is computed by the link function based on the features (or hidden node states). The darker the color in the adjacency matrix, the stronger the connectivity is. Then message functions compute incoming messages for each node as a weighted sum of the messages from other nodes.</p><formula xml:id="formula_0">Feature matrix F 0 Adjacency matrix A 0 Parse graph g 0 F 1 A 1 g 1 F S A S</formula><p>Thicker edges indicate larger information flows. The update functions update the hidden internal states of each node. Above process is repeated for several steps, iteratively and jointly learning the computation of graph structures and message passing. Finally, for each node, the readout functions output HOI action or object labels from the hidden node states. See § 3 for more details.</p><p>Formally, let G = (V, E, Y) denote the complete HOI graph. Nodes v ∈ V take unique values from {1, · · · , |V|}. Edges e ∈ E are two-tuples e = (v, w) ∈ V × V. Each node v has a output state y v ∈ Y that takes a value from a set of labels {1, · · · , Y v } (e.g., actions).</p><formula xml:id="formula_1">A parse graph g = (V g , E g , Y g ) is a sub-graph of G, where V g ⊆ V and E g ⊆ E.</formula><p>Given the node features Γ V and edge features Γ E , we want to infer the optimal parse graph g * that best explains the data according to a probability distribution p:</p><formula xml:id="formula_2">g * = argmax g p(g|Γ, G) = argmax g p(V g , E g , Y g |Γ, G) = argmax g p(Y g |V g , E g , Γ )p(V g , E g |Γ, G) (1) where Γ = {Γ V , Γ E }.</formula><p>Here p(V g , E g |Γ, G) evaluates the graph structure, and p(Y g |V g , E g , Γ ) is the labeling probability for the nodes in the parse graph. This formulation provides us a principled guideline for designing the GPNN. We design the network to approximate the computations of argmax g p(V g , E g |Γ, G) and argmax g p(Y g |V g , E g , Γ ). We introduce four types of functions as individual modules in the forward pass of a GPNN: link functions, message functions, update functions, and readout functions (as illustrated in <ref type="figure">Fig. 2</ref>). The link functions L(·) estimate the graph structure, giving an approximation of p(V g , E g |Γ, G). The message, update and readout functions together resemble the belief propagation process and approximate argmax Yg p(Y g |V g , E g , Γ ).</p><p>Specifically, the link function ( ) takes edge features ( ) as input and infers the connectivities between nodes. The soft adjacency matrix ( ) is thus constructed and used as weights for messages passing through edges between nodes. The incoming messages for a node are summarized by the message function ( ), then the hidden embedding state of the node is updated based on the messages by an update function ( ). Finally, readout functions ( ) compute the target outputs for each nodes. Those four types of functions are defined as follows: Link Function. We first infer an adjacency matrix that represents connectivities (i.e., the graph structure) between nodes by a link function. A link function L(·) takes the node features Γ V , and edge features Γ E as input and outputs an adjacency matrix A ∈ [0, 1] |V|×|V| :</p><formula xml:id="formula_3">A vw = L(Γ v , Γ w , Γ vw )<label>(2)</label></formula><p>where A vw denotes the (v, w)-th entry of the matrix A. Here we overload the notation and let Γ v denote node features and Γ vw denote edge features. In this way, the structure of a parse graph g can be approximated by the adjacency matrix. Then we start to propagate messages over the parse graph, where the soft adjacency matrix controls the information to be passed through edges.</p><p>Message and Update Functions. Based on the learned graph structure, the message passing algorithm is adopted for inference of node labels. During belief propagation, the hidden states of the nodes are iteratively updated by communicating with other nodes. Specially, message functions M (·) summarize messages to nodes coming from other nodes, and update functions U (·) update the hidden node states according to the incoming messages. At each iteration step s, the two functions computes:</p><formula xml:id="formula_4">m s v = w A vw M (h s−1 v , h s−1 w , Γ vw ) (3) h s v = U (h s−1 v , m s v )<label>(4)</label></formula><p>where m s v is the summarized incoming message for node v at s-th iteration and h s v is the hidden state for node v. The node connectivity A encourages the information flow between nodes in the parse graph. The message passing phase runs for S steps towards convergence. At the first step, the node hidden states h 0 v are initialized by node features Γ v . Readout Function. Finally, for each node, hidden state is fed into a readout function to output a label:</p><formula xml:id="formula_5">y v = R(h S v ).<label>(5)</label></formula><p>Here the readout function R(·) computes output y v for node v by activating its hidden state h S v (node embeddings). Iterative Parsing. Based on the above four functions, the messages are passed along the graph and weighted by the learned adjacency matrix A. We further extend above process into a joint learning framework that iteratively infers the graph structure and propagates the information to infer node labels. In particular, instead of learning A only at the beginning, we iteratively infer A with the updated node information and edge features at each step s:</p><formula xml:id="formula_6">A s vw = L(h s−1 v , h s−1 w , m s−1 vw ).<label>(6)</label></formula><p>Then the messages in Eq. 3 are redefined as:</p><formula xml:id="formula_7">m s v = w A s vw M (h s−1 v , h s−1 w , Γ vw ).<label>(7)</label></formula><p>In this way, both the graph structure and the message update can be jointly and iteratively learned in a unified framework. In practice, we find such a strategy would bring better performance (detailed in § 4.3). In next section, we show that by implementing each function by neural networks, the entire system is differentiable end-to-end. Hence all the parameters can be learned using gradient-based optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>Link Function. Given the complete HOI graph G = (V, E, Y), we use d V and d E to denote the dimension of the node features and the edge features, respectively. In a message passing step s, we first concatenate all the node features (hidden <ref type="figure">Fig. 2)</ref>. The link function is defined as a small neural network with one or several convolutional layer(s) (with 1 × 1 × (2d V + d E ) kernels) and a sigmoid activation. Then the adjacency matrix A s ∈ [0, 1] |V|×|V| can be computed as:</p><formula xml:id="formula_8">states) {h s v ∈ R d V } v and all the edge features (messages) {m s vw ∈ R d E } v,w to form a feature matrix F s ∈ R |V |×|V |×(2d V +d E ) (see in</formula><formula xml:id="formula_9">A s = σ(W L * F s ),<label>(8)</label></formula><p>where W L is the learnable parameters of the link function network L(·) and * denotes conv operation. The sigmoid operation σ(·) is for normalizing the values of the elements of A s into [0, 1]. The essential effect of multiple convolutional layers with 1 × 1 kernels is similar to fully connected layers applied to each individual edge features, except that the filter weights are shared by all the edges. In practice, we find such operation generates good enough results and leads to a high computation efficiency.</p><p>For spatial-temporal problems where the adjacency matrices should account for the previous states, we use convolutional LSTMs <ref type="bibr" target="#b38">[39]</ref> for modeling L(·) in temporal domain. At time t, the link function takes F s,t as input features and the previous adjacency matrix A s,t−1 as hidden state: A s,t = convLST M (F s,t , A s,t−1 ). Again, the kernel size for the conv layer in convLSTM is 1 × 1 × (2d V + d E ). Message Function. In our implementation, the message function M (·) in Eq. 3 is computed by:</p><formula xml:id="formula_10">M (h v , h w , Γ vw ) = [W M V h v , W M V h w , W M E Γ vw ],<label>(9)</label></formula><p>where [., .] denotes concatenation. It concatenates the outputs of linear transforms (i.e., fully connected layers parametrized by W M V and W M E ) that takes node hidden states h v or edge features Γ vw as input.</p><p>Update Function. Recurrent neural networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref> are natural choices for simulating the iterative update process, as done by previous works <ref type="bibr" target="#b11">[12]</ref>. Here we apply Gated Recurrent Unit (GRU) <ref type="bibr" target="#b4">[5]</ref> as the update function, because of its recurrent nature and smaller amount of parameters. Thus the update function in Eq. 4 is implemented as:</p><formula xml:id="formula_11">h s v = U (h s−1 v , m s v ) = GRU (h s−1 v , m s v ),<label>(10)</label></formula><p>where h s v is the hidden state and m s v is used as input features. As demonstrated in <ref type="bibr" target="#b24">[25]</ref>, the GRU is more effective than vanilla recurrent neural networks. Readout Function. A typical implementation of readout functions is combining several fully connected layers (parameterized by W R ) followed by an activation function:</p><formula xml:id="formula_12">y v = R(h S v ) = ϕ(W R h S v ).<label>(11)</label></formula><p>Here the activation function ϕ(·) can be used as softmax (one-class outputs) or sigmoid (multi-class outputs) according to different HOI tasks. In this way, the entire GPNN is implemented to be fully differentiable and end-to-end trainable. The loss for specific HOI task can be computed for the outputs of readout functions, and the error can propagate back according to chain rule. In next section, we will offer more details for implementing GPNN for HOI tasks on spatial and spatial-temporal settings and present qualitative as well as quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To verify the effectiveness and generic applicability of GPNN, we perform experiments on two HOI problems: i) HOI detection in images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>, and ii) HOI recognition and anticipation from videos <ref type="bibr" target="#b21">[22]</ref>. The first experiment is performed on HICO-DET <ref type="bibr" target="#b0">[1]</ref> and V-COCO <ref type="bibr" target="#b16">[17]</ref> datasets, showing that our approach is scalable to large datasets (about 60K images in total) and achieves a good detection accuracy over a large number of classes (more than 600 classes of HOIs). The second experiment is reported on CAD-120 dataset <ref type="bibr" target="#b21">[22]</ref>, showing that our method is well applicable to spatial-temporal domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Human-Object Interaction Detection in Images</head><p>For HOI detection in an image, the goal is to detect pairs of a human and an object bounding box with an interaction class label connecting them. Datasets. We use HICO-DET <ref type="bibr" target="#b0">[1]</ref> and V-COCO <ref type="bibr" target="#b16">[17]</ref> datasets for benchmarking our GPNN model. HICO-DET provides more than 150K annotated instances of human-object pairs in 47,051 images (37,536 training and 9,515 testing). It has the same 80 object categories as MS-COCO <ref type="bibr" target="#b27">[28]</ref> and 117 action categories. V-COCO is a subset of MS-COCO <ref type="bibr" target="#b27">[28]</ref>. It consists of a total of 10,346 images with 16,199 people instances, where ∼2.5K images in the train set, ∼2.8K images for validation and ∼4.9K images for testing. Each annotated person has binary   is set to be 3. For the readout function, we use a FC(d V -117)-Sigmoid(·) and FC(d V -26)-Sigmoid(·) for HICO-DET and V-COCO, respectively.</p><p>The probability of an HOI label of a human-object pair is given by the product of the final output probabilities from the human node and the object node. We employ an L1 loss for the adjacency matrix. For the node outputs, we use a weighted multi-class multi-label hinge loss. The reasons are two-folds: the training examples are not balanced, and it is essentially a multi-label problem for each node (there might not even exist a meaningful human-object interaction for detected humans and objects).</p><p>Our model is implemented using PyTorch and trained with a machine with a single Nvidia Titan Xp GPU. We start with a learning rate of 1e-3, and the rate decays every 5 epochs by 0.8. The training process takes about 20 epochs (∼15 hours) to roughly converge with a batch size of 32. Comparative Methods. We compare our method with eight baselines: (1) Fast-RCNN (union) <ref type="bibr" target="#b12">[13]</ref>: for each human-object proposal from detection results, their attention windows are used as the region proposal for Fast-RCNN. (2) Fast-RCNN (score) <ref type="bibr" target="#b12">[13]</ref>: given human-object proposals, HOI is predicted by linearly combining the human and object detection scores. (3) HO-RCNN [1]: a multistream architecture with a ConvNet to classify human, object and human-object proposals, respectively. The final output is computed by combining the scores from all the three streams. (4) HO-RCNN+IP <ref type="bibr" target="#b0">[1]</ref> and (5) HO-RCNN+IP+S <ref type="bibr" target="#b0">[1]</ref>: HO-RCNN with additional components. Interaction Patterns (IP) acts as a attention filter to images. S is an extra path with a single neuron that uses the raw object detection score to produce an offset for the final detection. More detailed descriptions of above five baselines can be found in <ref type="bibr" target="#b0">[1]</ref>. (6) Gupta et al. <ref type="bibr" target="#b16">[17]</ref>: trained based on Fast-RCNN <ref type="bibr" target="#b12">[13]</ref>. We use the scores reported in <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr" target="#b6">(7)</ref> Shen et al. <ref type="bibr" target="#b37">[38]</ref>: final predictions are from two Faster RCNN <ref type="bibr" target="#b35">[36]</ref> based networks which are trained for predicting verb and object classes, respectively. <ref type="bibr" target="#b7">(8)</ref> InteractNet <ref type="bibr" target="#b13">[14]</ref>: a modified Faster RCNN <ref type="bibr" target="#b35">[36]</ref> with an additional human-centric branch that estimates an action-specific density map for locating objects. Experiment Results. Following the standard settings in HICO-DET and V-COCO benchmarks, we evaluate HOI detection using mean average precision (mAP). An HOI detection is considered as a true positive when the human detection, the object detection, and the interaction class are all correct. The human and object bounding boxes are considered as true positives if they overlap with a ground truth bounding boxes of the same class with an intersection over union (IoU) greater than 0.5. For HICO-DET dataset, we report the mAP over three different HOI category sets: i) all 600 HOI categories in HICO (Full); ii) 138 HOI categories with less than 10 training instances (Rare); and iii) 462 HOI categories with 10 or more training instances (Non-Rare). For V-COCO dataset, since we concentrate on HOI detection, we report the mAP on three groups: i) 18 HOI action classes with one target object; ii) 3 HOI categories with two types of objects; iii) all 24 (=18 + 3 × 2) HOI classes. Results are evaluated on the test sets and reported in <ref type="table" target="#tab_2">Table 1 and Table 2</ref>.</p><p>As shown in <ref type="table" target="#tab_2">Table 1</ref>, the proposed GPNN substantially outperforms the comparative methods, achieving 31.89%, 30.45%, and 32.13% improvement over the second best methods on the three HOI category sets on the HICO-DET dataset. The results on V-COCO dataset (in <ref type="table" target="#tab_3">Table 2</ref>) also consistently demonstrate the superior performance of the proposed GPNN. Two important conclusions can be drawn from the results: i) our method is scalable to large datasets; ii) and our method performs better than pure neural network. Some visual results can be found in <ref type="figure" target="#fig_0">Fig. 3 and Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human-Object Interaction Recognition in Videos</head><p>The goal of this experiment is to detect and predict the human sub-activity labels and object affordance labels as the human-object interaction progresses in videos. The problem is challenging since it involves complex interactions that humans make with multiple objects, and objects also interact with each other. CAD-120 dataset <ref type="bibr" target="#b21">[22]</ref>. It has 120 RGB-D videos of 4 subjects performing 10 activities, each of which is a sequence of sub-activities involving 10 actions (e.g., reaching, opening), and 12 object affordances (e.g., reachable, openable) in total. Implementation Details. The link function is implemented as: convLSTM(1024-1024-1024-1)-Sigmoid(·) (i.e., a four-layer convLSTM). We use the same architecture as the previous experiment for message functions and update functions:</p><formula xml:id="formula_13">[FC(d V -d V ), FC(d E -d E )]</formula><p>for message function and GRU(d V ) for update function. The propagation step number S is set to be 3. We use a FC(d V -10)-Softmax(·) and a FC(d V -12)-Softmax(·) for readout functions of sub-activity and object affordance detection/anticipation, respectively. We employ an L1 loss for <ref type="table">Table 3</ref>. Human activity detection and future anticipation results on CAD-120 <ref type="bibr" target="#b21">[22]</ref> dataset, measured via F1-score.</p><formula xml:id="formula_14">Detection (F1-score) ↑ Anticipation (F1-score) ↑ Method Sub -activity(%) Object Affordance(%)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub -activity(%)</head><p>Object Affordance(%) ATCRF <ref type="bibr" target="#b21">[22]</ref> 80.4 81.5 37.9 36.7 S-RNN <ref type="bibr" target="#b19">[20]</ref> 83.2 88.7 62.3 80.7 S-RNN (multi-task) <ref type="bibr" target="#b19">[20]</ref> 82  the adjacency matrix and a cross entropy loss for the node outputs. We use the publicly available node and edge features from <ref type="bibr" target="#b22">[23]</ref>. Comparative Methods. We compare our method with two baselines: anticipatory temporal CRF (ATCRF) <ref type="bibr" target="#b21">[22]</ref> and structural RNN (S-RNN) <ref type="bibr" target="#b19">[20]</ref>. ATCRF is a top-performing graphical model approach for this problem, while S-RNN is the state-of-art method using structured neural networks. ATCRF models the human activities through a spatial-temporal conditional random field. S-RNN casts a pre-defined spatial-temporal graph as an RNN mixture by representing nodes and edges as LSTMs. Experiment Results. In <ref type="table">Table 3</ref> we show the quantitative comparison of our method with other competitors. It shows the F1-scores averaged over all classes on detection and activity anticipation tasks. GPNN greatly improves over ATCRF and S-RNN, especially on anticipation task. Our method outperforms the other two for the following reasons. i) Comparing to ATCRF limited to the Markov assumption, our method allows arbitrary graph structures with improved representation ability. ii) Our method enjoys the benefit of deep integration of graphical models and neural networks and can be learned in an end-to-end manner. iii) Rather than relying on a pre-fixed graph structure as in S-RNN, we infer the graph structure via learning an adjacency matrix and thus be able to control the information flow between nodes during massage passing. <ref type="figure" target="#fig_1">Fig. 5</ref> show the confusion matrices for detecting and predicting the sub-activities and object affordances, respectively. From above results we can draw two im-   <ref type="figure">Fig. 6</ref>. HOI detection results on a "cleaning objects" activity on CAD-120 <ref type="bibr" target="#b21">[22]</ref> dataset. Human are shown in red rectangle. Two objects are shown in green and blue rectangles, respectively. Detection and anticipation results are shown by different bars. For anticipation task, the label of the sub-activity at time t is anticipated at time t-1.</p><p>portant conclusions: i) our method is well applicable to the spatio-temporal domain; and ii) our method outperforms pure graphical models (e.g., ATCRF) and deep networks with pre-fixed graph structures (e.g., S-RNN). <ref type="figure">Fig. 6</ref> shows a qualitative visualization of "cleaning objects". We show one representative frame for each sub-activity as well as the corresponding detections and anticipations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we analyze the contributions of different model components to the final performance and examine the effectiveness of our main assumptions. <ref type="table" target="#tab_6">Table 4</ref> shows the detailed results on all three datasets. Integration of DNN with Graphical Model. We first examine the influence of integrating DNN with a graphical model. We directly feed the features, which are originally used for GPNN, into different fully connected networks for predicting HOI action or object classes. From <ref type="table" target="#tab_6">Table 4</ref>, we can observe the performance of w/o graph is significantly worse than GPNN model over various HOI datasets. This supports our view that modeling high-level structures and leveraging learning capabilities of DNNs together is essential for HOI tasks. GPNN with Fixed Graph Structures. In § 3, GPNN automatically infers graph structures (i.e., parse graph) via learning a soft adjacency matrix. To assess this strategy, we fix all the entries in the soft adjacency matrices to be constant 1. This way the graph structures are fixed and the information flow between nodes are not weighted. For constant graph baseline, we see obvious performance decrease, compared with the full GPNN model. This indicates that inferring graph structures is critical to get reasonable performance. GPNN without Supervision on Link Functions. We perform experiments by turning off the L1 loss on adjacency matrices (w/o graph loss in <ref type="table" target="#tab_6">Table 4</ref>). We can observe that the intermediate L1 loss is effective, further verifying our design to learn the graph structure. Another interesting observation is that training the model without this loss has a similar effect to training with constant graph. Hence supervision on the graph is fairly important. Jointly Learning Parse Graph and Message Passing. We next study the effect of jointly learning graph structures and message passing. By isolating graph parsing from message passing, we obtain w/o joint parsing, where the adjacency matrices are directly computed by link functions from edge features at the beginning. We observe a performance decrease in <ref type="table" target="#tab_6">Table 4</ref>, showing that learning graph structures and message passing together indeed boost the performance. Iterative Learning Process. Next we examine the effect of iterative message passing, we report three baselines: 1 iteration, 2 iterations, and 4 iterations, which correspond to the results from different message passing iterations. The baseline GPNN (first row in <ref type="table" target="#tab_6">Table 4</ref>) are the results after three iterations. From the results we observe that the iterative learning process is able to gradually improve the performance in general. We also observe that when the iteration round is increased to a certain extent, the performance drops slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose Graph Parsing Neural Network (GPNN) for inferring a parse graph in an end-to-end manner. The network can be decomposed into four distinct functions, namely link functions, message functions, update functions and readout functions, for iterative graph inference and message passing. GPNN provides a generic HOI representation that is applicable in both spatial and spatial-temporal domains. We demonstrate a substantial performance gain on three HOI datasets, showing the effectiveness of the proposed framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>HOI detection results on V-COCO<ref type="bibr" target="#b16">[17]</ref> test images. Human and objects are shown in red and green rectangles, respectively. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Confusion matrices of HOI detection (a)(b) and anticipation (c)(d) results on CAD-120<ref type="bibr" target="#b21">[22]</ref> dataset. Zoom in for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Containable Object Affordance Human Activity (a) Human-Object Interaction Detection in Still Images (b) Human-Object Interaction Recognition in Videos Lick</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Person Knife</cell><cell cols="2">Vase</cell><cell cols="2">Person Knife</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Opening</cell><cell>Reaching</cell><cell>Placing</cell></row><row><cell></cell><cell>(i) Image</cell><cell></cell><cell cols="2">(ii) HOI candidates</cell><cell></cell><cell></cell><cell cols="2">(iii) HOI result</cell><cell>t = 1</cell><cell>t = 2</cell><cell>t = 3</cell></row><row><cell>Person</cell><cell>Knife Vase</cell><cell>Person</cell><cell>Knife Vase</cell><cell>Person</cell><cell cols="2">Knife Vase</cell><cell>Person</cell><cell>Vase Knife</cell></row><row><cell></cell><cell>Table</cell><cell></cell><cell>Table</cell><cell></cell><cell cols="2">Table</cell><cell></cell><cell>Table</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Joint Inference</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(iv) Initial</cell><cell cols="2">(v) Parse</cell><cell cols="3">(vi ) Message</cell><cell cols="2">(vii) Final</cell><cell>Openable</cell><cell>Stationary</cell></row><row><cell cols="2">HOI graph</cell><cell cols="2">graph learning</cell><cell cols="3">passing</cell><cell cols="2">parse graph</cell><cell>Stationary</cell><cell>Reachable</cell><cell>Placeable</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>HOI detection results (mAP) on HICO-DET dataset<ref type="bibr" target="#b0">[1]</ref>. Higher values are better. The best scores are marked in bold.Fig. 3. HOI detection results on HICO-DET [1] test images. Human and objects are shown in red and green rectangles, respectively. Best viewed in color.labels for 26 different action classes. Note that three actions (i.e., cut, eat, and hit) are annotated with two types of targets: instrument and direct object. Implementation Details. Humans and objects are represented by nodes in the graph, while human-object interactions are represented by edges. In this experiment, we use a pre-trained deformable convolutional network<ref type="bibr" target="#b5">[6]</ref> for object detection and features extraction. Based on the detected bounding boxes, we extract node features (7 × 7 × 80) from the position-sensitive region of interest (PS RoI) pooling layer from the deformable ConvNet. We extract the edge feature from a combined bounding box, i.e., the smallest bounding box that contains both two nodes' bounding boxes. The functions of GPNN are implemented as follows. We use a convolutional network (128-128-1)-Sigmoid(·) with 1×1 kernels for the link function. The message functions are composed of a fully connected layer, concatenation, and summation. For a node v, the neighboring node feature Γ</figDesc><table><row><cell>Methods</cell><cell cols="4">Full (mAP %) ↑ Rare (mAP %) ↑ Non-rare (mAP %) ↑</cell></row><row><cell>Random</cell><cell></cell><cell>1.35 × 10 −3</cell><cell>5.72 × 10 −4</cell><cell>1.62 × 10 −3</cell></row><row><cell cols="2">Fast-RCNN(union) [13]</cell><cell>1.75</cell><cell>0.58</cell><cell>2.10</cell></row><row><cell cols="2">Fast-RCNN(score) [13]</cell><cell>2.85</cell><cell>1.55</cell><cell>3.23</cell></row><row><cell>HO-RCNN [1]</cell><cell></cell><cell>5.73</cell><cell>3.21</cell><cell>6.48</cell></row><row><cell>HO-RCNN+IP [1]</cell><cell></cell><cell>7.30</cell><cell>4.68</cell><cell>8.08</cell></row><row><cell cols="2">HO-RCNN+IP+S [1]</cell><cell>7.81</cell><cell>5.37</cell><cell>8.54</cell></row><row><cell>Gupta et al. [17]</cell><cell></cell><cell>9.09</cell><cell>7.02</cell><cell>9.71</cell></row><row><cell>Shen et al. [38]</cell><cell></cell><cell>6.46</cell><cell>4.24</cell><cell>7.12</cell></row><row><cell>InteractNet [14]</cell><cell></cell><cell>9.94</cell><cell>7.16</cell><cell>10.77</cell></row><row><cell>GPNN</cell><cell></cell><cell>13.11</cell><cell>9.34</cell><cell>14.23</cell></row><row><cell cols="2">Performance Gain(%)</cell><cell>31.89</cell><cell>30.45</cell><cell>32.13</cell></row><row><cell>Lie on bench</cell><cell>Eat at dining</cell><cell>Row boat</cell><cell>Hold book</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Board airplane</cell></row><row><cell></cell><cell>Type on keyboard</cell><cell>Fly kite</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hold cup</cell><cell>Read laptop</cell></row><row><cell>Sit on chair</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Wield knife</cell></row></table><note>w and edge feature Γ vw are passed through a fully connected layer and concate- nated. The final incoming message is a weighted sum of messages from all neigh- boring nodes. Specifically, the message for node v coming from node w through edge e = (v, w) is the concatenation of output from FC(d V -d V ) and FC(d E -d E ). A GRU(d V ) is used for the update function. The propagation step number S</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .Surf surfboard Look Cut scissors Throw Look Look frisbee Lick Sit chair Jump skateboard Kick ball Ride horse Ride horse</head><label>2</label><figDesc>HOI detection results (mAP) on V-COCO<ref type="bibr" target="#b16">[17]</ref> dataset. Legend: Set 1 indicates 18 HOI actions with one object, and Set 2 corresponds to 3 HOI actions (i.e., cut, eat, hit) with two objects (instrument and object).</figDesc><table><row><cell>Method</cell><cell cols="3">Set 1 (mAP %) ↑ Set 2 (mAP %) ↑ Ave. (mAP %) ↑</cell></row><row><cell>Gupta et al. [17]</cell><cell>33.5</cell><cell>26.7</cell><cell>31.8</cell></row><row><cell>InteractNet [14]</cell><cell>42.2</cell><cell>33.2</cell><cell>40.0</cell></row><row><cell>GPNN</cell><cell>44.5</cell><cell>42.8</cell><cell>44.0</cell></row><row><cell>Performance Gain(%)</cell><cell>5.5</cell><cell>28.9</cell><cell>10.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Sit bench</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Lay bench</cell></row><row><cell>Look laptop</cell><cell>Look laptop</cell><cell></cell><cell></cell></row><row><cell>Hold tennis_racket</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of GPNN model. Higher values are better.</figDesc><table><row><cell></cell><cell></cell><cell>V-COCO [17]</cell><cell cols="2">HICO-DET [1]</cell><cell></cell><cell cols="2">CAD-120 [22]</cell></row><row><cell></cell><cell></cell><cell>HOI Detection</cell><cell cols="2">HOI Detection</cell><cell cols="2">HOI Detec.</cell><cell>HOI Antici.</cell></row><row><cell></cell><cell></cell><cell>mAP(%) ↑</cell><cell cols="2">mAP(%) ↑</cell><cell cols="2">F1-score(%) ↑</cell><cell>F1-score(%) ↑</cell></row><row><cell cols="2">Aspect Method</cell><cell cols="2">Set 1 Set 2 Ave. Full Rare</cell><cell>Non-rare</cell><cell>Sub-activity</cell><cell>Object Aff.(%)</cell><cell>Sub-activity</cell><cell>Object Aff.(%)</cell></row><row><cell></cell><cell>GPNN (3 iterations)</cell><cell cols="4">44.5 42.8 44.0 13.11 9.34 14.23 88.9</cell><cell>88.8</cell><cell>75.6</cell><cell>81.9</cell></row><row><cell></cell><cell>w/o graph</cell><cell cols="3">27.4 30.0 28.1 7.88 2.04 9.62</cell><cell>50.2</cell><cell>20.8</cell><cell>32.3</cell><cell>19.6</cell></row><row><cell>graph</cell><cell>constant graph</cell><cell cols="4">34.6 33.3 34.3 8.75 1.94 10.79 85.3</cell><cell>85.6</cell><cell>73.8</cell><cell>79.1</cell></row><row><cell cols="2">structure w/o graph loss</cell><cell cols="3">37.7 40.5 38.4 8.15 6.24 8.72</cell><cell>85.2</cell><cell>85.8</cell><cell>74.7</cell><cell>79.2</cell></row><row><cell></cell><cell cols="5">w/o joint parsing 43.6 39.4 42.5 10.17 5.81 11.47 79.3</cell><cell>79.2</cell><cell>74.7</cell><cell>80.3</cell></row><row><cell cols="2">iterative 1 iteration</cell><cell cols="4">42.0 40.7 41.7 11.38 7.27 12.61 80.5</cell><cell>80.7</cell><cell>75.2</cell><cell>81.1</cell></row><row><cell cols="2">learning 2 iterations</cell><cell cols="4">44.1 42.2 43.6 12.37 9.01 13.38 87.9</cell><cell>86.1</cell><cell>76.1</cell><cell>81.5</cell></row><row><cell></cell><cell>4 iterations</cell><cell cols="4">43.6 40.9 42.9 12.39 8.95 13.41 87.9</cell><cell>85.7</cell><cell>75.5</cell><cell>80.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<title level="m">Learning to detect human-object interactions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">HICO: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches. Syntax, Semantics and Structure in Statistical Translation p</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning person-object interactions for action recognition in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Detecting actions, poses, and objects with relational phraselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Fast R-CNN. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Detecting and recognizing humanobject interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Objects in action: An approach for combining action understanding and object perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Recognising human-object interaction via exemplar based modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Structural-RNN: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from RGB-D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<title level="m">Interpretable structureevolving lstm</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<title level="m">Geometric deep learning on graphs and manifolds using mixture model cnns. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attribute and-or grammar for joint parsing of human pose, parts and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Predicting human activities using stochastic grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generalized earley parser: Bridging symbolic grammars and sequence data for future prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Attentive fashion grammar network for fashion landmark detection and clothing category classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep markov random field for image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Pose-guided human parsing by an And/Or graph using pose-context features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Modeling mutual context of object and human pose in humanobject interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Temporal dynamic graph LSTM for action-driven video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In: ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
