<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satya</forename><surname>Narayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst Amherst</orgName>
								<address>
									<postCode>01003</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shukla</forename></persName>
							<email>snshukla@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst Amherst</orgName>
								<address>
									<postCode>01003</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
							<email>marlin@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst Amherst</orgName>
								<address>
									<postCode>01003</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of our framework on interpolation and classification tasks using multiple datasets. Our results show that our approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1 1 Our implementation is available at :</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Irregularly sampled time series occur in applications including healthcare, climate science, ecology, astronomy, biology and others. It is well understood that irregular sampling poses a significant challenge to machine learning models, which typically assume fully-observed, fixed-size feature representations <ref type="bibr" target="#b31">(Yadav et al., 2018)</ref>. While recurrent neural networks (RNNs) have been widely used to model such data because of their ability to handle variable length sequences, basic RNNs assume regular spacing between observation times as well as alignment of the time points where observations occur for different variables (i.e., fully-observed vectors). In practice, both of these assumptions can fail to hold for real-world sparse and irregularly observed time series. To respond to these challenges, there has been significant progress over the last decade on building and adapting machine learning models that can better capture the structure of irregularly sampled multivariate time series <ref type="bibr" target="#b15">(Marlin et al., 2012;</ref><ref type="bibr" target="#b13">Li &amp; Marlin, 2015;</ref><ref type="bibr" target="#b14">Lipton et al., 2016;</ref><ref type="bibr" target="#b6">Futoma et al., 2017;</ref><ref type="bibr" target="#b1">Che et al., 2018;</ref><ref type="bibr" target="#b25">Shukla &amp; Marlin, 2019;</ref><ref type="bibr" target="#b21">Rubanova et al., 2019)</ref>.</p><p>In this work, we introduce a new model for multivariate, sparse and irregularly sampled time series that we refer to as Multi-Time Attention networks or mTANs. mTANs are fundamentally continuous-time interpolation-based models. Their primary innovations are the inclusion of a learned continuous-time embedding mechanism coupled with a time attention mechanism that replaces the use of a fixed similarity kernel when forming representation from continuous time inputs. This gives mTANs significantly more representational flexibility than previous interpolation-based models.</p><p>Our approach re-represents an irregularly sampled time series at a fixed set of reference points. The proposed time attention mechanism uses reference time points as queries and the observed time points as keys. We propose an encoder-decoder framework for end-to-end learning using an mTAN module to interface with given multivariate, sparse and irregularly sampled time series inputs. The encoder takes the irregularly sampled time series as input and produces a fixed-length latent representation over a set of reference points, while the decoder uses the latent representations to produce reconstructions conditioned on the set of observed time points. Learning uses methods established for variational autoencoders <ref type="bibr" target="#b20">(Rezende et al., 2014;</ref><ref type="bibr" target="#b11">Kingma &amp; Welling, 2014)</ref>.</p><p>Published as a conference paper at ICLR 2021</p><p>The main contributions of the mTAN model framework are: (1) It provides a flexible approach to modeling multivariate, sparse and irregularly sampled time series data (including irregularly sampled time series of partially observed vectors) by leveraging a time attention mechanism to learn temporal similarity from data instead of using fixed kernels. (2) It uses a temporally distributed latent representation to better capture local structure in time series data. (3) It provides interpolation and classification performance that is as good as current state-of-the-art methods or better, while providing significantly reduced training times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>An irregularly sampled time series is a sequence of samples with irregular time intervals between observations. In the multivariate setting, there can also be a lack of alignment across different variables within the same multivariate time series. Finally, when gaps between observation times are large, the time series is also considered to be sparse. Such data occur in electronic health records <ref type="bibr" target="#b31">(Yadav et al., 2018)</ref>, climate science <ref type="bibr" target="#b24">(Schulz &amp; Stattegger, 1997)</ref>, ecology <ref type="bibr" target="#b4">(Clark &amp; Bjørnstad, 2004)</ref>, biology <ref type="bibr" target="#b22">(Ruf, 1999)</ref>, and astronomy <ref type="bibr" target="#b23">(Scargle, 1982)</ref>. It is well understood that such data cause significant issues for standard supervised machine learning models that typically assume fully observed, fixed-size feature representations <ref type="bibr" target="#b31">(Yadav et al., 2018)</ref>.</p><p>A basic approach to dealing with irregular sampling is fixed temporal discretization. For example, <ref type="bibr" target="#b15">Marlin et al. (2012)</ref> and <ref type="bibr" target="#b14">Lipton et al. (2016)</ref> discretize continuous-time observations into hour-long bins. This has the advantage of simplicity, but requires ad-hoc handling of bins with more than one observation and results in missing data when bins are empty.</p><p>The alternative to temporal discretization is to construct models with the ability to directly use an irregularly sampled time series as input. <ref type="bibr" target="#b1">Che et al. (2018)</ref> present several methods based on gated recurrent unit networks (GRUs, <ref type="bibr" target="#b3">Chung et al. (2014)</ref>), including an approach that takes as input a sequence consisting of observed values, missing data indicators, and time intervals since the last observation. <ref type="bibr" target="#b19">Pham et al. (2017)</ref> proposed to capture time irregularity by modifying the forget gate of an LSTM <ref type="bibr" target="#b7">(Hochreiter &amp; Schmidhuber, 1997)</ref>, while <ref type="bibr" target="#b17">Neil et al. (2016)</ref> introduced a new time gate that regulates access to the hidden and cell state of the LSTM. While these approaches allow the network to handle event-based sequences with irregularly spaced vector-valued observations, they do not support learning directly from vectors that are partially observed, which commonly occurs in the multivariate setting because of lack of alignment of observation times across different variables.</p><p>Another line of work has looked at using observations from the future as well as from the past for interpolation. <ref type="bibr" target="#b32">Yoon et al. (2019)</ref>; <ref type="bibr" target="#b33">Yoon et al. (2018)</ref> presented an approach based on the multidirectional RNN (M-RNN) that can leverage observations from the relative past and future of a given time point. <ref type="bibr" target="#b25">Shukla &amp; Marlin (2019)</ref> proposed the interpolation-prediction network framework, consisting of several semi-parametric RBF interpolation layers that interpolate multivariate, sparse, and irregularly sampled input time series against a set of reference time points while taking into account all observed data in a time series. <ref type="bibr" target="#b8">Horn et al. (2020)</ref> proposed a set function based approach for classifying time-series with irregularly sampled and unaligned observation.</p><p>Chen et al. <ref type="bibr" target="#b3">(2018)</ref> proposed a variational auto-encoder model <ref type="bibr" target="#b11">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b20">Rezende et al., 2014)</ref> for continuous time data based on the use of a neural network decoder combined with a latent ordinary differential equation (ODE) model. They model time series data via a latent continuous-time function which is defined via a neural network representation of its gradient field. Building on this, <ref type="bibr" target="#b21">Rubanova et al. (2019)</ref>   <ref type="bibr" target="#b3">(Chung et al., 2014)</ref>. Instead of the encoder-decoder architecture where the ODE is decoupled from the input processing, GRU-ODE-Bayes provides a tighter integration by interleaving the ODE and the input processing steps.</p><p>Several recent approaches have also used attention mechanism to model irregularly sampled time series <ref type="bibr" target="#b27">(Song et al., 2018;</ref><ref type="bibr" target="#b28">Tan et al., 2020;</ref><ref type="bibr" target="#b34">Zhang et al., 2019)</ref> as well as medical concepts <ref type="bibr" target="#b18">(Peng et al., 2019;</ref><ref type="bibr" target="#b0">Cai et al., 2018)</ref>. Most of these approaches are similar to <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref> where they replace the positional encoding with an encoding of time and model sequences using self-attention.</p><p>Instead of adding the time encoding to the input representation as in <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref>, they concatenate it with the input representation. These methods use a fixed time encoding similar to the positional encoding of <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref>. <ref type="bibr" target="#b30">Xu et al. (2019)</ref> learn a functional time representation and concatenate it with the input event embedding to model time-event interactions.</p><p>Like <ref type="bibr" target="#b30">Xu et al. (2019)</ref> and <ref type="bibr" target="#b10">Kazemi et al. (2019)</ref>, our proposed method learns a time representation. However, instead of concatenating it with the input embedding, our model learns to attend to observations at different time points by computing a similarity weighting using only the time embedding. Our proposed model uses the time embedding as both the queries and keys in the attention formulation. It learns an interpolation over the query time points by attending to the observed values at key time points. Our proposed method is thus similar to kernel-based interpolations, but learning the time attention based similarity kernel gives our model significantly more flexibility compared to methods like that of Shukla &amp; Marlin (2019) that use similarity kernels with fixed functional forms. Another important difference relative to many of these previous methods is that our proposed approach attends only to the observed data dimensions at each time point and hence does not require a separate imputation step to handle vector valued observations with an arbitrary collection of dimensions missing at any given time point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE MULTI-TIME ATTENTION MODULE</head><p>In this section, we present the proposed Multi-Time Attention Module (mTAN). The role of this module is to re-represent a sparse and irregularly sampled time series in a fixed-dimensional representation. This module uses multiple continuous-time embeddings and attention-based interpolation. We begin by presenting notation followed by the time embedding and attention components.</p><p>Notation: In the case of a supervised learning task, we let D = {(s n , y n )|n = 1, ..., N } represent a data set containing N data cases. An individual data case consists of a single target value y n (discrete for classification), as well as a D-dimensional, sparse and irregularly sampled multivariate time series s n . Different dimensions d of the multivariate time series can have observations at different times, as well as different total numbers of observations L dn . Thus, we represent time series d for data case n as a tuple s dn = (t dn , x dn ) where t dn = [t 1dn , ..., t L dn dn ] is the list of time points at which observations are defined and x dn = [x 1dn , ..., x L dn dn ] is the corresponding list of observed values. In the case of an unsupervised task such as interpolation, each data case consists of a multivariate time series s n only. We drop the data case index n for brevity when the context is clear.</p><p>Time Embedding: Time attention module is based on embedding continuous time points into a vector space. We generalize the notion of a positional encoding used in transformer-based models to continuous time. Time attention networks simultaneously leverage H embedding functions φ h (t), each outputting a representation of size d r . Dimension i of embedding h is defined as follows:</p><formula xml:id="formula_0">φ h (t)[i] = ω 0h · t + α 0h , if i = 0 sin(ω ih · t + α ih ), if 0 &lt; i &lt; d r<label>(1)</label></formula><p>where the ω ih 's and α ih 's are learnable parameters. The periodic terms capture the periodicity in the time series. In this case, ω ih and α ih represent the frequency and phase of the sine function. The linear term, on the other hand, can capture non-periodic patterns dependent on the progression of time. For a given difference ∆, φ h (t + ∆) can be represented as a linear function of φ h (t).</p><p>Learning the periodic time embedding functions is equivalent to using a one-layer fully connected network with a sine function non-linearity to map the time values into a higher dimensional space. By contrast, the positional encoding used in transformer models is defined only for discrete positions. We note that our time embedding functions subsume positional encodings when evaluated at discrete positions.</p><p>Multi-Time Attention: The time embedding component described above takes a continuous time point and embeds it into H different d r -dimensional spaces. In this section, we describe how we leverage time embeddings to produce a continuous-time embedding module for sparse and irregularly sampled time series. This multi-time attention embedding module mTAN(t, s) takes as input a query time point t and a set of keys and values in the form of a D-dimensional multivariate sparse and irregularly sampled time series s (as defined in the notation section above), and returns a J dimensional embedding at time t. This process leverages a continuous-time attention mechanism applied to the H time embeddings. The complete computation is described below. As shown in Equation 3, the structure of the intermediate continuous-time functionx hd (t, s) is essentially a kernel smoother applied to the d th dimension of the time series. However, the interpolation weights κ h (t, t id ) are defined based on a time attention mechanism that leverages time embeddings, as shown in Equation 4. As we can see, the same time embedding function φ h (t) is applied for all data dimensions. The form of the attention mechanism is a softmax function over the observed time points t id for dimension d. The activation within the softmax is a scaled inner product between the time embedding φ h (t) of the query time point t and the time embedding φ h (t id ) of the observed time point, the key. The parameters W and V are each d r × d k matrices where d k ≤ d r . We use a scaling factor 1 √ d k to normalize the dot product to counteract the growth in the dot product magnitude with increase in the dimension d k .</p><formula xml:id="formula_1">mTAN(t, s)[j] = H h=1 D d=1x hd (t, s) · U hdj (2) x hd (t, s) = L d i=1 κ h (t, t id ) x id (3) κ h (t, t id ) = exp φ h (t)W V T φ h (t id ) T / √ d k L d i =1 exp φ h (t)wv T φ h (t i d ) T / √ d k<label>(4)</label></formula><p>Learning the time embeddings provides our model with flexibility to learn complex temporal kernel functions κ h (t, t ). The use of multiple simultaneous time embeddings φ h (t) and a final linear combination across time embedding dimensions and data dimensions means that the final output representation function mTAN(t, s) is extremely flexible. Different input dimensions can leverage different time embeddings via learned sparsity patterns in the parameter tensor U . Information from different data dimensions can also be mixed together to create compact reduced dimensional representations. We note that all of the required computations can be parallelized using masking variables to deal with unobserved dimensions, allowing for efficient implementation on a GPU.</p><p>Discretization: Since the mTAN module defines a continuous function of t given s, it can not be directly incorporated into neural network architectures that expect inputs in the form of fixeddimensional vectors or discrete sequences. However, the mTAN module can easily be adapted to produce such an output representation by materializing its output at a set of reference time points r = [r 1 , ..., r K ]. In some cases, we may have a fixed set of such points. In other cases, the set of  reference time points may need to depend on s itself. In particular, we define the auxiliary function ρ(s) to return the set of time points at which there is an observation on any dimension of s.</p><p>Given a collection of reference time points r, we define the discretized mTAN module mTAND(r, s) as mTAND(r, s)[i] = mTAN(r i , s). This module takes as input the set of reference time points r and the time series s and outputs a sequence of mTAN embeddings of length |r|, each of dimension J. The architecture of the mTAND module is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The mTAND module can be used to interface sparse and irregularly sampled multivariate time series data with any deep neural network layer type including fully-connected, recurrent, and convolutional layers. In the next section, we describe the construction of a temporal encoder-decoder architecture leveraging the mTAND module, which can be applied to both classification and interpolation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ENCODER-DECODER FRAMEWORK</head><p>As described in the last section, we leverage the discretized mTAN module in an encoder-decoder framework as our primary model in this paper, which we refer to as an mTAN network. We develop the encoder-decoder framework within the variational autoencoder (VAE) framework in this section.</p><p>The architecture for the model framework is shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>Model Architecture: As we are modeling time series data, we begin by defining a sequence of latent states z i . Each of these latent states are iid-distributed according to a standard multivariate normal distribution p(z i ). We let the set of latent states be z = [z 1 , ..., z K ] defined at K reference time points.</p><p>We define a three-stage decoder. First, the latent states are processed through an RNN decoder module to induce temporal dependencies, resulting in a first set of deterministic latent variables h dec RN N = [h dec 1,RN N , ..., h dec K,RN N ]. Second, the output of the RNN decoder stage and the K time points h dec RN N are provided to the mTAND module along with a set of T query time points t. The mTAND module outputs a sequence of embeddings h dec T AN = [h dec 1,T AN , ..., h dec T,T AN ] of length |t|. Third, the mTAN embeddings are independently decoded using a fully connected decoder f dec () and the result is used to parameterize an output distribution. In this work, we use a diagonal covariance Gaussian distribution with mean given by the final decoded representation and a fixed variance σ 2 . The final generated time series is given byŝ = (t, x) with all data dimensions observed. The full generative process is shown below. We let p θ (x|z, t) define the probability distribution over the values of the time series x given the time points t and the latent variables z. θ represents the parameters of all components of the decoder.</p><formula xml:id="formula_2">z k ∼ p(z k ) (5) h dec RN N = RNN dec (z)<label>(6)</label></formula><formula xml:id="formula_3">h dec T AN = mTAND dec (t, h dec RN N ) (7) x id ∼ N (x id ; f dec (h dec i,T AN )[d], σ 2 I)<label>(8)</label></formula><p>For an encoder, we simply invert the structure of the generative process. We begin by mapping the input time series s through the mTAND module along with a collection of K reference time points r.</p><p>We apply an RNN encoder to the mTAND model that outputs h enc T AN to encode longer-range temporal structure. Finally, we construct a distribution over latent variables at each reference time point using a diagonal Gaussian distribution with mean and variance output by fully connected layers applied to the RNN outputs h enc RN N . The complete encoder architecture is described below. We define q γ (z|r, s) to be the distribution over the latent variables induced by the input time series s and the reference time points r. γ represents all of the parameters in all of the encoder components.</p><formula xml:id="formula_4">h enc T AN = mTAND enc (r, s) (9) h enc RN N = RNN enc (h enc T AN ) (10) z k ∼ q(z k |µ k , σ 2 k ), µ k = f enc µ (h enc k,RN N ), σ 2 k = exp(f enc σ (h enc k,RN N ))<label>(11)</label></formula><p>Unsupervised Learning: To learn the parameters of our encoder-decoder model given a data set of sparse and irregularly sampled time series, we follow a slightly modified VAE training approach and maximize a normalized variational lower bound on the log marginal likelihood based on the evidence lower bound or ELBO. The learning objective is defined below where p θ (x jdn |z, t n ) and q γ (z|r, s n ) are defined in the previous section.</p><formula xml:id="formula_5">L NVAE (θ, γ) = N n=1 1 d L dn E qγ (z|r,sn) [log p θ (x n |z, t n )] − D KL (q γ (z|r, s n )||p(z))<label>(12)</label></formula><formula xml:id="formula_6">D KL (q γ (z|r, s n )||p(z)) = K i=1 D KL (q γ (z i |r, s n )||p(z i )) (13) log p θ (x n |z, t n ) = D d=1 L dn j=1 log p θ (x jdn |z, t jdn )<label>(14)</label></formula><p>Since irregularly sampled time series can have different numbers of observations across different dimensions as well as across different data cases, it can be helpful to normalize the terms in the standard ELBO objective to avoid the model focusing more on sequences that are longer at the expense of sequences that are shorter. The objective above normalizes the contribution of each data case by the total number of observations it contains. The fact that all data dimensions are not observed at all time points is accounted for in Equation 14. In practice, we use k samples from the variational distribution q γ (z|r, s n ) to compute the learning objective.</p><p>Supervised Learning: We can also augment the encoder-decoder model with a supervised learning component that leverages the latent states as a feature extractor. We define this component to be of the form p δ (y n |z) where δ are the model parameters. This leads to an augmented learning objective as shown in <ref type="figure" target="#fig_0">Equation 15</ref> where the λ term trades off the supervised and unsupervised terms. L supervised (θ, γ, δ) = L NVAE (θ, γ) + λE qγ (z|r,sn) log p δ (y n |z) (15) In this work, we focus on classification as an illustrative supervised learning problem. For the classification model p δ (y n |z), we use a GRU followed by a 2-layer fully connected network. We use a small number of samples to approximate the required intractable expectations during both learning and prediction. Predictions are computed by marginalizing over the latent variable as shown below.</p><formula xml:id="formula_7">y * = arg max y∈Y E qγ (z|r,s) [log p δ (y|z)]<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we present interpolation and classification experiments using a range of models and three real-world data sets (Physionet Challenge 2012, MIMIC-III, and a Human Activity dataset). Additional illustrative results on synthetic data can be found in Appendix A.2.</p><p>Datasets: The PhysioNet Challenge 2012 dataset <ref type="bibr" target="#b26">(Silva et al., 2012)</ref> consists of multivariate time series data with 37 variables extracted from intensive care unit (ICU) records. Each record contains sparse and irregularly spaced measurements from the first 48 hours after admission to ICU. We follow the procedures of <ref type="bibr" target="#b21">Rubanova et al. (2019)</ref>   <ref type="formula" target="#formula_0">(2019)</ref>, we extract 53, 211 records each containing 12 physiological variables. We focus on predicting in-hospital mortality using the first 48 hours of data. 8.1% of the instances have positive labels.</p><p>The human activity dataset consists of 3D positions of the waist, chest and ankles collected from five individuals performing various activities including walking, sitting, lying, standing, etc. We follow the data preprocessing steps of <ref type="bibr" target="#b21">Rubanova et al. (2019)</ref> and construct a dataset of 6, 554 sequences with 12 channels and 50 time points. We focus on classifying each time point in the sequence into one of eleven types of activities.</p><p>Experimental Protocols: We conduct interpolation experiments using the 8000 data cases in the PhysioNet data set. We randomly divide the data set into a training set containing 80% of the instances, and a test set containing the remaining 20% of instances. We use 20% of the training data for validation. In the interpolation task, we condition on a subset of available points and reconstruct/predict values for all points. We perform interpolation experiments with a varying percentage of observed points ranging from 50% to 90% of the available points. At test time, the values of observed points are conditioned on and each model is used to infer the values at all available time points in the test instance. We repeat each experiment five times using different random seeds to initialize the model parameters. We assess performance using mean squared error (MSE).</p><p>We use the labeled data in all three data sets to conduct classification experiments. The PhysioNet and MIMIC III problems are whole time series classification. Note that for the human activity dataset, we classify each time point in the time series. We treat this as a smoothing problem and condition on all available observations when producing the classification at each time-point (similar to labeling in a CRF). We use bidirectional RNNs as the RNN-based baselines for human activity dataset. We randomly divide each data set into a training set containing 80% of the time series, and a test set containing the remaining 20% of instances. We use 20% of the training set for validation. We repeat each experiment five times using different random seeds to initialize the model parameters. Due to class imbalance in the Physionet and MIMIC-III data sets, we assess classification performance using area under the ROC curve (the AUC score). For the Human Activity dataset, we evaluate models using accuracy.</p><p>For both interpolation and prediction, we select hyper-parameters on the held-out validation set using grid search, and then apply the best trained model to the test set. The hyper-parameter ranges searched for each model/dataset/task are fully described in Appendix A.4. Models: The model we focus on is the encoder-decoder architecture based on the discretized multitime attention module (mTAND-Full). In the classification experiments, the hidden state at the last observed point is passed to a two-layer binary classification module for all models. For each data set, the structure of this classifier is the same for all models. For the proposed model, the sequence of latent states is first passed through a GRU and then the final hidden state is passed through the same classification module. For the classification task only, we consider an ablation of the full model that uses the proposed mTAND encoder, which consists of our mTAND module followed by a GRU to extract a final hidden state, which is then passed to the classification module (mTAND-Enc). We compare to several deep learning models that expand on recurrent networks to accommodate irregular sampling. We also compare to several encoder-decoder approaches. The full list of model variants is briefly described below. We use a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">(Chung et al., 2014)</ref> module as the recurrent network throughout. Architecture details can be found in Appendix A.3.</p><p>• RNN-Impute: Missing observations replaced with weighted average of last observed measurement within that time series and global mean of the variable across training examples <ref type="bibr" target="#b1">(Che et al., 2018)</ref>.</p><p>• RNN-∆ t : Input is concatenated with masking variable and time interval ∆ t indicating how long the particular variable is missing.</p><p>• RNN-Decay: RNN with exponential decay on hidden states <ref type="bibr" target="#b16">(Mozer et al., 2017;</ref><ref type="bibr" target="#b1">Che et al., 2018)</ref>.</p><p>• GRU-D: combining hidden state decay with input decay <ref type="bibr" target="#b1">(Che et al., 2018)</ref>.</p><p>• Phased-LSTM: Captures time irregularity by a time gate that regulates access to the hidden and cell state of the LSTM <ref type="bibr" target="#b17">(Neil et al., 2016)</ref> with forward filling to handle partially observed vectors.</p><p>• IP-Nets: Interpolation prediction networks, which use several semi-parametric RBF interpolation layers, followed by a GRU (Shukla &amp; Marlin, 2019).</p><p>• SeFT: Uses a set function based approach where all the observations are modeled individually before pooling them together using an attention based approach <ref type="bibr" target="#b8">(Horn et al., 2020)</ref>.</p><p>• RNN-VAE: A VAE-based model where the encoder and decoder are standard RNN models.</p><p>• ODE-RNN: Uses neural ODEs to model hidden state dynamics and an RNN to update the hidden state in presence of a new observation <ref type="bibr" target="#b21">(Rubanova et al., 2019)</ref>.</p><p>• L-ODE-RNN: Latent ODE where the encoder is an RNN and decoder is a neural ODE (Chen et al., 2018).</p><p>• L-ODE-ODE: Latent ODE where the encoder is an ODE-RNN and decoder is a neural ODE <ref type="bibr" target="#b21">(Rubanova et al., 2019)</ref>.</p><p>Physionet Experiments: <ref type="table" target="#tab_2">Table 1</ref> compares the performance of all methods on the interpolation task where we observe 50% − 90% of the values in the test instances. As we can see, the proposed method (mTAND-Full) consistently and substantially out-performs all of the previous approaches. Further, the interpolation performance of the proposed method conditioned on just 50% of the observed values is similar to that of Latent-ODE model conditioned on all the observed values. <ref type="table" target="#tab_3">Table 2</ref> compares predictive performance on the PhysioNet mortality prediction task. The full Multi-Time Attention network model (mTAND-Full) and the classifier based only on the Multi-Time Attention network encoder (mTAND-Enc) achieve significantly improved performance relative to the current state-of-the-art methods (ODE-RNN and L-ODE-ODE) and other baseline methods.</p><p>We also report the time per epoch in minutes for all the methods. We note that the ODE-based models require substantially more run time than other methods due to the required use of an ODE solver <ref type="bibr">(Chen et al., 2018;</ref><ref type="bibr" target="#b21">Rubanova et al., 2019)</ref>. These methods also require taking the union of all observation time points in a batch, which further slows down the training process. As we can see, the proposed full Multi-Time Attention network (mTAND-Full) is over 85 times faster than ODE-RNN and over 100 times faster than L-ODE-ODE, the best-performing ODE-based models.</p><p>MIMIC-III Experiments: <ref type="table" target="#tab_3">Table 2</ref> compares the predictive performance of the models on the mortality prediction task on MIMIC-III. The Multi-Time Attention network-based encoder-decoder framework (mTAND-Full) achieves better performance than the recent IP-Net and SeFT model as well as all of the RNN baseline models. While ODE-RNN and L-ODE-ODE both have slightly better mean AUC than mTAND-Full, the differences are not statistically significant. Further, as shown on the PhysioNet classification problem, mTAND-Full is more than an order of magnitude faster than the ODE-based methods.</p><p>Human Activity Experiments: <ref type="table" target="#tab_3">Table 2</ref> shows that the mTAND-based classifiers achieve significantly better performance than the baseline models on this prediction task, followed by ODE-based models and IP-Nets.</p><p>Additional Experiments: In Appendix A.2, we demonstrate the effectiveness of learning temporally distributed latent representations with mTANs on a synthetic dataset. We show that mTANs are able to capture local structure in the time series better than latent ODE-based methods that encode to a single time point. This property of mTANs helps to improve the interpolation performance in terms of mean squared error.</p><p>We also perform ablation experiments to show the performance gain achieved by learning similarity kernels and time embeddings in Appendix A.1. In particular, we show that learning the time embedding improves classification performance compared to using fixed positional encodings. We also demonstrate the effectiveness of learning the similarity kernel by comparing to an approach that uses fixed RBF kernels. Appendix A.1 shows that learning the similarity kernel using the mTAND module performs as well as or better than using a fixed RBF kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND CONCLUSIONS</head><p>In this paper, we have presented the Multi-Time Attention (mTAN) module for learning from sparse and irregularly sampled data along with a VAE-based encoder-decoder model leveraging this module.</p><p>Our results show that the resulting model performs as well or better than a range of baseline and state-of-the-art models on both the interpolation and classification tasks, while offering training times that are one to two orders of magnitude faster than previous state of the art methods. While in this work we have focused on a VAE-based encoder-decoder architecture, the proposed mTAN module can be used to provide an interface between sparse and irregularly sampled time series and many different types of deep neural network architectures including GAN-based models. Composing the mTAN module with convolutional networks instead of recurrent architectures may also provide further computational enhancements due to improved parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 ABLATION STUDY</p><p>In this section, we perform ablation experiments to show the performance gain achieved by learning similarity kernel and time embedding. <ref type="table" target="#tab_4">Table 3</ref> shows the ablation results by substituting fixed positional encoding <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> in place of learnable time embedding defined in Equation 1 in mTAND-Full model on PhysioNet and MIMIC-III dataset for classification task. We report the average AUC score over 5 runs. As we can see from <ref type="table" target="#tab_4">Table 3</ref>, learning the time embedding improves AUC score by 1% as compared to using fixed positional encodings. Since mTANs are fundamentally continuous-time interpolation-based models, we perform an ablation study by comparing mTANs with the IP-nets <ref type="bibr" target="#b25">(Shukla &amp; Marlin, 2019)</ref>. IP-Nets use several semiparametric RBF interpolation layers, followed by a GRU to model irregularly sampled time series. In this framework, we replace the RBK kernel with a learnable similarity kernel using mTAND module, the corresponding model is mTAND-Enc. <ref type="table" target="#tab_5">Table 4</ref> compares the performance of the two methods on classification task on PhysioNet, MIMIC-III and Human Activity dataset. We report the average AUC score over 5 runs. <ref type="table" target="#tab_5">Table 4</ref> shows that learning the similarity kernel using mTAND module performs as well or better than using a fixed RBF kernel. PhysioNet IP-Nets 0.819 ± 0.006 mTAND-Enc 0.854 ± 0.001</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIMIC-III</head><p>IP-Nets 0.839 ± 0.001 mTAND-Enc 0.842 ± 0.001</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Activity</head><p>IP-Nets 0.869 ± 0.007 mTAND-Enc 0.907 ± 0.002</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 SYNTHETIC INTERPOLATION EXPERIMENTS</head><p>To demonstrate the capabilities of our model on the interpolation task, we generate a synthetic dataset consisting of 1000 trajectories each of 100 time points sampled over t ∈ [0, 1]. We fix 10 reference points and use RBF kernel with a fixed bandwidth of 100 for constructing local interpolations at 100 time points over <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. The values at the reference points are drawn from a standard normal distribution.</p><p>We randomly sample 20 observations from each trajectory to simulate a sparse and irregularly sampled multivariate time series. We use 80% of the data for training and 20% for testing. At test time, encoder conditions on 20 irregularly sampled time points and the decoder generates interpolations on all 100 time points. <ref type="figure" target="#fig_4">Figure 3</ref> illustrates the interpolation results on the test set for the Multi-Time Attention Network and Latent ODE model with ODE encoder <ref type="bibr" target="#b21">(Rubanova et al., 2019)</ref>. For both the models, we draw 100 samples from the approximate posterior distribution. As we can see from <ref type="figure" target="#fig_4">Figure  3</ref>, the ODE interpolations are much smoother and haven't been able to capture the local structure as well as mTANS.   <ref type="table" target="#tab_6">Table 5</ref> compares the proposed model with best performing baseline Latent-ODE with ODE encoder (L-ODE-ODE) on reconstruction and interpolation task. For both the tasks, we condition on the 20 irregularly sampled time points and reconstruct the input points (reconstruction) and the whole set of 100 time points (interpolation). We report the mean squared error on test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observed data Ground truth Reconstructions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ARCHITECTURE DETAILS</head><p>Multi-Time Attention Network (mTAND-Full): In our proposed encoder-decoder framework <ref type="figure" target="#fig_2">(Figure 2)</ref>, we use bi-directional GRU as the recurrent model in both encoder and decoder. In encoder, we use a 2 layer fully connected network with 50 hidden units and ReLU activations to map the RNN hidden state at each reference point to mean and variance. Similarly in decoder, mTAN embeddings are independently decoded using a 2 layer fully connected network with 50 hidden units and ReLU activations, and the result is used to parameterize the output distribution. For classification tasks, we use a separate GRU layer on top of the latent states followed by a 2-layer fully connected layer with 300 units and ReLU activations to output the class probabilities.</p><p>Multi-Time Attention Encoder (mTAND-Enc): As we show in the experiments, the proposed mTAN module can standalone be used for classification tasks. The mTAND-Enc consists of Multi-Time attention module followed by GRU to extract the final hidden state which is then passed to a 2-layer fully connected layer to output the class probabilities.</p><p>Loss Function: For computing the evidence lower bound (ELBO) during training, we use negative log-likelihood with fixed variance as the reconstruction loss. For all the datasets, we use a fixed variance of 0.01. For computing ELBO, we use 5 samples for interpolation task and 1 sample for classification tasks. We use cross entropy loss for classification. For the classification tasks, we tune the λ parameter in the supervised learning loss function <ref type="formula" target="#formula_0">(Equation 15</ref>). We achieved best performance using λ as 100 and 5 for Physionet, MIMIC-III respectively. For human activity dataset, we achieved best results without using the regulaizer or ELBO component. We found that KL annealing with coeff 0.99 improved the performance of interpolation and classification tasks on Physionet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 HYPERPARAMETERS</head><p>Baselines: For Physionet and Human Activity dataset, we use the reported hyperparameters for RNN baselines as well as ODE models from <ref type="bibr" target="#b21">Rubanova et al. (2019)</ref>. For MIMIC-III dataset, we independently tune the hyperparameters of the baseline models on the validation set. We search for GRU hidden units, latent dimension, number of hidden units in fully connected network for ODE function in recognition and generative model over the range {20, 32, 64, 128, 256}. For ODEs, we also searched the number of layers in fully connected network in the range {1, 2, 3}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mTAN:</head><p>We learn time embeddings of size 128. The number of embeddings H ∈ {1, 2, 4}. The linear projection matrices used for projecting time embedding W are each d k * d k /h where d k is the embedding size. We search the latent dimension and GRU encoder hidden size over the range {32, 64, 128}. We keep GRU decoder hidden size at 50. For the classification tasks, we use 128 reference points. For interpolation task, we search number of reference points over the range {8, 16, 32, 64, 128}. We use Adam Optimizer for training the models. For classification, experiments are run for 300 iteration with learning rate 0.0001, while for interpolation task experiments are run for 500 iterations with learning rate 0.001. Best hyperparameters are reported in the code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 VISUALIZING ATTENTION WEIGHTS</head><p>Irregularly sampled time points</p><p>Regularly sampled time points In this section, we visualize the attention weights learned by our proposed model. We experiment using synthetic dataset (described in A.2) which consists of univariate time series. <ref type="figure" target="#fig_5">Figure 4</ref> shows the attention weights learned by the encoder mTAND module. The input shown in the figure is the irregularly sampled time points and the edges show how the output at reference points attends to the values on the input time points. The final output can be computed by substituting the attention weights in Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 TRAINING DETAILS</head><p>A.6.1 DATA GENERATION AND PREPROCESSING All the datasets used in the experiments are publicly available and can be downloaded using the following links: PhysioNet: https://physionet.org/content/challenge-2012/ MIMIC-III: https://mimic.physionet.org/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of the mTAND module. It takes irregularly sampled time points and corresponding values as keys and values and produces fixed dimensional representation at the query time points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>As shown in Equation 2, dimension j of the mTAN embedding mTAN(t, s)[j] is given by a linear combination of intermediate univariate continuous-time functionsx hd (t, s). There is one such function defined for each input data dimension d and each time embedding h. The parameters U hdj are learnable linear combination weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of the proposed encoder-decoder framework mTAND-Full. Classifier component (faded region) is required only in classification experiments not in interpolation experiments. mTAND module is shown inFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and round the observation times to the nearest minute. This leads to 2880 possible measurement times per time series. The data set includes 4000 labeled instances and 4000 unlabeled instances. We use all 8000 instances for interpolation experiments and the 4000 labeled instances for classification experiments. We focus on predicting in-hospital mortality. 13.8% of examples are in the positive class. The MIMIC-III data set (Johnson et al., 2016) is a multivariate time series dataset consisting of sparse and irregularly sampled physiological signals collected at Beth Israel Deaconess Medical Center from 2001 to 2012. Following the procedures of Shukla &amp; Marlin</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Interpolations on the synthetic interpolation dataset. The columns represent 3 different examples. First row: Ground truth trajectories with observed points, second row: reconstructions on the complete range t ∈ [0, 1] using the proposed model mTAN, third row: reconstructions on the complete range t ∈ [0, 1] using the Latent ODE model with ODE encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of attention weights. mTAN learns an interpolation over the query time points by attending to the observed values at key time points. The brighter edges correspond to higher attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>proposed a latent ODE model which consists of an ODE-RNN model as the encoder. ODE-RNNs use neural ODEs to model the hidden state dynamics and uses an RNN to update the hidden state in the presence of a new observation. De Brouwer et al. (2019) proposed GRU-ODE-Bayes, a continuous-time version of the Gated Recurrent Unit</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Interpolation performance versus percent observed time points on Physionet</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">Mean Squared Error (×10 −3 )</cell><cell></cell></row><row><cell>RNN-VAE</cell><cell>56.25 ± 0.507</cell><cell>49.64 ± 0.658</cell><cell>43.02 ± 0.646</cell><cell>37.08 ± 0.516</cell><cell>30.57 ± 0.508</cell></row><row><cell>L-ODE-RNN</cell><cell>9.325 ± 0.574</cell><cell>8.160 ± 0.321</cell><cell>7.080 ± 0.205</cell><cell>5.975 ± 0.287</cell><cell>4.900 ± 0.200</cell></row><row><cell>L-ODE-ODE</cell><cell>6.525 ± 0.150</cell><cell>5.860 ± 0.152</cell><cell>5.000 ± 0.115</cell><cell>4.400 ± 0.115</cell><cell>3.480 ± 0.148</cell></row><row><cell>mTAND-Full</cell><cell>2.463 ± 0.036</cell><cell>2.053 ± 0.032</cell><cell>1.685 ± 0.077</cell><cell>1.292 ± 0.071</cell><cell>0.887 ± 0.032</cell></row><row><cell>Observed %</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Classification Performance on PhysioNet, MIMIC-III and Human Activity dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">AUC Score</cell><cell>Accuracy</cell><cell>time</cell></row><row><cell></cell><cell>PhysioNet</cell><cell>MIMIC-III</cell><cell cols="2">Human Activity per epoch</cell></row><row><cell>RNN-Impute</cell><cell>0.764 ± 0.016</cell><cell>0.8249 ± 0.0010</cell><cell>0.859 ± 0.004</cell><cell>0.5</cell></row><row><cell>RNN-∆t</cell><cell>0.787 ± 0.014</cell><cell>0.8364 ± 0.0011</cell><cell>0.857 ± 0.002</cell><cell>0.5</cell></row><row><cell>RNN-Decay</cell><cell>0.807 ± 0.003</cell><cell>0.8392 ± 0.0012</cell><cell>0.860 ± 0.005</cell><cell>0.7</cell></row><row><cell>RNN GRU-D</cell><cell>0.818 ± 0.008</cell><cell>0.8270 ± 0.0010</cell><cell>0.862 ± 0.005</cell><cell>0.7</cell></row><row><cell>Phased-LSTM</cell><cell>0.836 ± 0.003</cell><cell>0.8429 ± 0.0035</cell><cell>0.855 ± 0.005</cell><cell>0.3</cell></row><row><cell>IP-Nets</cell><cell>0.819 ± 0.006</cell><cell>0.8390 ± 0.0011</cell><cell>0.869 ± 0.007</cell><cell>1.3</cell></row><row><cell>SeFT</cell><cell>0.795 ± 0.015</cell><cell>0.8485 ± 0.0022</cell><cell>0.815 ± 0.002</cell><cell>0.5</cell></row><row><cell>RNN-VAE</cell><cell>0.515 ± 0.040</cell><cell>0.5175 ± 0.0312</cell><cell>0.343 ± 0.040</cell><cell>2.0</cell></row><row><cell>ODE-RNN</cell><cell>0.833 ± 0.009</cell><cell>0.8561 ± 0.0051</cell><cell>0.885 ± 0.008</cell><cell>16.5</cell></row><row><cell>L-ODE-RNN</cell><cell>0.781 ± 0.018</cell><cell>0.7734 ± 0.0030</cell><cell>0.838 ± 0.004</cell><cell>6.7</cell></row><row><cell>L-ODE-ODE</cell><cell>0.829 ± 0.004</cell><cell>0.8559 ± 0.0041</cell><cell>0.870 ± 0.028</cell><cell>22.0</cell></row><row><cell>mTAND-Enc</cell><cell>0.854 ± 0.001</cell><cell>0.8419 ± 0.0017</cell><cell>0.907 ± 0.002</cell><cell>0.1</cell></row><row><cell>mTAND-Full</cell><cell cols="3">0.858 ± 0.004 0.8544 ± 0.0024 0.910 ± 0.002</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation with time embedding</figDesc><table><row><cell>Dataset</cell><cell>Time Embedding</cell><cell>AUC Score</cell></row><row><cell>PhysioNet</cell><cell cols="2">Positional Encoding Learned Time Embedding 0.858 ± 0.004 0.845 ± 0.004</cell></row><row><cell>MIMIC-III</cell><cell cols="2">Positional Encoding Learned Time Embedding 0.854 ± 0.002 0.843 ± 0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparing interpolation kernels</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>AUC Score</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Synthetic Data: Mean Squared Error</figDesc><table><row><cell>Latent Dimension</cell><cell>Model</cell><cell cols="2">Reconstruction Interpolation</cell></row><row><cell>10</cell><cell>L-ODE-ODE mTAND-Full</cell><cell>0.0209 0.0088</cell><cell>0.0571 0.0409</cell></row><row><cell>20</cell><cell>L-ODE-ODE mTAND-Full</cell><cell>0.0191 0.0028</cell><cell>0.0541 0.0335</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human Activity: https://archive.ics.uci.edu/ml/datasets/Localization+ Data+for+Person+Activity.</p><p>We rescale each feature to be between 0 and 1 for Physionet and MIMIC-III dataset. We also rescale the time to be in [0, 1] for all datasets. In case of MIMIC-III dataset, for the time series missing entirely, we follow the preprocessing steps of Shukla &amp; Marlin (2019) and assign the starting point (time t=0) value of the time series to the global mean for that variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6.2 SOURCE CODE</head><p>The code for reproducing the results in this paper is available at https://github.com/ satyanshukla/mTANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6.3 COMPUTING INFRASTRUCTURE</head><p>All experiments were run on a Nvidia Titan X GPU.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Medical concept embedding with time-aware attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kee</forename><surname>Yuan Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beng</forename><forename type="middle">Chin</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<ptr target="https://arxiv.org/abs/1412.3555" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Presented at the Deep Learning workshop at NIPS2014</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Population time series: process variability, observation errors, missing values, lags, and hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">N</forename><surname>Bjørnstad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3140" to="3150" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gru-ode-bayes: Continuous modeling of sporadically-observed time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaak</forename><surname>Edward De Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7379" to="7390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to detect sepsis with a multitask gaussian process RNN classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Futoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1174" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Set functions for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Lehman</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Time2vec: Learning a vector representation of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepehr</forename><surname>Eghbali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janahan</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaspreet</forename><surname>Sahota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<idno>abs/1907.05321</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A scalable end-to-end gaussian process adapter for irregularly sampled time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin M Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1804" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification of sparse and irregularly sampled time series with mixtures of expected Gaussian kernels and random features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjmain</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Directly modeling missing data in sequences with rnns: Improved classification of clinical time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="253" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised pattern discovery in electronic health care data using probabilistic clustering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Robinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><forename type="middle">C</forename><surname>Khemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium</title>
		<meeting>the 2nd ACM SIGHIT International Health Informatics Symposium</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Discrete event, continuous time rnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">V</forename><surname>Kazakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindsey</surname></persName>
		</author>
		<idno>abs/1710.04110</idno>
		<ptr target="http://arxiv.org/abs/1710.04110" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Phased lstm: Accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3882" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporal self-attention network for medical concept embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueping</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blumenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting healthcare trajectories from medical records: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2017.04.001</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5320" to="5330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The lomb-scargle periodogram in biological rhythm research: analysis of incomplete and unequally spaced time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ruf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Rhythm Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="201" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Studies in astronomical time series analysis. ii-statistical aspects of spectral analysis of unevenly spaced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scargle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Astrophysical Journal</title>
		<imprint>
			<biblScope unit="volume">263</biblScope>
			<biblScope unit="page" from="835" to="853" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spectrum: Spectral analysis of unevenly spaced paleoclimatic time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stattegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Geosciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="929" to="945" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Interpolation Prediction Networks for Irregularly Sampled Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satya Narayan Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin M Marlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting in-hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikaro</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in cardiology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="245" to="248" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attend and diagnose: Clinical time series analysis using attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepta</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayaraman</forename><forename type="middle">J</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Spanias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI Conference on Artificial Intelligence, AAAI 2018</title>
		<imprint>
			<publisher>AAAI press</publisher>
			<date type="published" when="2018-01" />
			<biblScope unit="page" from="4091" to="4098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Data-gru: Dual-attention time-aware gated recurrent unit for irregular multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<editor>Andy Jinhua Ma, Terry Cheuk-Fung Yip, Grace Lai-Hung Wong, and Pongchi Yuen</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-attention with functional time representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanwei</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evren</forename><surname>Korpeoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushant</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kannan</forename><surname>Achan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15915" to="15925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mining electronic health records (ehrs): A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranjul</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Estimating missing data in temporal data streams using multi-directional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Zame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBME.2018.2874712</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1477" to="1490" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep sensing: Active sensing using multi-directional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>William R Zame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attain: Attention-based time-aware lstm networks for disease progression modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Ivy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4369" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
