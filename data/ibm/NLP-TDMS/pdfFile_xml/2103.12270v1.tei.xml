<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dilated SpineNet for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Rashwan</surname></persName>
							<email>arashwan@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
							<email>xianzhi@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Yin</surname></persName>
							<email>yinx@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><forename type="middle">Li</forename><surname>Google</surname></persName>
							<email>jingli@google.com</email>
						</author>
						<title level="a" type="main">Dilated SpineNet for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scale-permuted networks have shown promising results on object bounding box detection and instance segmentation. Scale permutation and cross-scale fusion of features enable the network to capture multi-scale semantics while preserving spatial resolution. In this work, we evaluate this meta-architecture design on semantic segmentationanother vision task that benefits from high spatial resolution and multi-scale feature fusion at different network stages. By further leveraging dilated convolution operations, we propose SpineNet-Seg, a network discovered by NAS that is searched from the DeepLabv3 system. SpineNet-Seg is designed with a better scale-permuted network topology with customized dilation ratios per block on a semantic segmentation task. SpineNet-Seg models outperform the DeepLabv3/v3+ baselines at all model scales on multiple popular benchmarks in speed and accuracy. In particular, our SpineNet-S143+ model achieves the new state-ofthe-art on the popular Cityscapes benchmark at 83.04% mIoU and attained strong performance on the PASCAL VOC2012 benchmark at 85.56% mIoU. SpineNet-Seg models also show promising results on a challenging Street View segmentation dataset. Code and checkpoints will be opensourced.</p><p>semantic segmentation as the pixel-wise classification task benefits from detailed spatial information and aggregation of features from multiple scales.</p><p>To solve these problems, researches have proposed better network operations and architecture designs. The dilated convolution operator <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> is one of the most popular methods that overcome the challenge of preserving feature resolution. The 'convolution with holes' design allows the network to use upsampled convolution kernels to extract abstract semantics without reducing feature resolution. Recently, scale-permuted networks discovered by neural architecture search (NAS) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> have shown promising results on the task of object detection. Scale permutation for the intermediate building blocks enables the network to capture strong semantics and retain high feature resolution throughout network stages. Cross-scale feature fusion aggregates multi-scale semantics that helps the network to recognize objects at different scales.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Preserving feature resolution and aggregating multiscale feature information have long been the challenges in achieving better semantic segmentation performance. Convolutional neural networks designed for image-level classification tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b19">20]</ref> successively reduce feature resolution by pooling operations and convolutions with strides at different network stages. Such networks only output low-resolution features with strong semantics. e.g. ResNet <ref type="bibr" target="#b18">[19]</ref> reduces feature resolution to 1/32 of the input resolution at the end of its C 5 stage and only outputs the C 5 features. This design is not optimal for * Authors contributed equally.  In this work, we first explore the effectiveness of scalepermuted networks on the task of semantic segmentation. We simplify the search space proposed in <ref type="bibr" target="#b13">[14]</ref> and use the backbone of DeepLabv3 <ref type="bibr" target="#b4">[5]</ref> as the baseline for NAS. The architecture found by NAS improves over the baseline DeepLabv3 model by +2.06% mIoU on the PASCAL VOC2012 benchmark while using less computational resources. Secondly, we combine dilation convolution with scale-permuted network to further improve semantic segmentation. We delicately design a joint search space for scale permutation, cross-scale connections, block adjustments and block dilation ratios. The final architecture, called SpineNet-Seg-49 (SpineNet-S49), improves mIoU by +2.47% over the baseline on the PASCAL VOC2012 benchmark while using 15% less computations. Lastly, we scale and modify the SpineNet-S49 architecture to generate two model families for regular-size semantic segmentation and mobile-size semantic segmentation. In particular, our SpineNet-S143+ model achieves new state-of-the-art performance on Cityscapes at 83.04% and strong performance on PASCAL VOC2012 at 85.56% mIoU, under the settings of single-model single-scale inference without using extra data. Our mobile-size SpineNet-S49-outperforms the Mo-bileNetv3 based DeepLab model by +2.5% while using less computatioinal resources.</p><p>Our contributions are summarized as below:</p><p>• We prove scale-permuted network improves semantic segmentation.</p><p>• We propose a novel search space that jointly search for 4 components for semantic segmentation and design a proxy task for NAS.</p><p>• We outperfrom the baseline DeepLabv3/v3+ models at all model scales by 2-3% mIoU on the PASCAL VOC2012 benchmark while using less computations.</p><p>• We achieve new state-of-the-art on the Cityscapes benchmark at 83.04% mIoU by using single-model single-scale inference without extra training data.</p><p>• We provide a family of Mobile SpineNet-Seg models for mobile-size semantic segmentaiton that outperform popular MobileNetv2/v3 based segmentation models.</p><p>The remaining contents of the paper are organized as follows. We discuss related works in Section 2. We describe our search space design and final architectures in Section 3. The application details for our regular-size and mobile-size segmentation systems are described in 4. Our main results and ablation studies are presented in Section 5. We conclude this work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic segmentation: Performance of the convolutional neural networks on the task of semantic segmenta-tion has been improved in the recent years by adopting better backbones and improving network designs for semantic segmentation. Since the development of convolutional neural network, researchers have proposed stronger network architectures in better designs and larger scales for the task of image classification, e.g. AlexNet <ref type="bibr" target="#b21">[22]</ref>, VGG <ref type="bibr" target="#b35">[36]</ref>, Inception <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, ResNet <ref type="bibr" target="#b18">[19]</ref>, Xception <ref type="bibr" target="#b7">[8]</ref>, DenseNet <ref type="bibr" target="#b20">[21]</ref>, MobileNet <ref type="bibr" target="#b32">[33]</ref>, Wide ResNet <ref type="bibr" target="#b48">[49]</ref> and ResNeXt <ref type="bibr" target="#b44">[45]</ref>. Such networks not only improve image classification, but also transfer to downstream tasks such as object detection, semantic segmentation, depth estimation, etc. On the other hand, better architecture designs for semantic segmentation have been proposed to preserve object details and to aggregate multi-scale contexts. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b54">55]</ref> propose to use an encoder-decoder design to first reduce feature resolution with an encoder to capture deep and coarse semantics then recover spatial resolution via upsampling or deconvolution <ref type="bibr" target="#b49">[50]</ref> with a decoder. Shortcut connections can be used between the two components to aggregate multi-scale contexts. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b17">18]</ref> propose to adopt the spatial pyramid pooling module to aggregate context information from local to global at multiple grid scales. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref> advocate to use dilated convolution at certain stages of existing architectures to expand receptive filed of convolution kernels without downsampling. The resulting architectures are able to capture dense semantics without losing resolution.</p><p>NAS and search space designs: NAS automates the design of neural network architecture to find better architectures in a predetermined search space on a target task. Recent architectures discovered by NAS have shown promising results than handcrafted models on vision tasks including image classification <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b19">20]</ref>, object detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b23">24]</ref>, semantic segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>, etc. For image classification, typical search space designs include searching for kernel size and filter size of convolutional layers, number of layers per network stage, additional operations such as shortcut connections, attention modules, activation functions, etc. Recent works have developed customized search space for downstream tasks. For object detection, NAS-FPN <ref type="bibr" target="#b15">[16]</ref> proposes a search space to search for layer scales and lateral connections for FPN <ref type="bibr" target="#b24">[25]</ref>. SpineNet <ref type="bibr" target="#b13">[14]</ref> designs a search space that searches for a new ordering of network blocks for a baseline architecture and cross-scale connections to connect all blocks. CR-NAS <ref type="bibr" target="#b23">[24]</ref> redistributes computational resources by searching for better block repeating times per network stage for ResNet models. Auto-DeepLab <ref type="bibr" target="#b26">[27]</ref> is one of the pioneering works to explore NAS for semantic segmentation. Auto-DeepLab proposes a two-level hierarchical search space that learns operations at block-level and learns block resolutions at network-level with a few hand-crafted constraints with respect to common network design choices for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section starts from introducing our search space for semantic segmentation in Section 3.1. The baseline architecture and the computation allocations for NAS are explained in Section 3.2. The final SpineNet-Seg architecture discovered by NAS and its variants are described in Section 3.3 and 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space Design</head><p>The proposed search space consists of 4 components: search a scale permutation for the building blocks of a baseline architecture; search one cross-scale connection for each block; search a level 1 adjustment for each block; search a dilation ratio for the convolution within each block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale permutations and cross-scale connections:</head><p>Inspired by <ref type="bibr" target="#b13">[14]</ref>, we define the search space for scalepermutation to be permuting the ordering of intermediate blocks. This results in a search space size of N !, where N is the total number of blocks to be permuted. Unlike <ref type="bibr" target="#b13">[14]</ref>, where two cross-scale connections are searched per block, we only search for one long-range connection for each block and simplify the short-range connection to be between each pair of adjacent blocks. This greatly reduces the number of candidates in the search space from (</p><formula xml:id="formula_0">N +m−1 i=m C i 2 ) to ( N +m−1 i=m i),</formula><p>where m is the number of initial blocks, while not introducing any performance drop in architecture search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block level adjustments:</head><p>As the default block level distribution might not be optimal for the target task, we allow each block to search for a level adjustment from a list of integer candidates {A 1 , A 2 , ..., A a }. This results in a search space size of a N .</p><p>Dilation ratios: Lastly, we introduce the popular dilated convolution operator to the search space. We allow each block to search for one dilation ratio from a list of candidates {D 1 , D 2 , ..., D d }. This results in a search space size of d N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baseline and Computation Allocations</head><p>Searching for a scale-permuted network starts from a baseline network. In this work, we adopt the ResNet-50 backbone of DeepLabv3 <ref type="bibr" target="#b4">[5]</ref> with an output stride of 16, and with stage 5 being repeated twice. Unlike DeepLabv3 that <ref type="bibr" target="#b0">1</ref> Following <ref type="bibr" target="#b13">[14]</ref>, we use "level" to represent the resolution of a block. L i indicates a block that has a resolution of 1 2 i of the input resolution. Block id BP CC LA DR FD  search space for cross-scale connections and reserve one L 3 block to construct an output block. For intermediate blocks, we first search for a permutation for the remaining block allocation {1×L 2 , 3×L 3 , 6×L 4 , 9×L 5 }. Secondly, for each block, we connect it to its immediate previous block and search for one cross-scale connection from the other previous blocks. The same resampling strategy as in <ref type="bibr" target="#b13">[14]</ref> is adopted when merging blocks. Thirdly, we search for one level adjustment within {−1, 0}. This is because ResNet-S50 keeps feature resolution but increases feature dimension for stage 4 and above. In order to constraint the computation of candidates in the search space to be no larger than the baseline, we only allow each block to either keep or decrease its level (i.e. increase feature resolution). Lastly, we search for a dilation ratio within {1, 2, 4}. The dilation ratio is applied to the 3×3 convolution in each bottleneck block. For the output block, following the common output design for semantic segmentation networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>, we append a L 3 block (i.e. output stride 8) at the top and only search for a cross-scale connection and a dilation ratio.</p><formula xml:id="formula_1">B 0 L 2 - - -64 B 1 L 2 - - -64 B 2 L 3 B 0 -1 1 64 B 3 L 4 B 1 -1 2 128 B 4 L 3 B 1 0 1 128 B 5 L 3 B 2 0 1 128 B 6 L 6 B 3 0 1 512 B 7 L 6 B 5 -1 2 512 B 8 L 7 B 4 0 1 512 B 9 L 7 B 6 0 4 512 B 10 L 5 B 6 0 1 512 B 11 L 5 B 7 0 2 512 B 12 L 4 B 8 0 4 256 B 13 L 4 B 9 0 1 256 B 14 L 5 B 11 0 4 512 B 15 L 4 B 11 0 4 256 B 16 L 4 B 12 0 1 256 B 17 L 2 B 14 0 4 64 B 18 L 7 B 16 -1 2 512 B 19 L 6 B 15 0 1 512 B 20 L 4 B 17 -1 4 128 B 21 -B 0 0 1 128</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SpineNet-Seg Architectures</head><p>SpineNet-Seg architectures are searched with a DeepLabv3 system on a semantic segmentation task. The final SpineNet-S49 configuration discovered by NAS are shown in Tab. 2. Feature dimension {32, 64, 128, 256, 512} are used for {L 1 , L 2 , L 3 , L 4 , L 5 } blocks, respectively. Based on SpineNet-S49, we construct three larger models, named SpineNet-S96, SpineNet-S143 and SpineNet-S143+, by repeating each block in SpineNet-S49 twice or three times. When repeating one block, we construct replicas of this block and connect them with the original block sequentially without introducing any cross-scale connections. For our largest model SpineNet-S143+, we further uniformly upscale the feature dimension of convolutional layers by 1.3×.</p><p>We control output stride of the model by changing the size of the output block and its cross-scale connection. We preserve the sizes of the rest of the layers. This is computationally more efficient than changing all layers that are smaller in size than the required output stride as proposed by DeepLabv3 <ref type="bibr" target="#b4">[5]</ref>. Unless stated, our models have an output stride of 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Mobile-size SpineNet-Seg Architectures</head><p>Unlike regular semantic segmentation systems that use standard convolution operations, mobile-size systems desire low-computation operations. Inspired by <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13]</ref>, we adopt the inverted bottleneck block that employs depthwise separable convolution <ref type="bibr" target="#b7">[8]</ref> as its main operator to build mobile-size SpineNet-Seg models.</p><p>We construct Mobile SpineNet-S49 and Mobile SpineNet-S49-by replacing all bottleneck blocks with inverted bottleneck blocks.</p><p>Feature dimension {16, 24, 40, 80, 112} and expansion ratio 6 are used for {L 1 , L 2 , L 3 , L 4 , L 5 } blocks in Mobile SpineNet-S49, respectively. Mobile SpineNet-S49-uniformly downscales the feature dimension of all convolutional layers by 0.65×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>We plug in SpineNet-Seg models as the backbones of the Deeplabv3 system for semantic segmentation. On top of the backbone, we apply an Atrous Spatial Pyramid Pooling (ASPP) module, n convolutions with kernel size 3 and feature dimension 256 followed by batch normalization and activation, and a final classification layer with kernel size 3 to compute pixel-wise predictions. The final architecture of the SpineNet-S49 model is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Normally, we directly build the final classification layer on top of the ASPP module (n = 0). However, we found that using 2 convolutional layers (n = 2) is essential for stable training when the output stride is 4, and with larger model sizes. In particular, SpineNet-S143+ model is trained with 2 convolutional </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>We evaluate our Spinenet-Seg models on the PASCAL VOC2012 benchmark <ref type="bibr" target="#b14">[15]</ref>, the Cityscapes benchmark <ref type="bibr" target="#b8">[9]</ref> and a challenging large-scale Street View dataset.</p><p>Unless stated, all experiments (including baselines) use SGD optimizer with momentum of 0.9, cosine learning rate schedule, and exponential moving average (EMA) optimizer with average decay of 0.9998. For pretraining, we use ImageNet <ref type="bibr" target="#b10">[11]</ref> and COCO <ref type="bibr" target="#b25">[26]</ref> datasets. Unless stated, the results (including DeepLabv3 and DeepLabv3+) reported are with ImageNet pretraining. All results in this section are computed with single scale inference. For fair comparison, we always scale mask predictions to original image sizes to compute mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pretraining</head><p>ImageNet pretrain: We pretrain the models on ImageNet-1k dataset for 350 epochs with batch size of 4096. We use the following experiment setup for ImageNet pretraining: cosine learning rate schedule with an initial learning rate of 1.6, regular batch normalization with momentum of 0.99, L2 weight decay of 4e-5, label smoothing of 0.1, we train on random crops of 320x320, we use RandAugment <ref type="bibr" target="#b9">[10]</ref> for image augmentations. EMA is not adopted for ImageNet pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO pretrain:</head><p>We use COCO-Things semantic segmentation labels where only annotations for things are used as forground classes, and the rest is used as background. We build the ASPP module such that it matches the ASPP used for the target dataset. We use the following experiment setup for COCO pretraining: we train on image sizes of 512x512 with random horizontal flips, and scale jittering of [0.5, 2.0], batch size of 256, sync batch normalization with momentum of 0.99 and L2 weight decay of 1e-5. We train the models for 64k steps with cosine learning rate schedule with an initial learning rate of 0.08. EMA is not used for COCO pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on PASCAL VOC2012</head><p>PASCAL VOC2012 <ref type="bibr" target="#b14">[15]</ref> is a semantic segmentation dataset with 20 forground classes and 1 background  <ref type="figure" target="#fig_1">Fig. 1</ref> shows performance comparisons of our SpineNet-Seg models vs. DeepLabv3 and DeepLabv3+ with counterpart ResNet backbones. All models are trained using the same experiment setup. Our results show consistent +3% and +2% improvements in mIoU across all model scales compared to DeepLabv3 and DeepLabv3+ models. Specifically, SpineNet-S49, SpineNet-S96, and SpineNet-S143 show improvements of +2%, +2.66%, and +2.29% in mIoU compared to DeepLabv3+ with ResNet-50, ResNet-101, and ResNet-152 backbones respectively. While having significant gain over DeepLab models, SpineNet models are less computationally expensive than their Deeplab ResNet counterparts.</p><p>Tab. 4 studies the effect of using different training setups on the PASCAL VOC2012 validation set. We found that using EMA of model weights improved mIoU by 0.37%. We also increase the training and evaluation image sizes to 640×640 and resize the prediction masks to its original image sizes for fair comparisons. Using image sizes of 640×640 shows an improvement of 0.7%. Finally, pretraining on COCO dataset shows an improvement of 1.4% in mIoU. As a result, the mIoU on PASCAL validation set of our SpineNet-S49 model achieves 83.49% mIoU.</p><p>Tab. 5 summarizes the effect of scaling up the model size by using different block repeats of 1, 2, and 3 (SpineNet-S49, SpineNet-S96, and SpineNet-S143 respectively). In these experiments, we used the best training setup in Tab. 4. SpineNet-S96 improves the mIoU by 1.67% on PASCAL Validation set, and SpineNet-S143 improves the mIoU by another 0.48%. Our best model using single scale inference achieves 85.64% mIoU on Pascal VOC validation set. We also compare our best models with previous work in Tab. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on Cityscapes</head><p>Cityscapes <ref type="bibr" target="#b8">[9]</ref> contains high quality pixel-level annotations of 5000 images (2975, 500, and 1525 for train, vali-dation, and test splits respectively). It also contains 20000 coarsely annotated images for training. In our experiments, we only used the high quality pixel-level annotation train split for training, and evaluate on the validation split. Following <ref type="bibr" target="#b8">[9]</ref>, we train and evaluate on 19 semantic labels and ignore the void label.</p><p>We train on crops of 512x1024 with scale jittering of [0.5, 2.0] and random horizontal image flipping. We use batch size of 64, and sync batch normalization with momentum of 0.99. We use dilation rates of 12, 24, 36, and 72 to build ASPP. We train each experiment for 100k iterations.</p><p>Tab. 6 compares SpineNet-S49 to DeepLabv3+ with ResNet-50 backbone. Both models are trained using the same training setup including ImageNet pretraining, batch size, and using EMA of model weights. Our SpineNet-S49 model shows an improvement of +1.22% in mIoU compared to its DeepLabv3+ ResNet-50 model counterpart. In the same table, we show the effect of scaling up the model using block repeats of 1, 2, and 3 (SpineNet-S49, SpineNet-S96, and SpienNet-S143 respectively). SpineNet-S96 model improves the performance by 0.39%, while SpineNet-S143 further improves the mIoU by another 0.66%.</p><p>Tab. 8 shows the effect of using different training setup on the Cityscapes validation set. We used SpineNet-S143 model (block repeats of 3 and output stride of 8) as the baseline for the ablation study. We found that using exponential moving average of model weights improved mIoU by 0.19%. Moreover, we adopt the largest backbone SpineNet-S143+ and change the output stride of the model to 4. SpineNet-S143+ improves the mIoU by 0.56% on Cityscapes validation set. Finally, we pretrain SpineNet-S143+ on COCO and finetune on Cityscapes which further improves performance by 0.37%. As shown in Tab. 7, our best model at 83.04% mIoU on Cityscapes validation set achieves the new state-of-the-art when using single scale inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Mobile SpineNet-Seg Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Params <ref type="formula">(</ref> We follow the training setup for SpineNet-Seg models to train two mobile-size models: Mobile SpineNet-S49 and Mobile SpineNet-S49-. As shown in Tab. 9, Mobile SpineNet-Seg models achieve significantly better mIoU in speed and accuracy compared to MobileNetV3 model. We further evaluate SpineNet-Seg on a challenging dataset from Street View images. The dataset contains 57k train images and 13k test images, with 44 semantic categories on typical street scenes, e.g., building, sidewalk, traffic sign, and cars. In addition, the dataset is collected across 6 continents, 39 countries and 80+ cities worldwide under diverse conditions. <ref type="figure" target="#fig_3">Fig. 3</ref> shows an typical example of the dataset. Given the complexity, size and geo-diversity, we believe this is a practical stress test for the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results on Street View</head><p>For controlled experiment, we compare our SpineNet-S143+ to ResNet-152 using the same experiment settings. We use train and eval image sizes of 1152x768 with scale jittering of [0.5, 2.0] and random horizontal image flipping. We use batch size of 128, and sync batch normalization with momentum of 0.99. We use dilation rates of 12, 24, 36, and 72 to build ASPP. We train each experiment for 100k iterations. We also compare to DeepLabv3+ with Xception65 backbone when trained according to experiment setting suggested in <ref type="bibr" target="#b5">[6]</ref>. All experiments are trained and evaluated on the same image sizes.</p><p>Tab. 10 compares SpineNet-Seg to DeepLabv3+ models. SpineNet model improves the mIoU by 1.64% compared to DeepLabv3+ with ResNet-152 backbone. We also observe 2.46% improvements on mIOU compared to DeepLabv3+ with Xception-65 backbone. The results support our claim that SpineNet-Seg outperform DeepLabv3+ models in challenging real world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Backbone mIoU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepLabv3+</head><p>Xception-65 57.06 DeepLabv3+</p><p>ResNet-152 57.88 SpineNet-Seg SpineNet-S143+ 59.52 <ref type="table">Table 10</ref>. Results on Street View Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">NAS Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">NAS implementation details</head><p>We run NAS for 10k trials, and we evaluate the best 10 architectures on PASCAL VOC2012. Due to the large number of trials, we design a proxy search task to quickly evaluate the architecture candidates. For all search experiments, we uniformly downscale the feature dimension of convolutional layers of the candidate models by 0.5× and use image sizes 384×384 for training and evaluation. We run training for 30k steps with batch size of 64 and collect the final evaluation mIoU as the reward for the controller <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Baseline R-S50 † R-S50 R-S101 † mIoU Gain (%) +0 +0.79 +2.47 +2.02 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Ablation studies of the search designs</head><p>First, we experiment with different baseline architectures to search from. Inspired by DeepLabv3, we consider three architectures, ResNet-S50 † , ResNet-S50 and ResNet-S101 † , where † indicates absence of stages 6 and 7. Tab. 11 shows that searching from ResNet-S50 yields best improvements with +2.47% in mIoU gain over the baseline. We also study the effect of searching for scale-permuted network and dilation ratios in Tab. 12. We found that searching for scalepermuted networks yields +2.15% in mIoU gain. Further searching for dilation ratios improves the mIoU by +0.32%. Finally, we study the effect of fixing the output stride to 4, 8, or 16 during search in Tab. 13, while also changing the feature dimension of the output block accordingly such that the model size is preserved among the search jobs. We found that output stride of 8 yields the best gain of +2.15% in mIoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">A study on search dataset</head><p>Search dataset is important since the evaluation signals influence the quality of the searched architectures. For instance, performance on the PASCAL VOC2012 dataset depends heavily on the quality of the pretrained checkpoint, hence it is difficult to use for NAS that trains proxty tasks from scratch. We decide to use COCO dataset since it is diverse, and unlike Cityscapes it has significantly smaller images. Training proxy tasks from scratch using the COCO dataset usually converges, and eval signals can be used as stable rewards to update the NAS controller.</p><p>We study the effect of using COCO Things annotations and COCO Things+Stuff annotations. Tab. 14 shows that COCO Things achieves better performance (+2.47% mIoU gain) compared to COCO Things+Stuff (+2.06%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we evaluated the effectiveness of scalepermuted architectures on the task of semantic segmentation, a vision task that benefits from high feature resolution and multi-scale feature fusion. We proposed a new search space that simplifies the SpineNet search space and introduces new search components for semantic segmentation and learned SpineNet-S49 architecture by NAS with a carefully designed proxy task. We further construct two families of models based on SpineNet-S49: SpnieNet-Seg models and Mobile SpineNet-Seg models. SpineNet-Seg models outperform the popular DeepLabv3/v3+ models on the PASCAL VOC2012 benchmark and a challenging Street View data and achieve state-of-the-art performance on the Cityscapes benchmark. Mobile SpineNet-Seg models achieve new state-of-the-art performance on mobile-size semantic segmentation, surpassing popular mobile segmentation systems such as MobileNetV2/V3. We expect scalepermuted network with task-specific designs to benefit more computer vision tasks in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Performance comparisons of SpineNet-Seg, DeepLabv3 and DeepLabv3+ on the PASCAL VOC2012 val set. The proposed SpineNet-Seg models outperform the other two families of models at all model scales. SpineNet-Seg adopts SpineNet-S49/S96/S143 backbones and DeepLabv3 and DeepLabv3+ adopt ResNet-50/101/152 backbones. Controlled experiments and detailed experimental settings can be found in Section 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The final SpineNet-S49 model for semantic segmentation. {L2, L3, L4, L5} blocks are colored in purple, yellow, green and blue, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Example Street View images with semantic labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 1. A performance comparison of the original DeepLabv3+ResNet-50 backbone that downsamples at the end of each stage and our modified ResNet-S50 backbone that downsamples at the beginning of each stage. Results are reported with the DeepLabv3+ system on the PASCAL VOC2012 val dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">Downsample FLOPs (B) mIoU</cell></row><row><cell>ResNet-50</cell><cell>end</cell><cell>117</cell><cell>79.4</cell></row><row><cell cols="2">ResNet-S50 beginning</cell><cell>85</cell><cell>79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>to downsample the features at the end of each network stage, we modify the downsampling to happen at the beginning of each stage. This saves 30% of the computational cost with negligible loss in performance. A study of the effect of such a modification is shown in Tab. 1. We refer to the modified backbone as ResNet-Seg-50 (ResNet-S50), while we refer to the original DeepLabv3+ backbones as ResNet-50/101/152 2 .ResNet-S50 provides a block allocation of {3×L 2 , 4×L 3 , 6×L 4 , 9×L 5 } bottleneck blocks. We take two L 2 blocks to build an initial network that forms the initial<ref type="bibr" target="#b1">2</ref> Unless stated otherwise, stage 5 is repeated twice when referring to different ResNet models (e.g. ResNet-50/101/152).</figDesc><table><row><cell></cell><cell>Predicted Masks</cell></row><row><cell></cell><cell>Segmentation Head</cell></row><row><cell></cell><cell>ASPP</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 4</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 2</cell></row><row><cell></cell><cell>Dilations: 4</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 4</cell></row><row><cell></cell><cell>Dilations: 4</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 4</cell></row><row><cell>SpineNet-S49</cell><cell>Dilations: 2</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 4</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 2</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 2</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Dilations: 1</cell></row><row><cell></cell><cell>Stem</cell></row><row><cell></cell><cell>Image</cell></row></table><note>Learned network configurations for the SpineNet-S49 architecture. We show the detailed configurations for each block for the search space components described in Section 3.1. BP: block permutation. CC: cross-scale connection. LA: level adjust- ment. DR: dilation ratio. FD: feature dimension.proposes</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Result comparisons on the PASCAL VOC2012 val set. The proposed SpineNet-Seg models outperform the DeepLabv3 baselines and DeepLabv3+ models at all scales. All models are trained under the same settings.</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell>Backbone</cell><cell>FLOPs (B) #Params (M) mIoU</cell></row><row><cell></cell><cell cols="3">DeepLabv3</cell><cell>ResNet-50</cell><cell>115</cell><cell>74</cell><cell>78.3</cell></row><row><cell></cell><cell cols="3">DeepLabv3+</cell><cell>ResNet-50</cell><cell>117</cell><cell>75</cell><cell>79.4</cell></row><row><cell></cell><cell cols="4">SpineNet-Seg SpineNet-S49</cell><cell>98</cell><cell>69</cell><cell>81.4</cell></row><row><cell></cell><cell cols="3">DeepLabv3</cell><cell>ResNet-101</cell><cell>193</cell><cell>93</cell><cell>79.3</cell></row><row><cell></cell><cell cols="3">DeepLabv3+</cell><cell>ResNet-101</cell><cell>195</cell><cell>94</cell><cell>79.9</cell></row><row><cell></cell><cell cols="4">SpineNet-Seg SpineNet-S96</cell><cell>154</cell><cell>116</cell><cell>82.6</cell></row><row><cell></cell><cell cols="3">DeepLabv3</cell><cell>ResNet-152</cell><cell>271</cell><cell>109</cell><cell>80.2</cell></row><row><cell></cell><cell cols="3">DeepLabv3+</cell><cell>ResNet-152</cell><cell>273</cell><cell>110</cell><cell>81.0</cell></row><row><cell></cell><cell cols="4">SpineNet-Seg SpineNet-S143</cell><cell>210</cell><cell>162</cell><cell>83.3</cell></row><row><cell cols="5">layers at the head (n = 2), while the rest of the models are</cell></row><row><cell cols="5">trained with n = 0. For mobile-size systems, we replace reg-</cell></row><row><cell cols="5">ular convolutions with depthwise separable convolutions in</cell></row><row><cell cols="2">ASPP and segmentation head.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">EMA 640×640 COCO mIoU</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">81.02</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">81.39</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell cols="2">82.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">83.49</cell></row></table><note>Table 4. An ablation study of the training settings. Results are re- ported with the SpineNet-S49 model on the PASCAL VOC2012 val set. EMA: refers to using exponential moving average of the model weights during training. 640x640: using training and eval- uation image sizes of 640×640. COCO: Model pretrained on the COCO dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 7 .Table 8 .</head><label>678</label><figDesc>Result comparisons on the Cityscapes val set. SpineNet-S49 outperforms DeepLabv3+ with a ResNet-50 backbone in both accuracy and speed. SpineNet-S49/S96/S143 and DeepLabv3+ models are trained under the same settings. SpineNet-S143+ marked with † adopts the best training recipe to achieve best performance. State-of-the-art on the Cityscapes val set. We compare our best model on the Cityscapes val set to other models reported in literature. Note that our model uses single-scale input for inference and is trained without using extra data. Cityscapes val set results. These results are obtained using single scale inference with no horizontal flipping. OS: refers to the output stride of the SpineNet-Seg model, normally the output stride is 8.</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell cols="2">Backbone</cell><cell>FLOPs (B) #Params (M) mIoU</cell></row><row><cell></cell><cell></cell><cell cols="2">DeepLabv3+</cell><cell cols="2">ResNet-50</cell><cell>1092</cell><cell>76</cell><cell>79.84</cell></row><row><cell></cell><cell></cell><cell cols="2">SpineNet-Seg</cell><cell cols="2">SpineNet-S49</cell><cell>798</cell><cell>69</cell><cell>81.06</cell></row><row><cell></cell><cell></cell><cell cols="2">SpineNet-Seg</cell><cell cols="2">SpineNet-S96</cell><cell>1272</cell><cell>117</cell><cell>81.45</cell></row><row><cell></cell><cell></cell><cell cols="4">SpineNet-Seg SpineNet-S143</cell><cell>1722</cell><cell>164</cell><cell>82.11</cell></row><row><cell></cell><cell></cell><cell cols="4">SpineNet-Seg  † SpineNet-S143+</cell><cell>2766</cell><cell>275</cell><cell>83.04</cell></row><row><cell></cell><cell></cell><cell cols="2">Model</cell><cell></cell><cell>Backbone</cell><cell>Multi-scale Test mIoU</cell></row><row><cell></cell><cell></cell><cell cols="3">DeepLabv3+ [6]</cell><cell>Xception-71</cell><cell>-</cell><cell>79.55</cell></row><row><cell></cell><cell></cell><cell cols="3">MDEQ-XL [2]</cell><cell>MDEQ</cell><cell>-</cell><cell>80.30</cell></row><row><cell></cell><cell></cell><cell cols="3">AutoDeeplab [27]</cell><cell>AutoDeeplab-L</cell><cell>-</cell><cell>80.33</cell></row><row><cell></cell><cell></cell><cell cols="3">RepVGG [12]</cell><cell>RepVGG-B2</cell><cell>-</cell><cell>80.57</cell></row><row><cell></cell><cell></cell><cell cols="3">HRNetV2 [42]</cell><cell>HRNetV2-W48</cell><cell>-</cell><cell>81.10</cell></row><row><cell></cell><cell></cell><cell cols="3">Panoptic-DeepLab [7]</cell><cell>-</cell><cell>-</cell><cell>81.50</cell></row><row><cell></cell><cell></cell><cell cols="4">HRNetV2 + OCR [42] HRNetV2-W48</cell><cell>-</cell><cell>81.60</cell></row><row><cell></cell><cell></cell><cell cols="3">ResNeSt [51]</cell><cell>ResNeSt-200</cell><cell>82.70</cell></row><row><cell></cell><cell></cell><cell cols="3">SpineNet-Seg</cell><cell>SpineNet-S143+</cell><cell>-</cell><cell>83.04</cell></row><row><cell cols="4">EMA OS=4 COCO mIoU</cell><cell></cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.92</cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>82.11</cell><cell></cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>82.67</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>83.04</cell><cell></cell></row><row><cell cols="6">class. For training, we use an augmented version of the</cell></row><row><cell cols="6">dataset [17] with extra annotations of 10582 images (train-</cell></row><row><cell cols="6">aug). The default training setup uses training image sizes of</cell></row><row><cell cols="6">512x512 with scale jittering of [0.5, 2.0] and random hor-</cell></row><row><cell cols="6">izontal image flipping. We use batch size of 32, and sync</cell></row><row><cell cols="6">batch normalization with momentum of 0.9997. We use di-</cell></row><row><cell cols="6">lation rates of 12, 24 and 36 to build ASPP. We train exper-</cell></row><row><cell>iments for 20k steps.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tab. 3 and</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Mobile SpineNet-Seg results on Cityscapes val dataset. We compare our models to MobileNetV3 version.</figDesc><table><row><cell></cell><cell></cell><cell>M) mIoU</cell></row><row><cell>MobileNetV3 [20]</cell><cell>3.60</cell><cell>72.64</cell></row><row><cell>Mobile SpineNet-S49-</cell><cell>3.15</cell><cell>75.18</cell></row><row><cell>Mobile SpineNet-S49</cell><cell>4.40</cell><cell>77.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 11 .Table 12 .Table 13 .Table 14 .</head><label>11121314</label><figDesc>Effectiveness of different search baselines. We show mIoU gain on PASCAL VOC2012 val set when using different architectures, ResNet-S50 and ResNet-S101, as the baseslines for NAS. Backbones marked with † indicate stage 5 not repeated. An ablation of searching for scale-permuted network and dilation ratios. We show mIoU gain on PASCAL VOC2012 val set when searching for scale-permuted network or jointly searching for scale-permuted network and dilation ratios. SP: scale-permuted network. DR: dilation ratio. Impact of different output strides. This table shows mIoU gain on PASCAL VOC2012 val set when searching architectures with different output strides. A study on the search dataset. We show mIoU gain on PASCAL VOC2012 val set when using COCO Things or COCO Stuff+Things for NAS.</figDesc><table><row><cell cols="5">Output Stride Baseline SP SP+DR</cell></row><row><cell cols="2">mIoU Gain</cell><cell>+0</cell><cell cols="2">+2.15 +2.47</cell></row><row><cell cols="3">Output Stride Baseline</cell><cell>4</cell><cell>8</cell><cell>16</cell></row><row><cell>mIoU gain</cell><cell>+0</cell><cell></cell><cell cols="2">+0.67 +2.15 +1.47</cell></row><row><cell cols="5">Search Dataset Baseline COCO Things Stuff+Things</cell></row><row><cell>mIoU Gain</cell><cell>+0</cell><cell></cell><cell>+2.47</cell><cell>+2.06</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5238" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7062</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation. ArXiv, abs/1706.05587</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno>abs/1802.02611</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12472" to="12482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Repvgg: Making vgg-style convnets great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiaohan Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno>abs/2101.03697</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient scale-permuted backbone with learned resource distribution. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spinenet: Learning scale-permuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.5" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computation reallocation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno>abs/1912.11234</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Savalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Squeezenas: Fast neural architecture search for faster semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">Eaton</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno>abs/1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">NAS-FCOS: Fast neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04423</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bridging category-level and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno>abs/1605.06885</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Auto-fpn: Automatic network architecture adaptation for object detection beyond classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1511.07122</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Resnest: Splitattention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scaleadaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2050" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/2006.06882</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
