<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PC-RGNN: Point Cloud Completion and Graph Neural Network for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Zhang</surname></persName>
							<email>zhangyanan@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
							<email>dhuang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
							<email>yhwang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PC-RGNN: Point Cloud Completion and Graph Neural Network for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LiDAR-based 3D object detection is an important task for autonomous driving and current approaches suffer from sparse and partial point clouds of distant and occluded objects. In this paper, we propose a novel two-stage approach, namely PC-RGNN, dealing with such challenges by two specific solutions. On the one hand, we introduce a point cloud completion module to recover high-quality proposals of dense points and entire views with original structures preserved. On the other hand, a graph neural network module is designed, which comprehensively captures relations among points through a local-global attention mechanism as well as multi-scale graph based context aggregation, substantially strengthening encoded features. Extensive experiments on the KITTI benchmark show that the proposed approach outperforms the previous state-of-the-art baselines by remarkable margins, highlighting its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D object detection in point clouds is eagerly in demand in autonomous driving, and LiDAR laser scanners are the most common instruments to collect such data. Compared to 2D images, LiDAR point clouds convey real 3D geometric structures and spatial locations of objects and are less sensitive to illumination variations, which enables more reliable detection results.</p><p>In recent years, several approaches have been proposed for 3D object detection and they follow either the one-stage framework or the two-stage one as in the 2D domain, where how to learn effective shape features is an essential issue. For instance, MV3D <ref type="bibr" target="#b1">(Chen et al. 2017</ref>) and AVOD <ref type="bibr" target="#b7">(Ku et al. 2018</ref>) transform point clouds to bird's eye view or front view as initial representation and apply 2D convolutions for feature map computation; VoxelNet <ref type="bibr" target="#b27">(Zhou and Tuzel 2018)</ref> and SECOND <ref type="bibr" target="#b23">(Yan, Mao, and Li 2018)</ref> voxelize the 3D space into regular cells and employ 3D convolutions to extract features; F-Pointnet <ref type="bibr" target="#b14">(Qi et al. 2018)</ref> and PointRCNN <ref type="bibr" target="#b18">(Shi, Wang, and Li 2019)</ref> take raw point clouds as input and encode features by PointNets <ref type="bibr">(Qi et al. 2017a,b)</ref>.</p><p>Those approaches indeed show great potentials and have consistently improved the performance of major bench- <ref type="figure">Figure 1</ref>: Illustration of the two major challenges in LiDARbased 3D object detection (best viewed with zoom-in). The left case shows a sparse point cloud for a car far away, while the right case shows the extremely incomplete point clouds for occluded cars. The red and green boxes indicate the predicted results and ground-truths respectively. marks. Unfortunately, they tend to fail in the presence of low-quality input point clouds, i.e. sparse and partial data due to distant and occluded objects, which often occur in the real world. As illustrated in <ref type="figure">Fig. 1, PointRCNN</ref>, the state of the art representative, misses many objects marked with the red ellipse and arrows for long-distance and severe occlusions. Such a limitation derives from two main aspects:</p><p>(1) point cloud representation in current 3D object detectors does not work well on large variations in sampling density and sampling integrity and (2) the PointNet-like networks adopted as the backbones in the leading 3D object detectors are not so powerful which make insufficient use of given point clouds.</p><p>Motivated by the analysis above, this paper proposes a novel two-stage approach, named PC-RGNN, to 3D object detection from LiDAR based point clouds. Specifically, the 3D proposal generation (3D PG) module first suggests bounding box candidates in a bottom-up manner via segmenting the whole scene into foreground and background. Since objects are usually sparsely and partially sampled, a point cloud completion (PC) module is then introduced to recover high-quality proposals with dense points and entire views. Further, we model each refined proposal as a graph and design a graph neural network module, called AMS-GNN, to capture its shape characteristics. AMS-GNN ag- gregates contextual clues by combining multi-scale graphs and learns different weights of neighboring nodes through a local-global attention mechanism, and geometric relations among points can thus be comprehensively exploited, leading to enhanced features for decision making. Thanks to these modules, PC-RGNN reaches very competitive scores on the KITTI database, and in particular, it facilitates the detection of 3D objects in very difficult scenes, as depicted in <ref type="figure">Fig. 1</ref>. In summary, the main contributions of this paper are:</p><p>• We highlight the challenge of low-quality input point clouds in LiDAR-based 3D object detection and propose a novel two-stage detection approach (PC-RGNN), which significantly boosts the performance.</p><p>• We present a new point cloud completion (PC) module to improve proposals of sparse and partial points. To the best of our knowledge, this is the first study that considers point cloud refinement in 3D object detection.</p><p>• We design a new graph neural network (AMS-GNN) module, which strengths the structure features by encoding geometric relations among points through attention based multi-scale graph aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section briefly reviews the major approaches to 3D object detection as well as the ones of point cloud completion. Grid-based detectors. A number of methods initially convert point clouds to regular grids by projecting them to the planes of specific views <ref type="bibr" target="#b1">(Chen et al. 2017;</ref><ref type="bibr" target="#b3">Engelcke et al. 2017;</ref><ref type="bibr" target="#b7">Ku et al. 2018;</ref><ref type="bibr" target="#b10">Liang et al. 2018</ref><ref type="bibr" target="#b9">Liang et al. , 2019</ref> or subdividing them to equally distributed voxels <ref type="bibr" target="#b21">(Wang and Posner 2015;</ref><ref type="bibr" target="#b27">Zhou and Tuzel 2018;</ref><ref type="bibr" target="#b23">Yan, Mao, and Li 2018;</ref><ref type="bibr" target="#b8">Lang et al. 2019</ref>) so that they can be processed by 2D or 3D CNNs to compute detection features. Although grid-based methods are generally straightforward and efficient, they inevitably incur much information loss and thus limit the performance because of the coarse quantization process.</p><p>Point-based detectors. Many methods directly take the raw unordered and irregular points as input and apply point cloud deep learning networks, such as PointNet <ref type="bibr" target="#b15">(Qi et al. 2017a</ref>) and PonintNet++ <ref type="bibr" target="#b16">(Qi et al. 2017b)</ref>, to encode structure features for detection <ref type="bibr" target="#b14">(Qi et al. 2018;</ref><ref type="bibr" target="#b2">Chen et al. 2019;</ref><ref type="bibr" target="#b13">Qi et al. 2019;</ref><ref type="bibr" target="#b18">Shi, Wang, and Li 2019;</ref><ref type="bibr" target="#b25">Yang et al. 2019;</ref><ref type="bibr" target="#b24">Yang et al. 2020</ref>). These methods outperform grid-based ones. However, without explicit modeling of point relations, they are not so competent at discriminative geometry representation.</p><p>Graph-based detectors. Recently, inspired by the success of graph convolutions in point cloud segmentation and classification tasks, Point-GNN <ref type="bibr" target="#b20">(Shi and Rajkumar 2020)</ref> investigates the graph neural network to extract shape features for 3D object detection, which proves a promising way. Nevertheless, in their model, each node is regarded to equally contribute in local and global representation and the singlescale graph does not make full use of the contextual information. Both the facts leave space for improvement.</p><p>Point cloud completion methods. Point cloud completion aims to estimate entire 3D shapes from partial point cloud inputs. L-GAN <ref type="bibr" target="#b0">(Achlioptas et al. 2018)</ref> introduces the first deep learning model with an Encoder-Decoder architecture. PCN <ref type="bibr" target="#b26">(Yuan et al. 2018</ref>) presents a coarse-to-fine procedure to synthesize dense and complete data by a specially designed decoder. RL-GAN-Net <ref type="bibr" target="#b17">(Sarmad, Lee, and Kim 2019)</ref> proposes a reinforcement learning agent controlled GAN to speed up the inference phase. PF-Net <ref type="bibr" target="#b6">(Huang et al. 2020)</ref> hierarchically recovers the point cloud by a featurepoint based multi-scale generation network. Different from the research aforementioned, for the first time, point cloud completion is attempted to ameliorate LiDAR-based 3D object detection, as objects are often very far away or seriously occluded, leading to sparse and partial sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PC-RGNN</head><p>In this section, we describe the proposed PC-RGNN in detail, including the entire framework as well as the modules of 3D proposal generation, point cloud completion, and attention based multi-scale graph feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>The framework overview of our proposed PC-RGNN is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The whole network consists of three main modules: (A) 3D proposal generation, (B) point cloud completion, and (C) attention based multi-scale graph neural network representation. Given a raw point cloud, the proposal generation module segments the foreground from background and generates 3D bounding box candidates simultaneously. The point cloud completion module then recovers dense and entire 3D shapes from sparse and/or partial proposal point clouds. Finally, the GNN module comprehensively encodes the structure characteristics to predict detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3D Proposal Generation</head><p>As described in <ref type="bibr" target="#b18">(Shi, Wang, and Li 2019)</ref>, objects in 3D scenes are naturally separated, and the segmentation masks can be directly acquired from their 3D bounding box annotations. Therefore, we follow PointRCNN and build a subnetwork to learn point-wise features to simultaneously locate the foreground areas and generate 3D proposals. Based on this bottom-up mechanism, we avoid using a large number of predefined 3D anchors and thus dramatically reduce the searching space in this phase.</p><p>Concretely, PointNet++ <ref type="bibr" target="#b16">(Qi et al. 2017b</ref>) with multi-scale grouping is adopted as the backbone, and a segmentation head and a regression head are added to estimate the foreground mask and generate bounding box candidates respectively. For large outdoor scenes, the number of background points is usually much larger than that of foreground, and we therefore use the focal loss <ref type="bibr" target="#b12">(Lin et al. 2017b</ref>) in segmentation to deal with the class imbalance problem as:</p><formula xml:id="formula_0">L seg (p t ) = −α t (1 − p t ) γ log(p t ),</formula><p>where p t = p for foreground points, 1 − p otherwise.</p><p>(1)</p><p>During training, we set α t = 0.25 and γ = 2. For proposal generation, box locations are only regressed from foreground points. Here, a 3D bounding box is described as <ref type="bibr">(x, y, z, h, w, l, θ)</ref> in the LiDAR coordinate, where (x, y, z) is the object center, (h, w, l) is the object size, and θ is the object orientation from the bird's eye view. For (z, h, w, l), the smooth L1 loss is utilized and for (x, y, θ), the bin-based loss ) is exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Point Cloud Completion</head><p>To address the challenges of sparse and partial data caused by distant and occluded objects, we propose a point cloud completion (PC) module for 3D detection. Unlike the existing 3D shape generation or reconstruction methods that output totally new point clouds, the proposed module only renders additional points as supplement with input data unchanged, aiming to preserve original spatial arrangement. As shown in <ref type="figure">Fig. 3</ref>, the overall architecture of the PC module is composed of three fundamental building blocks, i.e. Multi-Resolution Graph Encoder (MRGE), Point Pyramid Decoder (PPD), and Discriminator Network.</p><p>The input to MRGE is an N × 3 unordered point cloud. It is first down sampled to obtain two more views of smaller resolutions by farthest point sampling (FPS). Three independent GCN layers ) are then used to map those resolutions into individual latent vectors F i . Compared with the PointNet-like models, GCN captures extra geometry clues from connection relations of points, which is expected to facilitate low-quality point cloud refinement. All the F i are further concatenated to form a stronger feature map M in the size of 1920×3 and the feature maps are integrated into a final vector V. Inspired by Feature Pyramid Networks <ref type="bibr" target="#b11">(Lin et al. 2017a)</ref>, PPD conducts in a coarse to fine fashion. Three feature layers FC 1 , FC 2 , FC 3 (size: 1024, 512, 256) are computed by passing V through fullyconnected layers. Each feature layer is responsible for recovering point clouds in a specific resolution. The coarse center points Y coarse are predicted from FC 3 , which are of the size of M 1 × 3. The relative coordinates of middle center points Y middle are predicted from FC 2 . Each point in Y coarse serves as the center to generate M 2 /M 1 points of Y coarse . Thus, the size of Y middle is M 2 × 3. Fine points Y f ine are finally predicted by PPD, with the size of M × 3.</p><p>The loss function is made up of two parts: the multi-level completion loss and the adversarial loss. The Chamfer Distance (CD) is chosen as the completion loss:</p><formula xml:id="formula_1">d CD (S 1 , S 2 ) = 1 S1 x∈S1 min y∈S2 x − y 2 2 + 1 S2 y∈S2 min x∈S1 y − x 2 2<label>(2)</label></formula><p>It measures the average nearest squared distance between the predicted point set S 1 and the ground truth S 2 . Since PPD estimates three point clouds at different levels, the multi-level completion loss is calculated in (3), where d CD1 , d CD2 , and d CD3 are weighted by a hyperparameter α: put X into the predicted additional point set Y . Discriminator D tries to distinguish Y from the real point set Y. We first obtain a latent vector F after two GCN layers and F is then passed through fully-connected layers <ref type="bibr">[256,</ref><ref type="bibr">128,</ref><ref type="bibr">16,</ref><ref type="bibr">1]</ref> followed by a sigmoid-classifier to calculate the predicted value. In this case, the adversarial loss is defined as:</p><formula xml:id="formula_2">L com = αd CD1 (Y coarse , Y gt ) + 2αd CD2 (Y middle , Y gt ) + d CD3 (Y f ine , Y gt )<label>(3)</label></formula><formula xml:id="formula_3">L adv = 1≤i≤S log(D(y i )) + 1≤i≤S log(1 − D(F (x i )))</formula><p>(4) where x i ∈ X, y i ∈ Y , and S is the dataset size. The total loss is thus formulated as:</p><formula xml:id="formula_4">L com = λ com L com + λ adv L adv<label>(5)</label></formula><p>where λ com and λ adv are the weights of the completion loss and the adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Attention based Multi-Scale GNN</head><p>To comprehensively encode shape characteristics of point clouds in proposals refined by the PC module, we design a novel graph neural network module. It strengths the features delivered by the previous GNN counterpart by multi-scale contextual clue extraction and attention based discriminative point highlighting, thus named AMS-GNN. As demonstrated in <ref type="figure">Fig. 4</ref>, it is composed of four attention based multiscale graph convolution (AMS-GCN) layers and a global attention (GA) layer. Each AMS-GCN layer contains a multiscale graph aggregation operation and a local attention (LA) operation. Specifically, we encode each proposal point cloud in a graph by regarding the points as vertices and the connection between points as edges, which makes features flow between neighbors. We define a point cloud as a set V = {v 1 , ..., v i , ..., v N }, where v i = (p i , s i ) is a point with both the 3D coordi-   <ref type="bibr" target="#b5">(Geiger, Lenz, and Urtasun 2012)</ref>.</p><p>nates p i ∈ R 3 and the state value s i ∈ R c , a c-length vector that represents the point property. Given V, we construct a graph G = (V, E) by taking V as the vertices and connecting each point to its k neighbors as the edges E. Compared to the point cloud classification and segmentation tasks, point cloud detection is more complex as it regresses object positions and the points in different locations non-equally contribute to the results. Therefore, unlike the typical graph model preliminarily attempted by , to dynamically adapt to the geometric structure of the object, we automatically learn the weight of each neighbor node when aggregating edge features. To this end, we design a GNN to extend the states of the vertices to include explicit position information and introduce an attention mechanism to assign individual weights to different nodes:</p><formula xml:id="formula_5">∆p i t = M LP t 1 (s t i ) e t ij = M LP t 2 (concat(p j − p i + ∆p i t , s t j − s t i , s t i )) α t ij = sof tmax(M LP t 3 (e t ij )) = exp(M LP t 3 (e t ij )) k∈N i exp(M LP t 3 (e t ik )) s t+1 i = sum(α t ij e t ij |(i, j) ∈ E ) (6) where s t i , s t j</formula><p>are the vertex features of the current node and its connecting node, respectively. MLP 1 , MLP 2 and MLP 3 are used to learn position offset ∆p i t , edge feature e t ij and edge weight α t ij respectively. In addition, for 3D object detection, multiple instances belonging to the same category often have different point cloud discretization distributions, which makes the features learned by graph nodes sensitive to graph resolution and connection relationship. To alleviate this interference, we aggregate multi-scale graph contextual features, and besides the local attention applied to the neighborhood, we also employ an attention to weight the global feature after four AMS-GCNs, which is called global attention. This localglobal attention mechanism substantially makes the feature more powerful in detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we subsequently present experimental evaluation, containing datasets and implementation details, results, and ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Implementation Details</head><p>We evaluate our PC-RGNN on the KITTI 3D object detection benchmark <ref type="bibr" target="#b4">(Geiger et al. 2013</ref>) which contains 7481 training point clouds and 7518 testing ones. For fair comparison, we follow the previous studies <ref type="bibr" target="#b1">(Chen et al. 2017;</ref><ref type="bibr" target="#b14">Qi et al. 2018)</ref> to subdivide the original training data into a training set and a validation set, resulting in 3712 samples for training and 3769 for validation. Since the point cloud in KITTI does not have a complete object shape, we first use ShapeNetCars with 1824 samples derived from <ref type="bibr" target="#b26">(Yuan et al. 2018)</ref> to train our point cloud completion module. In this way, we incorporate prior knowledge of the car class. Considering the variations in point cloud distribution, we extract 2000 object point clouds located in the ground-truth boxes with more than 2048 points from the KITTI training split to finetune the PC module. The ground-truth point cloud is created by uniformly sampling 2048 points on each shape. The low-quality point cloud is generated by randomly selecting a viewpoint and removing the points within a certain radius from original data. During training, each point cloud is transformed into the bounding box's coordinates and projected back to the world frame after completion.</p><p>Our PC-RGNN is trained on 4 GTX 1080Ti GPUs using PyTorch. The stage-1 sub-network is trained for 200 epochs with the batch size at 16 and the learning rate of 0.002. The PC module is first pretrained for 60 epochs by using the Adam optimizer with an initial learning rate of 0.0001 and a batch size of 32. Combining the PC and AMS-GNN modules, the stage-2 sub-network is trained for 80 epochs with  the batch size at 8 and the learning rate of 0.002 in an endto-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In the KITTI dataset, all the samples are divided into three sets with increasing difficulties, i.e. Easy, Moderate and Hard, according to different bounding box heights and occlusion/truncation levels. For example, the occlusion levels in the three difficulties are 'Fully visible', 'Partly occluded', and 'Difficult to see', respectively. Firstly, we evaluate PC-RGNN on the test set by submitting detection results to the official server. The results are summarized in <ref type="table" target="#tab_1">Table 1</ref>. PC-RGNN significantly outperforms the previously published state-of-the-art counterparts in all the tasks and difficulties. Point-GNN <ref type="bibr" target="#b20">(Shi and Rajkumar 2020)</ref> is the pioneer graph-based detector, and PointR-CNN <ref type="bibr" target="#b18">(Shi, Wang, and Li 2019)</ref> is a representative two-stage approach. They are related to our method and compared to them, PC-RGNN makes two improvements at the second stage, i.e. point cloud refinement by the PC module and feature enhancement by the AMS-GNN module. Therefore, we choose the two methods as the baselines. The performance gains delivered by PC-RGNN are emphasized in slanted bold font which indicate that our method is effective in LiDAR-based 3D object detection in the challenging scenes.</p><p>We then report the performance achieved on the KITTI validation set in <ref type="table" target="#tab_3">Table 2</ref>. We follow the official KITTI evaluation protocol, where the IoU threshold is 0.7 for the car class. The proposed PC-RGNN also outperforms all the other approaches by remarkable margins, especially in the Moderate and Hard difficulties. We present several challenging 3D detection examples in <ref type="figure" target="#fig_2">Fig. 5</ref>. We can observe that even in very difficult cases with distant and occluded objects, our network still reaches decent results, thanks to point cloud completion based data refining and attention multiscale GNN based feature strengthening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To better verify our contributions, we conduct ablation studies on the KITTI validation set. We primarily investigate the   impacts of our proposed PC module as well as the AMS-GNN module to the final results. We remove the PC module from PC-RGNN and replace our AMS-GNN with Point-Net++ as our baseline, which achieves a 3D mAP of 81.63%. We then add the PC and AMS-GNN modules separately on the baseline for comparison. Finally, we combine both the two modules to update the scores.</p><p>The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. Only with either the PC module or the AMS-GNN module, the performance is boosted to 82.54% and 83.18%, respectively. Further, when combining both of them, it yields an mAP of 84.27%, largely superior to that of the baseline model by 2.64%. It clearly demonstrates the credits of the PC and AMS-GNN modules in PC-RGNN. Additionally, we visualize the object proposals before and after point cloud completion. As shown in <ref type="figure" target="#fig_3">Fig.  6</ref>, the original cars are barely recognizable due to the lowquality of the input data (i.e. long-distance and heavy occlusion). In contrast, the refined point clouds display much more reasonable geometric shapes. It confirms the fact that the point cloud completion operation significantly increases the accuracy of 3D object detection in difficult scenes.</p><p>To further analyze the effectiveness of AMS-GNN, we take a graph neural network with four typical graph convolution layers derived from ) as our baseline. We successively add the multi-scale (MS) graph aggregation, the local attention (LA), and the global attention (GA) to the baseline. All the results are listed in <ref type="table" target="#tab_6">Table 4</ref>. The improvements of the three parts are 0.21%, 0.62%, 0.16%, respectively. This shows that both the local-global attention mechanism and the multi-scale graph feature aggregation improve the discriminative power of GNN in capturing geometric characteristics of point clouds.</p><p>Besides, we validate the PC module. We take the complete PC-RGNN as our baseline, and its point cloud completion module (PC-M) only predicts additional points for refinement according to low-quality input. We then replace the graph encoder in PC-M with PointNet and remove the multi-resolution branch, generating two new models named PC-M without GE and PC-M without MR, respectively. Meanwhile, we change PC-M to the module which generates totally new shapes from low-quality input as existing   <ref type="table" target="#tab_8">Table 5</ref>, compared with our baseline, PC-M without GE and PC-M without MR decrease by 0.60% and 0.67%, respectively. This proves that the multi-resolution graph encoding strategy indeed captures additional geometric features. Compared with the baseline, PC-O encounters a drop of 0.86%. It suggests that despite the low-quality, the original points are critical to regress the object locations, and the way proposed in this study, i.e. PC-M, well handles this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a novel two-stage approach, namely PC-RGNN, to LiDAR-based 3D object detection. It aims to  address low-quality inputs, i.e. sparsely and partially sampled point clouds, caused by distant and/or occluded objects in challenging scenarios. To this end, we introduce a point cloud completion module for data refinement, and to the best of our knowledge, this is the first attempt to integrate this technique into 3D object detection framework. Furthermore, we design a new graph neural network for feature enhancement, which comprehensively captures the geometric relations among points by a local-global attention mechanism and multi-scale graph based contextual information aggregation. Extensive experiments are carried out on the KITTI dataset and state of the art results are reached, which demonstrate the effectiveness of the proposed PC-RGNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Framework overview. The whole PC-RGNN network consists of three main modules: (A) 3D proposal generation, (B) point cloud completion, and (C) attention based multi-scale GNN representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>We define F : X → Y , which maps the low-quality in-Architecture of the PC module. With input point clouds, it predicts additional parts of sparse and partial data by a Multi-Resolution Graph Encoder (MRGE) and a Point Pyramid Decoder (PPD). The Discriminator tries to distinguish the predicted regions from the real ones. Architecture of the Attention based Multi-Scale Graph Neural Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of PC-RGNN on the KITTI val split (best viewed with zoom-in). For each sample, the upper part is the image and the lower part is a representative view of the corresponding point cloud. The red boxes indicate the predicted results while the green ones denote the ground-truths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Point cloud completion results delivered by the PC module. Blue and red point clouds represent the data before and after this phase, respectively. point cloud completion networks do and name it as PC-O. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison in 3D object detection with previous state-of-the-art methods in terms of the car class on the KITTI test split (the results are computed by the official test server). 3D object detection and bird's eye view detection are evaluated by Average Precision (AP) with IoU threshold 0.7, while orientation estimation is validated by Average Orientation Similarity (AOS) as mentioned in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with previous state-ofthe-art methods in terms of the car class on the KITTI val split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the proposed PC and AMS-GNN modules.</figDesc><table><row><cell>Module</cell><cell cols="3">AP 3D (IoU=0.7) (%)</cell><cell></cell></row><row><cell cols="5">MS LA GA Easy Moderate Hard 3D mAP Gain</cell></row><row><cell>90.02</cell><cell>80.39</cell><cell>79.44</cell><cell>83.28</cell><cell>-</cell></row><row><cell>90.16</cell><cell>80.57</cell><cell>79.73</cell><cell>83.49</cell><cell>↑0.21</cell></row><row><cell>90.78</cell><cell>81.25</cell><cell>80.29</cell><cell>84.11</cell><cell>↑0.83</cell></row><row><cell>90.94</cell><cell>81.43</cell><cell>80.45</cell><cell>84.27</cell><cell>↑0.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiments on the AMS-GNN module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation experiments on the PC module.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiview 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast point rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PF-Net: Point fractal network for 3D point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7662" to="7670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rl-gan-net: A reinforcement learning agent controlled gan network for real-time point cloud shape completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5898" to="5907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="15607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointbased 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
