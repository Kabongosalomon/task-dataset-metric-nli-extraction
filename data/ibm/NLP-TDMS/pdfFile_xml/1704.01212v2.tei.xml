<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
						</author>
						<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The past decade has seen remarkable success in the use of deep neural networks to understand and translate natural language <ref type="bibr" target="#b37">(Wu et al., 2016)</ref>, generate and decode complex audio signals , and infer features from real-world images and videos <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref>. Although chemists have applied machine learning to many problems over the years, predicting the properties of molecules and materials using machine learning (and especially deep learning) is still in its infancy. To date, most research applying machine learning to chemistry tasks <ref type="bibr" target="#b11">(Hansen et al., 2015;</ref><ref type="bibr" target="#b16">Huang &amp; von Lilienfeld, 2016;</ref><ref type="bibr"></ref> Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DFT</head><p>⇠ 10 3 seconds Message Passing Neural Net ⇠ 10 2 seconds E,! 0 , ... <ref type="figure">Figure 1</ref>. A Message Passing Neural Network predicts quantum properties of an organic molecule by modeling a computationally expensive DFT calculation. <ref type="bibr" target="#b31">Rupp et al., 2012;</ref><ref type="bibr" target="#b29">Rogers &amp; Hahn, 2010;</ref><ref type="bibr" target="#b26">Montavon et al., 2012;</ref><ref type="bibr" target="#b2">Behler &amp; Parrinello, 2007;</ref><ref type="bibr" target="#b33">Schoenholz et al., 2016)</ref> has revolved around feature engineering. While neural networks have been applied in a variety of situations <ref type="bibr" target="#b24">(Merkwirth &amp; Lengauer, 2005;</ref><ref type="bibr" target="#b25">Micheli, 2009;</ref><ref type="bibr" target="#b22">Lusci et al., 2013;</ref><ref type="bibr" target="#b9">Duvenaud et al., 2015)</ref>, they have yet to become widely adopted. This situation is reminiscent of the state of image models before the broad adoption of convolutional neural networks and is due, in part, to a dearth of empirical evidence that neural architectures with the appropriate inductive bias can be successful in this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targets</head><p>Recently, large scale quantum chemistry calculation and molecular dynamics simulations coupled with advances in high throughput experiments have begun to generate data at an unprecedented rate. Most classical techniques do not make effective use of the larger amounts of data that are now available. The time is ripe to apply more powerful and flexible machine learning methods to these problems, assuming we can find models with suitable inductive biases. The symmetries of atomic systems suggest neural networks that operate on graph structured data and are invariant to graph isomorphism might also be appropriate for molecules. Sufficiently successful models could someday help automate challenging chemical search problems in drug discovery or materials science.</p><p>In this paper, our goal is to demonstrate effective machine learning models for chemical prediction problems arXiv:1704.01212v2 <ref type="bibr">[cs.</ref>LG] 12 Jun 2017 that are capable of learning their own features from molecular graphs directly and are invariant to graph isomorphism. To that end, we describe a general framework for supervised learning on graphs called Message Passing Neural Networks (MPNNs) that simply abstracts the commonalities between several of the most promising existing neural models for graph structured data, in order to make it easier to understand the relationships between them and come up with novel variations. Given how many researchers have published models that fit into the MPNN framework, we believe that the community should push this general approach as far as possible on practically important graph problems and only suggest new variations that are well motivated by applications, such as the application we consider here: predicting the quantum mechanical properties of small organic molecules (see task schematic in figure 1).</p><p>In general, the search for practically effective machine learning (ML) models in a given domain proceeds through a sequence of increasingly realistic and interesting benchmarks. Here we focus on the QM9 dataset as such a benchmark <ref type="bibr" target="#b28">(Ramakrishnan et al., 2014)</ref>. QM9 consists of 130k molecules with 13 properties for each molecule which are approximated by an expensive 1 quantum mechanical simulation method (DFT), to yield 13 corresponding regression tasks. These tasks are plausibly representative of many important chemical prediction problems and are (currently) difficult for many existing methods. Additionally, QM9 also includes complete spatial information for the single low energy conformation of the atoms in the molecule that was used in calculating the chemical properties. QM9 therefore lets us consider both the setting where the complete molecular geometry is known (atomic distances, bond angles, etc.) and the setting where we need to compute properties that might still be defined in terms of the spatial positions of atoms, but where only the atom and bond information (i.e. graph) is available as input. In the latter case, the model must implicitly fit something about the computation used to determine a low energy 3D conformation and hopefully would still work on problems where it is not clear how to compute a reasonable 3D conformation.</p><p>When measuring the performance of our models on QM9, there are two important benchmark error levels. The first is the estimated average error of the DFT approximation to nature, which we refer to as "DFT error." The second, known as "chemical accuracy," is a target error that has been established by the chemistry community. Estimates of DFT error and chemical accuracy are provided for each of the 13 targets in <ref type="bibr" target="#b10">Faber et al. (2017)</ref>. One important goal of this line of research is to produce a model which can achieve chemical accuracy with respect to the true targets as measured by an extremely precise experiment. The dataset containing the true targets on all 134k molecules does not currently exist. However, the ability to fit the DFT approximation to within chemical accuracy would be an encouraging step in this direction. For all 13 targets, achieving chemical accuracy is at least as hard as achieving DFT error. In the rest of this paper when we talk about chemical accuracy we generally mean with respect to our available ground truth labels.</p><p>In this paper, by exploring novel variations of models in the MPNN family, we are able to both achieve a new state of the art on the QM9 dataset and to predict the DFT calculation to within chemical accuracy on all but two targets. In particular, we provide the following key contributions:</p><p>• We develop an MPNN which achieves state of the art results on all 13 targets and predicts DFT to within chemical accuracy on 11 out of 13 targets.</p><p>• We develop several different MPNNs which predict DFT to within chemical accuracy on 5 out of 13 targets while operating on the topology of the molecule alone (with no spatial information as input).</p><p>• We develop a general method to train MPNNs with larger node representations without a corresponding increase in computation time or memory, yielding a substantial savings over previous MPNNs for high dimensional node representations.</p><p>We believe our work is an important step towards making well-designed MPNNs the default for supervised learning on modestly sized molecules. In order for this to happen, researchers need to perform careful empirical studies to find the proper way to use these types of models and to make any necessary improvements to them, it is not sufficient for these models to have been described in the literature if there is only limited accompanying empirical work in the chemical domain. Indeed convolutional neural networks existed for decades before careful empirical work applying them to image classification <ref type="bibr" target="#b20">(Krizhevsky et al., 2012)</ref> helped them displace SVMs on top of handengineered features for a host of computer vision problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Message Passing Neural Networks</head><formula xml:id="formula_0">m t+1 v = w∈N (v) M t (h t v , h t w , e vw ) (1) h t+1 v = U t (h t v , m t+1 v )<label>(2)</label></formula><p>where in the sum, N (v) denotes the neighbors of v in graph G. The readout phase computes a feature vector for the whole graph using some readout function R according tô</p><formula xml:id="formula_1">y = R({h T v | v ∈ G}).<label>(3)</label></formula><p>The message functions M t , vertex update functions U t , and readout function R are all learned differentiable functions. R operates on the set of node states and must be invariant to permutations of the node states in order for the MPNN to be invariant to graph isomorphism. In what follows, we define previous models in the literature by specifying the message function M t , vertex update function U t , and readout function R used. Note one could also learn edge features in an MPNN by introducing hidden states for all edges in the graph h t evw and updating them analogously to equations 1 and 2. Of the existing MPNNs, only <ref type="bibr" target="#b17">Kearnes et al. (2016)</ref> has used this idea.</p><p>Convolutional Networks for Learning Molecular Fingerprints, <ref type="bibr" target="#b9">Duvenaud et al. (2015)</ref> The message function used is M (h v , h w , e vw ) = (h w , e vw ) where (., .) denotes concatenation. The vertex update function used is</p><formula xml:id="formula_2">U t (h t v , m t+1 v ) = σ(H deg(v) t m t+1 v )</formula><p>, where σ is the sigmoid function, deg(v) is the degree of vertex v and H N t is a learned matrix for each time step t and vertex degree N . R has skip connections to all previous hidden states h t v and is equal to f</p><formula xml:id="formula_3">v,t softmax(W t h t v ) ,</formula><p>where f is a neural network and W t are learned readout matrices, one for each time step t. This message passing scheme may be problematic since the resulting message vector is m t+1 v = ( h t w , e vw ) , which separately sums over connected nodes and connected edges. It follows that the message passing implemented in <ref type="bibr" target="#b9">Duvenaud et al. (2015)</ref> is unable to identify correlations between edge states and node states.</p><p>Gated Graph Neural Networks (GG-NN), <ref type="bibr" target="#b21">Li et al. (2016)</ref> The message function used is M t (h t v , h t w , e vw ) = A evw h t w , where A evw is a learned matrix, one for each edge label e (the model assumes discrete edge types). The update func-</p><formula xml:id="formula_4">tion is U t = GRU(h t v , m t+1 v ),</formula><p>where GRU is the Gated Recurrent Unit introduced in <ref type="bibr" target="#b7">Cho et al. (2014)</ref>. This work used weight tying, so the same update function is used at each time step t. Finally,</p><formula xml:id="formula_5">R = v∈V σ i(h (T ) v , h 0 v ) j(h (T ) v )<label>(4)</label></formula><p>where i and j are neural networks, and denotes elementwise multiplication.</p><p>Interaction Networks, <ref type="bibr" target="#b0">Battaglia et al. (2016)</ref> This work considered both the case where there is a target at each node in the graph, and where there is a graph level target. It also considered the case where there are node level effects applied at each time step, in such a case the update function takes as input the concatenation</p><formula xml:id="formula_6">(h v , x v , m v ) where x v is an external vector representing some outside influence on the vertex v. The message func- tion M (h v , h w , e vw ) is a neural network which takes the concatenation (h v , h w , e vw ). The vertex update function U (h v , x v , m v ) is a neural network which takes as input the concatenation (h v , x v , m v ). Finally, in the case where there is a graph level output, R = f ( v∈G h T v )</formula><p>where f is a neural network which takes the sum of the final hidden states h T v . Note the original work only defined the model for T = 1.</p><p>Molecular Graph Convolutions, <ref type="bibr" target="#b17">Kearnes et al. (2016)</ref> This work deviates slightly from other MPNNs in that it introduces edge representations e t vw which are updated during the message passing phase. The message function used for node messages is</p><formula xml:id="formula_7">M (h t v , h t w , e t vw ) = e t vw . The vertex update function is U t (h t v , m t+1 v ) = α(W 1 (α(W 0 h t v ), m t+1 v ))</formula><p>where (., .) denotes concatenation, α is the ReLU activation and W 1 , W 0 are learned weight matrices. The edge state update is defined by e t+1 vw = U t (e t vw , h t v , h t w ) = α(W 4 (α(W 2 , e t vw ), α(W 3 (h t v , h t w )))) where the W i are also learned weight matrices.</p><p>Deep Tensor Neural Networks, <ref type="bibr" target="#b34">Schütt et al. (2017)</ref> The message from w to v is computed by</p><formula xml:id="formula_8">M t = tanh W f c ((W cf h t w + b 1 ) (W df e vw + b 2 )) where W f c , W cf , W df are matrices and b 1 , b 2 are bias vectors. The update function used is U t (h t v , m t+1 v ) = h t v + m t+1 v .</formula><p>The readout function passes each node independently through a single hidden layer neural network and sums the outputs, in particular</p><formula xml:id="formula_9">R = v NN(h T v ).</formula><p>Laplacian Based Methods, <ref type="bibr" target="#b5">Bruna et al. (2013)</ref>; Defferrard et al. <ref type="formula" target="#formula_0">(2016)</ref>; <ref type="bibr" target="#b19">Kipf &amp; Welling (2016)</ref> These methods generalize the notion of the convolution operation typically applied to image datasets to an operation that operates on an arbitrary graph G with a real valued adjacency matrix A. The operations defined in <ref type="bibr" target="#b5">Bruna et al. (2013)</ref>; <ref type="bibr" target="#b8">Defferrard et al. (2016)</ref> </p><formula xml:id="formula_10">result in message functions of the form M t (h t v , h t w ) = C t vw h t w ,</formula><p>where the matrices C t vw are parameterized by the eigenvectors of the graph laplacian L, and the learned parameters of the model.</p><formula xml:id="formula_11">The vertex update function used is U t (h t v , m t+1 v ) = σ(m t+1 v ) where σ is some pointwise non-linearity (such as ReLU).</formula><p>The <ref type="bibr" target="#b19">Kipf &amp; Welling (2016)</ref> </p><formula xml:id="formula_12">model results in a mes- sage function M t (h t v , h t w ) = c vw h t w where c vw = (deg(v)deg(w)) −1/2 A vw . The vertex update function is U t v (h t v , m t+1 v ) = ReLU(W t m t+1 v ).</formula><p>For the exact expressions for the C t vw and the derivation of the reformulation of these models as MPNNs, see the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Moving Forward</head><p>Given how many instances of MPNNs have appeared in the literature, we should focus on pushing this general family as far as possible in a specific application of substantial practical importance. This way we can determine the most crucial implementation details and potentially reach the limits of these models to guide us towards future modeling improvements.</p><p>One downside of all of these approaches is computation time. Recent work has adapted the GG-NN architecture to larger graphs by passing messages on only subsets of the graph at each time step <ref type="bibr" target="#b23">(Marino et al., 2016)</ref>. In this work we also present a MPNN modification that can improve the computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Although in principle quantum mechanics lets us compute the properties of molecules, the laws of physics lead to equations that are far too difficult to solve exactly. Therefore scientists have developed a hierarchy of approximations to quantum mechanics with varying tradeoffs of speed and accuracy, such as Density Functional Theory (DFT) with a variety of functionals <ref type="bibr" target="#b1">(Becke, 1993;</ref><ref type="bibr" target="#b14">Hohenberg &amp; Kohn, 1964)</ref>, the GW approximation <ref type="bibr" target="#b12">(Hedin, 1965)</ref>, and Quantum Monte-Carlo <ref type="bibr" target="#b6">(Ceperley &amp; Alder, 1986)</ref>. Despite being widely used, DFT is simultaneously still too slow to be applied to large systems (scaling as O(N 3 e ) where N e is the number of electrons) and exhibits systematic as well as random errors relative to exact solutions to Schrödinger's equation. For example, to run the DFT calculation on a single 9 heavy atom molecule in QM9 takes around an hour on a single core of a Xeon E5-2660 (2.2 GHz) using a version of Gaussian G09 (ES64L-G09RevD.01) . For a 17 heavy atom molecule, computation time is up to 8 hours. Empirical potentials have been developed, such as the Stillinger-Weber potential <ref type="bibr" target="#b35">(Stillinger &amp; Weber, 1985)</ref>, that are fast and accurate but must be created from scratch, from first principles, for every new composition of atoms. <ref type="bibr" target="#b15">Hu et al. (2003)</ref> used neural networks to approximate a particularly troublesome term in DFT called the exchange correlation potential to improve the accuracy of DFT. However, their method fails to improve upon the efficiency of DFT and relies on a large set of ad hoc atomic descriptors. Two more recent approaches by <ref type="bibr" target="#b2">Behler &amp; Parrinello (2007)</ref> and <ref type="bibr" target="#b31">Rupp et al. (2012)</ref> attempt to approximate solutions to quantum mechanics directly without appealing to DFT. In the first case single-hidden-layer neural networks were used to approximate the energy and forces for configurations of a Silicon melt with the goal of speeding up molecular dynamics simulations. The second paper used Kernel Ridge Regression (KRR) to infer atomization energies over a wide range of molecules. In both cases hand engineered features were used (symmetry functions and the Coulomb matrix, respectively) that built physical symmetries into the input representation. Subsequent papers have replaced KRR by a neural network.</p><p>Both of these lines of research used hand engineered features that have intrinsic limitations. The work of <ref type="bibr" target="#b2">Behler &amp; Parrinello (2007)</ref> used a representation that was manifestly invariant to graph isomorphism, but has difficulty when applied to systems with more than three species of atoms and fails to generalize to novel compositions. The representation used in <ref type="bibr" target="#b31">Rupp et al. (2012)</ref> is not invariant to graph isomorphism. Instead, this invariance must be learned by the downstream model through dataset augmentation.</p><p>In addition to the eight MPNNs discussed in Section 2 there have been a number of other approaches to machine learning on graphical data which take advantage of the symmetries in a number of ways. One such family of approaches define a preprocessing step which constructs a canonical graph representation which can then be fed into into a standard classifier. Examples in this family include <ref type="bibr" target="#b27">Niepert et al. (2016)</ref> and <ref type="bibr" target="#b31">Rupp et al. (2012)</ref>. Finally <ref type="bibr" target="#b32">Scarselli et al. (2009)</ref> define a message passing process on graphs which is run until convergence, instead of for a finite number of time steps as in MPNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">QM9 Dataset</head><p>To investigate the success of MPNNs on predicting chemical properties, we use the publicly available QM9 dataset <ref type="bibr" target="#b28">(Ramakrishnan et al., 2014)</ref>. Molecules in the dataset consist of Hydrogen (H), Carbon (C), Oxygen (O), Nitrogen (N), and Flourine (F) atoms and contain up to 9 heavy (non Hydrogen) atoms. In all, this results in about 134k drug-like organic molecules that span a wide range of chemistry. For each molecule DFT is used to find a reasonable low energy structure and hence atom "positions" are available. Additionally a wide range of interesting and fundamental chemical properties are computed. Given how fundamental some of the QM9 properties are, it is hard to believe success on more challenging chemical tasks will be possible if we can't make accurate statistical predictions for the properties computed in QM9.</p><p>We can group the different properties we try to predict into four broad categories. First, we have four properties related to how tightly bound together the atoms in a molecule are. These measure the energy required to break up the molecule at different temperatures and pressures. These include the atomization energy at 0K, U 0 (eV), atomization energy at room temperature, U (eV), enthalpy of atomization at room temperature, H (eV), and free energy of atomization, G (eV).</p><p>Next there are properties related to fundamental vibrations of the molecule, including the highest fundamental vibrational frequency ω 1 (cm −1 ) and the zero point vibrational energy (ZPVE) (eV).</p><p>Additionally, there are a number of properties that concern the states of the electrons in the molecule. They include the energy of the electron in the highest occupied molecular orbital (HOMO) ε HOMO (eV), the energy of the lowest unoccupied molecular orbital (LUMO) ε LUMO (eV), and the electron energy gap (∆ε (eV)). The electron energy gap is simply the difference ε HOMO − ε LUMO .</p><p>Finally, there are several measures of the spatial distribution of electrons in the molecule. These include the electronic spatial extent R 2 (Bohr 2 ), the norm of the dipole moment µ (Debye), and the norm of static polarizability α (Bohr 3 ). For a more detailed description of these properties, see the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MPNN Variants</head><p>We began our exploration of MPNNs around the GG-NN model which we believe to be a strong baseline. We focused on trying different message functions, output functions, finding the appropriate input representation, and properly tuning hyperparameters.</p><p>For the rest of the paper we use d to denote the dimension of the internal hidden representation of each node in the graph, and n to denote the number of nodes in the graph. Our implementation of MPNNs in general operates on directed graphs with a separate message channel for incoming and outgoing edges, in which case the incoming message m v is the concatenation of m in v and m out v , this was also used in <ref type="bibr" target="#b21">Li et al. (2016)</ref>. When we apply this to undirected chemical graphs we treat the graph as directed, where each original edge becomes both an incoming and outgoing edge with the same label. Note there is nothing special about the direction of the edge, it is only relevant for parameter tying. Treating undirected graphs as directed means that the size of the message channel is 2d instead of d.</p><p>The input to our MPNN model is a set of feature vectors for the nodes of the graph, x v , and an adjacency matrix A with vector valued entries to indicate different bonds in the molecule as well as pairwise spatial distance between two atoms. We experimented as well with the message function used in the GG-NN family, which assumes discrete edge labels, in which case the matrix A has entries in a discrete alphabet of size k. The initial hidden states h 0 v are set to be the atom input feature vectors x v and are padded up to some larger dimension d. All of our experiments used weight tying at each time step t, and a GRU <ref type="bibr" target="#b7">(Cho et al., 2014)</ref> for the update function as in the GG-NN family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Message Functions</head><p>Matrix Multiplication: We started with the message function used in GG-NN which is defined by the equation</p><formula xml:id="formula_13">M (h v , h w , e vw ) = A evw h w .</formula><p>Edge Network: To allow vector valued edge features we propose the message function M (h v , h w , e vw ) = A(e vw )h w where A(e vw ) is a neural network which maps the edge vector e vw to a d × d matrix.</p><p>Pair Message: One property that the matrix multiplication rule has is that the message from node w to node v is a function only of the hidden state h w and the edge e vw . In particular, it does not depend on the hidden state h t v . In theory, a network may be able to use the message channel more efficiently if the node messages are allowed to depend on both the source and destination node. Thus we also tried using a variant on the message function as described in <ref type="bibr" target="#b0">(Battaglia et al., 2016)</ref>. Here the message from w to v along edge e is m wv = f (h t w , h t v , e vw ) where f is a neural network.</p><p>When we apply the above message functions to directed graphs, there are two separate functions used, M in and an M out . Which function is applied to a particular edge e vw depends on the direction of that edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Virtual Graph Elements</head><p>We explored two different ways to change how the messages are passed throughout the model. The simplest modification involves adding a separate "virtual" edge type for pairs of nodes that are not connected. This can be implemented as a data preprocessing step and allows information to travel long distances during the propagation phase.</p><p>We also experimented with using a latent "master" node, which is connected to every input node in the graph with a special edge type. The master node serves as a global scratch space that each node both reads from and writes to in every step of message passing. We allow the master node to have a separate node dimension d master , as well as separate weights for the internal update function (in our case a GRU). This allows information to travel long distances during the propagation phase. It also, in theory, allows additional model capacity (e.g. large values of d master ) without a substantial hit in performance, as the complexity of the master node model is O(|E|d 2 + nd 2 master ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Readout Functions</head><p>We experimented with two readout functions. First is the readout function used in GG-NN, which is defined by equation 4. Second is a set2set model from <ref type="bibr" target="#b36">Vinyals et al. (2015)</ref>. The set2set model is specifically designed to operate on sets and should have more expressive power than simply summing the final node states. This model first applies a linear projection to each tuple (h T v , x v ) and then takes as input the set of projected tuples T = {(h T v , x v )}. Then, after M steps of computation, the set2set model produces a graph level embedding q * t which is invariant to the order of the of the tuples T . We feed this embedding q * t through a neural network to produce the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Multiple Towers</head><p>One issue with MPNNs is scalability. In particular, a single step of the message passing phase for a dense graph requires O(n 2 d 2 ) floating point multiplications. As n or d get large this can be computationally expensive. To address this issue we break the d dimensional node embeddings h t v into k different d/k dimensional embeddings h t,k v and run a propagation step on each of the k copies separately to get temporary embeddings {h t+1,k v , v ∈ G}, using separate message and update functions for each copy. The k temporary embeddings of each node are then mixed together according to the equation</p><formula xml:id="formula_14">h t,1 v , h t,2 v , . . . , h t,k v = g h t,1 v ,h t,2 v , . . . ,h t,k v<label>(5)</label></formula><p>where g denotes a neural network and (x, y, . . .) denotes concatenation, with g shared across all nodes in the graph. This mixing preserves the invariance to permutations of the nodes, while allowing the different copies of the graph to communicate with each other during the propagation phase. This can be advantageous in that it allows larger hidden states for the same number of parameters, which yields a computational speedup in practice. For example, when the message function is matrix multiplication (as in GG-NN) a propagation step of a single copy takes O n 2 (d/k) 2 time, and there are k copies, therefore the overall time complexity is O n 2 d 2 /k , with some additional overhead due to the mixing network. For k = 8, n = 9 and d = 200 we see a factor of 2 speedup in inference time over a k = 1, n = 9, and d = 200 architecture. This variation would be most useful for larger molecules, for instance molecules from GDB-17 <ref type="bibr" target="#b30">(Ruddigkeit et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Input Representation</head><p>There are a number of features available for each atom in a molecule which capture both properties of the electrons in the atom as well as the bonds that the atom participates in. For a list of all of the features see table 1. We experimented with making the hydrogen atoms explicit nodes in the graph (as opposed to simply including the count as a node feature), in which case graphs have up to 29 nodes. Note that having larger graphs significantly slows training time, in this case by a factor of roughly 10. For the adjacency matrix there are three edge representations used depending on the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chemical Graph:</head><p>In the abscence of distance information, adjacency matrix entries are discrete bond types: single, double, triple, or aromatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distance bins:</head><p>The matrix multiply message function assumes discrete edge types, so to include distance information we bin bond distances into 10 bins, the bins are obtained by uniformly partitioning the interval [2, 6] into 8 bins, followed by adding a bin [0, 2] and [6, ∞]. These bins were hand chosen by looking at a histogram of all distances. The adjacency matrix then has entries in an alphabet of size 14, indicating bond type for bonded atoms and distance bin for atoms that are not bonded. We found the distance for bonded atoms to be almost completely determined by bond type.</p><p>Raw distance feature: When using a message function which operates on vector valued edges, the entries of the adjacency matrix are then 5 dimensional, where the first dimension indicates the euclidean distance between the pair of atoms, and the remaining four are a one-hot encoding of the bond type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Training</head><p>Each model and target combination was trained using a uniform random hyper parameter search with 50 trials. T was constrained to be in the range 3 ≤ T ≤ 8 (in practice, any T ≥ 3 works). The number of set2set computations M was chosen from the range 1 ≤ M ≤ 12. All models were trained using SGD with the ADAM optimizer <ref type="bibr" target="#b18">(Kingma &amp; Ba (2014)</ref>), with batch size 20 for 3 million steps ( 540 epochs). The initial learning rate was chosen uniformly between 1e −5 and 5e −4 . We used a linear learning rate decay that began between 10% and 90% of the way through training and the initial learning rate l decayed to a final learning rate l * F , using a decay factor F in the range [.01, 1].</p><p>The QM-9 dataset has 130462 molecules in it. We randomly chose 10000 samples for validation, 10000 samples for testing, and used the rest for training. We use the validation set to do early stopping and model selection and we report scores on the test set. All targets were normalized to have mean 0 and variance 1. We minimize the mean squared error between the model output and the target, although we evaluate mean absolute error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Results</head><p>In all of our tables we report the ratio of the mean absolute error (MAE) of our models with the provided estimate of chemical accuracy for that target. Thus any model with error ratio less than 1 has achieved chemical accuracy for that target. In the supplementary material we list the chemical accuracy estimates for each target, these are the same estimates that were given in <ref type="bibr" target="#b10">Faber et al. (2017)</ref>. In this way, the MAE of our models can be calculated as (Error Ratio) × (Chemical Accuracy). Note, unless otherwise indicated, all tables display result of models trained individually on each target (as opposed to training one model to predict all 13).</p><p>We performed numerous experiments in order to find the best possible MPNN on this dataset as well as the proper input representation. In our experiments, we found that including the complete edge feature vector (bond type, spatial distance) and treating hydrogen atoms as explicit nodes in the graph to be very important for a number of targets. We also found that training one model per target consistently outperformed jointly training on all 13 targets. In some cases the improvement was up to 40%. Our best MPNN variant used the edge network message function, set2set output, and operated on graphs with explicit hydrogens. We were able to further improve performance on the test set by ensembling the predictions of the five models with lowest validation error.</p><p>In table 2 we compare the performance of our best MPNN variant (denoted with enn-s2s) and the corresponding ensemble (denoted with enn-s2s-ens5) with the previous state of the art on this dataset as reported in <ref type="bibr" target="#b10">Faber et al. (2017)</ref>. For clarity the error ratios of the best non-ensemble models are shown in bold. This previous work performed a comparison study of several existing ML models for QM9 and we have taken care to use the same train, validation, and test split. These baselines include 5 different hand engineered molecular representations, which then get fed through a standard, off-the-shelf classifier. These input representations include the Coulomb Matrix (CM, <ref type="bibr" target="#b31">Rupp et al. (2012)</ref>), Bag of Bonds (BoB, <ref type="bibr" target="#b11">Hansen et al. (2015)</ref>), Bonds Angles, Machine Learning (BAML, <ref type="bibr" target="#b16">Huang &amp; von Lilienfeld (2016)</ref>), Extended Connectivity Fingerprints (ECPF4, <ref type="bibr" target="#b29">Rogers &amp; Hahn (2010)</ref>), and "Projected Histograms" <ref type="bibr">(HDAD, Faber et al. (2017)</ref>) representations. In addition to these hand engineered features we include two existing baseline MPNNs, the Molecular Graph Convolutions model (GC) from <ref type="bibr" target="#b17">Kearnes et al. (2016)</ref>, and the original GG-NN model <ref type="bibr" target="#b21">Li et al. (2016)</ref> trained with distance bins. Overall, our new MPNN achieves chemical accuracy on 11 out of 13 targets and state of the art on all 13 targets.</p><p>Training Without Spatial Information: We also experimented in the setting where spatial information is not included in the input. In general, we find that augmenting the MPNN with some means of capturing long range interactions between nodes in the graph greatly improves performance in this setting. To demonstrate this we performed 4 experiments, one where we train the GG-NN model on the sparse graph, one where we add virtual edges, one where we add a master node, and one where we change the graph level output to a set2set output. The error ratios averaged across the 13 targets are shown in table 3. Overall, these three modifications help on all 13 targets, and the Set2Set output achieves chemical accuracy on 5 out of 13 targets. For more details, consult the supplementary material. The experiments shown tables 3 and 4 were run with a partial charge feature as a node input. This feature is an output of the DFT calculation and thus could not be used in an applied setting. The state of art numbers we report in table 2 do not use this feature.</p><p>Towers: Our original intent in developing the towers variant was to improve training time, as well as to allow the model to be trained on larger graphs. However, we also found some evidence that the multi-tower structure improves generalization performance. In table 4 we compare GG-NN + towers + set2set output vs a baseline GG-NN + set2set output when distance bins are used. We do this comparison in both the joint training regime and when training one model per target. The towers model outperforms the baseline model on 12 out of 13 targets in both  individual and joint target training. We believe the benefit of towers is that it resembles training an ensemble of models. Unfortunately, our attempts so far at combining the towers and edge network message function have failed to further improve performance, possibly because the combination makes training more difficult. Further training details, and error ratios on all targets can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Experiments:</head><p>In preliminary experiments, we tried disabling weight tying across different time steps. However, we found that the most effective way to increase performance was to tie the weights and use a larger hidden dimension d. We also early on found the pair message function to perform worse than the edge network function. This included a toy pathfinding problem which was originally 2 As reported in <ref type="bibr" target="#b34">Schütt et al. (2017)</ref>. The model was trained on a different train/test split with 100k training samples vs 110k used in our experiments. designed to benefit from using pair messages. Also, when trained jointly on the 13 targets the edge network function outperforms pair message on 11 out of 13 targets, and has an average error ratio of 1.53 compared to 3.98 for pair message. Given the difficulties with training this function we did not pursue it further. For performance on smaller sized training sets, consult the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions and Future Work</head><p>Our results show that MPNNs with the appropriate message, update, and output functions have a useful inductive bias for predicting molecular properties, outperforming several strong baselines and eliminating the need for complicated feature engineering. Moreover, our results also reveal the importance of allowing long range interactions between nodes in the graph with either the master node or the set2set output. The towers variation makes these models more scalable, but additional improvements will be needed to scale to much larger graphs.</p><p>An important future direction is to design MPNNs that can generalize effectively to larger graphs than those appearing in the training set or at least work with benchmarks designed to expose issues with generalization across graph sizes. Generalizing to larger molecule sizes seems particularly challenging when using spatial information. First of all, the pairwise distance distribution depends heavily on the number of atoms. Second, our most successful ways of using spatial information create a fully connected graph where the number of incoming messages also depends on the number of nodes. To address the second issue, we believe that adding an attention mechanism over the incoming message vectors could be an interesting direction to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.">Interpretation of Laplacian based models as MPNNs</head><p>Another family of models defined in <ref type="bibr" target="#b8">Defferrard et al. (2016)</ref>, <ref type="bibr" target="#b5">Bruna et al. (2013)</ref>, <ref type="bibr" target="#b19">Kipf &amp; Welling (2016)</ref> can be interpreted as MPNNs. These models generalize the notion of convolutions a general graph G with N nodes. In the language of MPNNs, these models tend to have very simple message functions and are typically applied to much larger graphs such as social network data. We closely follow the notation defined in <ref type="bibr">Bruna et al. (2013) equation (3.</ref>2). The model discussed in <ref type="bibr" target="#b8">Defferrard et al. (2016)</ref>  <ref type="table" target="#tab_4">(equation 5)</ref> and <ref type="bibr" target="#b19">Kipf &amp; Welling (2016)</ref> can be viewed as special cases. Given an adjacency matrix W ∈ R N ×N we define the graph Laplacian to be L = I n − D −1/2 W D −1/2 where D is the diagonal degree matrix with D ii = deg(v i ). Let V denote the eigenvectors of L, ordered by eigenvalue. Let σ be a real valued nonlinearity (such as ReLU). We now define an operation which transforms an input vector x of size N × d 1 to a vector y of size N × d 2 (the full model can defined as stacking these operations).</p><formula xml:id="formula_15">y j = σ d1 i=1 V F i,j V T x i (j = 1 . . . d 2 )<label>(6)</label></formula><p>Here y j and x i are all N dimensional vectors corresponding to a scalar feature at each node. The matrices F i,j are all diagonal N × N matrices and contain all of the learned parameters in the layer. We now expand equation 6 in terms of the full N × d 1 vector x and N × d 2 vector y, using v and w to index nodes in the graph G and i, j to index the dimensions of the node states. In this way x w,i denotes the i'th dimension of node w, and y v,j denotes the j'th dimension of node v, furthermore we use x w to denote the d 1 dimensional vector for node state w, and y v to denote the d 2 dimensional vector for node v. Define the rank 4 tensorL of dimension N ×</p><formula xml:id="formula_16">N × d 1 × d 2 wherẽ L v,w,i,j = (V F i,j V T ) v,w . We will useL i,j to denote the N × N dimensional matrix where (L i,j ) v,w =L v,w,i,j andL v,w to denote the d 1 × d 2 dimensional matrix where (L v,w ) i,j =L v,w,i,j .</formula><p>Writing equation 6 in this notation we have</p><formula xml:id="formula_17">y j = σ d1 i=1L i,j x i y v,j = σ   d1,N i=1,w=1L v,w,i,j x w,i   y v = σ N w=1L v,w x w .</formula><p>Relabelling y v as h t+1 v and x w as h t w this last line can be interpreted as the message passing update in an MPNN where</p><formula xml:id="formula_18">M (h t v , h t w ) =L v,w h t w and U (h t v , m t+1 v ) = σ(m t+1 v ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.1.">THE SPECIAL CASE OF KIPF AND WELLING (2016)</head><p>Motivated as a first order approximation of the graph laplacian methods, <ref type="bibr" target="#b19">Kipf &amp; Welling (2016)</ref> propose the following layer-wise propagation rule:</p><formula xml:id="formula_19">H l+1 = σ D −1/2ÃD−1/2 H l W l<label>(7)</label></formula><p>HereÃ = A + I N where A is the real valued adjacency matrix for an undirected graph G. Adding the identity matrix I N corresponds to adding self loops to the graph. Alsõ D ii = jÃ ij denotes the degree matrix for the graph with self loops, W l ∈ R D×D is a layer-specific trainable weight matrix, and σ(·) denotes a real valued nonlinearity. Each H l is a R N ×D dimensional matrix indicating the D dimensional node states for the N nodes in the graph.</p><p>In what follows, given a matrix M we use M (v) to denote the row in M indexed by v (v will always correspond to a node in the graph G). Let L =D −1/2ÃD−1/2 . To get the updated node state for node v we have</p><formula xml:id="formula_20">H l+1 (v) = σ L (v) H l W l = σ w L vw H l (w) W l Relabelling the row vector H l+1 (v) as an N dimensional col- umn vector h t+1 v the above equation is equivalent to h t+1 v = σ (W l ) T w L vw h t w (8) which is equivalent to a message function M t (h t v , h t w ) = L vw h t w =Ã vw (deg(v)deg(w)) −1/2 h t w , and update function U t (h t v , m t+1 v ) = σ((W t ) T m t+1 )</formula><p>. Note that the L vw are all scalar valued, so this model corresponds to taking a certain weighted average of neighboring nodes at each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.">A More Detailed Description of the Quantum Properties</head><p>First there the four atomization energies.</p><p>• Atomization energy at 0K U 0 (eV): This is the energy required to break up the molecule into all of its constituent atoms if the molecule is at absolute zero. This calculation assumes that the molecules are held at fixed volume.</p><p>• Atomization energy at room temperature U (eV): Like U 0 , this is the energy required to break up the molecule if it is at room temperature.</p><p>• Enthalpy of atomization at room temperature H (eV):</p><p>The enthalpy of atomization is similar in spirit to the energy of atomization, U . However, unlike the energy this calculation assumes that the constituent molecules are held at fixed pressure.</p><p>• Free energy of atomization G (eV): Once again this is similar to U and H, but assumes that the system is held at fixed temperature and pressure during the dissociation.</p><p>Next there are properties related to fundamental vibrations of the molecule:</p><p>• Highest fundamental vibrational frequency ω 1 (cm −1 ): Every molecule has fundamental vibrational modes that it can naturally oscillate at. ω 1 is the mode that requires the most energy.</p><p>• Zero Point Vibrational Energy (ZPVE) (eV): Even at zero temperature quantum mechanical uncertainty implies that atoms vibrate. This is known as the zero point vibrational energy and can be calculated once the allowed vibrational modes of a molecule are known.</p><p>Additionally, there are a number of properties that concern the states of the electrons in the molecule:</p><p>• Highest Occupied Molecular Orbital (HOMO) ε HOMO (eV): Quantum mechanics dictates that the allowed states that electrons can occupy in a molecule are discrete. The Pauli exclusion principle states that no two electrons may occupy the same state. At zero temperature, therefore, electrons stack in states from lowest energy to highest energy. HOMO is the energy of the highest occupied electronic state.</p><p>• Lowest Unoccupied Molecular Orbital (LUMO) ε LUMO (eV): Like HOMO, LUMO is the lowest energy electronic state that is unoccupied.</p><p>• Electron energy gap ∆ε (eV): This is the difference in energy between LUMO and HOMO. It is the lowest energy transition that can occur when an electron is excited from an occupied state to an unoccupied state. ∆ε also dictates the longest wavelength of light that the molecule can absorb.</p><p>Finally, there are several measures of the spatial distribution of electrons in the molecule:</p><p>• Electronic Spatial Extent R 2 (Bohr 2 ): The electronic spatial extent is the second moment of the charge distribution, ρ(r), or in other words R 2 = drr 2 ρ(r).</p><p>• Norm of the dipole moment µ (Debye): The dipole moment, p(r) = dr p(r )(r − r ), approximates the electric field far from a molecule. The norm of the dipole moment is related to how anisotropically the charge is distributed (and hence the strength of the field far from the molecule). This degree of anisotropy is in turn related to a number of material properties (for example hydrogen bonding in water causes the dipole moment to be large which has a large effect on dynamics and surface tension).</p><p>• Norm of the static polarizability α (Bohr 3 ): α measures the extent to which a molecule can spontaneously incur a dipole moment in response to an external field. This is in turn related to the degree to which i.e. Van der Waals interactions play a role in the dynamics of the medium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3.">Chemical Accuracy and DFT Error</head><p>In <ref type="table" target="#tab_4">Table 5</ref> we list the mean absolute error numbers for chemical accuracy. These are the numbers used to compute the error ratios of all models in the tables. The mean absolute errors of our models can thus be calculated as (Error Ratio) × (Chemical Accuracy). We also include some estimates of the mean absolute error for the DFT calculation to the ground truth. Both the estimates of chemical accruacy and DFT error were provided in <ref type="bibr" target="#b10">Faber et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.">Additional Results</head><p>In <ref type="table" target="#tab_5">Table 6</ref> we compare the performance of the best architecture (edge network + set2set output) on different sized training sets. It is surprising how data efficient this model is on some targets. For example, on both R2 and Omega our models are equal or better with 11k samples than the best baseline is with 110k samples.</p><p>In <ref type="table" target="#tab_6">Table 7</ref> we compare the performance of several models trained without spatial information. The left 4 columns show the results of 4 experiments, one where we train the GG-NN model on the sparse graph, one where we add virtual edges (ve), one where we add a master node (mn), and one where we change the graph level output to a set2set output (s2s). In general, we find that it's important to allow the model to capture long range interactions in these graphs.</p><p>In <ref type="table">Table 8</ref> we compare GG-NN + towers + set2set output (tow8) vs a baseline GG-NN + set2set output (GG-NN) when distance bins are used. We do this comparison in both the joint training regime <ref type="formula">(</ref> In <ref type="table" target="#tab_7">Table 9</ref> right 2 columns compare the edge network (enn) with the pair message network (pm) in the joint training regime (j). The edge network consistently outperforms the pair message function across most targets.</p><p>In <ref type="table" target="#tab_1">Table 10</ref> we compare our MPNNs with different input featurizations. All models use the Set2Set output and GRU update functions. The no distance model uses the matrix multiply message function, the distance models use the edge neural network message function.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>j) and when training one model per target (i). For joint training of the baseline we used 100 trials with d = 200 as well as 200 trials where d was chosen randomly in the set {43, 73, 113, 153}, we report the minimum test error across all 300 trials. For individual training of the baseline we used 100 trials where d was chosen uniformly in the range [43, 200]. The towers model was always trained with d = 200 and k = 8, with 100 tuning trials for joint training and 50 trials for individual training. The towers model outperforms the baseline model on 12 out of 13 targets in both individual and joint target training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>There are at least eight notable examples of models from the literature that we can describe using our Message Pass-time steps and is defined in terms of message functions M t and vertex update functions U t . During the message passing phase, hidden states h t v at each node in the graph are updated based on messages m t+1</figDesc><table /><note>ing Neural Networks (MPNN) framework. For simplicity we describe MPNNs which operate on undirected graphs G with node features x v and edge features e vw . It is triv- ial to extend the formalism to directed multigraphs. The forward pass has two phases, a message passing phase and a readout phase. The message passing phase runs for Tv according to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Atom Features</figDesc><table><row><cell>Feature</cell><cell>Description</cell></row><row><cell>Atom type</cell><cell>H, C, N, O, F (one-hot)</cell></row><row><cell>Atomic number</cell><cell>Number of protons (integer)</cell></row><row><cell>Acceptor</cell><cell>Accepts electrons (binary)</cell></row><row><cell>Donor</cell><cell>Donates electrons (binary)</cell></row><row><cell>Aromatic</cell><cell>In an aromatic system (binary)</cell></row><row><cell>Hybridization</cell><cell>sp, sp2, sp3 (one-hot or null)</cell></row><row><cell cols="2">Number of Hydrogens (integer)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of Previous Approaches (left) with MPNN baselines (middle) and our methods (right)</figDesc><table><row><cell>Target</cell><cell cols="5">BAML BOB CM ECFP4 HDAD GC</cell><cell cols="4">GG-NN DTNN enn-s2s enn-s2s-ens5</cell></row><row><cell>mu</cell><cell>4.34</cell><cell>4.23</cell><cell>4.49 4.82</cell><cell>3.34</cell><cell cols="2">0.70 1.22</cell><cell>-</cell><cell>0.30</cell><cell>0.20</cell></row><row><cell>alpha</cell><cell>3.01</cell><cell>2.98</cell><cell>4.33 34.54</cell><cell>1.75</cell><cell cols="2">2.27 1.55</cell><cell>-</cell><cell>0.92</cell><cell>0.68</cell></row><row><cell cols="2">HOMO 2.20</cell><cell>2.20</cell><cell>3.09 2.89</cell><cell>1.54</cell><cell cols="2">1.18 1.17</cell><cell>-</cell><cell>0.99</cell><cell>0.74</cell></row><row><cell>LUMO</cell><cell>2.76</cell><cell>2.74</cell><cell>4.26 3.10</cell><cell>1.96</cell><cell cols="2">1.10 1.08</cell><cell>-</cell><cell>0.87</cell><cell>0.65</cell></row><row><cell>gap</cell><cell>3.28</cell><cell>3.41</cell><cell>5.32 3.86</cell><cell>2.49</cell><cell cols="2">1.78 1.70</cell><cell>-</cell><cell>1.60</cell><cell>1.23</cell></row><row><cell>R2</cell><cell>3.25</cell><cell>0.80</cell><cell>2.83 90.68</cell><cell>1.35</cell><cell cols="2">4.73 3.99</cell><cell>-</cell><cell>0.15</cell><cell>0.14</cell></row><row><cell>ZPVE</cell><cell>3.31</cell><cell>3.40</cell><cell cols="2">4.80 241.58 1.91</cell><cell cols="2">9.75 2.52</cell><cell>-</cell><cell>1.27</cell><cell>1.10</cell></row><row><cell>U0</cell><cell>1.21</cell><cell>1.43</cell><cell>2.98 85.01</cell><cell>0.58</cell><cell cols="2">3.02 0.83</cell><cell>-</cell><cell>0.45</cell><cell>0.33</cell></row><row><cell>U</cell><cell>1.22</cell><cell>1.44</cell><cell>2.99 85.59</cell><cell>0.59</cell><cell cols="2">3.16 0.86</cell><cell>-</cell><cell>0.45</cell><cell>0.34</cell></row><row><cell>H</cell><cell>1.22</cell><cell>1.44</cell><cell>2.99 86.21</cell><cell>0.59</cell><cell cols="2">3.19 0.81</cell><cell>-</cell><cell>0.39</cell><cell>0.30</cell></row><row><cell>G</cell><cell>1.20</cell><cell>1.42</cell><cell>2.97 78.36</cell><cell>0.59</cell><cell cols="2">2.95 0.78</cell><cell>.84 2</cell><cell>0.44</cell><cell>0.34</cell></row><row><cell>Cv</cell><cell>1.64</cell><cell>1.83</cell><cell>2.36 30.29</cell><cell>0.88</cell><cell cols="2">1.45 1.19</cell><cell>-</cell><cell>0.80</cell><cell>0.62</cell></row><row><cell>Omega</cell><cell>0.27</cell><cell>0.35</cell><cell>1.32 1.47</cell><cell>0.34</cell><cell cols="2">0.32 0.53</cell><cell>-</cell><cell>0.19</cell><cell>0.15</cell></row><row><cell cols="2">Average 2.17</cell><cell>2.08</cell><cell>3.37 53.97</cell><cell>1.35</cell><cell cols="2">2.59 1.36</cell><cell>-</cell><cell>0.68</cell><cell>0.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Models Trained Without Spatial Information</figDesc><table><row><cell>Model</cell><cell>Average Error Ratio</cell></row><row><cell>GG-NN</cell><cell>3.47</cell></row><row><cell cols="2">GG-NN + Virtual Edge 2.90</cell></row><row><cell cols="2">GG-NN + Master Node 2.62</cell></row><row><cell>GG-NN + set2set</cell><cell>2.57</cell></row><row><cell cols="2">Table 4. Towers vs Vanilla GG-NN (no explicit hydrogen)</cell></row><row><cell>Model</cell><cell>Average Error Ratio</cell></row><row><cell>GG-NN + joint training</cell><cell>1.92</cell></row><row><cell>towers8 + joint training</cell><cell>1.75</cell></row><row><cell cols="2">GG-NN + individual training 1.53</cell></row><row><cell cols="2">towers8 + individual training 1.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Chemical Accuracy For Each Target</figDesc><table><row><cell>Target</cell><cell cols="2">DFT Error Chemical Accuracy</cell></row><row><cell>mu</cell><cell>.1</cell><cell>.1</cell></row><row><cell>alpha</cell><cell>.4</cell><cell>.1</cell></row><row><cell cols="2">HOMO 2.0</cell><cell>.043</cell></row><row><cell cols="2">LUMO 2.6</cell><cell>.043</cell></row><row><cell>gap</cell><cell>1.2</cell><cell>.043</cell></row><row><cell>R2</cell><cell>-</cell><cell>1.2</cell></row><row><cell>ZPVE</cell><cell>.0097</cell><cell>.0012</cell></row><row><cell>U0</cell><cell>.1</cell><cell>.043</cell></row><row><cell>U</cell><cell>.1</cell><cell>.043</cell></row><row><cell>H</cell><cell>.1</cell><cell>.043</cell></row><row><cell>G</cell><cell>.1</cell><cell>.043</cell></row><row><cell>Cv</cell><cell>.34</cell><cell>.050</cell></row><row><cell cols="2">Omega 28</cell><cell>10.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Results from training the edge network + set2set model on different sized training sets (N denotes the number of training samples)</figDesc><table><row><cell>Target</cell><cell cols="5">N=11k N=35k N=58k N=82k N=110k</cell></row><row><cell>mu</cell><cell>1.28</cell><cell>0.55</cell><cell>0.44</cell><cell>0.32</cell><cell>0.30</cell></row><row><cell>alpha</cell><cell>2.76</cell><cell>1.59</cell><cell>1.26</cell><cell>1.09</cell><cell>0.92</cell></row><row><cell cols="2">HOMO 2.33</cell><cell>1.50</cell><cell>1.34</cell><cell>1.19</cell><cell>0.99</cell></row><row><cell cols="2">LUMO 2.18</cell><cell>1.47</cell><cell>1.19</cell><cell>1.10</cell><cell>0.87</cell></row><row><cell>gap</cell><cell>3.53</cell><cell>2.34</cell><cell>2.07</cell><cell>1.84</cell><cell>1.60</cell></row><row><cell>R2</cell><cell>0.28</cell><cell>0.22</cell><cell>0.21</cell><cell>0.21</cell><cell>0.15</cell></row><row><cell>ZPVE</cell><cell>2.52</cell><cell>1.78</cell><cell>1.69</cell><cell>1.68</cell><cell>1.27</cell></row><row><cell>U0</cell><cell>1.24</cell><cell>0.69</cell><cell>0.58</cell><cell>0.62</cell><cell>0.45</cell></row><row><cell>U</cell><cell>1.05</cell><cell>0.69</cell><cell>0.60</cell><cell>0.52</cell><cell>0.45</cell></row><row><cell>H</cell><cell>1.14</cell><cell>0.64</cell><cell>0.65</cell><cell>0.53</cell><cell>0.39</cell></row><row><cell>G</cell><cell>1.23</cell><cell>0.62</cell><cell>0.64</cell><cell>0.49</cell><cell>0.44</cell></row><row><cell>Cv</cell><cell>1.99</cell><cell>1.24</cell><cell>0.93</cell><cell>0.87</cell><cell>0.80</cell></row><row><cell cols="2">Omega 0.28</cell><cell>0.25</cell><cell>0.24</cell><cell>0.15</cell><cell>0.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison of models when distance information is excluded</figDesc><table><row><cell>Target</cell><cell cols="2">GG-NN ve</cell><cell>mn</cell><cell>s2s</cell></row><row><cell>mu</cell><cell>3.94</cell><cell>3.76</cell><cell>4.02</cell><cell>3.81</cell></row><row><cell>alpha</cell><cell>2.43</cell><cell>2.07</cell><cell>2.01</cell><cell>2.04</cell></row><row><cell cols="2">HOMO 1.80</cell><cell>1.60</cell><cell>1.67</cell><cell>1.71</cell></row><row><cell>LUMO</cell><cell>1.73</cell><cell>1.48</cell><cell>1.48</cell><cell>1.41</cell></row><row><cell>gap</cell><cell>2.48</cell><cell>2.33</cell><cell>2.23</cell><cell>2.26</cell></row><row><cell>R2</cell><cell>14.74</cell><cell cols="3">17.11 13.16 13.67</cell></row><row><cell>ZPVE</cell><cell>5.93</cell><cell>3.21</cell><cell>3.26</cell><cell>3.02</cell></row><row><cell>U0</cell><cell>1.98</cell><cell>0.89</cell><cell>0.90</cell><cell>0.72</cell></row><row><cell>U</cell><cell>2.48</cell><cell>0.93</cell><cell>0.99</cell><cell>0.79</cell></row><row><cell>H</cell><cell>2.19</cell><cell>0.86</cell><cell>0.95</cell><cell>0.80</cell></row><row><cell>G</cell><cell>2.13</cell><cell>0.85</cell><cell>1.02</cell><cell>0.74</cell></row><row><cell>Cv</cell><cell>1.96</cell><cell>1.61</cell><cell>1.63</cell><cell>1.71</cell></row><row><cell>Omega</cell><cell>1.28</cell><cell>1.05</cell><cell>0.78</cell><cell>0.78</cell></row><row><cell cols="2">Average 3.47</cell><cell>2.90</cell><cell>2.62</cell><cell>2.57</cell></row><row><cell cols="5">Table 8. Towers vs Vanilla Model (no explicit hydrogen)</cell></row><row><cell>Target</cell><cell cols="4">GG-NN-j tow8-j GG-NN-i tow8-i</cell></row><row><cell>mu</cell><cell>2.73</cell><cell>2.47</cell><cell>2.16</cell><cell>2.23</cell></row><row><cell>alpha</cell><cell>1.66</cell><cell>1.50</cell><cell>1.47</cell><cell>1.34</cell></row><row><cell cols="2">HOMO 1.33</cell><cell>1.19</cell><cell>1.27</cell><cell>1.20</cell></row><row><cell>LUMO</cell><cell>1.28</cell><cell>1.12</cell><cell>1.24</cell><cell>1.11</cell></row><row><cell>gap</cell><cell>1.73</cell><cell>1.55</cell><cell>1.78</cell><cell>1.68</cell></row><row><cell>R2</cell><cell>6.07</cell><cell>6.16</cell><cell>4.78</cell><cell>4.11</cell></row><row><cell>ZPVE</cell><cell>4.07</cell><cell>3.43</cell><cell>2.70</cell><cell>2.54</cell></row><row><cell>U0</cell><cell>1.00</cell><cell>0.86</cell><cell>0.71</cell><cell>0.55</cell></row><row><cell>U</cell><cell>1.01</cell><cell>0.88</cell><cell>0.65</cell><cell>0.52</cell></row><row><cell>H</cell><cell>1.01</cell><cell>0.88</cell><cell>0.68</cell><cell>0.50</cell></row><row><cell>G</cell><cell>0.99</cell><cell>0.85</cell><cell>0.66</cell><cell>0.50</cell></row><row><cell>Cv</cell><cell>1.40</cell><cell>1.27</cell><cell>1.27</cell><cell>1.09</cell></row><row><cell>Omega</cell><cell>0.66</cell><cell>0.62</cell><cell>0.57</cell><cell>0.50</cell></row><row><cell cols="2">Average 1.92</cell><cell>1.75</cell><cell>1.53</cell><cell>1.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Pair Message vs Edge Network in joint training</figDesc><table><row><cell>Target</cell><cell>pm-j</cell><cell>enn-j</cell></row><row><cell>mu</cell><cell>2.10</cell><cell>2.24</cell></row><row><cell>alpha</cell><cell>2.30</cell><cell>1.48</cell></row><row><cell cols="2">HOMO 1.20</cell><cell>1.30</cell></row><row><cell>LUMO</cell><cell>1.46</cell><cell>1.20</cell></row><row><cell>gap</cell><cell>1.80</cell><cell>1.75</cell></row><row><cell>R2</cell><cell cols="2">10.87 2.41</cell></row><row><cell>ZPVE</cell><cell cols="2">16.53 3.85</cell></row><row><cell>U0</cell><cell>3.16</cell><cell>0.92</cell></row><row><cell>U</cell><cell>3.18</cell><cell>0.93</cell></row><row><cell>H</cell><cell>3.20</cell><cell>0.93</cell></row><row><cell>G</cell><cell>2.97</cell><cell>0.92</cell></row><row><cell>Cv</cell><cell>2.17</cell><cell>1.28</cell></row><row><cell>Omega</cell><cell>0.83</cell><cell>0.74</cell></row><row><cell cols="2">Average 3.98</cell><cell>1.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Performance With Different Input Information</figDesc><table><row><cell>Target</cell><cell cols="3">no distance distance dist + exp hydrogen</cell></row><row><cell>mu</cell><cell>3.81</cell><cell>0.95</cell><cell>0.30</cell></row><row><cell>alpha</cell><cell>2.04</cell><cell>1.18</cell><cell>0.92</cell></row><row><cell cols="2">HOMO 1.71</cell><cell>1.10</cell><cell>0.99</cell></row><row><cell>LUMO</cell><cell>1.41</cell><cell>1.06</cell><cell>0.87</cell></row><row><cell>gap</cell><cell>2.26</cell><cell>1.74</cell><cell>1.60</cell></row><row><cell>R2</cell><cell>13.67</cell><cell>0.57</cell><cell>0.15</cell></row><row><cell>ZPVE</cell><cell>3.02</cell><cell>2.57</cell><cell>1.27</cell></row><row><cell>U0</cell><cell>0.72</cell><cell>0.55</cell><cell>0.45</cell></row><row><cell>U</cell><cell>0.79</cell><cell>0.55</cell><cell>0.45</cell></row><row><cell>H</cell><cell>0.80</cell><cell>0.59</cell><cell>0.39</cell></row><row><cell>G</cell><cell>0.74</cell><cell>0.55</cell><cell>0.44</cell></row><row><cell>Cv</cell><cell>1.71</cell><cell>0.99</cell><cell>0.80</cell></row><row><cell>Omega</cell><cell>0.78</cell><cell>0.41</cell><cell>0.19</cell></row><row><cell cols="2">Average 2.57</cell><cell>0.98</cell><cell>0.68</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Google Brain 2 Google 3 Google DeepMind. Correspondence to: Justin Gilmer &lt;gilmer@google.com&gt;, George E. Dahl &lt;gdahl@google.com&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">By comparison, the inference time of the neural networks discussed in this work is 300k times faster.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Lukasz Kaiser, Geoffrey Irving, Alex Graves, and Yujia Li for helpful discussions. Thank you to Adrian Roitberg for pointing out an issue with the use of partial charges in an earlier version of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Density-functional thermochemistry. iii. the role of exact exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><forename type="middle">D</forename><surname>Becke</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.464913</idno>
		<idno>doi: 10.1063/1. 464913</idno>
		<ptr target="http://dx.doi.org/10.1063/1.464913" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5648" to="5652" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalized neural-network representation of high-dimensional potential-energy surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Behler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Parrinello</surname></persName>
		</author>
		<idno type="DOI">http:/link.aps.org/doi/10.1103/PhysRevLett.98.146401</idno>
		<ptr target="http://link.aps.org/doi/10" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">146401</biblScope>
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">http:/link.aps.org/doi/10.1103/PhysRevLett.98.146401</idno>
		<idno>98.146401</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lillenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Bakowies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>personal communication</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Cun</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Yann. Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantum monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ceperley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast machine learning models of electronic and energetic properties consistently reach approximation errors better than dft accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anatole</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1702.05532" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Machine learning predictions of molecular properties: Accurate many-body potentials and nonlocality in chemical space. The journal of physical chemistry letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franziska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jpclett.5b00831</idno>
		<ptr target="http://dx.doi.org/10.1021/acs.jpclett.5b00831" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2326" to="2331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">New method for calculating the one-particle green&apos;s function with application to the electron-gas problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Hedin</surname></persName>
		</author>
		<idno type="DOI">http:/link.aps.org/doi/10.1103/PhysRev.139.A796</idno>
		<ptr target="http://link.aps.org/doi/10.1103/PhysRev.139.A796" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="796" to="823" />
			<date type="published" when="1965-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inhomogeneous electron gas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hohenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kohn</surname></persName>
		</author>
		<idno type="DOI">http:/link.aps.org/doi/10.1103/PhysRev.136.B864</idno>
		<idno>doi: 10.1103/ PhysRev.136.B864</idno>
		<ptr target="http://link.aps.org/doi/10.1103/PhysRev.136.B864" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="864" to="871" />
			<date type="published" when="1964-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combined first-principles calculation and neural-network correction approach for heat of formation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiujun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laiho</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="11501" to="11507" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Communication: Understanding molecular representations in machine learning: The role of uniqueness and target similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anatole</surname></persName>
		</author>
		<idno type="DOI">10.1063/1.4964627</idno>
		<ptr target="http://dx.doi.org/10.1063/1.4964627" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">161102</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: Moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lusci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Pollastri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1563" to="1575" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Abhinav. The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupta</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04844</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic generation of complementary descriptors with molecular graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Merkwirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural network for graphs: A contextual constructive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning invariant representations of molecules for atomization energy prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fazli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siamac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franziska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ziehe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual international conference on machine learning</title>
		<meeting>the 33rd annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reymond</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Louis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast and accurate modeling of molecular atomization energies with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Haand Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anatole</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.108.058301</idno>
		<ptr target="http://dx.doi.org/10.1103/PhysRevLett.108.058301" />
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">58301</biblScope>
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A structural approach to relaxation in glassy liquids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Sussman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthimios</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Physics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quantumchemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Computer simulation of local order in condensed phases of silicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">H</forename><surname>Stillinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">http:/link.aps.org/doi/10.1103/PhysRevB.31.5262</idno>
		<idno>doi: 10.1103/ PhysRevB.31.5262</idno>
		<ptr target="http://link.aps.org/doi/10.1103/PhysRevB.31.5262" />
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. B</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5262" to="5271" />
			<date type="published" when="1985-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolfgang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maxim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
