<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PISEP 2 : Pseudo Image Sequence Evolution based 3D Pose Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">PISEP 2 : Pseudo Image Sequence Evolution based 3D Pose Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-pose prediction</term>
					<term>CNN</term>
					<term>3D skeleton</term>
					<term>video pre- diction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pose prediction is to predict future poses given a window of previous poses. In this paper, we propose a new problem that predicts poses using 3D joint coordinate sequences. Different from the traditional pose prediction based on Mocap frames, this problem is convenient to use in real applications due to its simple sensors to capture data. We also present a new framework, PISEP 2 (Pseudo Image Sequence Evolution based 3D Pose Prediction), to address this new problem. Specifically, a skeletal representation is proposed by transforming the joint coordinate sequence into an image sequence, which can model the different correlations of different joints. With this image based skeletal representation, we model the pose prediction as the evolution of image sequence. Moreover, a novel inference network is proposed to predict all future poses in one step by decoupling the decoders in a non-recursive manner. Compared with the recursive sequence to sequence model, we can improve the computational efficiency and avoid error accumulation significantly. Extensive experiments are carried out on two benchmark datasets (e.g. G3D and FNTU). The proposed method achieves the state-ofthe-art performance on both datasets, which demonstrates the effectiveness of our proposed method.</p><p>Index Terms-pose prediction, CNN, 3D skeleton, video prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P OSE prediction is widely applied to human-computer collaboration, family service robots, intelligent security and so on <ref type="bibr" target="#b0">[1]</ref>. It is important to predict future dynamics before it happens, which can provide more time for the robot to react and prepare ahead. With the development of the lowcost depth sensor (such as Kinect) and the 3D human pose estimation technique <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, we can easily and effectively acquire 3D skeletal data of humans. Moreover, joints position representation is effective, which is closely matches the visual dissimilarity in Euclidean space <ref type="bibr" target="#b3">[4]</ref>. Therefore, we present to predict poses using a 3D joint coordinate sequence. As is shown in <ref type="figure" target="#fig_1">Figure 1</ref>, the blue poses are the previous poses, and the red poses are the future poses. Our goal is to predict future joint coordinate sequence given a window of previous joint coordinate sequence.</p><p>From the perspective of the problem, the most similar existing works are mocap based pose prediction and video prediction, which intrinsically belong to the sequence to sequence modeling. However, the input and output are different from  ours. (1) Mocap based pose prediction <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>: on one hand, the input and output of these works are mocap frames, and the human pose is represented as a mocap vector parameterized by the exponential map which is easy to predict to a great extent <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>; on the other hand, the acquisition of mocap data is difficult and expensive, and it needs lots of preprocessing to visualize its performance <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>,which is inefficient and not intuitive. Therefore, this motivates us to predict human poses using joint coordinate sequence, which can be acquired cheaply, easily and efficiently. Moreover, due to the various distances or views of the placement of the camera, the problem of pose prediction with a joint coordinate sequence is very challenging. For example, the x or y-axis joint coordinate value of the nearby will be larger than the distant. It will lead to the different physical structure characteristics of the human body. Therefore, mocap based pose prediction and joint coordinate sequence based pose prediction are two different problems, which are not comparable. <ref type="bibr" target="#b1">(2)</ref> Video prediction: the input and output of video prediction is the image frame, which is different from ours. Specially, the 3D skeletal data has a sparse data structure, which is very different from the dense video data. This may lead to a huge gap between video prediction and pose prediction. Moreover, great success has been made in video prediction <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b16">[17]</ref> while our pose prediction is rarely researched. Therefore, this motivates us to pursue a dense representation of the 3D skeletal data to smooth this gap.</p><p>From the perspective of the methodology, there are two main problems needed to be solved: skeletal representation, sequence to sequence modeling. (1) Skeletal representation: As is shown in <ref type="figure" target="#fig_2">Figure 2a</ref>, most of the image-based skeletal representations are formulated by transforming a joint coordinate sequence into a fixed size image <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b24">[25]</ref>. In these rep- resentations, the spatial and temporal information are coupled together, while they are different. Recent works are proposed to model the spatial and temporal information separately, and achieve great performance significantly <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b29">[30]</ref>. Therefore, as is shown in <ref type="figure" target="#fig_2">Figure 2b</ref>, by decoupling the "time" dimension information, we propose a new representation by transforming a joint coordinate sequence into an image sequence, which can conveniently model the spatial and temporal information differently. Here, m is the number of previous frames. At layers m/2, we can capture the global temporal evolution information. Inspired by <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, the human body is naturally divided into five parts. Moreover, during the process of human motion, the joints of each part may have strong correlations, and the joints of different parts may have weak correlations. For example, the joints of the left upper limb may have strong correlations, and the joints between upper limbs and lower limbs may have weak correlations. Therefore, to model those different correlations, our new skeletal representation is formulated by ensuring that the joints of each part in the adjacent positions, and the upper limbs and lower limbs in the distant positions separated by the trunk. With this image based skeletal representation, we model the pose prediction problem as the image sequence evolution. (2) Sequence to sequence modeling: the state-of-the-art sequence to sequence models predict multiple future frames recursively, which easily suffer from costly computation and error accumulation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Moreover, the model complexity increases significantly with the increased length of input or output frames, which is time-consuming. To address those problems, a new framework, PISEP 2 , is proposed to predict various future frames in a nonrecursive manner by decoupling all the decoders.</p><p>In this paper, we present to predict human poses based on the joint coordinate sequence directly. To capture the diverse correlations of different limbs and the local characteristic of the human body, we propose a new skeletal representation by encoding the joint coordinate sequence into the image sequence, which conveniently models the spatial and temporal information of previous frames separately. To efficiently predict future dynamics, we propose a new framework to predict all future frames only in one step. Finally, our method is evaluated comprehensively on G3D <ref type="bibr" target="#b34">[35]</ref> dataset and FNTU dataset (collected from NTU RGB+D <ref type="bibr" target="#b35">[36]</ref>), and achieves the state-of-the-art performance. The main contributions of this paper are summarized as follows:</p><p>1. To the best of our knowledge, this is the first time the problem of pose prediction using 3D joint coordinate sequence has been explored, which is intuitive and efficient to evaluate and visualize its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A new skeletal representation that preserves the local characteristic of the human body is proposed, which can conveniently model different correlations of different limbs, and also model the spatial and temporal information separately. With this new representation, the pose prediction is modeled as an image sequence evolution problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A new sequence to sequence model that models the spatial and temporal information differently, PISEP 2 , is proposed to predict multiple future frames in one step, which can avoid error accumulation.</p><p>4. The proposed method achieves the state-of-the-art performance on two challenging datasets, which shows strong ability of generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Pose prediction receives growing interest recently. In this section, we review the related works from two folds: video prediction and pose prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video prediction</head><p>Video prediction has been extensively studied in recent years <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b40">[41]</ref>, but is an unsolved problem. The main issue of video prediction is spatio-temporal modeling. In the following, we review from these perspectives: spatio-temporal modeling using CNN (Convolution Neural Network), spatio-temporal modeling using ConvLSTM (Convolutional Long Short-Term Memory), spatio-temporal modeling using CNN and LSTM.</p><p>Spatio-temporal modeling using CNN: many approaches were proposed to address the spatio-temporal modeling using CNN in video prediction <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b43">[44]</ref>. Traditional CNN have shown its power in spatial modeling, and can't efficiently model the temporal information. Therefore, Oh et al. <ref type="bibr" target="#b41">[42]</ref> and Zhang et al. <ref type="bibr" target="#b42">[43]</ref> proposed to concatenate the previous frames along with the axis(i.e. time interval or channels) as one tensor, and then apply a CNN based module to capture the spatio-temporal information. Xu et al. <ref type="bibr" target="#b11">[12]</ref> proposed to model the spatial information of each frame by a CNN based block (Residual Multiplicative Block, RMB), and capture the temporal evolution of previous frames hierarchically by a cascade multiplicative unit (CMU) that receives two consecutive frames as input. Zhang et al. <ref type="bibr" target="#b43">[44]</ref> proposed to extract spatio-temporal information by convolutions over sequences of tensors.</p><p>Spatio-temporal modeling using ConvLSTM: ConvL-STM <ref type="bibr" target="#b44">[45]</ref> is a special structure of LSTM that combines the effectiveness of CNN and RNN, which can model the spatial and temporal information simultaneously. <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> used ConvLSTM to capture the spatio-temporal information of previous frames for video prediction. For example, Finn et al. <ref type="bibr" target="#b45">[46]</ref> used CDNA (Convolutional Dynamic Neural Advection), a ConvLSTM based framework, to estimate the distribution in the previous frames for each pixel in the new frame, and then used CDNA kernels to predict the motion information. However, conventional ConvLSTM with a layer-independent memory mechanism ignores the memorized information in the previous layers, which is important to predict video sequences. Therefore, Wang et al. <ref type="bibr" target="#b33">[34]</ref> proposed a novel framework based on ConvLSTM, ST-LSTM (Spatiotemporal LSTM), to predict multiple future video frames recursively, which can extract and memory the spatial and temporal information simultaneously.</p><p>Spatio-temporal modeling using CNN and LSTM: Most of the existing works were proposed based on CNN and LSTM to extract spatio-temporal features to predict future video frames recursively <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b47">[48]</ref>- <ref type="bibr" target="#b52">[53]</ref>. The commonly modeling of these models are two folds: CNN+ConvLSTM+CNN/3D CNN, CNN+ConvLSTM+Deconvolution.</p><p>(1) CNN+ConvLSTM+CNN or 3D CNN <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b51">[52]</ref>: among which, CNNs were commonly used to model the spatial information of previous frames, and ConvLSTMs were used to model the temporal dynamics with local spatial information of the previous frames. Finally, another CNN was applied to predict future video frames. For example, Kalchbrenner et al. <ref type="bibr" target="#b51">[52]</ref> proposed to model the spatial information with RMBs of each frame, then used the ConvLSTM to model the dynamic information of previous frames, finally used another RMB to restore the spatial information of future frames. (2) CNN+ConvLSTM+Deconvolution <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b52">[53]</ref>: in these models, CNN was used to model the spatial information of previous frames and ConvLSTM was used to capture the spatio-temporal of previous frames similarly. Differently, deconvolution was used to predict future frames. For example, Liang et al. <ref type="bibr" target="#b47">[48]</ref> used VAE (variational autoencoder) to model the distribution of input frames, and then applied ConvLSTM to model the temporal dynamics of previous frames. Then used five deconvolutional layers to predict future frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pose prediction</head><p>The related works for pose prediction mainly include: video data based pose prediction, mocap data based pose prediction.</p><p>Video data based pose prediction: there are lots of works related to the pose prediction based on video data <ref type="bibr" target="#b53">[54]</ref>- <ref type="bibr" target="#b59">[60]</ref>, which aim to predict human pose sequence with a few image frames. Most of these works used CNN to model the spatial information from the image data, and then used LSTM or DMM to model their temporal dynamics of previous representations or future pose representations <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b59">[60]</ref>. For example, <ref type="bibr" target="#b57">[58]</ref> proposed to predict 3D pose from a static image using CNN based framework to learn the latent pose representation form the static image. <ref type="bibr" target="#b56">[57]</ref> proposed to model the spatial information of the static image, and then used LSTM to model temporal dynamics of the futures. Both <ref type="bibr" target="#b56">[57]</ref> and <ref type="bibr" target="#b57">[58]</ref> used MSE loss to optimize their models.</p><p>Mocap data based pose prediction: many works were proposed to predict human poses based on mocap data <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b60">[61]</ref>- <ref type="bibr" target="#b62">[63]</ref>. The input and output of these models are mocap vector parameterized by the exponential map <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, which is different from a joint coordinate sequence. Most of these works are based on recursive structure, and the model complexity increases with the predictive length of the future pose <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>. Moreover, most of these models mainly focus on temporal evolution modeling of the previous poses <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>, which ignores the spatial modeling of the human body. For example, Gui et al. <ref type="bibr" target="#b61">[62]</ref> proposed to predict future human poses using the encoder-decoder network constructed by GRU (Gated Recurrent Unit), then used a residual connection to model the motion velocities of the previous poses. One similar work is based on mocap converted data <ref type="bibr" target="#b60">[61]</ref>. The author first covert the mocap frame to the coordinate space in Cartesian coordinates to get a standardized body model. And then designed different encoding-decoding networks based on a fully connected network that model the spatial and temporal information equally, which ignores the differences between the spatial and temporal information.</p><p>Different from all previous works discussed above, we formulate a new problem of 3D pose prediction with a 3D joint coordinate sequence instead mocap frame which can be acquired cheaply and efficiently. Moreover, to address this new proposed problem, we propose: (1) a new skeletal representation, which models the problem of pose prediction as the video prediction; (2) a new framework, PISEP 2 , to predict all future frames in non-recursive manner, which can avoid error accumulation and improve computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>Our main framework (Image Sequence Evolution based Pose Prediction, PISEP 2 ) is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. It mainly includes four parts: (1) Input: our input is the joint coordinate sequence. (2) Skeletal representation: the skeletal representation aims to transform the joint coordinate sequence into an image sequence. And the detail of the skeletal representation as described in the following section. (3) Encoder-Dynamics-Decoder (EDD): the goal of this phase is to infer future dynamics through history poses. For this, EDD described in the following is applied. (4) Output: finally, the output is the future joint coordinate sequence.</p><p>Therefore, in the following sections. We first describe our skeletal representations in detail. Then, we develop a novel framework, EDD, to infer the future dynamics with previous frames. Finally, we briefly introduce the loss function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Skeletal Representation</head><p>Different from image data, the skeleton sequence is a set of joints coordinates. Therefore, in this section, as is shown in <ref type="figure" target="#fig_4">Figure 4</ref>, we propose to represent the joint coordinate sequence with the image sequence, which is formulated by preserving the local characteristic of the human body. The left part of <ref type="figure" target="#fig_4">Figure 4</ref> is the skeleton of the human body. The numbers in the circle are the index of the skeletal joints. The yellowcolored joints are informative enough for representing human motion <ref type="bibr" target="#b63">[64]</ref>, which is relatively stable. And the blue-colored joints may have a limited effect on the sequence of the poses. Therefore, we will select the yellow-colored joints to formulate the skeletal motion frame.</p><p>Given the skeleton of a person in frame i, this new representation is represented by transforming the joints coordinates S i into a one-channel image F i denoted as equation 1. Where S i = {J 1 , J 2 , · · · , J N }, J = (x,y,z)and N is the number of joint. With this representation, the joint coordinate sequence is represented by an image sequence:</p><formula xml:id="formula_0">{S 1 , S 2 , · · · , S m } ⇒ {F 1 , F 2 , · · · , F m },</formula><p>where m is the length of a sequence. Besides, motivated by <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, the human body can be divided into five parts: left arm, right arm, trunk, left leg, and right leg. To model the different relationships of different limbs, as shown in the right part of <ref type="figure" target="#fig_4">Figure 4</ref>, we propose to: (1) place the two limbs or two legs in the adjacent areas, which can conveniently model the correlation of two limbs or two legs; (2) place the two limbs and the two legs in the distance areas separated by trunk, which can efficiently capture their weak correlations. Moreover, to maintain the local characteristic, we place the joints of each limb in the continuous adjacent areas, which keep the physical connection relationship of the local joints. Then, the joints of the five parts are concatenated in the order: left arm, right arm, trunk, left leg, right leg. Finally, our joints orders are: <ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">0,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14</ref>. Different from <ref type="bibr" target="#b30">[31]</ref>, our pose is represented by a two-dimension matrix that preserves the local structure of the human body, while <ref type="bibr" target="#b30">[31]</ref> represents their pose with a one-dimension vector by orderly concatenating the x,y,z coordinates of each joint that loses the local structure of the human body to some extent.</p><formula xml:id="formula_1">F i =      x 1 y 1 z 1 x 2 y 2 z 2 . . . . . . . . . x N y N z N     <label>(1)</label></formula><p>The right image in <ref type="figure" target="#fig_4">Figure 4</ref> is the transformational pseudo image. The size of the transformed images is 18 × 3. Here, in our dataset, the value of the x or y coordinate is very small, and the value of the z coordinate is relatively large. Therefore, we translate the x, y, and z coordinate values appropriately to obtain a better display effect. The same processing is applied in <ref type="figure" target="#fig_2">Figure 2</ref> and <ref type="figure" target="#fig_3">Figure 3</ref> for a better visual presentation. In the skeletal representation, the pixel value represents the x/y/z coordinates of the corresponding joint, and the change of the pixel value represents the movement of the joint in the x/y/zaxis direction. Notably, because the coordinate value may be affected by the distance of the position of the camera, the coordinate value may exceed the effective representation of the image. Therefore, we can force the coordinate value of joints in the effective representation range of the image without changing the pose shape and the temporal evolution process of pose sequence through uniform translation and scale scaling operations. In this paper, since our joints coordinate values are far from this range, we have not made any processing in all experiments. Notably, the pixel values are real numbers, including positive real numbers, negative real numbers and 0, which is consistent with the joint coordinate values, and different from the general image.</p><p>The advantages of our skeletal representation are summarized as follows:</p><p>1. Modeling the different correlations of joints of the human body: On one hand, by ensuring the joints of each limb in the adjacent areas, we can conveniently model the strong correlations of joints of each limb. On the other hand, by placing the upper limbs and the lower limbs in the distant areas, we can conveniently model their weak correlations.</p><p>2.</p><p>Modeling the spatial and temporal information of previous frames separately: as shown in <ref type="figure" target="#fig_2">Figure 2b</ref>, by decoupling the "time" dimension, we can conveniently model the spatial and temporal information separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EDD</head><p>Most existing sequence-to-sequence models likely suffer from costly computation and error accumulation with its recurrent structure <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Inspired by the architecture of <ref type="bibr" target="#b11">[12]</ref> for spatio-temporal modeling, we propose a new structure, EDD (Encoder-Dynamics-Decoder), to predict multiple future frames in one step, which can significantly improve the computational efficiency and avoid error accumulation. <ref type="figure" target="#fig_5">Figure  5</ref> is the framework of EDD. The framework mainly includes three parts: (1) Encoder: this module aims to model the spatial structure information of the pose. Residual multiplicative block (RMB) <ref type="bibr" target="#b51">[52]</ref> has power for spatial modeling with its LSTM-like structure. Therefore, we introduce RMB as the basic unit. The encoder is stacked by l e residual multiplicative blocks (RMBs) to enlarge its receptive field for better spatial modeling. To reduce the model complexity, the encoders share weights. (2) Dynamics module: the dynamic module aims to capture the general temporal evolution of previous poses. For this, we introduce the cascade multiplicative unit (CMU) proposed in <ref type="bibr" target="#b11">[12]</ref> as the building block which models the dynamics of the adjacent frames. To capture the global temporal evolution information of previous frames, we model the different scales of temporal information in a hierarchical way. More specially, 2 l continuous frames temporal dynamics will be captured at layer l in the dynamics module. At layer m/2, we can model the global temporal information of previous m frames. Therefore, we can model the different scales of temporal evolution information at different layers. To reduce the computational cost, CMUs at each layer share weights; to capture different scales temporal information, CMUs at different layers are decoupled with each other. Different from <ref type="bibr" target="#b11">[12]</ref>, we remove the chain structure of this model module to learn the general temporal evolution information only once at all time steps, while <ref type="bibr" target="#b11">[12]</ref> need to learn the temporal evolution of previous frames recursively at each step. (3) Decoder: this module proposes to predict all future frames in one step. The decoder is stacked by l d RMBs, which reconstruct the spatial structure of predictive pose. Different future poses are generated by decoupling all the decoders with each other based on the general temporal evolution information produced by dynamics module. Differently, all decoders in <ref type="bibr" target="#b11">[12]</ref> share weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss function</head><p>Our goal is to predict future poses as close as possible to groundtruth. L2 norm loss is the commonly used loss function for the similar problem. Obviously, when the number is smaller than one, its square value smaller than absolute. In this case, the L1 norm loss can better reflect the difference between these two similar poses. Therefore, we propose to achieve a more accurate pose prediction using L1 norm loss that may better guide training well. Our loss function can be formulated as equation 2:</p><formula xml:id="formula_2">l = y − y (2)</formula><p>Where y is the groundtruth pose, y is the predictive pose.</p><p>During training, we aim to minimize the loss function to obtain the optimization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate our model on two challenging datasets: G3D <ref type="bibr" target="#b34">[35]</ref> and FNTU. Where FNTU dataset is collected from NTU RGB+D <ref type="bibr" target="#b35">[36]</ref> dataset to ensure the quality of the learning data. We first introduce the datasets and implementation details. Then, we briefly explain our baselines. Next, we compare our method with the state-of-the-art methods to verify the performance of PISEP 2 . And extensive experiments are conducted to evaluate the effectiveness of our proposed method. Moreover, we further evaluate the performance of our model for unseen data. Finally, we show some qualitative results of pose prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Implementation Details</head><p>G3D: G3D <ref type="bibr" target="#b34">[35]</ref> dataset contains 10 subjects performing 20 gaming actions captured by the Microsoft Kinect sensor. G3D is an unsegmented dataset, and each video may contain multiple actions. It consists of 210 samples in total. We randomly select 70 samples as a test set and the rest as a training set. <ref type="bibr" target="#b0">1</ref> Filtered NTU RGB+D (FNTU) 1 : NTU RGB+D <ref type="bibr" target="#b35">[36]</ref> dataset is collected by the Microsoft Kinect v2 sensor. The dataset includes 60 action classes performed by 40 subjects and consists of 56, 880 video clips in total. NTU RGB+D dataset is a well-segmented dataset, and each video contains one action. However, the skeletal data captured from the side or back of the human body is very noisy. And those noisy skeletal sequences are not suitable for pose prediction. Therefore, we form our dataset based on NTU RGB+D dataset, named filtered NTU RGB+D (FNTU), by: (1) filtering the mutual actions since our method focus on the analysis of single person movement of the human body; (2) selecting the relative forward skeleton of the human body to ensure the quality of learning data. The FNTU dataset consists of 18102 samples. We randomly select one of 12001 samples for training and the rest for testing. <ref type="bibr" target="#b0">1</ref> Implementation Details: in experiments, we use an overlap sliding window to clip the skeleton sequence on the training set and test set respectively. We aim to use the previous 10 frames to predict future 10 frames. Therefore, the window size is set to 20. To ensuring the continuity of the segmented sequence, the overlap size is set to 5. Our final training set contains 3543 sequence clips, and the test set contains 1637 sequence clips on the G3D dataset. And the training set consists of 53843 samples, and the test set consists of 26819 samples on FNTU. To avoid over-fitting, we stack 2 RMBs as the encoder and stack 3 RMBs as the decoder on G3D dataset. And we stack 4 RMBs for the encoder and stack 6 RMBs for the decoder on FNTU dataset. We train all models using Adam optimizer, and our learning rate is initial with 0.0001. All the experiments are implemented with TensorFlow.</p><p>Metrics: We choose the mean squared error (MSE) per frame and the mean absolute error (MAE) per frame as our evaluation metrics. Specially, as shown in equations 3 and 4, we calculate the MSE or MAE between the joint coordinate of the groundtruth pose and the joint coordinate of the predictive pose and normalize with the length of the predictive sequence.</p><formula xml:id="formula_3">e mse = N i 3 j (p i,j − p i,j ) 2 (3) e mae = N i 3 j |p i,j − p i,j |<label>(4)</label></formula><p>Where p and p represented by equations (1) are the groundtruth pose and predictive pose respectively, N is the number of joint, p i,j is the value of the ith joint, jth dimension of the groundtruth pose, and p i,j is the value of the ith joint, jth dimension of the predictive pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>Since pose prediction with a joint coordinate sequence is the newly proposed problem, there exists no baseline for comparison. Our pose prediction is modeled as the problem of image evolution, we consider the PredCNN proposed in <ref type="bibr" target="#b11">[12]</ref> as our baseline. Moreover, the most similar work in <ref type="bibr" target="#b60">[61]</ref> converts the mocap frame into the joint coordinate frame in Cartesian coordinates. Therefore, for comparison, we reproduce the framework (Symmetric Temporal Encoder, S-TE) as our baseline.</p><p>PredCNN: the non-overlapping PredCNN is the most similar to ours. For a fair comparison, the non-overlapping Pred-CNN with our new skeletal representation is introduced to address the problem of our proposed pose prediction.</p><p>S-TE: as shown in <ref type="bibr" target="#b60">[61]</ref>, the framework of S-TE has five fully connected layers, and all layers are of dimensions </p><formula xml:id="formula_4">(3 × N joints × N i , 300, 100, 300, (3 × N joints × N o ) respec- tively.</formula><p>Where the N joints , N i and N o are the joints number of a pose, the input length of the previous frames and output length of the future frames respectively. In this paper, the joints number of a pose is 18 (i.e. N joints = 18). Since the joints order does not affect the network performance with fully connected structure, given a window frame of size N , we process the data by flattening our skeletal representation described above into a vector with dimensions (3 × N joints × N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with baselines</head><p>To evaluate the performance of our framework, we compare our method with the above baselines. The experimental results are shown in <ref type="table" target="#tab_0">Table I</ref>. And our model achieves the stateof-the-art performance, which demonstrates the effectiveness of our proposed method. Compared with PredCNN <ref type="bibr" target="#b11">[12]</ref>, our method significantly gains in accuracy on both datasets. For example, the MSE decreases from 0.1882 to 0.1199, and the MAE decreases from 1.5713 to 1.1101 on G3D. The experimental results show that our framework can avoid error accumulations. Besides, our model captures the spatial information by an LSTM-like block, and the temporal information with a hierarchical structure, which can treat the spatial and temporal information unequally, while the S-TE <ref type="bibr" target="#b60">[61]</ref> treat the spatial and the temporal information equally. Therefore, compared with the S-TE <ref type="bibr" target="#b60">[61]</ref>, our framework can handle the spatial-temporal information well. This may be the possible reason that our model leads to slightly better performance. For example, compared with S-TE <ref type="bibr" target="#b60">[61]</ref>, the MSE increases by 0.0208 and 0.0215, and the MAE increases by 0.0994 and 0.1443 on G3D and FNTU, respectively. Quantitative analysis of frame-wise results: to analyze the performance of each time-step, the frame-wise performance of different methods are as shown in <ref type="figure" target="#fig_6">Figure 6</ref>. Where the horizontal axis represents by frames and the vertical axis represents by MSE or MAE of each frame. The mean MSE of predictive poses for each frame is 0.1199 and 0.1210, and the mean MAE of predictive poses for each pose is 1.1101 and 1.1651 on G3D and FNTU, respectively. Compared with PredCNN <ref type="bibr" target="#b11">[12]</ref>, our method significantly decreases error at all time-steps, especially for the long-term prediction. Moreover, PredCNN easily suffers from error accumulation that may lead to its poor performance, and it may more obvious in the later time-step. The experimental results show that our method can significantly enhance the predictive performance on both shortterm and long-term predictions. And our framework achieves the best performance, which further evidence the effectiveness of our proposed method.</p><p>Quantitative analysis of joint-wise results: to further analyze the performance of our method, we measure the error of each joint. <ref type="figure" target="#fig_7">Figure 7</ref> is the joint-wise performance of different methods. Where the horizontal axis represents by frames and the vertical axis represents by MSE or MAE of each of joint. (1) On G3D, the errors of the joints of the upper limbs are relatively large, while the errors of the joints of lower limbs or trunk are relatively small. The possible reason for this phenomenon is that most of the movement occurs mainly in the joints of the upper limbs. Compared with the upper limbs joints, the joints of lower limbs or trunk are relatively stable. Therefore, the errors of the joints of the upper limbs are larger than the errors of the joints of the lower limbs or trunk. Compared with PredCNN, the performance of our method is significantly extends their performance at all joints, which demonstrates that our method can avoid error accumulation well. Compared with S-TE, our method outperforms S-TE overall for both MSE and MAE at all joints. Since our model treats the spatial and temporal information differently, while S-TE model the spatial and temporal information equally, our method can better capture the temporal evolution information.</p><p>And the experimental results have demonstrated this to a great extent. (2) On FNTU, compared with PredCNN and S-TE, our method achieves the best results for both MSE and MAE. More specially, the errors of the upper limbs joints are relatively large, and the errors of the joints of the lower limbs or trunk are relatively small, which demonstrates similar results on G3D. Similarly, most of the actions on FNTU are the upper limbs related action. The movement of the upper limbs joints is relatively violent. This may be the main reason for the above phenomenon. The experimental results on FNTU are consistent with the results on G3D, which intensively verify the effectiveness of our proposed method.</p><p>Quantitative analysis of axis-wise results: because the z coordinate value of each joint is significantly larger than the x or y coordinate value of each joint, to further analyze the error of each predictive pose, the errors are calculated along the axis x, axis y, and axis z respectively. <ref type="figure" target="#fig_9">Figure 8</ref> is the axis-wise performance of different methods. Where "*-x" denotes the error of method "*" on the axis x, "*-y" denotes the error of method "*" on the axis y, "*-z" denotes the error of method "*" on the axis z. (1) On G3D: in general, the errors of x, y, and z coordinate of joints of upper limbs are relatively large, especially for the "wrist righ","hand right", "hand left", and "wrist lef" joint, on both MSE and MAE. The possible reason for this is that: the actions on G3D are the upper limbs related actions, and these joints are the most active. Therefore, this may lead to a large error of these joints. For all the methods, generally, the errors of the x coordinate of each joint are the smallest. The errors of z coordinate of the joints of the upper limbs are relatively large, while the errors of the y coordinate of the joints of the lower limbs or trunk are relatively large. The possible reason is: for all actions, in general, (a) the movement of the x-axis is the smallest for all joints; (b) the movement of the y-axis of upper limb joints is more violent; (c) the movement of the z-axis of the trunk or lower limbs joints is more violent. This may cause the smallest errors of the x coordinate of each joint, the errors of the y coordinate of the joints of the upper limbs may larger than the trunk or lower limbs joints, and the errors of z coordinate of the trunk or lower limbs joints may larger than the joints of the upper limbs. Compared with <ref type="bibr" target="#b11">[12]</ref>, the errors of our method are decreased significantly for both x, y, and z coordinate of all joints, which demonstrate that our model can efficiently avoid error accumulation. Compared with <ref type="bibr" target="#b60">[61]</ref>, the performance of our method is slightly better, which further demonstrates that our method can handle the temporal evolution of previous frames. (2) On FNTU, this shows similar results for both MSE and MAE, which demonstrate the effectiveness of our method again. Specially, the x coordinate of all joints achieves the best performance. For the joints of the upper limbs, the errors of the y coordinate of each joint are relatively large; for the joints of the trunk or lower limbs, the errors of the z coordinate of each joint are relatively large. The possible reason is similar to the results on G3D as discussed above. Compared with <ref type="bibr" target="#b11">[12]</ref>, the performance of our method significantly outperforms <ref type="bibr" target="#b11">[12]</ref> for all joints on both x, y, and z-axis, especially for the joints of the upper limbs, which demonstrates that our method captures the temporal evolution well and can efficiently avoid error accumulation. Compared with <ref type="bibr" target="#b60">[61]</ref>, our method outperforms <ref type="bibr" target="#b60">[61]</ref> at all joints overall, especially for the joints of the upper limbs that movement violently, which further shows the effectiveness of our proposed method to capture temporal evolution information of previous poses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation of PISEP 2</head><p>To further evaluate the effectiveness of our proposed method, in this section, we will comprehensively verify from there three aspects: skeletal representation, network architecture, and loss function.  Evaluation of skeletal representation: to evaluate the performance of our skeletal representation, we randomly disrupt the order of the joints to evaluate our network. <ref type="table" target="#tab_0">Table II</ref> is the results of different representations. Where "disorder1" and "disorder2" are two group experiments of random joints order  <ref type="figure">Fig. 9</ref>. Frame-wise performance of different representations.</p><p>representation. Compared with the disorder joints representation, our method outperforms the performances of "disorder1" and "disorder2" representation on both G3D and FNTU, which demonstrates our skeletal representation can preserve the local characteristic of the human body. Besides, with our skeletal representation, we can efficiently model the different correlations of different limbs. However, the improvement of our skeletal representation is limit. For example, the MSE only decreases by 0.0033 or 0.0064, and the MAE decreases by 0.0180 or 0.0437 on G3D. The possible reason is that our new representation is too small, and, to some extent, different joints may be affected by each other under the operation of convolution. <ref type="figure">Figure 9</ref> is the frame-wise performance of different representations. As is shown in <ref type="figure">Figure 9</ref>, the results of our representation are slightly better than the performances of the "disorder1" and "disorder2" joints representations at all future frames, which demonstrate that the effectiveness of our skeletal representation. But our effectiveness is limited, the possible reason is: due to the small size of our representation, to some extent, all joint can be affected with each other under the convolution operation.</p><p>Evaluation of our network architecture: PredCNN [12] is the typical representation of the chain network, which is the most similar to ours. To evaluate the effectiveness of our framework, we compare it with PredCNN using our skeletal representation. And the experimental results are shown in <ref type="table" target="#tab_0">Table III</ref> and <ref type="figure" target="#fig_1">Figure 10</ref>. As is shown in <ref type="table" target="#tab_0">Table III</ref>, our performance efficiently outperforms PredCNN on both accuracy and computational cost. Our network removed the chain structure proposes to predict all future frames in one step, which can efficiently avoid the error accumulation. The experimental results further evidence the efficiency of our network. <ref type="figure" target="#fig_1">Figure 10</ref> is the frame-wise performance of different archi-   tectures. As is shown in <ref type="figure" target="#fig_1">Figure 10</ref>, the performance of our network significantly exceeds the performance of PredCNN at all time-steps, especially for the long-term prediction. Because PredCNN is a network with a recursive structure, the output of the current step is the input of the next step. Therefore, the performance of each step is vulnerable to the performance of the previous step. This may lead to the poor performance of PredCNN, especially for the later time-step. The experimental results show that our network can effectively avoid error accumulation again.</p><p>Evaluation of Loss function: when two poses are too similar, their difference is very small (far less than one). Their square value is smaller. So, in this case, the L2 loss can't guide training well, but the L1 norm loss directly reflects the difference between these two similar poses, which can better guide training. Therefore, we assume that the L1 norm loss better than L2 norm loss for more accurate pose prediction.</p><p>To verify the effectiveness of L1 norm loss, we carry out two experiments using our skeletal representation: (1) PISEP 2 with L2 loss; (2) PISEP 2 with L1 norm loss. <ref type="table" target="#tab_0">Table IV</ref> shows the performance of PISEP 2 with different losses on two challenge datasets. The performance of PISEP 2 with L1 norm loss significantly outperforms it with L2 loss on both datasets. For example, the MSE decreases from 0.1434 to 0.1199, and the MAE decreases from 1.4038 to 1.1101 on G3D. <ref type="figure" target="#fig_1">Figure 11</ref> is the frame-wise performance of different losses. Compared with L2 loss, the performance of L1 norm loss outperforms of L2 loss at all timestamps on these two datasets. The  experimental results further demonstrate the effectiveness of L1 norm loss which is more suitable for more accurate to predict poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation of generalization</head><p>The actions on G3D are the gaming related actions, which are different from the actions on FNTU, because most of the actions on FNTU are the daily activity actions. Therefore, to further analyze the performance of our model on unseen data, we present to carry out two groups experiments: (1) pre-trained on FNTU, test on G3D directly; (2) pre-trained on FNTU, finetuned on G3D.</p><p>The first group experimental results are shown in <ref type="table" target="#tab_4">Table V</ref>. And our model shows the best performance on unseen data. Compared with PredCNN, our model decreases by 0.0986 and 0.8993 for MSE and MAE respectively. Compared with S-TE <ref type="bibr" target="#b60">[61]</ref>, the MSE and MAE of our model decrease by 0.0606 and 0.4194 respectively. The experimental results show that our model is more general, which is more robust to unseen data. The possible reasons are two folds: (1) model spatial and temporal information differently: more precisely, we can model the spatial information using, an LSTM like block, RMBs <ref type="bibr" target="#b51">[52]</ref> powerfully. And the global temporal information is modeled hierarchically using CMUs <ref type="bibr" target="#b11">[12]</ref>. (2) Non-recursively structure: different from the commonly used recursive network <ref type="bibr" target="#b11">[12]</ref>, we propose to predict all future frames in one step, which can avoid error accumulation efficiently and also improve their predictive performance.</p><p>To further verify the predictive power of our network, we present to carry out the second group experiment. And the experimental results are shown in <ref type="table" target="#tab_0">Table VI</ref>. Our network achieves optimal experimental results, which is consistent with the experimental results as discussed above. <ref type="figure" target="#fig_1">Figure 12</ref> is the frame-wise performance on unseen data. Before fine-tuning, as shown in the left part of <ref type="figure" target="#fig_1">Figure 12</ref>,  our method significantly outperforms all the baselines, which indicates the power of our network to learn the general motion representation. After fine-tuning, as shown in the right part of <ref type="figure" target="#fig_1">Figure 12</ref>, all of the methods can learn the motion representation on new data well. Moreover, the performance of PredCNN is approximate to the performance of the S-TE. However, our method still surpasses PredCNN and S-TE, especially for the long-term motion prediction, which further evinces the power of our network to efficiently model the temporal evolution of pose sequence.</p><p>To analyze the general performance of our proposed method carefully, joint-wise evaluation is carried out as shown in <ref type="figure" target="#fig_1">Figure 13</ref>. Similarly, the errors of the joints of the upper limbs are larger than the errors of the joints of the trunk or lower limbs both on MSE and MAE. The possible reason is the same as discussed above. (1) Before fine-tuning, the performance of our method better than most of the joints of all the baselines for both MSE and MAE. Among them, the performance of PredCNN achieves the worst performance and is severely unstable. For example, the error of the joint "spine mid" fluctuates greatly, and its error at short-term motion prediction is larger than the long-term motion prediction, which is converse to the normal trend. The possible reason is that their recursive structure cause error accumulation, which leads to the poor generalization ability of PredCNN. Compared with PredCNN, the performance of S-TE seems more stable. Because S-TE treats the spatial and temporal information equally, which may not capture the temporal evolution of the pose sequence well. But our model removes the recursive structure, and presents to predict all future poses at one time, which can effectively improve the computation efficiency and avoid error accumulation. Besides, our model significantly outperforms PredCNN and S-TE, which, to a great extent, shows the powerful generalization ability of our network. (2) After finetuning, all models can learn the specific representation of new data, and our model gains the best performance. This may benefit from our non-chain framework, which can capture the temporal information well and avoid error accumulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Analysis of the Experimental Results</head><p>To show the performance of our proposed method, we visualize the predictive pose frame by frame qualitatively. <ref type="figure" target="#fig_1">Figure 14</ref> is the visualization of frame-wise performance on two challenging datasets. Here, for each group pose sequences, the first sequence denotes the groundtruth sequence, the second sequence corresponds to the performance of S-TE, the third sequence corresponds to the results of the PredCNN, and the last sequence produces the results of our model. Moreover, all the predictive future poses are marked in red.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 14</ref>, (a) on G3D, our model achieves more reasonable performance in general, which further evidences the effectiveness of our proposed method. For example, the top left group pose sequences, the long-term performance of the third sequence performs seems terrible, which is very different from the groundtruth poses. Compared with PredCNN, our predict poses seems more reasonable. For the top right group pose sequences, the S-TE model shows the worst results. The possible reason is that S-TE treats spatial and temporal information equally, which is likely to fail to carefully capture the temporal evolution of the pose sequence. Compared with PredCNN, owe to the non-recurrent structure, our model achieves superior performance, which can avoid error accumulation. Although our result is still far from the groundtruth in the top right and the bottom left sequence, the evolution direction of the pose movement is approximately correct. (b) On FNTU, the visualization performance of our method outperforms PredCNN's in general, which demonstrates that our method can efficiently avoid error accumulation. Compared with S-TE, our performance seems more reasonable. For example, for the top left group sequences, although our performance is not good enough, our result seems more reasonable than S-TE. Since S-TE may not distinguish the spatial and temporal information, while our method model the spatial and temporal information with different kinds of blocks, our model can better capture the temporal information than S-TE. This may be the possible reason that our model can predict more reasonable poses.</p><p>To further analysis the visualization performance of our model on unseen data, the predictive poses with different models are visualized as shown in <ref type="figure" target="#fig_1">Figure 15.</ref> (1) Before finetuning, compared with PredCNN, in most cases, our method achieves better visualization performance. For example, for the top right group pose sequence, our predictive poses are more reasonable since the direction of the motion is almost consistent with the groundtruth. But there are exceptions, such as bottom right group sequences. Our method is slightly worse. Compared with S-TE, our approach is much better. The experimental results show that the generalization performance of our method is much better since our method models spatial and temporal information differently, and can avoid error accumulation efficiently. (2) After fine-tuning, the visualization performances of all methods have been improved significantly, but the effect of our approach is the best, which further verify the effectiveness of the proposed method from the side. Among them, there may be some unreasonable phenomena in PredCNN's predictive pose due to the accumulation of errors. For example, for the top right group sequences, the joints of the poses in the long-term prediction seem unreasonable since the evolution direction of the joints is inconsistent with the groundtruth. But our performance is approximately consistent with the groundtruth. This shows again that our method can predict future poses more reasonably.</p><p>The best performance of our method, to a great extent, reveals two facts: (1) modeling the spatial and temporal information differently can explore the temporal evolution information better; (2) remove the recursive structure of the sequence to sequence model may can avoid error accumulation efficiently and improve the performance both at accuracy and computation efficiency significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we have formulated a new problem of 3D pose prediction using joint coordinate sequence data and proposed a conceptually simple but efficient framework to address this new problem. Specifically, we provide a conveniently 3D pose prediction method, which can efficiently evaluate and visualize its performance. Furthermore, we present a new skeletal representation, which can conveniently model diverse correlations of different limbs, the local characteristic of the human body, and the global temporal evolution of previous poses. Besides, a new sequence to sequence model is proposed to predict all future frames in one step, and also achieves the state-of-the-art performance, which can significantly improve the computational efficiency and avoid error accumulation. In sum, we have shown the effectiveness of the proposed new skeletal representation and the proposed framework, which can provide an efficient pose prediction method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Xiaoli</head><label></label><figDesc>Liu and Jianqin Yin are with the Automation School of Beijing University of Posts and Telecommunications, Beijing 100876, China (Li-uxiaoli134@bupt.edu.cn; jqyin@bupt.edu.cn). (Corresponding author: Jianqin Yin) Huaping Liu is with the Department of Computer Science and Technology of Tsinghua University, Beijing 100084, China. Yilong Yin is with the school of Computer Science and Technology of Shandong University, Jinan 250022, China.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>pose prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>arXiv:1909.01818v1 [cs.CV] 4 Sep 2019 Temporal evolution of previous frames. (a) Commonly spatio-temporal modeling of most of the existing methods. (b) The spatio-temporal modeling of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The representation of skeletal data. The left part of the figure is the skeleton of the human body and the right part of the figure is the transformational images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The framework of EDD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Frame-wise performance of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Joint-wise performance of different methods. (a) Joint-wise MSE of different methods on G3D. (b) Joint-wise MAE of different methods on G3D. (c) Joint-wise MSE of different methods on FNTU. (d) Joint-wise MAE of different methods on FNTU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>performance of different methods on FNTU</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Axis-wise performance of different methods. (a) Axis-wise MSE of different methods on G3D. (b) Axis-wise MAE of different methods on G3D. (c) Axis-wise MSE of different methods on FNTU. (d) Axis-wise MAE of different methods on FNTU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Frame-wise performance of different architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Frame-wise performance of different losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Frame-wise performance on unseen data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Joint-wise performance of unseen data. (a) Joint-wise MSE of general results. (b) Joint-wise MAE of general results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Visualization of frame-wise predictive performance. (a) Visualization of frame-wise performance on G3D. (b) Visualization of frame-wise performance on FNTU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Visualization of frame-wise performance on unseen data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">WITH THE STATE-OF-THE-ART METHODS</cell></row><row><cell></cell><cell></cell><cell>MSE</cell><cell></cell><cell>MAE</cell></row><row><cell></cell><cell>G3D</cell><cell>FNTU</cell><cell>G3D</cell><cell>FNTU</cell></row><row><cell cols="5">PredCNN [12] 0.1882 0.1665 1.5713 1.6394</cell></row><row><cell>S-TE [61]</cell><cell cols="4">0.1407 0.1425 1.2095 1.3094</cell></row><row><cell>PISEP</cell><cell></cell><cell></cell><cell></cell></row></table><note>2 0.1199 0.1210 1.1101 1.1651</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RESULTS</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="8">OF DIFFERENT REPRESENTATIONS</cell></row><row><cell cols="4">Representation</cell><cell></cell><cell>G3D</cell><cell cols="3">MSE FNTU</cell><cell cols="2">G3D</cell><cell>MAE FNTU</cell></row><row><cell cols="4">disorder1</cell><cell cols="7">0.1232 0.1273 1.1281 1.1944</cell></row><row><cell cols="4">disorder2</cell><cell cols="7">0.1263 0.1330 1.1538 1.2241</cell></row><row><cell></cell><cell></cell><cell>Our</cell><cell></cell><cell cols="7">0.1199 0.1210 1.1101 1.1651</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">G3D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FNTU</cell></row><row><cell></cell><cell cols="2">0.25</cell><cell>Disorder1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Disorder1</cell></row><row><cell></cell><cell></cell><cell>0.2</cell><cell>Disorder2 PISEP 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell>Disorder2 PISEP 2</cell></row><row><cell>MSE</cell><cell cols="2">0.15</cell><cell></cell><cell></cell><cell></cell><cell>MSE</cell><cell cols="2">0.15</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell></cell><cell cols="2">0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.05</cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Frames</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Frames</cell></row><row><cell></cell><cell></cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.6</cell><cell></cell></row><row><cell></cell><cell></cell><cell>1.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4</cell><cell></cell></row><row><cell cols="2">MAE</cell><cell>1 1.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MAE</cell><cell>1 1.2</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Frames</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Frames</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">RESULTS OF DIFFERENT ARCHITECTURES</cell><cell></cell></row><row><cell>Model</cell><cell>G3D</cell><cell>MSE FNTU</cell><cell>G3D</cell><cell>MAE FNTU</cell><cell>Test time/ms G3D FNTU</cell></row><row><cell cols="6">PredCNN [12] 0.1876 0.1665 1.5539 1.6394 4.3828 3.2649</cell></row><row><cell>PISEP 2</cell><cell cols="5">0.1199 0.1210 1.1101 1.1651 2.0372 3.1961</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell cols="5">PERFORMANCE OF DIFFERENT LOSSES</cell></row><row><cell>Model</cell><cell>G3D</cell><cell>MSE FNTU</cell><cell>G3D</cell><cell>MAE FNTU</cell></row><row><cell>PISEP 2 (L2)</cell><cell cols="4">0.1434 0.1354 1.4038 1.4063</cell></row><row><cell cols="5">PISEP 2 (L1 norm) 0.1199 0.1210 1.1101 1.1651</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="3">RESULTS OF GENERAL PREDICTION</cell></row><row><cell>Model</cell><cell>MSE</cell><cell>MAE</cell></row><row><cell cols="3">PredCNN [12] 0.2432 2.1706</cell></row><row><cell>S-TE [61]</cell><cell cols="2">0.2052 1.6907</cell></row><row><cell>PISEP 2</cell><cell cols="2">0.1446 1.2713</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI RESULTS</head><label>VI</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">OF FINE-TUNED PREDICTION</cell></row><row><cell></cell><cell></cell><cell cols="2">Model</cell><cell></cell><cell>MSE</cell><cell>MAE</cell></row><row><cell></cell><cell></cell><cell cols="4">PredCNN [12] 0.1315 1.2808</cell></row><row><cell></cell><cell></cell><cell cols="3">S-TE [61]</cell><cell>0.1289 1.1150</cell></row><row><cell></cell><cell></cell><cell cols="2">PISEP 2</cell><cell></cell><cell>0.1040 0.9379</cell></row><row><cell></cell><cell cols="5">General frame-wise performances</cell></row><row><cell></cell><cell>0.4</cell><cell>PredCNN S-TE</cell><cell></cell><cell></cell></row><row><cell>MSE</cell><cell>0.2 0.3</cell><cell>PISEP 2</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell cols="2">Frames</cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell cols="2">Frames</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our datasets: https://drive.google.com/drive/folders/1bqNyIk2O0NIf5Hv 2sMfwsuPjwbpZK-n5</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Human action recognition and prediction: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.11230</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhanced computer vision with microsoft kinect sensor: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1318" to="1334" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2500" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A deep learning framework for character motion synthesis and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2891" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Actionagnostic human pose forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1423" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Practical parameterization of rotations using the exponential map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Grassia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Graphics Tools</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="29" to="48" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1345" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predcnn: Predictive learning with cascade convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2940" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flexible spatio-temporal networks for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6523" to="6531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to decompose and disentangle representations for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prediction and tracking of moving objects in image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1441" to="1445" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Block-based spatial prediction and transforms based on 2d markov processes for image and video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kamisli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1247" to="1260" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 3rd IAPR Asian Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia &amp; Expo Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint distance maps based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="624" to="628" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Representation learning of temporal dynamics for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3010" to="3022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning clip representations for skeleton-based 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2842" to="2855" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">View-invariant human action recognition based on a 3d bio-constrained skeleton model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1430" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bioinspired dynamic 3d discriminative skeletal features for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Few-shot human motion prediction via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="432" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">G3d: A gaming action dataset and real time action recognition evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">One-step time-dependent future video frame prediction with a convolutional encoder-decoder neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vukotić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="140" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Frequency domain transformer networks for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00271</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Patch to the future: Unsupervised visual prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3302" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal residual networks for citywide crowd flows prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dnn-based prediction model for spatio-temporal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Predrnn++: Towards a resolution of the deep-in-time dilemma in spatiotemporal predictive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dual motion gan for future-flow embedded video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1744" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01523</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Structure preserving video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1460" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep generative video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video pixel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1771" to="1779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep video generation, prediction and completion of human action sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="366" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The pose knows: Video forecasting by generating pose futures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3332" to="3341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Human pose forecasting via deep markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Forecasting human dynamics from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05180</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Forecasting human pose and motion with multibody dynamic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Real-time human motion forecasting using a rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Virtual Reality Software and Technology</title>
		<meeting>the 24th ACM Symposium on Virtual Reality Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Butepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6158" to="6166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adversarial geometry-aware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Long-term human motion prediction by modeling motion context and enhancing motion dynamic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02513</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bio-inspired predictive orientation decomposition of skeleton trajectories for real-time human activity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3053" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
