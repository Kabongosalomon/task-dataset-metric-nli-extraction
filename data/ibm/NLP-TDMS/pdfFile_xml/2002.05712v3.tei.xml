<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Iteration Batch Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
							<email>gaohuang@tsinghua.edu.cnyuecao</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Iteration Batch Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A well-known issue of Batch Normalization is its significantly reduced effectiveness in the case of small mini-batch sizes. When a mini-batch contains few examples, the statistics upon which the normalization is defined cannot be reliably estimated from it during a training iteration. To address this problem, we present Cross-Iteration Batch Normalization (CBN), in which examples from multiple recent iterations are jointly utilized to enhance estimation quality. A challenge of computing statistics over multiple iterations is that the network activations from different iterations are not comparable to each other due to changes in network weights. We thus compensate for the network weight changes via a proposed technique based on Taylor polynomials, so that the statistics can be accurately estimated and batch normalization can be effectively applied. On object detection and image classification with small mini-batch sizes, CBN is found to outperform the original batch normalization and a direct calculation of statistics over previous iterations without the proposed compensation technique. Code is available at https://github.com/ Howal/Cross-iterationBatchNorm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Batch Normalization (BN) <ref type="bibr" target="#b9">[10]</ref> has played a significant role in the success of deep neural networks. It was introduced to address the issue of internal covariate shift, where the distribution of network activations changes during training iterations due to the updates of network parameters. This shift is commonly believed to be disruptive to network training, and BN alleviates this problem through normalization of the network activations by their mean and variance, computed over the examples within the mini-batch at each iteration. With this normalization, network training can be performed at much higher learning rates and with less sensitivity to weight initialization.</p><p>In BN, it is assumed that the distribution statistics for the examples within each mini-batch reflect the statistics over the full training set. While this assumption is generally valid for large batch sizes, it breaks down in the small batch size regime <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b8">9]</ref>, where noisy statistics computed from small sets of examples can lead to a dramatic drop in performance. This problem hinders the application of BN to memory-consuming tasks such as object detection <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b2">3]</ref>, semantic segmentation <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b1">2]</ref> and action recognition <ref type="bibr" target="#b30">[30]</ref>, where batch sizes are limited due to memory constraints.</p><p>Towards improving estimation of statistics in the small batch size regime, alternative normalizers have been proposed. Several of them, including Layer Normalization (LN) <ref type="bibr" target="#b0">[1]</ref>, Instance Normalization (IN) <ref type="bibr" target="#b28">[28]</ref>, and Group Normalization (GN) <ref type="bibr" target="#b31">[31]</ref>, compute the mean and variance over the channel dimension, independent of batch size. Different channel-wise normalization techniques, however, tend to be suitable for different tasks, depending on the set of channels involved. Although GN is designed for detection task, the slow inference speed limits its practical usage. On the other hand, synchronized BN (SyncBN) <ref type="bibr" target="#b18">[18]</ref> yields consistent improvements by processing larger batch sizes across multiple GPUs. These gains in performance come at the cost of additional overhead needed for synchronization across the devices.</p><p>A seldom explored direction for estimating better statistics is to compute them over the examples from multiple recent training iterations, instead of from only the current iteration as done in previous techniques. This can substantially enlarge the pool of data from which the mean and variance are obtained. However, there exists an obvious drawback to this approach, in that the activation values from different iterations are not comparable to each other due to the changes in network weights. As shown in <ref type="figure">Figure 1</ref>, directly calculating the statistics over multiple iterations, which we refer to as Naive CBN, results in lower accuracy.</p><p>In this paper, we present a method that compensates for the network weight changes among iterations, so that examples from preceding iterations can be effectively used to improve batch normalization. Our method, called Cross-Iteration Batch Normalization (CBN), is motivated by the  observation that network weights change gradually, instead of abruptly, between consecutive training iterations, thanks to the iterative nature of Stochastic Gradient Descent (SGD). As a result, the mean and variance of examples from recent iterations can be well approximated for the current network weights via a low-order Taylor polynomial, defined on gradients of the statistics with respect to the network weights. The compensated means and variances from multiple recent iterations are averaged with those of the current iteration to produce better estimates of the statistics. In the small batch size regime, CBN leads to appreciable performance improvements over the original BN, as exhibited in <ref type="figure">Figure 1</ref>. The superiority of our proposed approach is further demonstrated through more extensive experiments on ImageNet classification and object detection on COCO. These gains are obtained with negligible overhead, as the statistics from previous iterations have already been computed and Taylor polynomials are simple to evaluate. With this work, it is shown that cues for batch normalization can successfully be extracted along the time dimension, opening a new direction for investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The importance of normalization in training neural networks has been recognized for decades <ref type="bibr" target="#b10">[11]</ref>. In general, normalization can be performed on three components: input data, hidden activations, and network parameters. Among them, input data normalization is used most commonly be-cause of its simplicity and effectiveness <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>After the introduction of Batch Normalization <ref type="bibr" target="#b9">[10]</ref>, the normalization of activations has become nearly as prevalent. By normalizing hidden activations by their statistics within each mini-batch, BN effectively alleviates the vanishing gradient problem and significantly speeds up the training of deep networks. To mitigate the mini-batch size dependency of BN, a number of variants have been proposed, including Layer Normalization (LN) <ref type="bibr" target="#b0">[1]</ref>, Instance Normalization (IN) <ref type="bibr" target="#b28">[28]</ref>, Group Normalization (GN) <ref type="bibr" target="#b31">[31]</ref>, and Batch Instance Normalization (BIN) <ref type="bibr" target="#b17">[17]</ref>. The motivation of LN is to explore more suitable statistics for sequential models, while IN performs normalization in a manner similar to BN but with statistics only for each instance. GN achieves a balance between IN and LN, by dividing features into multiple groups along the channel dimension and computing the mean and variance within each group for normalization. BIN introduces a learnable method for automatically switching between normalizing and maintaining style information, enjoying the advantages of both BN and IN on style transfer tasks. Cross-GPU Batch Normalization (CGBN or SyncBN) <ref type="bibr" target="#b18">[18]</ref> extends BN across multiple GPUs for the purpose of increasing the effective batch size. Though providing higher accuracy, it introduces synchronization overhead to the training process. Kalman Normalization (KN) <ref type="bibr" target="#b29">[29]</ref> presents a Kalman filtering procedure for estimating the statistics for a network layer from the layer's observed statistics and the computed statistics of previous layers.</p><p>Batch Renormalization (BRN) <ref type="bibr" target="#b8">[9]</ref> is the first attempt to utilize the statistics of recent iterations for normalization. It does not compensate for the statistics from recent iterations, but rather it down-weights the importance of statistics from distant iterations. This down-weighting heuristic, however, does not make the resulting statistics "correct", as the statistics from recent iterations are not of the current network weights. BRN can be deemed as a special version of our Naive CBN baseline (without Taylor polynomial approximation), where distant iterations are down-weighted.</p><p>Recent work have also investigated the normalization of network parameters. In Weight Normalization (WN) <ref type="bibr" target="#b22">[22]</ref>, the optimization of network weights is improved through a reparameterization of weight vectors into their length and direction. Weight Standardization (WS) <ref type="bibr" target="#b19">[19]</ref> instead reparameterizes weights based on their first and second moments for the purpose of smoothing the loss landscape of the optimization problem. To combine the advantages of multiple normalization techniques, Switchable Normalization (SN) <ref type="bibr" target="#b16">[16]</ref> and Sparse Switchable Normalization (SSN) <ref type="bibr" target="#b24">[24]</ref> make use of differentiable learning to switch among different normalization methods.</p><p>The proposed CBN takes an activation normalization approach that aims to mitigate the mini-batch dependency of BN. Different from existing techniques, it provides a way to effectively aggregate statistics across multiple training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting Batch Normalization</head><p>The original batch normalization (BN) <ref type="bibr" target="#b9">[10]</ref> whitens the activations of each layer by the statistics computed within a mini-batch. Denote θ t and x t,i (θ t ) as the network weights and the feature response of a certain layer for the i-th example in the t-th mini-batch. With these values, BN conducts the following normalization:</p><formula xml:id="formula_0">x t,i (θ t ) = x t,i (θ t ) − µ t (θ t ) σ t (θ t ) 2 + ,<label>(1)</label></formula><p>wherex t,i (θ t ) is the whitened activation with zero mean and unit variance, is a small constant added for numerical stability, and µ t (θ t ) and σ t (θ t ) are the mean and variance computed for all the examples from the current mini-batch, i.e.,</p><formula xml:id="formula_1">µ t (θ t ) = 1 m m i=1 x t,i (θ t ),<label>(2)</label></formula><formula xml:id="formula_2">σ t (θ t ) = 1 m m i=1 (x t,i (θ t ) − µ t (θ t )) 2 = ν t (θ t ) − µ t (θ t ) 2 ,<label>(3)</label></formula><formula xml:id="formula_3">where ν t (θ t ) = 1 m m i=1 x t,i (θ t ) 2</formula><p>, and m denotes the number of examples in the current mini-batch. The whitened activationx t,i (θ t ) further undergoes a linear transform with learnable weights, to increase its expressive power:</p><formula xml:id="formula_4">y t,i (θ t ) = γx t,i (θ t ) + β,<label>(4)</label></formula><p>where γ and β are the learnable parameters (initialized to γ = 1 and β = 0 in this work). When the batch size m is small, the statistics µ t (θ t ) and σ t (θ t ) become noisy estimates of the training set statistics, thus degrading the effects of batch normalization. In the ImageNet classification task for which the BN module was originally designed, a batch size of 32 is typical. However, for other tasks requiring larger models and/or higher image resolution, such as object detection, semantic segmentation and video recognition, the typical batch size may be as small as 1 or 2 due to GPU memory limitations. The original BN becomes considerably less effective in such cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Leveraging Statistics from Previous Iterations</head><p>To address the issue of BN with small mini-batches, a naive approach is to compute the mean and variance over the current and previous iterations. However, the statistics µ t−τ (θ t−τ ) and ν t−τ (θ t−τ ) of the (t − τ )-th iteration are computed under the network weights θ t−τ , making them obsolete for the current iteration. As a consequence, directly aggregating statistics from multiple iterations produces inaccurate estimates of the mean and variance, leading to significantly worse performance.</p><p>We observe that the network weights change smoothly between consecutive iterations, due to the nature of gradient-based training.</p><p>This allows us to approxi-</p><formula xml:id="formula_5">mate µ t−τ (θ t ) and ν t−τ (θ t ) from the readily available µ t−τ (θ t−τ ) and ν t−τ (θ t−τ ) via a Taylor polynomial, i.e., µ t−τ (θ t ) =µ t−τ (θ t−τ ) + ∂µ t−τ (θ t−τ ) ∂θ t−τ (θ t − θ t−τ ) + O(||θ t − θ t−τ || 2 ),<label>(5)</label></formula><formula xml:id="formula_6">ν t−τ (θ t ) =ν t−τ (θ t−τ ) + ∂ν t−τ (θ t−τ ) ∂θ t−τ (θ t − θ t−τ ) + O(||θ t − θ t−τ || 2 ),<label>(6)</label></formula><p>where ∂µ t−τ (θ t−τ )/∂θ t−τ and ∂ν t−τ (θ t−τ )/∂θ t−τ are gradients of the statistics with respect to the network weights, and O(||θ t − θ t−τ || 2 ) denotes higher-order terms of the Taylor polynomial, which can be omitted since the first-order term dominates when (θ t − θ t−τ ) is small. In Eq. (5) and Eq. <ref type="formula" target="#formula_6">(6)</ref>, the gradients ∂µ t−τ (θ t−τ )/∂θ t−τ and ∂ν t−τ (θ t−τ )/∂θ t−τ cannot be precisely determined at a negligible cost because the statistics µ l t−τ (θ t−τ ) and ν l t−τ (θ t−τ ) for a node at the l-th network layer depend on all the network weights prior to the l-th layer, i.e., ∂µ l t−τ (θ t−τ )/∂θ r t−τ = 0 and ∂ν l t−τ (θ t−τ )/∂θ r t−τ = 0 for r ≤ l, where θ r t−τ denotes the network weights at the r-th layer. Only when r = l can these gradients be derived in closed form efficiently.</p><p>Empirically, we find that as the layer index r decreases (r ≤ l), the partial gradients ∂µ l t (θt) θ r t and ∂ν l t (θt) θ r t rapidly diminish. These reduced effects of network weight changes at earlier layers on the activation distributions in later layers may perhaps be explained by the reduced internal covariate shift of BN. Motivated by this phenomenon, which is studied in Appendix C, we propose to truncate these partial gradients at layer l.</p><p>Thus, we further approximate Eq. (5) and Eq. <ref type="formula" target="#formula_6">(6)</ref> by</p><formula xml:id="formula_7">µ l t−τ (θt) ≈ µ l t−τ (θt−τ ) + ∂µ l t−τ (θt−τ ) ∂θ l t−τ (θ l t − θ l t−τ ),<label>(7)</label></formula><formula xml:id="formula_8">ν l t−τ (θt) ≈ ν l t−τ (θt−τ ) + ∂ν l t−τ (θt−τ ) ∂θ l t−τ (θ l t − θ l t−τ ). (8) A naive implementation of ∂µ l t−τ (θ t−τ )/∂θ l t−τ and ∂ν l t−τ (θ t−τ )/∂θ l t−τ involves computational overhead of O(C out × C out × C in × K),</formula><p>where C out and C in denote the output and input channel dimension of the l-th layer, respectively, and K denotes the kernel size of θ l t−τ . Here,</p><formula xml:id="formula_9">Iteration t-2 CBN mean, variance ( , ) μ t − 2 δ t − 2 Compensated mean, variance ( , ) μ ¯ t − 2 δ ¯ t − 2</formula><p>Normalize, Affine transform Normalize, Affine transform</p><formula xml:id="formula_10">BN Iteration t-1 mean, variance ( , ) μ t − 1 δ t − 1 Compensated mean, variance ( , ) μ ¯ t − 1 δ ¯ t − 1</formula><p>Normalize, Affine transform we find that the operation can be implemented efficiently in O(C out × C in × K), thanks to the averaging over feature responses of µ and ν. See Appendix B for the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-Iteration Batch Normalization</head><p>After compensating for network weight changes, we aggregate the statistics of the k − 1 most recent iterations with those of the current iteration t to obtain the statistics used in CBN:</p><formula xml:id="formula_11">µ l t,k (θ t ) = 1 k k−1 τ =0 µ l t−τ (θ t ),<label>(9)</label></formula><formula xml:id="formula_12">ν l t,k (θ t ) = 1 k k−1 τ =0 max ν l t−τ (θ t ), µ l t−τ (θ t ) 2 ,<label>(10)</label></formula><formula xml:id="formula_13">σ l t,k (θ t ) = ν l t,k (θ t ) −μ l t,k (θ t ) 2 ,<label>(11)</label></formula><p>where µ l t−τ (θ t ) and ν l t−τ (θ t ) are computed from Eq. <ref type="formula" target="#formula_7">(7)</ref> and Eq. (8). In Eq.</p><formula xml:id="formula_14">(10),ν l t,k (θ t ) is determined from the maximum of ν l t−τ (θ t ) and µ l t−τ (θ t ) 2 in each iteration be- cause ν l t−τ (θ t ) ≥ µ l t−τ (θ t ) 2</formula><p>should hold for valid statistics but may be violated by Taylor polynomial approximations in Eq. <ref type="bibr" target="#b6">(7)</ref> and Eq. <ref type="bibr" target="#b7">(8)</ref>. Finally,μ l t,k (θ t ) andσ l t,k (θ t ) are applied to normalize the corresponding feature responses</p><formula xml:id="formula_15">{x l t,i (θ t )} m i=1</formula><p>at the current iteration:</p><formula xml:id="formula_16">x l t,i (θ t ) = x l t,i (θ t ) −μ l t,k (θ t ) σ l t,k (θ t ) 2 + .<label>(12)</label></formula><p>With CBN, the effective number of examples used to compute the statistics for the current iteration is k times as large as that for the original BN. In training, the loss gradients are backpropagated to the network weights and activations at the current iteration, i.e., θ l t and x l t,i (θ t ). Those of the previous iterations are fixed and do not receive gradients. Hence, the computation cost of CBN in backpropagation is the same as that of BN.</p><p>Replacing the BN modules in a network by CBN leads to only minor increases in computational overhead and memory footprint. For computation, the additional overhead mainly comes from computing the partial derivatives ∂µ t−τ (θ t−τ )/∂θ l t−τ and ∂ν t−τ (θ t−τ )/∂θ l t−τ , which is insignificant in relation to the overhead of the whole network. For memory, the module requires access to the statistics (</p><formula xml:id="formula_17">{µ l t−τ (θ t−τ )} k−1 τ =1 and {ν l t−τ (θ t−τ )} k−1 τ =1 ) and the gradients ({∂µ t−τ (θ t−τ )/∂θ l t−τ } k−1 τ =1 and {∂ν t−τ (θ t−τ )/∂θ l t−τ } k−1 τ =1</formula><p>) computed for the most recent k − 1 iterations, which is also minor compared to the rest of the memory consumed in processing the input examples. The additional computation and memory of CBN is reported for our experiments in <ref type="table">Table 8</ref>.</p><p>A key hyper-parameter in the proposed CBN is the temporal window size, k, of recent iterations used for statistics estimation. A broader window enlarges the set of examples, but the example quality becomes increasingly lower for more distant iterations, since the differences in network parameters θ t and θ t−τ become more significant and are compensated less well using a low-order Taylor polynomial. Empirically, we found that CBN is effective with a window size up to k = 8 in a variety of settings and tasks. The only trick is that the window size should be kept small at the beginning of training, when the network weights change quickly. Thus, we introduce a burn-in period of length T burn-in for the window size, where k = 1 and CBN degenerates to the original BN. In our experiments, the burn-in period is set to <ref type="bibr" target="#b25">25</ref>  default. <ref type="table">Table 1</ref> compares CBN with other feature normalization methods. The key difference among these approaches is the axis along which the statistics are counted and the features are normalized. The previous techniques are all designed to exploit examples from the same iteration. By contrast, CBN explores the aggregation of examples along the temporal dimension. As the data utilized by CBN lies in a direction orthogonal to that of previous methods, the proposed CBN could potentially be combined with other feature normalization approaches to further enhance statistics estimation in certain challenging applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Classification on ImageNet</head><p>Experimental settings. ImageNet <ref type="bibr" target="#b21">[21]</ref> is a benchmark dataset for image classification, containing 1.28M training images and 50K validation images from 1000 classes. We follow the standard setting in <ref type="bibr" target="#b4">[5]</ref> to train deep networks on the training set and report the single-crop top-1 accuracy on the validation set. Our preprocessing and augmentation strategy strictly follows the GN baseline <ref type="bibr" target="#b31">[31]</ref>. We use a weight decay of 0.0001 for all weight layers, including γ and β. We train standard ResNet-18 for 100 epochs on 4 GPUs, and decrease the learning rate by the cosine decay strategy <ref type="bibr" target="#b6">[7]</ref>. We perform the experiments for five trials, and report their mean and standard deviation (error bar). ResNet-18 with BN is our base model. To compare with other normalization methods, we directly replace BN with IN, LN, GN, BRN, and our proposed CBN.</p><p>Comparison of feature normalization methods. In Table 2, we compare the performance of each normalization method with a batch size, 32, sufficient for computing reliable statistics. Under this setting, BN clearly yields the highest top-1 accuracy. Similar to results found in previous works <ref type="bibr" target="#b31">[31]</ref>, the performance of IN and LN is significantly worse than that of BN. GN works well on image classification but falls short of BN by 1.2%. Among all the methods, our CBN is the only one that is able to achieve accuracy comparable to BN, as it converges to the procedure of BN at larger batch sizes.</p><p>Sensitivity to batch size. We compare the behavior of CBN, original BN <ref type="bibr" target="#b9">[10]</ref>, GN <ref type="bibr" target="#b31">[31]</ref>, and BRN <ref type="bibr" target="#b8">[9]</ref>  number of images per GPU on ImageNet classification. For CBN, the recent iterations are utilized so as to ensure that the number of effective examples is no fewer than 16. For BRN, the settings strictly follow the original paper. We adopt a learning rate of 0.1 for the batch size of 32, and linearly scale the learning rate by N/32 for a batch size of N .</p><p>The results are shown in <ref type="table">Table 3</ref>. For the original BN, its accuracy drops noticeably as the number of images per GPU is reduced from 32 to 2. BRN suffers a significant performance drop as well. GN maintains its accuracy by utilizing the channel dimension but not batch dimension. For CBN, its accuracy holds by exploiting the examples of recent iterations. Also, CBN outperforms GN by 0.9% on average top-1 accuracy with different batch sizes. This is reasonable, because the statistics computation of CBN introduces uncertainty caused by the stochastic batch sampling like in BN, but this uncertainty is missing in GN which results in some loss of regularization ability. For the extreme case that the number of images per GPU is 1 <ref type="figure">, BN and BRN</ref>   <ref type="table">Table 6</ref>. Results with stronger backbones on COCO object detection and instance segmentation. and GN using different network architectures. The results are shown in <ref type="table">Table 4</ref>.1. We choose five typres of architectures, i.e., ResNet-50 <ref type="bibr" target="#b5">[6]</ref>, VGG-16 <ref type="bibr" target="#b25">[25]</ref>, Inception-v3 <ref type="bibr" target="#b27">[27]</ref>, DenseNet-121 <ref type="bibr" target="#b7">[8]</ref>, and MobileNet-v2 <ref type="bibr" target="#b23">[23]</ref>. This set of architectures represents the majority of modern network choices for computer vision tasks. BN (bs=32) is the ideal upper bound of this experiment. All the other three normalization methods are trained with a batch size of 4. BN (bs=4) clearly suffers from the limitations of small batchsize regime. Also, GN leads to a about a 0.5% performance drop. Our CBN is the only one that obtains results comparable to BN with large batch size. These results demonstrate that our proposed CBN can be used in most modern convolutional neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection and Instance Segmentation on COCO</head><p>Experimental settings. COCO <ref type="bibr" target="#b13">[14]</ref> is chosen as the benchmark for object detection and instance segmentation. Models are trained on the COCO 2017 train split with 118k images, and evaluated on the COCO 2017 validation split with 5k images. Following the standard protocol in <ref type="bibr" target="#b13">[14]</ref>, the object detection and instance segmentation accuracies are measured by the mean average precision (mAP) scores at different intersection-over-union (IoU) overlaps at the box and the mask levels, respectively.</p><p>Following <ref type="bibr" target="#b31">[31]</ref>, Faster R-CNN <ref type="bibr" target="#b20">[20]</ref> and Mask R-CNN <ref type="bibr" target="#b3">[4]</ref> with FPN <ref type="bibr" target="#b12">[13]</ref> are chosen as the baselines for object detection and instance segmentation, respectively. For both, the 2fc box head is replaced by a 4conv1fc head for better use of the normalization mechanism <ref type="bibr" target="#b31">[31]</ref>. The backbone networks are ImageNet pretrained ResNet-50 (default) or ResNet-101, with specific normalization. Finetuning is performed on the COCO train set for 12 epochs on 4 GPUs by SGD, where each GPU processes 4 images (default). Note that the mean and variance statistics in CBN are computed within each GPU. The learning rate is initialized to be 0.02 * N/16 for a batch size per GPU of N , and is decayed by a factor of 10 at the 9-th and the 11-th epochs. The weight decay and momentum parameters are set to 0.0001 and 0.9, respectively. We use the average over 5 trials for all results. As the values of standard deviation of all methods are less than 0.1 on COCO, they are ignored here.</p><p>As done in <ref type="bibr" target="#b31">[31]</ref>, we experiment with two settings where the normalizers are activated only at the task-specific heads with frozen BN at the backbone (default), or the normalizers are activated at all the layers except for the early conv1 and conv2 stages in ResNet.</p><p>Normalizers at backbone and task-specific heads. We further study the effect of different normalizers on the backbone network and task-specific heads for object detection on COCO. CBN, original BN, syncBN, and GN are in-cluded in the comparison. For BRN, it is unclear <ref type="bibr" target="#b16">[16]</ref> how to apply it in tasks like object detection. Directly replacing BN with BRN leads to 0.3% performance drop on AP bbox score. <ref type="table" target="#tab_3">Table 5</ref> presents the results. When BN is frozen in the backbone and no normalizer is applied at the head, the AP bbox score is 36.9%. When the original BN is applied at the head only and at both the backbone and the head, the accuracy drops to 36.3% and 35.5%, respectively. For CBN, the accuracy is 37.7% and 37.7% at these two settings, respectively. Without any synchronization across GPUs, CBN can achieve performance on par with syncBN and GN, showing the superiority of the proposed approach.</p><p>Instance segmentation and stronger backbones. Results of object detection (Faster R-CNN <ref type="bibr" target="#b20">[20]</ref>) and instance segmentation (Mask R-CNN <ref type="bibr" target="#b3">[4]</ref>) with ResNet-50 and ResNet-101 are presented in <ref type="table">Table 6</ref>. We can observe that our CBN achieves performance comparable to syncBN and GN with R50 and R101 as the backbone on both Faster R-CNN and Mask R-CNN, which demonstrates that CBN is robust and versatile to various deep models and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Effect of temporal window size k. We conduct this ablation on ImageNet image classification and COCO object detection, with each GPU processing 4 images. <ref type="figure" target="#fig_2">Figure 3</ref> presents the results. When k = 1, only the batch from the current iteration is utilized; therefore, CBN degenerates to the original BN. The accuracy suffers due to the noisy statistics on small batch sizes. As the window size k increases, more examples from recent iterations are utilized for statistics estimation, leading to greater accuracy. Accuracy saturates at k = 8 and even drops slightly. For more distant iterations, the network weights differ more substantially and Taylor polynomial approximation becomes less accurate.</p><p>On the other hand, it is empirically observed that the original BN saturates at a batch size of 16 or 32 for numerous applications <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b31">31]</ref>, indicating that the computed statistics become accurate. Thus, a temporal window size of k = min( 16 bs per GPU , 8) is suggested. Effect of compensation. To study this, we compare CBN to 1) Naive CBN, where statistics from recent iterations are directly aggregated without compensation via Taylor polynomial; and 2) the original BN applied with the same effective example number as CBN (i.e., its batch size per GPU is set to the product of batch size per GPU and temporal window size of CBN), which does not require any compensation and serves as an upper performance bound.</p><p>The experimental results are also presented in <ref type="figure" target="#fig_2">Figure 3</ref>. CBN clearly surpasses Naive CBN when the previous iterations are included. Actually, Naive CBN fails when the temporal window size grows to k = 8 as shown in <ref type="figure" target="#fig_2">Figure 3(a</ref>    <ref type="figure" target="#fig_3">Figure 4</ref> presents the train and test curves of CBN, Naive CBN, BN-bs4, and BN-bs16 on ImageNet, with 4 images per GPU and a temporal window size of 4 for CBN, Naive CBN, and BN-bs4, and 16 images per GPU for BN-bs16. The train curve of CBN is close to BN-bs4 at the beginning, and approaches BN-bs16 at the end. The reason is that we adopt a burn-in period to avoid the disadvantage of rapid statistics change at the beginning of training. The gap between the train curve of Naive CBN and CBN shows that Naive CBN cannot even converge well on the training set. The test curve of CBN is close to BN-bs16 at the end, while Naive CBN exhibits considerable jitter. All these phenomena indicate the effectiveness of our proposed Taylor polynomial compensation.</p><p>Effect of burn-in period length T . We study the effect of varying the burn-in period length T burn-in , at 4 images per GPU on both ImageNet image classification (ResNet-18) and COCO object detection (Faster R-CNN with FPN and ResNet-50). <ref type="figure" target="#fig_4">Figure 5</ref>(a) and 5(b) present the results. When the burn-in period is too short, the accuracy suffers. This is because at the beginning of training, the network weights change rapidly, causing the compensation across iterations Epoch-8 Epoch-9 Epoch-10 Epoch-11 AP bbox 37.7 37.7 37.6 37.3 <ref type="table">Table 7</ref>. Results on switching from BN to syncBN at different epochs on COCO.</p><p>to be less effective. When the burn-in period is too long, i.e., CBN is involved too late and the overall performance drops to the BN baseline.</p><p>An interesting observation is that the accuracy is stable for a wide range of burn-in periods T burn-in . This leads to a question of whether BN in the small batch-size regime only suffers in terms of generalization performance in later stages of training. For further exploration, we design an experiment to remove other influences: we first train the model on COCO with standard BN and a small batch size, then switch BN to syncBN. We present the experimental results in <ref type="table">Table 7</ref>. Results show that syncBN works similarly to CBN, which further verifies the high performance of CBN. It also supports our assumption that BN in the small batch-size regime only suffers in terms of generalization performance in later stages of training, which may shed some light on the small batch-size regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis</head><p>Computational cost, memory footprint, and training/inference speed. We examine the computational cost, memory footprint, and the training and inference speed of BN, GN and CBN in a practical COCO object detection task using R50-Mask R-CNN, shown in <ref type="table">Table 8</ref>. The batch size per GPU and window size of CBN are set to 4.</p><p>Compared to BN and GN, CBN consumes about 7% extra memory and 11% more computational cost. The extra memory mainly contains the statistics (µ and ν), their respective gradients, and the network parameters (θ t−1 · · · θ t−(k−1) ) of previous iterations, while the computational cost comes from calculations of the statistics' respective gradients, Taylor compensations, and averaging operations.</p><p>The overall training speed of CBN is close to both BN and GN. It is worth noting that the inference speed of CBN is equal to BN, which is much faster than GN. The inference stage of CBN is the same as that of BN, where pre-recorded statistics can be used instead of online statistics calculation. From these results, the additional overhead of CBN is seen to be minor. Also, merging BN/CBN into convolution in inference <ref type="bibr" target="#b11">[12]</ref> could be utilized for further speedup.</p><p>Using second order statistics for compensation. Results of CBN with different orders of Taylor expansion (batch size = 4, #iterations for approximation = 3) are shown in <ref type="table">Table 9</ref>. By directly using the statistics of recent iterations without compensation, Naive CBN outperforms BN with batch size 4 by 1.7% in accuracy. Via compensating the statistics of recent iterations with a first-order Taylor expansion, CBN <ref type="bibr" target="#b0">(1)</ref> can further improve the accuracy by 3.2% compared to Naive CBN. However, CBN (2) using a second-order approximation does not achieve better performance than CBN <ref type="bibr" target="#b0">(1)</ref> . This may be because CBN <ref type="bibr" target="#b0">(1)</ref> already achieves performance comparable to BN with large batch size, which serves as the upper bound of our approach, indicating that a first-order approximation is enough for image classification on ImageNet. Therefore, first-order compensation for CBN is adopted by default.</p><p>Using more than one layer for compensation. We also study the influence of applying compensation over more than one layer. CBN using two layers for compensation achieves 70.1 on ImageNet (batch size per GPU=4, k=4), which is comparable to CBN using only one layer. However, the efficient implementation can no longer be used when more than one layer of compensation is employed. As using more layers does not further improve performance but consumes more FLOPs, we adopt one-layer compensation for CBN in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In the small batch size regime, batch normalization is widely known to drop dramatically in performance. To address this issue, we propose to enhance the quality of statistics via utilizing examples from multiple recent iterations. As the network activations from different iterations are not comparable to each other due to changes in network weights, we compensate for the network weight changes based on Taylor polynomials, so that the statistics can be accurately estimated. In the experiments, the proposed ap-proach is found to outperform original batch normalization and a direct calculation of statistics over previous iterations without compensation. Moreover, it achieves performance on par with SyncBN, which can be regarded as the upper bound, on both ImageNet and COCO object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of BN and the proposed Cross-Iteration Batch Normalization (CBN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The effect of temporal window size (k) on ImageNet (ResNet-18) and COCO (Faster R-CNN with ResNet-50 and FPN) with #bs/GPU = 4 for CBN and Naive CBN. Naive CBN directly utilizes statistics from recent iterations, while BN uses the equivalent #examples as CBN for statistics computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Training and test curves for CBN, Naive CBN, and BN on ImageNet, with batch size per GPU of 4 and temporal window size k = 4 for CBN, Naive CBN, and BN-bs4, and batch size per GPU of 16 for BN-bs16. The plot of BN-bs16 is the ideal bound. network weights over iterations. Compared with the original BN upper bound, CBN achieves similar accuracy at the same effective example number. This result indicates that the compensation using a low-order Taylor polynomial by CBN is effective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Results of different burn-in periods (in epochs) on CBN, with batch size per iteration of 4, on ImageNet and COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>#GPU #bs/GPU * temporal window (batch, spatial, iteration)Table 1. Comparison of different feature normalization methods. #bs/GPU denotes batch size per GPU.</figDesc><table><row><cell></cell><cell>batch size per iter</cell><cell>#examples for statistics</cell><cell>Norm axis</cell></row><row><cell>IN</cell><cell>#bs/GPU * #GPU</cell><cell>1</cell><cell>(spatial)</cell></row><row><cell>LN</cell><cell>#bs/GPU * #GPU</cell><cell>1</cell><cell>(channel, spatial)</cell></row><row><cell>GN</cell><cell>#bs/GPU * #GPU</cell><cell>1</cell><cell>(channel group, spatial)</cell></row><row><cell>BN</cell><cell>#bs/GPU * #GPU</cell><cell>#bs/GPU</cell><cell>(batch, spatial)</cell></row><row><cell cols="2">syncBN #bs/GPU * #GPU</cell><cell>#bs/GPU * #GPU</cell><cell>(batch, spatial, GPU)</cell></row><row><cell>CBN</cell><cell>#bs/GPU *</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>epochs on ImageNet image</cell></row><row><cell></cell><cell></cell><cell cols="2">classification and 3 epochs on COCO object detection by</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Top-1 accuracy of normalization methods with different batch sizes using ResNet-18 as the base model on ImageNet. Top-1 accuracy of normalization methods with different network architectures on ImageNet.</figDesc><table><row><cell>at the same</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>fails to produce results, while CBN outperforms GN by 0.4% on top-1 accuracy in this case.Different network architectures. To verify the generalization ability of CBN, we also compared CBN to BN backbone box head AP bbox AP bbox Results of feature normalization methods on Faster R-CNN with FPN and ResNet50 on COCO. As the values of standard deviation of all methods are less than 0.1 on COCO, we ignore them here.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>AP bbox 75</cell><cell>AP bbox S</cell><cell>AP bbox M</cell><cell>AP bbox L</cell></row><row><cell cols="2">fixed BN</cell><cell>-</cell><cell>36.9</cell><cell>58.2</cell><cell>39.9</cell><cell>21.2</cell><cell>40.8</cell><cell>46.9</cell></row><row><cell cols="2">fixed BN</cell><cell>BN</cell><cell>36.3</cell><cell>57.3</cell><cell>39.2</cell><cell>20.8</cell><cell>39.7</cell><cell>47.3</cell></row><row><cell cols="2">fixed BN</cell><cell>syncBN</cell><cell>37.7</cell><cell>58.5</cell><cell>41.1</cell><cell>22.0</cell><cell>40.9</cell><cell>49.0</cell></row><row><cell cols="2">fixed BN</cell><cell>GN</cell><cell>37.8</cell><cell>59.0</cell><cell>40.8</cell><cell>22.3</cell><cell>41.2</cell><cell>48.4</cell></row><row><cell cols="2">fixed BN</cell><cell>BRN</cell><cell>37.4</cell><cell>58.1</cell><cell>40.3</cell><cell>22.0</cell><cell>40.7</cell><cell>48.3</cell></row><row><cell cols="2">fixed BN</cell><cell>CBN</cell><cell>37.7</cell><cell>59.0</cell><cell>40.7</cell><cell>22.1</cell><cell>40.9</cell><cell>48.8</cell></row><row><cell></cell><cell>BN</cell><cell>BN</cell><cell>35.5</cell><cell>56.4</cell><cell>38.7</cell><cell>19.7</cell><cell>38.8</cell><cell>47.3</cell></row><row><cell cols="2">syncBN</cell><cell>syncBN</cell><cell>37.9</cell><cell>58.5</cell><cell>41.1</cell><cell>21.7</cell><cell>41.5</cell><cell>49.7</cell></row><row><cell></cell><cell>GN</cell><cell>GN</cell><cell>37.8</cell><cell>59.1</cell><cell>40.9</cell><cell>22.4</cell><cell>41.2</cell><cell>49.0</cell></row><row><cell></cell><cell>CBN</cell><cell>CBN</cell><cell>37.7</cell><cell>58.9</cell><cell>40.6</cell><cell>22.0</cell><cell>41.4</cell><cell>48.9</cell></row><row><cell>Backbone</cell><cell cols="2">method</cell><cell>norm</cell><cell cols="2">AP bbox AP bbox 50</cell><cell>AP bbox 75</cell><cell>AP bbox S</cell><cell>AP bbox M</cell><cell>AP bbox L</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GN</cell><cell>37.8</cell><cell>59.0</cell><cell>40.8</cell><cell>22.3</cell><cell>41.2</cell><cell>48.4</cell></row><row><cell>R50+FPN</cell><cell cols="2">Faster R-CNN</cell><cell>syncBN</cell><cell>37.7</cell><cell>58.5</cell><cell>41.1</cell><cell>22.0</cell><cell>40.9</cell><cell>49.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CBN</cell><cell>37.7</cell><cell>59.0</cell><cell>40.7</cell><cell>22.1</cell><cell>40.9</cell><cell>48.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GN</cell><cell>39.3</cell><cell>60.6</cell><cell>42.7</cell><cell>22.5</cell><cell>42.5</cell><cell>51.3</cell></row><row><cell cols="3">R101+FPN Faster R-CNN</cell><cell>syncBN</cell><cell>39.2</cell><cell>59.8</cell><cell>43.0</cell><cell>22.2</cell><cell>42.9</cell><cell>51.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CBN</cell><cell>39.2</cell><cell>60.0</cell><cell>42.6</cell><cell>22.3</cell><cell>42.6</cell><cell>51.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AP bbox AP bbox 50</cell><cell>AP bbox 75</cell><cell cols="2">AP mask AP mask 50</cell><cell>AP mask 75</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GN</cell><cell>38.6</cell><cell>59.8</cell><cell>41.9</cell><cell>35.0</cell><cell>56.7</cell><cell>37.3</cell></row><row><cell>R50+FPN</cell><cell cols="2">Mask R-CNN</cell><cell>syncBN</cell><cell>38.5</cell><cell>58.9</cell><cell>42.3</cell><cell>34.7</cell><cell>56.3</cell><cell>36.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CBN</cell><cell>38.5</cell><cell>59.2</cell><cell>42.1</cell><cell>34.6</cell><cell>56.4</cell><cell>36.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GN</cell><cell>40.3</cell><cell>61.2</cell><cell>44.2</cell><cell>36.6</cell><cell>58.5</cell><cell>39.2</cell></row><row><cell cols="3">R101+FPN Mask R-CNN</cell><cell>syncBN</cell><cell>40.3</cell><cell>60.8</cell><cell>44.2</cell><cell>36.0</cell><cell>57.7</cell><cell>38.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CBN</cell><cell>40.1</cell><cell>60.5</cell><cell>44.1</cell><cell>35.8</cell><cell>57.3</cell><cell>38.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Comparison of theoretical memory, FLOPs and practical training and inference speed between original BN, GN, and CBN in both training and inference on COCO. Top-1 accuracy of CBN that compensating with different orders and batch size per iter = 4 on ImageNet.</figDesc><table><row><cell></cell><cell>Memory</cell><cell>FLOPs</cell><cell cols="2">Training</cell><cell>Inference</cell></row><row><cell></cell><cell>(GB)</cell><cell>(M)</cell><cell cols="2">Speed (iter/s)</cell><cell>Speed (iter/s)</cell></row><row><cell>BN</cell><cell>14.1</cell><cell>5155.1</cell><cell>1.3</cell><cell></cell><cell>6.2</cell></row><row><cell>GN</cell><cell>14.1</cell><cell>5274.2</cell><cell>1.2</cell><cell></cell><cell>3.7</cell></row><row><cell>CBN</cell><cell>15.1</cell><cell>5809.7</cell><cell>1.0</cell><cell></cell><cell>6.2</cell></row><row><cell></cell><cell cols="5">BN Naive CBN CBN (1) CBN (2)</cell></row><row><cell cols="3">Top-1 acc 65.1</cell><cell>66.8</cell><cell>70.0</cell><cell>70.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* This work is done when Zhuliang Yao is an intern at Microsoft Research Asia. Correspondence to: Yue Cao (yuecao@microsoft.com).</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Cross-Iteration Batch Normalization(CBN)</head><p>Input: Feature responses of a network node of the l-th layer at the t-th iteration</p><p>Let C out and C in denote the output and input channel dimension of the l-th layer, respectively, and K denotes the kernel size of θ l t−τ . µ l t−τ and ν l t−τ are thus of C out dimensions in channels, and</p><p>Here we find that the operations of µ and ν can be implemented efficiently in O(C in × K) and O(C out × C in × K), respectively, thanks to the averaging of feature responses in µ and ν.</p><p>Here we derive the efficient implementation of ∂µ l t−τ (θ t−τ )/∂θ l t−τ . That of ∂ν l t−τ (θ t−τ )/∂θ l t−τ is about the same. Let us first simplify the notations a bit. Let µ l and θ l denote µ l t−τ (θ t−τ ) and θ l t−τ respectively, by removing the irrelevant notations for iterations. The element-wise computation in the forward pass can be computed as</p><p>where µ l j denotes the j-th channel in µ l , and x l i,j denotes the j-th channel in the i-th example. x l i,j is computed as</p><p>where n and k enumerate the input feature dimension and the convolution kernel index, respectively, offset(k) denotes the spatial offset in applying the k-th kernel, and y l−1 is the output of the (l − 1)-th layer. The element-wise calculation of ∂µ l /∂θ l ∈ R Cout×Cout×Cin×K is as follows, taking Eq. (13) and Eq. (14) into consideration: </p><p>Thus, [ ∂µ l ∂θ l ] j,q,p,η takes non-zero values only when j = q. This operation can be implemented efficiently in O(C in × K). Similarly, the calculation of ∂ν l /∂θ l can be obtained in O(C out × C in × K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Observation of the gradients diminishing</head><p>The key assumption in Eq. <ref type="bibr" target="#b6">(7)</ref> and Eq. <ref type="formula">(8)</ref> is that for a node at the l-th layer, the gradient of its statistics with respect to the network weights at the l-th layer is larger than that of weights from the prior layers, i.e., , and || · || F denotes the Frobenius norm. Here, we examine this assumption empirically for networks trained on ImageNet image recognition. Both</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks with dense connectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch renormalization: Towards reducing minibatch dependence in batch-normalized models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade, this book is an outgrowth of a 1996 NIPS workshop</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeprebirth: Accelerating deep neural network execution on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deguang</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Differentiable learning-to-normalize via switchable normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanglin</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10779</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch-instance normalization for adaptively style-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyo-Eun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2563" to="2572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10520</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Weight standardization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ssn: Learning sparse switchable normalization via sparsestmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03793</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Importance of input data normalization for the application of neural networks to complex industrial problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Sevilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on nuclear science</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1464" to="1468" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kalman normalization: Normalizing internal representations across network layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="21" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Algorithm Outline Algorithm 1 presents an outline of our proposed Cross-Iteration Batch Normalization (CBN)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Comparison of gradients of statistics w.r.t. current layer vs</title>
		<imprint/>
	</monogr>
	<note>Figure 6. that w.r.t. previous layers on ImageNet</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">|| F for r ∈ {l − 1, l − 2} are averaged over all CBN layers of the network at different training epochs (Figure 6). The results suggest that the key assumption holds well, thus validating the approximation in Eq</title>
	</analytic>
	<monogr>
		<title level="m">||g µ (r)|| F /||g µ (l)|| F and ||g ν (r)|| F /||g ν (l)</title>
		<imprint/>
	</monogr>
	<note>We also study the gradients of non-ResNet models. The ratios of ||g µ || F and ||g ν || F are (0.20 and 0.41) for VGG-16 and (0.15 and 0.37) for Inception-V3. which is similar to ResNet (0.12 and 0.39), indicating that the assumption should also hold for the VGG and Inception series</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
