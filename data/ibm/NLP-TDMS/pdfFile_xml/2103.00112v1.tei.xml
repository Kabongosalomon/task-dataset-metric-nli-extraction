<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer in Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Lab of Computer Science</orgName>
								<orgName type="institution">ISCAS &amp; UCAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
							<email>ehwu@um.edu.mo</email>
							<affiliation key="aff1">
								<orgName type="department">State Key Lab of Computer Science</orgName>
								<orgName type="institution">ISCAS &amp; UCAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
							<email>yunhe.wang@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer in Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer is a type of self-attention-based neural networks originally applied for NLP tasks. Recently, pure transformer-based models are proposed to solve computer vision problems. These visual transformers usually view an image as a sequence of patches while they ignore the intrinsic structure information inside each patch. In this paper, we propose a novel Transformer-iN-Transformer (TNT) model for modeling both patch-level and pixel-level representation. In each TNT block, an outer transformer block is utilized to process patch embeddings, and an inner transformer block extracts local features from pixel embeddings. The pixel-level feature is projected to the space of patch embedding by a linear transformation layer and then added into the patch. By stacking the TNT blocks, we build the TNT model for image recognition. Experiments on ImageNet benchmark and downstream tasks demonstrate the superiority and efficiency of the proposed TNT architecture. For example, our TNT achieves 81.3% top-1 accuracy on ImageNet which is 1.5% higher than that of DeiT with similar computational cost. The code will be available at https://github.com/huawei-noah/noah-research/tree/ master/TNT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer is a type of neural network mainly based on self-attention mechanism <ref type="bibr" target="#b33">[34]</ref>. Transformer is widely used in the field of natural language processing (NLP), e.g., the famous BERT <ref type="bibr" target="#b8">[9]</ref> and GPT-3 <ref type="bibr" target="#b3">[4]</ref> models. Inspired by the breakthrough of transformer in NLP, researchers have recently applied transformer to computer vision (CV) tasks such as image recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>, object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>, and image processing <ref type="bibr" target="#b5">[6]</ref>. For example, DETR <ref type="bibr" target="#b4">[5]</ref> treats object detection as a direct set prediction problem and solve it using a transformer encoder-decoder architecture. IPT <ref type="bibr" target="#b5">[6]</ref> utilizes transformer for handling multiple low-level vision tasks in a single model. Compared to the mainstream CNN models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30]</ref>, these transformer-based models have also shown promising performance on visual tasks <ref type="bibr" target="#b10">[11]</ref>.</p><p>CV models purely based on transformer are attractive because they provide an computing paradigm without the image-specific inductive bias, which is completely different from convolutional neural networks (CNNs). Chen et.al proposed iGPT <ref type="bibr" target="#b6">[7]</ref>, the pioneering work applying pure transformer model on image recognition by self-supervised pre-training. ViT <ref type="bibr" target="#b9">[10]</ref> views an image as a sequence of patches and perform classification with a transformer encoder. DeiT <ref type="bibr" target="#b30">[31]</ref> further explores the data-efficient training and distillation of ViT. Compared to CNNs, transformer-based models can also achieve competitive accuracy without inductive bias, e.g., DeiT trained from scratch has an 81.8% ImageNet top-1 accuracy by using 86.4M parameters and 17.6B FLOPs.</p><p>In ViT <ref type="bibr" target="#b9">[10]</ref>, an image is split into a sequence of patches and each patch is simply transformed into a vector (embedding). The embeddings are processed by vanilla transformer block. By doing so, ViT can process images by a standard transformer with few modifications. This design also affects the subsequent works including DeiT <ref type="bibr" target="#b30">[31]</ref>, ViT-FRCNN <ref type="bibr" target="#b1">[2]</ref>, IPT <ref type="bibr" target="#b5">[6]</ref> and SETR <ref type="bibr" target="#b37">[38]</ref>. However, these (a) TNT framework.  visual transformers ignore the local relation and structure information inside the patch which is important for visual recognition <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref>. By projecting the patch into a vector, the spatial structure is corrupted and hard to learn.</p><p>In this paper, we propose a novel Transformer-iN-Transformer (TNT) architecture for visual recognition as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. Specifically, an image is split into a sequence of patches and each patch is reshaped to (super) pixel sequence. The patch embeddings and pixel embeddings are obtained by a linear transformation from the patches and pixels respectively, and then they are fed into a stack of TNT blocks for representation learning. In the TNT block, there are two transformer blocks where the outer transformer block models the global relation among patch embeddings, and the inner one extracts local structure information of pixel embeddings. The local information is added on the patch embedding by linearly projecting the pixel embeddings into the space of patch embedding. Patch-level and pixel-level position embeddings are introduced in order to retain spatial information. Finally, the class token is used for classification via a MLP head. Through the proposed TNT model, we can model both global and local structure information of the images and improve the representation ability of the feature. Experiments on ImageNet benchmark and downstream tasks demonstrate the superiority of our method in terms of accuracy and FLOPs. For example, our TNT-S achieves 81.3% ImageNet top-1 with only 5.2B FLOPs, which is 1.5% higher than that of DeiT with similar computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we describe the proposed transformer-in-transformer architecture and analyze the complexity in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>We first briefly describe the basic components in transformer <ref type="bibr" target="#b33">[34]</ref>, including MSA (Multi-head Self-Attention), MLP (Multi-Layer Perceptron) and LN (Layer Normalization).</p><p>MSA. In the self-attention module, the inputs X ∈ R n×d are linearly transformed to three parts, i.e., queries Q ∈ R n×d k , keys K ∈ R n×d k and values V ∈ R n×dv where n is the sequence length, d, d k , d v are the dimensions of inputs, queries (keys) and values, respectively. The scaled dot-product attention is applied on Q, K, V :</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax( QK T √ d k )V.<label>(1)</label></formula><p>Finally, a linear layer is used to produce the output. Multi-head self-attention splits the queries, keys and values for h times and perform the attention function in parallel, and then the output values of each head are concatenated and linearly projected to form the final output.</p><p>MLP. The MLP is applied between self-attention layers for feature transformation and non-linearity:</p><formula xml:id="formula_1">MLP(X) = σ(XW 1 + b 1 )W 2 + b 2 ,<label>(2)</label></formula><p>where W 1 ∈ R d×dm and W 2 ∈ R dm×d are weights of the two fully-connected layers respectively, b 1 ∈ R dm and b 2 ∈ R d are the bias terms, and σ(·) is the activation function such as GELU <ref type="bibr" target="#b14">[15]</ref>.</p><p>LN. Layer normalization <ref type="bibr" target="#b0">[1]</ref> is a key part in transformer for stable training and faster convergence. LN is applied over each sample x ∈ R d as follows:</p><formula xml:id="formula_2">LN(x) = x − µ δ • γ + β<label>(3)</label></formula><p>where µ ∈ R, δ ∈ R are the mean and standard deviation of the feature respectively, • is the element-wise dot, and γ ∈ R d , β ∈ R d are learnable affine transform parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer in Transformer</head><p>Given a 2D image, we uniformly split it into n patches X = [X 1 , X 2 , · · · , X n ] ∈ R n×p×p×3 , where (p, p) is the resolution of each image patch. ViT <ref type="bibr" target="#b9">[10]</ref> just utilizes a standard transformer to process the sequence of patches which corrupts the local structure of a patch, as shown in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. Instead, we propose Transformer-iN-Transformer (TNT) architecture to learn both global and local information in an image. In TNT, each patch is further transformed into the target size (p , p ) with pixel unfold <ref type="bibr" target="#b25">[26]</ref>, and with a linear projection, the sequence of patch tensors is formed as</p><formula xml:id="formula_3">Y 0 = [Y 1 0 , Y 2 0 , · · · , Y n 0 ] ∈ R n×p ×p ×c ,<label>(4)</label></formula><p>where Y i 0 ∈ R p ×p ×c , i = 1, 2, · · · , n, and c is the number of channels. In particular, we view each patch tensor Y i 0 as a sequence of pixel embeddings:</p><formula xml:id="formula_4">Y i 0 = [y i,1 0 , y i,2 0 , · · · , y i,m 0 ]<label>(5)</label></formula><p>where m = p 2 , and y i,j 0 ∈ R c , j = 1, 2, · · · , m. In TNT, we have two data flows in which one flow operates across the patch and the other processes the pixels inside each patch. For the pixel embeddings, we utilize a transformer block to explore the relation between pixels:</p><formula xml:id="formula_5">Y i l = Y i l−1 + MSA(LN(Y i l−1 )),<label>(6)</label></formula><formula xml:id="formula_6">Y i l = Y i l + MLP(LN(Y i l )).<label>(7)</label></formula><p>where l = 1, 2, · · · , L is index of the l-th layer, and L is the total number of layers. All patch tensors after transformation are</p><formula xml:id="formula_7">Y l = [Y 1 l , Y 2 l , · · · , Y n l ]</formula><p>. This can be viewed as an inner transformer block, denoted as T in . This process builds the relationship among pixels by computing interactions between any two pixels. For example, in a patch of human face, a pixel belonging to the eye is more related to other pixels of eyes while interacts less with forehead pixels.</p><p>For the patch level, we create the patch embedding memories to store the sequence of patch-level representations:</p><formula xml:id="formula_8">Z 0 = [Z class , Z 1 0 , Z 2 0 , · · · , Z n 0 ] ∈ R (n+1)×d</formula><p>where Z class is the classification token similar to ViT <ref type="bibr" target="#b9">[10]</ref>, and all of them are initialized as zero. In each layer, the patch tensors are transformed into the domain of patch embeddings by linear projection and added into the patch embeddings:</p><formula xml:id="formula_9">Z i l−1 = Z i l−1 + Vec(Y i l−1 )W l−1 + b l−1 ,<label>(8)</label></formula><p>where Z i l−1 ∈ R d , Vec(·) flattens the input to a vector, and W l−1 ∈ R mc×d and b l−1 ∈ R d are the weights and bias respectively. We use the standard transformer block for transforming the patch embeddings:</p><formula xml:id="formula_10">Z i l = Z i l−1 + MSA(LN(Z i l−1 )),<label>(9)</label></formula><formula xml:id="formula_11">Z i l = Z i l + MLP(LN(Z i l )).<label>(10)</label></formula><p>This outer transformer block T out is used for modeling relationship among patch embeddings.</p><p>In summary, the inputs and outputs of the TNT block include the pixel embeddings and patch embeddings as shown in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>, so the TNT can be formulated as Y l , Z l = TNT(Y l−1 , Z l−1 ). (11) In our TNT block, the inner transformer block is used to model the relationship between pixels for local feature extraction, and the outer transformer block captures the intrinsic information from the sequence of patches. By stacking the TNT blocks for L times, we build the transformer-in-transformer network. Finally, the classification token serves as the image representation and a fully-connected layer is applied for classification. Position encoding. Spatial information is an important factor in image recognition. For patch embeddings and pixels embeddings, we both add the corresponding position encodings to retain spatial information as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. The standard learnable 1D position encodings are utilized here. Specifically, each patch is assigned with a position encodings:</p><formula xml:id="formula_12">Z 0 ← Z 0 + E patch ,<label>(12)</label></formula><p>where E patch ∈ R (n+1)×d are the patch position encodings. As for the pixels in a patch, a pixel position encoding is added to each pixel embedding:</p><formula xml:id="formula_13">Y i 0 ← Y i 0 + E pixel , i = 1, 2, · · · , n<label>(13)</label></formula><p>where E pixel ∈ R m×c are the pixel position encodings which are shared across patches. In this way, patch position encoding can maintain the global spatial information, while pixel position encoding is used for preserving the local relative position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Complexity Analysis</head><p>A standard transformer block includes two parts, i.e., the multi-head self-attention and multi-layer perceptron. The FLOPs of MSA are 2nd(d k + d v ) + n 2 (d k + d v ), and the FLOPs of MLP are 2nd v rd v where r is the dimension expansion ratio of hidden layer in MLP. Overall, the FLOPs of a standard transformer block are</p><formula xml:id="formula_14">FLOPs T = 2nd(d k + d v ) + n 2 (d k + d v ) + 2nddr.<label>(14)</label></formula><p>Since r is usually set as 4, and the dimensions of input, key (query) and value are usually set as the same, the FLOPs calculation can be simplified as FLOPs T = 2nd(6d + n).</p><p>(15) The number of parameters can be obtained as</p><formula xml:id="formula_15">Params T = 12dd.<label>(16)</label></formula><p>Our TNT block consists of three parts: an inner transformer block T in , an outer transformer block </p><p>Although we add two more components in our TNT block, the increase of FLOPs is small since c d and O(m) ≈ O(n) in practice. For example, in the ViT-B/16 configuration, we have d = 768 and n = 196. We set c = 12 and m = 64 in our structure of TNT-B correspondingly. From Eq. 15 and Eq. 17, we can obtain that FLOPs T = 1446M and FLOPs T N T = 1603M . The FLOPs ratio of TNT block over standard transformer block is about 1.09×. Similarly, the parameters ratio is about 1.08×. With a small increase of computation and memory cost, our TNT block can efficiently model the local structure information and achieve a much better trade-off between accuracy and complexity as demonstrated in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Network Architecture</head><p>We build our TNT architectures by following the basic configuration of ViT <ref type="bibr" target="#b9">[10]</ref> and DeiT <ref type="bibr" target="#b30">[31]</ref>. The patch size is set as 16×16. The unfolded patch size p is set as 4 by default, and other size values are evaluated in the ablation studies. As shown in <ref type="table" target="#tab_0">Table 1</ref>, there are two variants of TNT networks with different model size, namely, TNT-S and TNT-B. They consist of 23.8M and 65.6M parameters respectively. The corresponding FLOPs for processing a 224×224 image are 5.2B and 14.1B respectively.</p><p>Operational optimizations. Inspired by squeeze-and-excitation (SE) network for CNNs <ref type="bibr" target="#b16">[17]</ref>, we propose to explore channel-wise attention for transformers. We first average all the patch (pixel) embeddings and use a two-layer MLP to calculate the attention values. The attention is multiplied to all the embeddings. The SE module only brings in a few extra parameters but is able to perform dimension-wise attention for feature enhancement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we conduct extensive experiments on visual benchmarks to evaluate the effectiveness of the proposed TNT architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Experimental Settings</head><p>Datasets. ImageNet ILSVRC 2012 <ref type="bibr" target="#b27">[28]</ref> is an image classification benchmark consisting of 1.2M training images belonging to 1000 classes, and 50K validation images with 50 images per class. We adopt the same data augmentation strategy as that in DeiT <ref type="bibr" target="#b30">[31]</ref> including random crop, random clip, Rand-Augment <ref type="bibr" target="#b7">[8]</ref>, Mixup <ref type="bibr" target="#b36">[37]</ref> and CutMix <ref type="bibr" target="#b35">[36]</ref>.</p><p>In addition to ImageNet, we also test on the downstream tasks with transfer learning to evaluate the generalization of TNT. The details of used visual datasets are listed in <ref type="table" target="#tab_1">Table 2</ref>. The data augmentation strategy are the same as that of ImageNet. Superordinate-level CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> 50,000 10,000 10 CIFAR-100 <ref type="bibr" target="#b17">[18]</ref> 50,000 10,000 100 Implementation Details. We utilize the training strategy provided in DeiT <ref type="bibr" target="#b30">[31]</ref>. The main advanced technologies apart from common setting <ref type="bibr" target="#b13">[14]</ref> include AdamW <ref type="bibr" target="#b20">[21]</ref>, label smoothing <ref type="bibr" target="#b28">[29]</ref>, DropPath <ref type="bibr" target="#b18">[19]</ref>, repeated augmentation <ref type="bibr" target="#b15">[16]</ref>. We list the hyper-parameters in <ref type="table" target="#tab_2">Table 3</ref> for better understanding. All the models are implemented using PyTorch <ref type="bibr" target="#b25">[26]</ref> and trained on NVIDIA Tesla V100 GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TNT on ImageNet</head><p>We train our TNT models with the same training setting as that of DeiT <ref type="bibr" target="#b30">[31]</ref>. The recent transformerbased models like ViT <ref type="bibr" target="#b9">[10]</ref> and DeiT <ref type="bibr" target="#b30">[31]</ref> are compared. To have a better understanding of current progress of visual transformers, we also include the representative CNN-based models such as ResNet <ref type="bibr" target="#b13">[14]</ref>, RegNet <ref type="bibr" target="#b26">[27]</ref> and EfficientNet <ref type="bibr" target="#b29">[30]</ref>. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. We can see that our transformer-based model, i.e., TNT outperforms all the other visual transformer models. In particular, TNT-S achieves 81.3% top-1 accuracy which is 1.5% higher than the baseline model DeiT-S, indicating the benefit of the introduced TNT framework to preserve local structure information inside the patch. By adding SE module, TNT-S model can be further improved to obtain 81.6% top-1. Compared to CNNs, TNT can outperform the widely-used ResNet and RegNet. Note that all the transformer-based models are still inferior to EfficientNet which utilizes special depth-wise convolutions, so it is yet a challenge of how to beat EfficientNet using pure transformer. We also plot the accuracy-parameters and accuracy-FLOPs line charts in <ref type="figure" target="#fig_4">Fig. 3</ref> to have an intuitive comparison of these models. Our TNT models consistently outperform other transformer-based models by a significant margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>Effect of position encodings. Position information is important for image recognition. In TNT structure, patch position encoding is for maintaining global spatial information, and pixel position encoding is used to preserve local relative position. We verify their effect by removing them separately. As shown in <ref type="table" target="#tab_4">Table 5</ref>, we can see that TNT-S with both patch position encoding and pixel position encoding performs the best by achieving 81.3% top-1 accuracy. Removing patch/pixel position encoding brings in a 0.6%/0.5% accuracy drop respectively, and removing all position encodings heavily decrease the accuracy by 0.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of #heads.</head><p>The effect of #heads in standard transformer has been investigated in multiple works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref> and a head width of 64 is recommended for visual tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>. We adopt the head width of 64 in outer transformer block in our model. The number of heads in inner transformer block is another hyper-parameter for investigation. We evaluate the effect of #heads in inner transformer block ( <ref type="table" target="#tab_5">Table 6</ref>). We can see that a proper number of heads (e.g., 2 or 4) achieve the best performance.  Transformed patch size (p , p ). In TNT, the input image is split into a number of 16×16 patches and each patch is further transformed into size (p , p ) for computational efficiency. Here we test the effect of hyper-parameter p on TNT-S architecture. As shown in <ref type="table" target="#tab_6">Table 7</ref>, we can see that the value of p has slight influence on the performance, and we use p = 4 by default for its efficiency, unless stated otherwise.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visualization</head><p>We visualize the learned features of DeiT and TNT to further understand the effect of the proposed method. For better visualization, the input image is resized to 1024×1024. The feature maps are formed by reshaping the patch embeddings according to their spatial positions. The feature maps in the 1-st, 6-th and 12-th blocks are shown in <ref type="figure" target="#fig_6">Fig. 4(a)</ref> where 12 feature maps are randomly sampled for these blocks each. In TNT, the local information are better preserved compared to DeiT. We also visualize all the 384 feature maps in the 12-th block using t-SNE <ref type="bibr" target="#b32">[33]</ref>  <ref type="figure" target="#fig_6">(Fig. 4(b)</ref>). We can see that the features of TNT are more diverse and contain richer information than those of DeiT. These benefits owe to the introduction of inner transformer block for modeling local features.</p><p>In addition to the patch-level features, we also visualize the pixel embeddings of TNT in <ref type="figure" target="#fig_7">Fig. 5</ref>. For each patch, we reshape the pixel embeddings according to their spatial positions to form the feature maps and then average these feature maps by the channel dimension. The averaged feature maps corresponding to the 14×14 patches are shown in <ref type="figure" target="#fig_7">Fig. 5</ref>. We can see that the local information is well preserved in the shallow layers, and the representations become more abstract gradually as the network goes deeper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Transfer Learning</head><p>To demonstrate the strong generalization ability of TNT, we tranfer TNT-S, TNT-B models trained on ImageNet to other benchmark datasets. More specifically, we evaluate our models on 4 image classification datasets with traning set size ranging from 2,040 to 50,000 images. These datasets include superordinate-level object classification (CIFAR-10 <ref type="bibr" target="#b17">[18]</ref>, CIFAR-100 <ref type="bibr" target="#b17">[18]</ref>) and fine-grained object classification (Oxford-IIIT Pets <ref type="bibr" target="#b24">[25]</ref>, Oxford 102 Flowers <ref type="bibr" target="#b23">[24]</ref>), shown in <ref type="table" target="#tab_1">Table 2</ref>. All models are fine-tuned with an image resolution of 384×384. <ref type="table" target="#tab_7">Table 8</ref> compares the transfer learing results of TNT to those of VIT, DeiT and other convlutional networks. We find that TNT outperforms DeiT in most datasets with less parameters, which shows the supreority of modeling pixel-level relations to get better feature resprentation.</p><p>Fine-tuning Details. We adopt the same training settings as those at the pre-training stage by perserving all data augmentation strategies. In order to fine-tune in a different resolution, we also interpolate the position embeddings of new patches. For CIFAR-10 and CIFAR-100, we fine-tune the models for 64 epochs, and for fine-grained datasets, we fine-tune the models for 300 epochs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel Transformer-iN-Transformer (TNT) network architecture for visual recognition. In particular, we uniformly split the image into a sequence of patches and view each patch as a sequence of pixels. We introduce a TNT block in which an outer transformer block is utilized for processing the patch embeddings and an inner transformer block is used to model the relation among pixel embeddings. The information of pixel embeddings is added on the patch embedding after the projection of a linear layer. We build our TNT architecture by stacking the TNT blocks. Compared to the conventional vision transformers (ViT) which corrupts the local structure of the patch, our TNT can better preserve and model the local information for visual recognition. Extensive experiments on ImageNet and downstream tasks have demonstrate the effectiveness of the proposed TNT architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) TNT block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the proposed Transformer-iN-Transformer (TNT) framework. The position embedding is not drawn for neatness. T-Block denotes transformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Patch-level and pixel-level position encodings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>T</head><label></label><figDesc>out and a linear layer. The computation complexity of T in and T out are 2nmc(6c + m) and 2nd(6d + n) respectively. The linear layer has FLOPs of nmcd. In total, the FLOPs of TNT block are FLOPs T N T = 2nmc(6c + m) + nmcd + 2nd(6d + n). (17) Similarly, the parameter complexity of TNT block is calculated as Params T N T = 12cc + mcd + 12dd.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Performance comparison of the representative visual backbone networks on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Feature maps in Block-1/6/12. (b) T-SNE of Block-12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the features of DeiT-S and TNT-S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the averaged pixel embeddings of TNT-S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Variants of our TNT architecture. 'S' means small, 'B' means base. The FLOPs are calculated for images at resolution 224×224.</figDesc><table><row><cell>Model</cell><cell>Depth</cell><cell cols="6">Inner transformer dim c #heads MLP r dim d #heads MLP r Outer transformer</cell><cell cols="2">Params FLOPs (M) (B)</cell></row><row><cell>TNT-S (ours)</cell><cell>12</cell><cell>24</cell><cell>4</cell><cell>4</cell><cell>384</cell><cell>6</cell><cell>4</cell><cell>23.8</cell><cell>5.2</cell></row><row><cell>TNT-B (ours)</cell><cell>12</cell><cell>40</cell><cell>4</cell><cell>4</cell><cell>640</cell><cell>10</cell><cell>4</cell><cell>65.6</cell><cell>14.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Details of used visual datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Train size Test size #Classes</cell></row><row><cell>ImageNet [28]</cell><cell>1,281,167</cell><cell>50,000</cell><cell>1000</cell></row><row><cell>Fine-grained</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Oxford 102 Flowers [24]</cell><cell>2,040</cell><cell>6,149</cell><cell>102</cell></row><row><cell>Oxford-IIIT Pets [25]</cell><cell>3,680</cell><cell>3,669</cell><cell>37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Default training hyper-parameters used in our method, unless stated otherwise.</figDesc><table><row><cell cols="2">Epochs Optimizer</cell><cell cols="2">Batch Learning size rate</cell><cell>LR decay</cell><cell cols="2">Weight Warmup decay epochs</cell><cell cols="3">Label smooth path Drop Repeated Aug</cell></row><row><cell>300</cell><cell>AdamW</cell><cell>1024</cell><cell>1e-3</cell><cell>cosine</cell><cell>0.05</cell><cell>5</cell><cell>0.1</cell><cell>0.1</cell><cell>√</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of TNT and other networks on ImageNet.</figDesc><table><row><cell>Model</cell><cell>Resolution</cell><cell>Params (M)</cell><cell>FLOPs (B)</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell>CNN-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 [14]</cell><cell>224×224</cell><cell>25.6</cell><cell>4.1</cell><cell>76.2</cell><cell>92.9</cell></row><row><cell>ResNet-152 [14]</cell><cell>224×224</cell><cell>60.2</cell><cell>11.5</cell><cell>78.3</cell><cell>94.1</cell></row><row><cell>RegNetY-8GF [27]</cell><cell>224×224</cell><cell>39.2</cell><cell>8.0</cell><cell>79.9</cell><cell>-</cell></row><row><cell>RegNetY-16GF [27]</cell><cell>224×224</cell><cell>83.6</cell><cell>15.9</cell><cell>80.4</cell><cell>-</cell></row><row><cell>GhostNet-A [12, 13]</cell><cell>240×240</cell><cell>11.9</cell><cell>0.6</cell><cell>79.4</cell><cell>94.5</cell></row><row><cell>EfficientNet-B3 [30]</cell><cell>300×300</cell><cell>12.0</cell><cell>1.8</cell><cell>81.6</cell><cell>94.9</cell></row><row><cell>EfficientNet-B4 [30]</cell><cell>380×380</cell><cell>19.0</cell><cell>4.2</cell><cell>82.9</cell><cell>96.4</cell></row><row><cell>Transformer-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeiT-S [31]</cell><cell>224×224</cell><cell>22.1</cell><cell>4.6</cell><cell>79.8</cell><cell>-</cell></row><row><cell>T2T-ViT_t-14 [35]</cell><cell>224×224</cell><cell>21.5</cell><cell>5.2</cell><cell>80.7</cell><cell>-</cell></row><row><cell>TNT-S (ours)</cell><cell>224×224</cell><cell>23.8</cell><cell>5.2</cell><cell>81.3</cell><cell>95.6</cell></row><row><cell>TNT-S + SE (ours)</cell><cell>224×224</cell><cell>24.7</cell><cell>5.2</cell><cell>81.6</cell><cell>95.7</cell></row><row><cell>ViT-B/16 [10]</cell><cell>384×384</cell><cell>86.4</cell><cell>55.5</cell><cell>77.9</cell><cell>-</cell></row><row><cell>DeiT-B [31]</cell><cell>224×224</cell><cell>86.4</cell><cell>17.6</cell><cell>81.8</cell><cell>-</cell></row><row><cell>TNT-B (ours)</cell><cell>224×224</cell><cell>65.6</cell><cell>14.1</cell><cell>82.8</cell><cell>96.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Effect of position encoding.</figDesc><table><row><cell cols="2">Model Patch position encoding Pixel position encoding Top-1 (%)</cell></row><row><cell></cell><cell>80.5</cell></row><row><cell>TNT-S</cell><cell>80.8</cell></row><row><cell></cell><cell>80.7</cell></row><row><cell></cell><cell>81.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Effect of #heads in inner transformer block in TNT-S.</figDesc><table><row><cell>#heads</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>Top-1</cell><cell>81.0</cell><cell>81.3</cell><cell>81.3</cell><cell>81.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Effect of p .</figDesc><table><row><cell>p</cell><cell cols="3">Params FLOPs Top-1</cell></row><row><cell>2</cell><cell>23.8M</cell><cell>5.1B</cell><cell>81.0%</cell></row><row><cell>4</cell><cell>23.8M</cell><cell>5.2B</cell><cell>81.3%</cell></row><row><cell>8</cell><cell>25.1M</cell><cell>6.0B</cell><cell>81.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Results on downstream tasks with ImageNet pre-training. ↑ 384 denotes fine-tuning with 384×384 resolution.</figDesc><table><row><cell>Model</cell><cell cols="6">Params (M) ImageNet CIFAR10 CIFAR100 Flowers Pets</cell></row><row><cell>CNN-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Grafit ResNet-50 [32]</cell><cell>25.6</cell><cell>79.6</cell><cell>-</cell><cell>-</cell><cell>98.2</cell><cell>-</cell></row><row><cell>Grafit RegNetY-8GF [32]</cell><cell>39.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>99.1</cell><cell>-</cell></row><row><cell>EfficientNet-B5 [30]</cell><cell>30</cell><cell>83.6</cell><cell>98.7</cell><cell>91.1</cell><cell>98.5</cell><cell>-</cell></row><row><cell>Transformer-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-B/16 ↑384</cell><cell>86.4</cell><cell>77.9</cell><cell>98.1</cell><cell>87.1</cell><cell>89.5</cell><cell>93.8</cell></row><row><cell>DeiT-B ↑384</cell><cell>86.4</cell><cell>83.1</cell><cell>99.1</cell><cell>90.8</cell><cell>98.4</cell><cell>-</cell></row><row><cell>TNT-S ↑384 (ours)</cell><cell>23.8</cell><cell>83.1</cell><cell>98.7</cell><cell>90.1</cell><cell>98.8</cell><cell>94.7</cell></row><row><cell>TNT-B ↑384 (ours)</cell><cell>65.6</cell><cell>83.9</cell><cell>99.1</cell><cell>91.1</cell><cell>99.0</cell><cell>95.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Toward transformer-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09958</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Approximating CNNs with bag-of-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<title level="m">A survey on visual transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Model rubik&apos;s cube: Twisting resolution, depth and width for tinynets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiulin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fractalnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12982</idno>
		<title level="m">Grafit: Learning fine-grained image representations with coarse labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
