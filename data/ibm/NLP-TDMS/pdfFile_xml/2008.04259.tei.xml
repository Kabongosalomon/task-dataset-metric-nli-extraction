<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Perceptually-Motivated Approach for Low-Complexity, Real-Time Enhancement of Fullband Speech</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Valin</surname></persName>
							<email>jmvalin@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
							<email>umutisik@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neerad</forename><surname>Phansalkar</surname></persName>
							<email>neeradp@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
							<email>ritwikg@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Helwani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
							<email>arvindhk@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Perceptually-Motivated Approach for Low-Complexity, Real-Time Enhancement of Fullband Speech</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech enhancement</term>
					<term>pitch filtering</term>
					<term>postfilter</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past few years, speech enhancement methods based on deep learning have greatly surpassed traditional methods based on spectral subtraction and spectral estimation. Many of these new techniques operate directly in the the short-time Fourier transform (STFT) domain, resulting in a high computational complexity. In this work, we propose PercepNet, an efficient approach that relies on human perception of speech by focusing on the spectral envelope and on the periodicity of the speech. We demonstrate high-quality, real-time enhancement of fullband (48 kHz) speech with less than 5% of a CPU core.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the past few years, speech enhancement methods based on deep learning have greatly surpassed traditional methods based on spectral subtraction [1] and spectral estimation <ref type="bibr">[2]</ref>. Many of these techniques operate directly on the short-time Fourier transform (STFT), estimating either magnitudes <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b3">5]</ref> or ideal ratio masks (IRM) <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7]</ref>. This typically requires a large number of neurons and weights, resulting in a high complexity.</p><p>It also partly explains why many of those methods are restricted to 8 or 16 kHz. The use of the STFT also brings up a tradeoff with the window length -long windows can cause musical noise and reverb-like effects, whereas short windows do not provide sufficient frequency resolution for removing noise between pitch harmonics. These problems can be mitigated by the use of complex ratio masks <ref type="bibr" target="#b6">[8]</ref> or time-domain processing <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b9">11]</ref>, at the cost of further increasing complexity.</p><p>We propose PercepNet, an efficient approach that relies heavily on human perception of speech signals and improves on RNNoise <ref type="bibr" target="#b10">[12]</ref>. More precisely, we rely on the perception of audio in critical bands (Section 2) and on the perception of tones and noise (Section 3) with a new acausal comb filter. The deep neural network (DNN) model we use is trained using perceptual criteria (Section 4). We propose a novel envelope postfilter (Section 5) that further improves the enhanced signal.</p><p>The PercepNet algorithm operates on 10-ms frames with 40 ms of look-ahead and can enhance 48 kHz speech in real time using just 4.1% of an x86 CPU core. We show that its quality significantly exceeds that of RNNoise (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Signal Model</head><p>Let x (n) be a clean speech signal, the signal captured by a hands-free microphone in a noisy room is given by</p><formula xml:id="formula_0">y (n) = x (n) h (n) + η (n) ,<label>(1)</label></formula><p>where η (n) is the additive noise from the room, h (n) is the impulse response from the talker to the microphone, and de- notes the convolution. Furthermore, the clean speech can be expressed as x (n) = p (n) + u (n), where p (n) is a locally periodic component and u (n) is a stochastic component (here we consider transients such as stops as part of the stochastic component). In this work, we attempt to compute an enhanced signal x (n) =p (n)+û (n) which is as perceptually close to the clean speech x (n) as possible. Separating the stochastic component u (n) from the environmental noise η (n) is a very hard problem. Fortunately, we only needû (n) to sound like u (n), which can be achieved by filtering the mixture u (n) h (n) + η (n) to have the same spectral envelope as u (n). Since p (n) is periodic and the noise is assumed not to have strong periodicity, p (n) should be easier to estimate. Again, we mostly needp (n) to have the same spectral envelope and the same period as p (n).</p><p>We seek to construct an enhanced signal with the same 1) spectral envelope, and 2) frequency-dependent periodic-tostochastic ratio, as the clean signal. For both these properties, we use a resolution that matches human perception.</p><p>We use the short-time Fourier transform (STFT) with 20-ms windows and 50% overlap. We use the Vorbis window function <ref type="bibr" target="#b11">[13]</ref> -which satisfies the Princen-Bradley perfect reconstruction criterion <ref type="bibr" target="#b12">[14]</ref> -for analysis and synthesis, as shown in <ref type="figure">Fig. 1</ref>. An overview of the algorithm is shown in <ref type="figure">Fig. 2</ref>  <ref type="figure">Figure 3</ref>: Frequency response of the proposed comb filter (red) vs the filter used in <ref type="bibr" target="#b10">[12]</ref> (blue) for a pitch of 200 Hz.</p><formula xml:id="formula_1">|C 5 (z)| |C (0) (z)|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Bands</head><p>The vast majority of noise signals have a wide bandwidth with a smooth spectrum. Similarly, both the periodic and the stochastic components of speech have a smooth spectral envelope. This allows us to represent their envelope from 0 to 20 kHz using 34 bands, spaced according to the human hearing equivalent rectangular bandwidth (ERB) <ref type="bibr" target="#b13">[15]</ref>. To avoid bands with just one DFT bin, we impose a minimum band width of 100 Hz. For each band of the enhanced signal to be perceptually close to the clean speech, both their total energy and their periodic content should be the same. In this paper, we denote the complex-valued spectrum of the signal x (n) for band b in frame as x b ( ). We also denote the L2-norm of that band as X b ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Gains</head><p>From the magnitude of the noisy speech signal in band b, we compute the ideal ratio mask, i.e. the gain that needs to be applied to y b such that it has the same energy as x b ( ):</p><formula xml:id="formula_2">g b ( ) = X b ( ) Y b ( ) .<label>(2)</label></formula><p>In the case where the speech only has a stochastic component, applying the gain g b ( ) to the magnitude spectrum in band b should result in an enhanced signal that is almost indistinguishable from the clean speech signal. On the other hand, when the speech is perfectly periodic, applying the gain g b ( ) results in an enhanced signal that sounds rougher than the clean speech; even though the energy is the same, the enhanced signal is less harmonic than the clean speech. In that case, the noise is particularly perceptible due to the fact that tones have relatively little masking effect on noise <ref type="bibr" target="#b14">[16]</ref>. In that situation, we use the comb filter described in the next section to remove the noise between the pitch harmonics and make the signal more periodic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pitch Filtering</head><p>To reconstruct the harmonic properties of the clean speech, we use comb filtering based on the pitch frequency. The comb filter can achieve much finer frequency resolution than would otherwise be possible with the STFT (50 Hz using 20-ms frames). We estimate the pitch period using a correlation-based method combined with a dynamic programming search <ref type="bibr" target="#b15">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Filter</head><p>For a voiced speech signal with period T , a simple comb filter</p><formula xml:id="formula_3">C (0) (z) = 1 + z −T 2<label>(3)</label></formula><p>introduces zeros at regular interval between harmonics and attenuates the noisy part of the signal by around 3 dB. This pro-vided a small, but noticeable quality improvement in <ref type="bibr" target="#b10">[12]</ref>. In this work, we extend the comb filtering to more than one period, including non-causal taps using the following filter:</p><formula xml:id="formula_4">CM (z) = M k=−M w k z −kT ,<label>(4)</label></formula><p>where M is the number of periods on each side of the central tap and w k is a window function satisfying k w k = 1.</p><p>Using CM (z), the noisy part of the signal is attenuated by</p><formula xml:id="formula_5">σ 2 w = k w 2 k .</formula><p>Although a rectangular window would minimize σ 2 w , we use a Hann window, which shapes the remaining noise to be lower between harmonics. Due to the behavior of tone masking <ref type="bibr" target="#b13">[15]</ref>, this results in a lower perceptual noise. For M = 5, we have σw = −9 dB and the full response is shown in <ref type="figure">Fig. 3</ref>. In practice, since the maximum look-ahead is bounded, we truncate the window w k to values of kT that are permitted.</p><p>The filtering occurs in the time domain, with the output denotedp (n) since it approximates the "perfect" periodic component p (n) from the clean speech. Its STFT is denotedp b ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Filtering Strength</head><p>The amount of comb filtering is important: not enough filtering results in roughness, whereas too much results in a robotic voice. The strength of the comb filtering in <ref type="bibr" target="#b10">[12]</ref> is controlled by a heuristic. In this work, we instead have the neural network learn the strength that best preserves the ratio of periodic to stochastic energy in each band. The equations below describe what that ideal strength should be. Since they rely on properties of the clean speech, they are only used at training time.</p><p>We define the pitch coherence q x,b ( ) of the clean signal as the cosine distance between the complex spectra of the signal with its periodic component (both and b are omitted for clarity)</p><formula xml:id="formula_6">qx p H x p · x ,<label>(5)</label></formula><p>where · H denotes the Hermitian transpose and [·] denotes the real component. Similarly, we define qy as the pitch coherence of the noisy signal. Since the ground truth p is not available, the coherence values need to be estimated. Considering that the noise inp is attenuated by a factor σ 2 w , the pitch coherence of the estimated periodic signalp itself can be approximated as</p><formula xml:id="formula_7">qp = qy (1 − σ 2 w ) q 2 y + σ 2 w .<label>(6)</label></formula><p>We define the pitch filtering strength r ∈ [0, 1], where r = 0 causes no filtering to occur and r = 1 replaces the signal witĥ p. Let z = (1 − r) y + rp be a pitch-enhanced signal, we want the pitch coherence of z to match the clean signal:</p><formula xml:id="formula_8">qz = p · ((1 − r) y + rp) p · (1 − r) y + rp = qx .<label>(7)</label></formula><p>Solving <ref type="formula" target="#formula_8">(7)</ref> for r results in</p><formula xml:id="formula_9">r = α 1 + α ,<label>(8)</label></formula><formula xml:id="formula_10">α = b 2 + a q 2 x − q 2 y − b a ,<label>(9)</label></formula><p>where a = q 2 p − q 2 In very noisy conditions, it is possible for the periodic estimatep to have a lower coherence than the clean speech in a band (qp &lt; qx). In that case, we set r = 1 and compute a gain attenuation term that will ensure that the stochastic component of the enhanced speech matches the level of the clean speech (at the expense of making the periodic component too quiet)</p><formula xml:id="formula_11">x and b = qpqy 1 − q 2 x . G R U C O N V 1 x 5 F C 128 C O N V 1 x 3 G R U G R U G R U G R U F C</formula><formula xml:id="formula_12">g (att) = 1 + n0 − q 2 x 1 + n0 − q 2 p ,<label>(10)</label></formula><p>where n0 = 0.03 (or 15 dB) limits the maximum attenuation to the noise-masking-tone threshold <ref type="bibr" target="#b16">[18]</ref>. For the normal case (qp ≥ qx), then g (att) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DNN Model</head><p>The model uses both convolutional layers (a 1x5 layer followed by a 1x3 layer), and GRU <ref type="bibr" target="#b17">[19]</ref> layers, as shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. The convolutional layers are aligned in time such as to use up to M frames into the future. To achieve 40 ms look-ahead including the 10-ms overlap, we use M = 3. The input features used by the model are tied to the 34 ERB bands. For each band, we use two features: the magnitude of the band with look-ahead Y b ( + M ) and the pitch coherence without look-ahead q y,b ( ) (the coherence estimation itself uses the full look-ahead). In addition to those 68 band-related features, we use the pitch period T ( ), as well as an estimate of the pitch correlation <ref type="bibr" target="#b18">[20]</ref> with look-ahead, for a total of 70 input features. For each band b, we also have 2 outputs: the gainĝ b ( ) approximates g</p><formula xml:id="formula_13">(att) b ( ) g b ( ) and the strengthr b ( ) approxi- mates r b ( ).</formula><p>The weights of the model are forced to a ± 1 2 range and quantized to 8-bit integers. This reduces the memory requirement (and bandwidth), while also reducing the computational complexity of the inference by taking advantage of vectorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Data</head><p>We train the model on synthetic mixtures of clean speech and noise with SNRs ranging from -5 dB to 45 dB, with some noise-free examples included. The clean speech data includes 120 hours of 48 kHz speech from different public and internal databases, including more than 200 speakers and more than 20 different languages. The noise data includes 80 hours of various noise types, also sampled at 48 kHz.</p><p>To ensure robustness in reverberated conditions, the noisy signal is convolved with simulated and measured room impulse responses. Inspired by <ref type="bibr" target="#b19">[21]</ref>, the target includes the early reflections so that only late reverberation is attenuated.</p><p>We improve the generalization of the model by applying a different random second-order pole-zero filter to both the speech and the noise. We also apply the same random spectral tilt to both signals to better generalize across different microphone frequency responses. To achieve bandwidthindependence, we apply a low-pass filter with a random cutoff frequency between 3 kHz and 20 kHz. This makes it possible to use the same model on narrowband to fullband audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Loss function</head><p>We use a different loss function for the gain and for the pitch filtering strength. For the gain, we consider that the perceptual loudness of a signal is proportional to its energy raised to a power γ/2, where we use γ = 0.5. For that reason, we raise the gains to the power γ before computing the metrics. In addition to the squared error, we also use the fourth power to overemphasize the cost of making large errors (e.g. completely attenuating speech):</p><formula xml:id="formula_14">Lg = b (g γ b −ĝ γ b ) 2 + C4 b (g γ b −ĝ γ b ) 4 ,<label>(11)</label></formula><p>where we use C4 = 10 to balance between the L2 and L4 terms. Although simple, the loss function in (11) implicitly incorporates many of the characteristics of the improved loss function proposed in <ref type="bibr" target="#b20">[22]</ref>, including scale-invariance, SNRinvariance, power-law compression, and non-linear frequency resolution.</p><p>For the pitch filtering strength, we use the same principle as for Lg but evaluating the loudness of the noisy component of the enhanced speech. Since the comb filter with strength r b attenuates the noise by a factor (1 − r b ), we use the strength loss</p><formula xml:id="formula_15">Lr = b ((1 − r b ) γ − (1 −r b ) γ ) 2 .<label>(12)</label></formula><p>Since the enhancement is not overly sensitive to errors in the value ofr b , we do not use a fourth power term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Envelope Postfiltering</head><p>To further enhance the speech, we slightly deviate from the gainsĝ b produced by the DNN. The deviation is inspired by the formant postfilters <ref type="bibr" target="#b21">[23]</ref> often used in CELP codecs. We intentionally de-emphasize noisier bands slightly further than they would be in the clean signal, while overemphasizing clean bands to compensate. This is done by computing a warped gain</p><formula xml:id="formula_16">g (w) b =ĝ b sin π 2ĝ b ,<label>(13)</label></formula><p>which leavesĝ b essentially unaffected for clean bands, while squaring it (like the gain of a Wiener filter) for very noisy bands.</p><p>To avoid over-attenuating the enhanced signal as a whole, we also apply a global gain compensation heuristic computed as</p><formula xml:id="formula_17">G = (1 + β) E 0 E 1 1 + β E 0 E 1 2 ,<label>(14)</label></formula><p>where E0 is the total energy of the enhanced signal using the original gainĝ b and E1 is the total energy when using the warped gainĝ (w) b . We use β = 0.02, which results in a maximum theoretical gain of 5.5 dB for clean bands. Scaling the final signal for the frame by G results in a perceptually cleaner signal that is about as loud as the clean signal. The band energy after that postfilter is given bŷ</p><formula xml:id="formula_18">X b = Gĝ (w) b Y b .<label>(15)</label></formula><p>When listening to the enhanced speech through loudspeakers in a room, the impulse response of the room is added back to the signal such that it blends with any speech coming from the room. However, when listening through headphones, the lack of any reverberation can make the enhanced signal sound overly dry and unnatural. This is addressed by enforcing a minimum decay in the energy, subject to never exceeding the energy of the noisy speech:</p><formula xml:id="formula_19">X (r) b ( ) = min max X b ( ) , δX (r) b ( − 1) ,Ŷ b ( ) ,<label>(16)</label></formula><p>where δ is chosen to be equivalent to a reverberation time T60 = 100 ms.</p><p>After the frequency-domain enhanced speech is converted back to the time domain, a high-pass filter is applied to the output. The filter helps eliminating some remaining low-frequency noise and its cutoff frequency is determined by the estimated pitch of the talker <ref type="bibr" target="#b18">[20]</ref> to avoid attenuating the fundamental.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Results</head><p>We evaluate the quality of the enhanced speech with two mean opinion score (MOS) <ref type="bibr" target="#b22">[24]</ref> tests conducted using the crowdsourcing methodology P.808 <ref type="bibr" target="#b23">[25]</ref>. First, we use the 48 kHz noisy VCTK test set provided in <ref type="bibr" target="#b24">[26]</ref> to compare PercepNet to the original RNNoise <ref type="bibr" target="#b10">[12]</ref>, while also conducting an ablation study. The test includes 824 samples, rated by 8 listeners each, resulting in a 95% confidence interval of 0.04. We also provide PESQ-WB <ref type="bibr" target="#b25">[27]</ref> results as a reference for comparison with other methods like SEGAN <ref type="bibr" target="#b7">[9]</ref>. The results in <ref type="table" target="#tab_1">Table 1</ref> not only demonstrate a base improvement over RNNoise, but also show that both the pitch filter and the envelope postfilter help improve the quality of the enhanced speech. In addition, subjective testing clearly shows the limitations of PESQ-WB when evaluating the envelope postfilter -even though the subjective evaluation shows a strong improvement from the postfilter, PESQ-WB considers it a degradation. Note that the unusually high absolute numbers in the MOS results are likely due to the fullband samples in that test.</p><p>In the second test, the DNS challenge <ref type="bibr" target="#b26">[28]</ref> organizers evaluated blind test samples processed with PercepNet and provided us with the results in <ref type="table" target="#tab_2">Table 2</ref>. The test set includes 150 synthetic samples without reverberation, 150 synthetic samples with reverberation, and 300 real recordings. Each sample was rated by 10 listeners, leading to a 95% confidence interval of 0.02 for all algorithms. Since PercepNet operates at 48 kHz, the 16-kHz challenge test data was internally up-sampled (and later downsampled) in the STFT domain, avoiding any additional algorithmic delay. The same model parameters were used for both the challenge 16-kHz evaluation and our own 48-kHz VCTK evaluation, demonstrating the capability to operate on speech with different bandwidths. The quality also exceeds that of the baseline <ref type="bibr" target="#b27">[29]</ref> algorithm.</p><p>The algorithm complexity is mostly dictated by the neural network, and thus the number of weights. For a frame size of 10 ms and 8M weights, the complexity is around 800 MMACS  (one multiply-and-accumulate per weight per frame/second). By quantizing the weights with 8 bits, vectorization makes it possible to run the network efficiently. With the default frame size of 10 ms, PercepNet requires 5.2% of one mobile x86 core (1.8 GHz Intel i7-8565U CPU) for real-time operation. Evaluated with a frame size of 40 ms (four internal frames of 10 ms each to improve cache efficiency), the complexity is reduced to 4.1% on the same CPU core with an identical output. Despite a much lower complexity than the maximum allowed by the DNS challenge, PercepNet ranked second in the real-time track. Qualitatively 1 , the use of ERB bands -rather than operating directly on frequency bins -makes the algorithm incapable of producing musical noise (aka birdie artifacts) in the output. Similarly, the short window used for analysis avoids reverb-like smearing in the time domain. Instead, the main noticeable artifact is a certain amount of roughness caused by some noise remaining between pitch harmonics, especially for loud car noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have demonstrated an efficient speech enhancement algorithm that focuses on the main perceptual characteristics of speech -spectral envelope and periodicity -to produce highquality fullband speech in real time with low complexity. The proposed PercepNet model uses a band structure to represent the spectrum, along with pitch filtering and an additional envelope postfiltering step. Evaluation results show significant quality improvements for both wideband and fullband speech and demonstrate the effectiveness of both the pitch filtering and the postfilter. We believe the results demonstrate the benefits of modeling speech using perceptually-relevant parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">References</head><p>[1] S. Boll. Suppression of acoustic noise in speech using spectral subtraction. IEEE Transactions on acoustics, speech, and signal processing, 27(2):113-120, 1979.</p><p>[2] Y. Ephraim and D. Malah. Speech enhancement using a minimum mean-square error log-spectral amplitude estimator. IEEE Trans-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>AmplitudeFigure 1 :Figure 2 :</head><label>12</label><figDesc>The current window being synthesized is shown in solid red. We use three windows of look-ahead (shown in dashed lines) such that samples up to time t = 40 ms are used to compute the audio output up to t = 0. Overview of the PercepNet algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Overview of the DNN architecture computing the 34 gainsĝ b and 34 strengthsr b from the 70-dimensional input feature vector f . The number of units on each layer is indicated above the layer type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>P.808 MOS results based on internal testing on the VCTK test set at 48 kHz.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">PESQ-WB MOS (P.808)</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>3.40</cell></row><row><cell>SEGAN [9]</cell><cell>2.16</cell><cell>-</cell></row><row><cell>RNNoise (original) [12]</cell><cell>2.29</cell><cell>3.70</cell></row><row><cell>PercepNet (no pitch, no pf)</cell><cell>2.64</cell><cell>3.81</cell></row><row><cell>PercepNet (no pf)</cell><cell>2.73</cell><cell>3.91</cell></row><row><cell>PercepNet (no pitch)</cell><cell>2.47</cell><cell>3.93</cell></row><row><cell>PercepNet</cell><cell>2.54</cell><cell>4.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Challenge official P.808 MOS results. The baseline model is provided by the challenge organizers.</figDesc><table><row><cell>Algorithm</cell><cell>Synthetic</cell><cell>Synthetic</cell><cell>Real</cell><cell>Overall</cell></row><row><cell></cell><cell cols="3">w/o reverb w/ reverb record</cell><cell></cell></row><row><cell>Noisy</cell><cell>3.32</cell><cell>2.78</cell><cell>2.97</cell><cell>3.01</cell></row><row><cell>Baseline</cell><cell>3.49</cell><cell>2.64</cell><cell>3.00</cell><cell>3.03</cell></row><row><cell>PercepNet</cell><cell>3.92</cell><cell>3.16</cell><cell>3.51</cell><cell>3.52</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Audio samples available at: https://www.amazon.science/interspeech-2020-deep-noise-suppression-audio-samples</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="443" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Experiments on deep learning for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fifteenth Annual Conference of the International Speech Communication Association</title>
		<meeting>Fifteenth Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A convolutional recurrent neural network for real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTER-SPEECH</title>
		<meeting>INTER-SPEECH</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="3229" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ideal ratio mask estimation using deep neural networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7092" to="7096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dnn-based enhancement of noisy and reverberant speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Merks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6525" to="6529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09452</idno>
		<title level="m">SEGAN: Speech enhancement generative adversarial network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5069" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved speech enhancement with the wave-u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11307</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hybrid DSP/deep learning approach to real-time full-band speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Multimedia Signal Processing (MMSP) Workshop</title>
		<meeting>IEEE Multimedia Signal Processing (MMSP) Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Vorbis I specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Montgomery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis/synthesis filter bank design based on time domain aliasing cancellation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Princen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1153" to="1161" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An introduction to the psychology of hearing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Brill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asymmetry of masking between complex tones and noise: Partial loudness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gockel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="349" to="360" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A robust algorithm for pitch tracking (RAPT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Talkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Coding and Synthesis</title>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="495" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual coding of digital audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spanias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="515" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</title>
		<meeting>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Voice coding with Opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 135 th AES Convention</title>
		<meeting>the 135 th AES Convention</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Late reverberation suppression using recurrent neural networks with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5434" to="5438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Investigations on data augmentation and loss functions for deep learning based speech-background separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3499" to="3503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive postfiltering for quality enhancement of coded speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="71" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Methods for subjective determination of transmission quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Itu-T. Recommendation</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">800</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Subjective evaluation of speech quality with a crowdsourcing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Itu-T. Recommendation</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">808</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Investigating rnn-based speech enhancement methods for noiserobust text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISCA Speech Synthesis Workshop (SSW)</title>
		<meeting>ISCA Speech Synthesis Workshop (SSW)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">862.2: Wideband extension to recommendation P.862 for the assessment of wideband telephone networks and speech codecs (PESQ-WB)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">P</forename><surname>Itu-T</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13981</idno>
		<title level="m">The INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Weighted speech distortion losses for neural-network-based real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10601</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
