<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
						</author>
						<title level="a" type="main">Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a novel method termed Frustum ConvNet (F-ConvNet) for amodal 3D object detection from point clouds. Given 2D region proposals in an RGB image, our method first generates a sequence of frustums for each region proposal, and uses the obtained frustums to group local points. F-ConvNet aggregates point-wise features as frustumlevel feature vectors, and arrays these feature vectors as a feature map for use of its subsequent component of fully convolutional network (FCN), which spatially fuses frustum-level features and supports an end-to-end and continuous estimation of oriented boxes in the 3D space. We also propose component variants of F-ConvNet, including an FCN variant that extracts multi-resolution frustum features, and a refined use of F-ConvNet over a reduced 3D space. Careful ablation studies verify the efficacy of these component variants. F-ConvNet assumes no prior knowledge of the working 3D environment and is thus dataset-agnostic. We present experiments on both the indoor SUN-RGBD and outdoor KITTI datasets. F-ConvNet outperforms all existing methods on SUN-RGBD, and at the time of submission it outperforms all published works on the KITTI benchmark. Code has been made available at: https: //github.com/zhixinwang/frustum-convnet.</p><p>wang.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Detection of object instances in 3D sensory data has tremendous importance in many applications including autonomous driving, robotic object manipulation, and augmented reality. Among others, RGB-D images and LiDAR point clouds are the most representative formats of 3D sensory data. In practical problems, these data are usually captured by viewing objects/scenes from a single perspective; consequently, only partial surface depth of the observed objects/scenes can be captured. The task of amodal 3D object detection is thus to estimate oriented 3D bounding boxes enclosing the full objects, given partial observations of object surface. In this work, we focus on object detection from point clouds, and assume the availability of accompanying RGB images.</p><p>Due to the discrete, unordered, and possibly sparse nature of point clouds, detecting object instances from them is challenging and requires learning techniques that are different from the established ones <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> for object detection in RGB images. In order to leverage the expertise in 2D object detection, existing methods convert 3D point clouds either into 2D images by view projection <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, or into regular grids of voxels by quantization <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Although 2D object detection can be readily applied to the converted images or volumes, these methods suffer from loss of critical 3D information in the projection or quantization process.</p><p>With the progress of point set deep learning <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, recent methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> resort to learning features directly from raw point clouds. For example, the seminal work of F-PointNet <ref type="bibr" target="#b12">[13]</ref> first finds local points corresponding to pixels inside a 2D region proposal, and then uses PointNet <ref type="bibr" target="#b10">[11]</ref> to segment from these local points the foreground ones; the amodal 3D box is finally estimated from the foreground points. Performance of this method is limited due to the reasons that (1) it is not of end-to-end learning to estimate oriented boxes, and (2) final estimation relies on too few foreground points which themselves are possibly segmented wrongly. Methods of VoxelNet style <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref> overcome both of the above limitations by partitioning 3D point cloud into a regular grid of equally spaced voxels; voxel-level features are learned and extracted, again using methods similar to PointNet <ref type="bibr" target="#b10">[11]</ref>, and are arrayed together to form feature maps that are processed subsequently by convolutional (conv) layers; amodal 3D boxes are estimated in an end-toend fashion using spatially convolved voxel-level features. For the other side of the coin, due to unawareness of objects, sizes and positions of grid partitioning in VoxelNet <ref type="bibr" target="#b13">[14]</ref> methods do not take object boundaries into account, and their settings usually assume prior knowledge of the 3D environment (e.g., only one object in vertical space of the KITTI dataset <ref type="bibr" target="#b16">[17]</ref>), which, however, are not always suitable.</p><p>Motivated to address the limitations in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we propose in this paper a novel method of amodal 3D object detection termed Frustum ConvNet (F-ConvNet). Similar to <ref type="bibr" target="#b12">[13]</ref>, our method assumes the availability of 2D region proposals in RGB images, which can be easily obtained from off-the-shelf object detectors <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, and identifies 3D points corresponding to pixels inside each region pro-posal. Different from <ref type="bibr" target="#b12">[13]</ref>, our method generates for each region proposal a sequence of (possibly overlapped) frustums by sliding along the frustum axis 1 (cf. <ref type="figure" target="#fig_0">Fig. 1</ref> for an illustration). These obtained frustums define groups of local points. Given the sequence of frustums and point association, our F-ConvNet starts with lower, parallel layer streams of PointNet style to aggregate point-wise features as a frustum-level feature vector; it then arrays at its early stage these feature vectors of individual frustums as 2D feature maps, and uses a subsequent fully convolutional network (FCN) to down-sample and up-sample frustums such that their features are fully fused across the frustum axis at a higher frustum resolution. Together with a final detection header, our proposed F-ConvNet supports an end-to-end and continuous estimation of oriented 3D boxes, where we also propose an FCN variant that extracts multi-resolution frustum features. Given an initial estimation of 3D box, a final refinement using the same F-ConvNet often improves the performance further. We present careful ablation studies that verify the efficacy of different components of F-ConvNet. On the SUN-RGBD dataset <ref type="bibr" target="#b17">[18]</ref>, our method outperforms all existing ones. On the KITTI benchmark <ref type="bibr" target="#b16">[17]</ref>, our method outperforms all published works at the time of submission, including those working on point clouds and those working on a combination of point clouds and RGB images. We summarize our contributions as follows.</p><p>• We propose a novel method termed Frustum ConvNet (F-ConvNet) for amodal 3D object detection from point clouds. We use a novel grouping mechanism -sliding frustums to aggregate local point-wise features for use of a subsequent FCN. Our proposed method supports an end-to-end estimation of oriented boxes in the 3D space that is determined by 2D region proposals. • We propose component variants of F-ConvNet, including an FCN variant that extracts multi-resolution frustum features, and a refined use of F-ConvNet over a reduced 3D space. Careful ablation studies verify the efficacy of these components and variants. • F-ConvNet assumes no prior knowledge of the working 3D environment, and is thus dataset-agnostic. On the indoor SUN-RGBD dataset <ref type="bibr" target="#b17">[18]</ref>, F-ConvNet outperforms all existing methods; on the outdoor dataset of KITTI benchmark <ref type="bibr" target="#b16">[17]</ref>, it outperforms all published works at the time of submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In this section, we briefly review existing methods of amodal 3D object detection. We organize our reviews into two categories of technical approaches, namely those based on conversion of 3D point clouds as images/volumes, and those admitting operation directly on raw point clouds. <ref type="bibr" target="#b0">1</ref> For any image region, a square pyramid passing though the image region can be specified by the viewing camera and the farthest plane that is perpendicular to the optical axis of the camera. Starting from the image plane, a frustum is formed by truncating the pyramid with a pair of parallel planes perpendicular to the optical axis, which is also the frustum axis.</p><p>Methods based on data conversion MV3D <ref type="bibr" target="#b4">[5]</ref> projects LiDAR point clouds to bird eye view (BEV), and then employs a Faster-RCNN <ref type="bibr" target="#b1">[2]</ref> for 3D object detection. AVOD <ref type="bibr" target="#b5">[6]</ref> extends MV3D by aggregating the multi-modal features to generate more reliable 3D object proposals. Some existing methods also use depth images as converted data of point clouds. Deng et al. <ref type="bibr" target="#b18">[19]</ref> directly estimate 3D bounding boxes from RGB-D images based on the Fast-RCNN framework <ref type="bibr" target="#b0">[1]</ref>. Luo et al. <ref type="bibr" target="#b19">[20]</ref> explore the SSD pipeline <ref type="bibr" target="#b2">[3]</ref> to fuse RGB and depth images for 3D bounding box estimation. DSS <ref type="bibr" target="#b20">[21]</ref> encodes a depth image as a grid of 3D voxels by TSDF, and uses 3D CNNs for classification and box estimation. PIXOR <ref type="bibr" target="#b8">[9]</ref> also encodes point clouds as grids of voxels. The above methods based on data conversion can leverage the expertise in mature 2D detection, but the projection or quantization process would cause loss of critical information. Methods working on raw point clouds We have reviewed in introduction the seminal work of F-PointNet <ref type="bibr" target="#b12">[13]</ref> that works directly on raw point clouds but is not of end-to-end learning to estimate oriented boxes since before that it has to do an instance segmentation and T-Net alignment, and the methods of VoxelNet style <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref> that resolve this issue but with the shortcoming of object unawareness in 3D point clouds. We note that PointPillars <ref type="bibr" target="#b15">[16]</ref> explores pillar shape instead of voxel design to aggregate point-wise features. For multi-stage methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, subsequent works of IPOD <ref type="bibr" target="#b21">[22]</ref> and PointRCNN <ref type="bibr" target="#b22">[23]</ref> explore different proposal methods. Our proposed F-ConvNet is motivated and designed to combine the benefits of both worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED FRUSTUM CONVNET</head><p>In this section, we present our proposed Frustum ConvNet (F-ConvNet) that supports end-to-end learning of amodal 3D object detection. Design of F-ConvNet centers on the notion of square frustum, and a sequence of frustums along the same frustum axis connect a cloud of discrete, unordered points with an FCN that enables oriented 3D box estimation in a continuous 3D space. <ref type="figure" target="#fig_1">Fig.2</ref> give an illustration. By assuming the availability of 2D region proposals in RGB images, we will first introduce our way of point association with sequences of (possibly overlapped) frustums that are obtained by sliding along frustum axes determined by 2D region proposals, and compare with alternative ways of point association/grouping. We will then present the architecture of F-ConvNet, and specify how point-wise features inside individual frustums are aggregated and re-formed as 2D feature maps for a continuous frustum-level feature fusion and 3D box estimation. We finally explain how an F-ConvNet can be trained using losses borrowed from the literature of 2D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Associating Point Clouds with Sliding Frustums</head><p>Learning semantics from point clouds grounds on extraction of low-level geometric features that are defined over local groups of neighboring points. Due to the discrete, unordered nature of point cloud, there exists no oracle way to associate individual points into local groups. In the literature of point set classification/segmentation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, local groupings are formed by searching nearest neighbors, with seed points sampled from a point cloud using farthest point sampling (FPS). FPS can efficiently cover a point cloud, but it is unaware of object positions; consequently, grouping based on FPS is not readily useful for tasks concerning with detection of object instances from a point cloud. Rather than sampling seed points for local groupings, methods of VoxelNet style <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref> define a regular grid of equally spaced voxels in the 3D space, and points falling in same voxels are grouped together. Although voxels may densely cover the entire 3D space, their sizes and grid positions do not take object boundaries into account. In addition, their settings usually assume prior knowledge of the 3D environment and the contained object categories (e.g., car and pedestrian in the KITTI dataset <ref type="bibr" target="#b16">[17]</ref>), which, however, are not always available.</p><p>To address these limitations, we propose the following scheme to group local points. We assume that an RGB image is available accompanying the 3D point cloud, and that 2D region proposals are also provided by off-the-shelf object detectors [1]- <ref type="bibr" target="#b2">[3]</ref>. A sequence of (possibly overlapped) frustums can be obtained by sliding a pair of parallel planes along the frustum axis with an equal stride, where the pair of planes are also perpendicular to the frustum axis. We also assume the optical axis of the camera is perpendicular to this 2D region, which suggests an initial adjustment of camera coordinate system has already been performed, as shown in <ref type="figure" target="#fig_2">Fig.3</ref>. We generate such a sequence of frustums for each 2D region proposal, and we use thus obtained frustum sequences to group points, i.e., points falling inside the same frustums are grouped together. Assuming that 2D region proposals are accurate enough, our frustums mainly contain foreground points, and are aware of object boundaries. We note that for each 2D region proposal, a single frustum of larger size (defined by the image plane and the farthest plane) is generated in <ref type="bibr" target="#b12">[13]</ref> and all points falling inside this frustum are grouped together; consequently, an initial stage of foreground point segmentation has to be performed before amodal 3D box estimation. In contrast, we generate for each region proposal a sequence of frustums whose feature vectors are arrayed as a feature map and used in a subsequent FCN for an end-to-end estimation of oriented boxes in the continuous 3D space, as presented shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Architecture of Frustum ConvNet</head><p>Given a sequence of frustums generated from a region proposal, the key design of an F-ConvNet is to aggregate at its early stage point-wise features inside each frustum as a frustum-level feature vector, and then array as a 2D feature map these feature vectors of individual frustums for use of a subsequent FCN, which, together with a detection header, supports an end-to-end and continuous estimation of oriented 3D boxes. <ref type="figure" target="#fig_1">Fig. 2</ref> gives the architecture. We present separate components of the F-ConvNet as follows. Frustum-level feature extraction via PointNet For a 2D region proposal in an RGB image, assume a sequence of T frustums of height u are generated by sliding along the frustum axis with a stride s. For any one of them, assume it contains M local points whose coordinates in the camera coordinate system are denoted as</p><formula xml:id="formula_0">{x i = (x i , y i , z i )} M i=1 .</formula><p>To learn and extract point-wise features, we use PointNet <ref type="bibr" target="#b10">[11]</ref> in this work that stacks three fully-connected (FC) layers, followed by a final layer that aggregates features of individual points as a frustum-level feature vector via element-wise max pooling, as shown in <ref type="figure" target="#fig_1">Fig.2(a)</ref>. We apply the PointNet to each frustum, and thus the T duplicate PointNets, with shared weights, form the lower, parallel streams of our F-ConvNet. Instead of using {x i } M i=1 as input of PointNet directly, we use relative coordinates {x i = (x i ,ȳ i ,z i )} M i=1 that are obtained by subtracting each x i with the centroid c of the frustum, i.e., x i = x i − c for i = 1, . . . , M. We note that choices other than PointNet are applicable as well, such as PointCNN <ref type="bibr" target="#b23">[24]</ref>.</p><p>Fully convolutional network Denote the extracted frustumlevel feature vectors as {f i } L i=1 , with f i ∈ R d . We array these L vectors to form a 2D feature map F of the size L × d, which will be used as input of a subsequent FCN. As shown in <ref type="figure" target="#fig_1">Fig.2(b)</ref>, our FCN consists of blocks of conv layers, and de-conv layers corresponding to each block. Convolution in conv layers is applied across the frustum dimension by using kernels of the size 3 × d. The final layer of each of the conv blocks, except the first block, also down-samples (halves) the 2D feature map at the frustum dimension by using stride-2 convolution. Convolution and down-sampling fuse features across frustums and produce at different conv blocks virtual frustums of varying heights (along the frustum axis direction). Given output feature map of each conv block, a corresponding de-conv layer is used that up-samples at the frustum dimension the feature map to a specified (higher) resolutionL; outputs of all de-conv layers are then concatenated together along the feature dimension. Feature concatenation from virtual frustums of varying sizes provides a hierarchical granularity of frustum covering, which would be useful to estimate 3D boxes of object instances whose sizes are unknown and vary. In this work, we use an FCN of 4 conv blocks and 3 de-conv layers for KITTI, and an FCN of <ref type="figure">Fig. 4</ref>: Illustration of our multi-resolution frustum feature integration. We show an example between Block2 and Block3. We also use it in between Block3 and Block4, and between Block4 and DeConv4. Including the first resolution, we have in total four kinds of resolutions in KITTI dataset. 5 conv blocks and 4 de-conv layers for SUN-RGBD. Layer specifics of these FCNs are given in the appendix. A multi-resolution frustum feature integration variant We already know that output feature maps of conv blocks in FCN are of reduced resolutions by a power of 2 at the frustum dimension. Take the one with the size of L/2 × d as an example. For the same 2D region proposal, a new sequence of T /2 frustums can be generated by sliding along the frustum axis with a stride 2s. Applying PointNet to each of the generated frustums and arraying the resulting feature vectors produce a new feature map of the same size L/2 × d. When frustum height is doubled as 2u, the new sequence covers the same 3D space at a half coarser resolution, while its feature map being compatible with its corresponding one in FCN. We then concatenate along the feature dimension the two feature maps of the same size, giving rise to a new one of the size L/2 × 2d. A final conv layer is used to resize it back as a feature map of size L/2 × d, so that it can be placed back in FCN with no change of other FCN layers. <ref type="figure">Fig.4</ref> illustrates the above procedure. The procedure can be used for each down-sampled feature maps in FCN. We refer to this scheme as a multi-resolution frustum feature integration variant of F-ConvNet. Ablation studies in Section IV-B verify its efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detection Header and Training of Frustum ConvNet</head><p>On top of FCN is the detection header composed of two, parallel conv layers, as shown in <ref type="figure" target="#fig_1">Fig.2</ref>. They are respectively used as the classification and regression branches. The whole F-ConvNet is trained using a multi-task fashion, similar to those in 2D object detection <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>.</p><p>Suppose we have K object categories. The classification branch is trained to output aL × (K + 1) frustum-wise probability map of object categories, plus the background one. In this work, we use focal loss <ref type="bibr" target="#b24">[25]</ref> for classification branch to cope with imbalance of foreground and background samples.</p><p>Ground truth of an oriented 3D bounding box is parameterized as {x g c , y g c , z g c , l g , w g , h g , θ g }, where {x g c , y g c , z g c } denote coordinates of box center, {l g , w g , h g } denote three side Leftmost legend is the camera coordinate system. Normalized coordinate systems are shown with each cuboid box, where the origin of each system is located at each cuboid center and its direction is aligned with our first predicted box. lengths of the box, and θ g denotes the yaw angle that means the in-plane rotation perpendicular to the gravity direction. We discretize the range [−π, π) of yaw angles into N bins, and define for each frustum KN anchor boxes, i.e., N ones per foreground category. For any one of them parameterized as {x a c , y a c , z a c , l a , w a , h a , θ a }, we use centroid of the frustum as {x a c , y a c , z a c }, compute from training samples the categorywise averages of the three side lengths as {l a , w a , h a }, and set θ a as one of the bin centers of yaw angles. This gives the following offset formulas:</p><formula xml:id="formula_1">∆x = x g c − x a c , ∆y = y g c − y a c , ∆z = z g c − z a c , ∆l = l g − l a l a , ∆w = w g − w a w a , ∆h = h g − h a h a , ∆θ = θ g − θ a .<label>(1)</label></formula><p>Regression loss for the center is based on the Euclidean distance and smooth l1 regression loss for offsets of size and angle. Besides, we also use a corner loss <ref type="bibr" target="#b12">[13]</ref> to regularize box regression of all parameters. Together with the focal loss for classification branch, the whole F-ConvNet is trained using a total of three losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Final Refinement</head><p>We have assumed for now that the 2D region proposals are accurate enough. In practice, region proposals provided by object detectors do not bound object instances precisely. As a remedy, we propose a refinement that applies the same F-ConvNet architecture to points falling inside the oriented 3D boxes that we have just estimated. Specifically, we first expand each estimated box by a specified factor -we set the factor as 1.2 in this work, and normalize points inside the expanded box by translation and rotation, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. These normalized points are used as input of a second F-ConvNet for a final refinement. Ablation studies show that this refinement resolves well the issue caused by inaccurate 2D object proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Datasets and Implementation Details KITTI The KITTI dataset <ref type="bibr" target="#b16">[17]</ref> contains 7,481 training pairs and 7,518 testing pairs of RGB images and point clouds of three object categories (i.e., Car, Pedestrian, and Cyclist). For each category, detection results are evaluated based on three levels of difficulty (i.e., easy, moderate, and hard). We train two separate F-ConvNets respectively for Car and Pedestrian/Cyclist. Since ground truth of the test set is unavailable, we follow existing works <ref type="bibr" target="#b4">[5]</ref> and split the original training set into the new training and validation ones respectively of 3,712 and 3,769 samples. We conduct our ablation experiments using this splitting, and our final results on the KITTI test set are obtained by server submission. We use the official 3D IoU evaluation metrics of 0.7, 0.5, and 0.5 respectively for the categories of Car, Cyclist, and Pedestrian. SUN-RGBD The SUN-RGBD dataset <ref type="bibr" target="#b17">[18]</ref> contains 10,355 RGB-D images (5,285 training ones and 5,050 testing ones) of 10 object categories. We convert the depth images as point clouds for use of our method. Results are evaluated on the 10 categories under 0.25 3D IoU threshold. For this dataset, we do not use the final refinement of our method. Implementation details Our use of 2D object detectors is as follows. For the KITTI validation dataset, we use the 2d detection results provided by F-PointNet <ref type="bibr" target="#b12">[13]</ref>. For the KITTI test dataset, we use RRC <ref type="bibr" target="#b25">[26]</ref> for the car category and MSCNN <ref type="bibr" target="#b26">[27]</ref> for the pedestrians and cyclist categories. We directly use the release models provided by these methods. As for SUN-RGBD, we train Faster-RCNN <ref type="bibr" target="#b1">[2]</ref> with the backbone network of ResNet-50 <ref type="bibr" target="#b27">[28]</ref>. We do data augmentation to the obtained 2D region proposals by translation and scaling during training. We randomly sample from 3D points corresponding to each region proposal to have a fixed number 1,024 for KITTI and 2,048 for SUN-RGBD. For final refinement, we use a fixed number of 512. We also do random flipping and shifting to these points, similar to <ref type="bibr" target="#b12">[13]</ref>.</p><p>To prepare positive and negative training samples, we shrink ground-truth boxes by a ratio of 0.5, and count anchor boxes whose centers fall in the shrunken groundtruth boxes as foreground ones, count the others as background. We ignore the anchor boxes whose centers fall between the shrunken boxes and ground-truth boxes. We train F-ConvNets with a mini-batch size 32 on one GPU. We use ADAM optimizer with weight decay of 0.0001. Learning rates start from 0.001 and decay by a factor of 10 every 20 th epoch of the total 50 epoches. We consider a depth range of [0, 70] meters in KITTI and that of    At evaluation time, we only keep predicted foreground samples and apply an NMS module with a 3D IoU threshold of 0.1 to reduce redundancy. The final 3D detection scores are computed by adding 2D detection scores and predicted 3D bounding box scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>In this section, we verify components and variants of our proposed F-ConvNet by conducting ablation studies on the train/val split of KITTI. We follow the convention and use the car category that contains the most training examples. Before individual studies, we first report in Tab.I and Tab.II our results of 3D detection and BEV detection on the validation set. For the most important "Moderate" column, our method outperforms existing ones on both of the two tasks. Influence of 2D region proposal Our method relies on accuracy of 2D region proposals. To investigate how much it affects the performance, we use three 2D object detectors with increased practical performance, namely a baseline Faster-RCNN <ref type="bibr" target="#b1">[2]</ref> with a backbone network of ResNet-50 <ref type="bibr" target="#b27">[28]</ref>, a detector provided by <ref type="bibr" target="#b12">[13]</ref>, and an oracle one of ground-truth 2D boxes. Results in Tab.III confirm that better performance of 2D detection positively affects our method.    Effect of focal loss and final refinement We use focal loss <ref type="bibr" target="#b24">[25]</ref> to cope with imbalance of foreground and background training samples. We also propose a final refinement step to cope with less accurate 2D region proposals.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with the State of the Art</head><p>The KITTI Results Tab.VII shows the performance of our method on the KITTI test set, which is obtained by server submission. Our method outperforms all existing published works, and at the time of submission it ranks 4 th on the KITTI leaderboard. We also show performance of our method on 3D object localization in Tab. VIII. For this detection task, the 3D bounding boxes are projected to birdeye view plane and IoU is evaluated on oriented 2D boxes. Representative results of our method are visualized in <ref type="figure">Fig.6</ref>. The SUN-RGBD Results We also apply our proposed F-ConvNet to the indoor environment of SUN-RGBD. Our results in Tab.IX are better than those of all existing methods, showing the general usefulness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented a novel method of Frustum ConvNet (F-ConvNet) for amodal 3D object detection in an end-toend and continuous fashion. The proposed method is datasetagnostic and demonstrates state-of-the-art performance on both the indoor SUN-RGBD and outdoor KITTI datasets. The method is useful for many applications such as autonomous driving and robotic object manipulation. In future      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration for how a sequence of frustums are generated for a region proposal in an RGB image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The whole framework of our F-ConvNet. We group points and extract features by PointNet from a sequence of frustums, and for 3D box estimation, frustum-level features are re-formed as a 2D feature map for use of our fully convolutional network (FCN) and detection header (CLS and REG).(a) The architecture of PointNet. (b) The architecture of FCN used in Frustum ConvNet for KITTI dataset. Each convolutional layer is followed by Batch Normalization and ReLU nonlinearity. Blue-colored bar in (b) represents the 2D feature map of arrayed frustum-level feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>An illustration of our frustums (coded as different colors) for point association and frustum-level feature aggregation. A sequence of non-overlapped frustums are shown here for simplicity. Actually, we set u = 2s in our experiments. We show the top view on the right to denote clearly the sliding stride s and height u of frustums.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Normalization of point coordinates for final refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>[0, 8] in SUN-RGBD. For KITTI, we use 4 frustum resolutions of u = [0.5, 1.0, 2.0, 4.0] and s = [0.25, 0.5, 1.0, 2.0] for the car category, with d = [128, 128, 256, 512], L = 280, and L = 140, and 4 frustum resolutions of u = [0.2, 0.4, 0.8, 1.6] and s = [0.1, 0.2, 0.4, 0.8] for the pedestrian and cyclist categories, with d = [128, 128, 256, 512], L = 700, andL = 350. For SUN-RGBD, we use 5 frustum resolutions of u = [0.2, 0.4, 0.8, 1.6, 3.2] and s = [0.1, 0.2, 0.4, 0.8, 1.6], with d = [128, 128, 256, 512, 512], L = 80, andL = 40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>3D object detection AP (%) on KITTI val set.</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>MV3D [5]</cell><cell>86.55</cell><cell>78.10</cell><cell>76.67</cell></row><row><cell>VoxelNet [14]</cell><cell>89.60</cell><cell>84.81</cell><cell>78.57</cell></row><row><cell>F-PointNet [13]</cell><cell>88.16</cell><cell>84.92</cell><cell>76.44</cell></row><row><cell cols="2">ContFusion [10] 95.44</cell><cell>87.34</cell><cell>82.43</cell></row><row><cell>IPOD [22]</cell><cell>88.3</cell><cell>86.4</cell><cell>84.6</cell></row><row><cell>Ours</cell><cell>90.23</cell><cell>88.79</cell><cell>86.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>BEV detection AP (%) on KITTI val set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Influence of 2D region proposal. Each line corresponds to results from a different 2D object detector.</figDesc><table><row><cell></cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>PointNet</cell><cell>84.09</cell><cell>75.32</cell><cell>67.45</cell></row><row><cell>PointCNN</cell><cell>81.91</cell><cell>73.83</cell><cell>66.37</cell></row><row><cell>Effect of frustum feature extractor We use PointNet [11]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>to extract and aggregate point-wise features as frustum-level</cell><cell></cell><cell></cell><cell></cell></row><row><cell>feature vectors. Other choices such as PointCNN [24] are</cell><cell></cell><cell></cell><cell></cell></row><row><cell>applicable as well. To compare, we replace the element-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>wise max pooling in PointNet by the X-Conv operation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>in PointCNN for feature aggregation. Tab.IV shows that</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointCNN is also a possible choice of frustum feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>extractor; however, its performance is not necessarily better</cell><cell></cell><cell></cell><cell></cell></row><row><cell>than the simple PointNet. We use one resolution for this</cell><cell></cell><cell></cell><cell></cell></row><row><cell>experiment.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison between frustum feature extractors.Effect of the multi-resolution frustum feature integration variant To investigate the effect of this variant, we plug in various resolution combinations into F-ConvNet. Results in Tab.V confirm the efficacy.</figDesc><table><row><cell>(0.5, 1.0) (1.0, 2.0) (2.0, 4.0)</cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell></cell><cell>84.09</cell><cell>75.32</cell><cell>67.45</cell></row><row><cell></cell><cell>84.19</cell><cell>74.88</cell><cell>66.95</cell></row><row><cell></cell><cell>85.41</cell><cell>75.63</cell><cell>67.44</cell></row><row><cell></cell><cell>86.12</cell><cell>76.04</cell><cell>67.97</cell></row><row><cell></cell><cell>86.21</cell><cell>76.12</cell><cell>67.96</cell></row><row><cell></cell><cell>86.69</cell><cell>76.30</cell><cell>68.02</cell></row><row><cell></cell><cell>86.51</cell><cell>76.57</cell><cell>68.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell>: Investigation of the multi-resolution frustum</cell></row><row><cell>feature integration variant. We show different combinations</cell></row><row><cell>of pair (s, u), where s denotes sliding stride of frustums and</cell></row><row><cell>u for height of frustums.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The effects of these two components are clearly demonstrated in Tab.VI.</figDesc><table><row><cell></cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell></row><row><cell>w/o FL and w/o RF</cell><cell>83.78</cell><cell>74.05</cell><cell>65.96</cell></row><row><cell>w/o RF</cell><cell>86.51</cell><cell>76.57</cell><cell>68.17</cell></row><row><cell>Ours</cell><cell>89.02</cell><cell>78.80</cell><cell>77.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI :</head><label>VI</label><figDesc>Effects of focal loss (FL) and final refinement (RF).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>3D object detection AP (%) on KITTI test set.</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>Cars Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>Pedestrians Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>Cyclists Moderate</cell><cell>Hard</cell></row><row><cell>MV3D [5]</cell><cell>86.02</cell><cell>76.90</cell><cell>68.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VoxelNet [14]</cell><cell>77.47</cell><cell>65.11</cell><cell cols="2">57.73 39.48</cell><cell>33.69</cell><cell cols="2">31.51 61.22</cell><cell>48.36</cell><cell>44.37</cell></row><row><cell>F-PointNet [13]</cell><cell>88.70</cell><cell>84.00</cell><cell cols="2">75.33 58.09</cell><cell>50.22</cell><cell>47.57</cell><cell>75.38</cell><cell>61.96</cell><cell>54.68</cell></row><row><cell>AVOD-FPN [6]</cell><cell>88.53</cell><cell>83.79</cell><cell cols="2">77.90 58.75</cell><cell>51.05</cell><cell cols="2">47.54 68.06</cell><cell>57.48</cell><cell>50.77</cell></row><row><cell>SECOND [15]</cell><cell>88.07</cell><cell>79.37</cell><cell cols="2">77.95 55.10</cell><cell>46.27</cell><cell cols="2">44.76 73.67</cell><cell>56.04</cell><cell>48.78</cell></row><row><cell>IPOD [22]</cell><cell>86.93</cell><cell>83.98</cell><cell cols="2">77.85 60.83</cell><cell>51.24</cell><cell>45.40</cell><cell>77.10</cell><cell>58.92</cell><cell>51.01</cell></row><row><cell>PointPillars [16]</cell><cell>88.35</cell><cell>86.10</cell><cell cols="2">79.83 58.66</cell><cell>50.23</cell><cell cols="2">47.19 79.14</cell><cell>62.25</cell><cell>56.00</cell></row><row><cell>PointRCNN [23]</cell><cell>89.47</cell><cell>85.68</cell><cell cols="2">79.10 55.92</cell><cell>47.53</cell><cell cols="2">44.67 81.52</cell><cell>66.77</cell><cell>60.78</cell></row><row><cell>Ours</cell><cell>89.69</cell><cell>83.08</cell><cell cols="2">74.56 58.90</cell><cell>50.48</cell><cell cols="2">46.72 82.59</cell><cell>68.62</cell><cell>60.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VIII :</head><label>VIII</label><figDesc>3D object localization AP (BEV) (%) on KITTI test set.Fig. 6: Qualitative results on the KITTI test set. Best view in color with zoom-in. Different color bounding boxes denote different categories, with green for car, orange for pedestrian, and yellow for cyclist.</figDesc><table><row><cell>Method</cell><cell>bathtub</cell><cell>bed</cell><cell>bookshelf</cell><cell>chair</cell><cell>desk</cell><cell cols="2">dresser nightstand</cell><cell>soft</cell><cell>table</cell><cell>toilet</cell><cell>mean</cell></row><row><cell>DSS [21]</cell><cell>44.2</cell><cell>78.8</cell><cell>11.9</cell><cell>61.2</cell><cell>20.5</cell><cell>6.4</cell><cell>15.4</cell><cell>53.5</cell><cell>50.3</cell><cell>78.9</cell><cell>42.1</cell></row><row><cell>COG [29]</cell><cell>58.26</cell><cell>63.67</cell><cell>31.80</cell><cell>62.17</cell><cell>45.19</cell><cell>15.47</cell><cell>27.36</cell><cell cols="4">51.02 51.29 70.07 47.63</cell></row><row><cell>2Ddriven3D [30]</cell><cell>43.45</cell><cell>64.48</cell><cell>31.40</cell><cell cols="2">48.27 27.93</cell><cell>25.92</cell><cell>41.92</cell><cell cols="4">50.39 37.02 80.40 45.12</cell></row><row><cell>PointFusion [31]</cell><cell>37.26</cell><cell>68.57</cell><cell>37.69</cell><cell cols="2">55.09 17.16</cell><cell>23.95</cell><cell>32.33</cell><cell cols="4">53.83 31.03 83.80 45.38</cell></row><row><cell>Ren et al. [32]</cell><cell>76.2</cell><cell>73.2</cell><cell>32.9</cell><cell>60.5</cell><cell>34.5</cell><cell>13.5</cell><cell>30.4</cell><cell>60.4</cell><cell>55.4</cell><cell>73.7</cell><cell>51.0</cell></row><row><cell>F-PointNet [13]</cell><cell>43.3</cell><cell>81.1</cell><cell>33.3</cell><cell>64.2</cell><cell>24.7</cell><cell>32.0</cell><cell>58.1</cell><cell>61.1</cell><cell>51.1</cell><cell>90.9</cell><cell>54.0</cell></row><row><cell>Ours</cell><cell>61.32</cell><cell>83.19</cell><cell>36.46</cell><cell>64.40</cell><cell>29.67</cell><cell>35.10</cell><cell>58.42</cell><cell cols="3">66.61 53.34 86.99</cell><cell>57.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE IX :</head><label>IX</label><figDesc>3D object detection AP (%) on the SUN-RGBD test set (IoU 0.25).research, we will investigate more seamless ways of integrating point-wise and RGB features and we expect even better performance would be achieved.APPENDIXLayer specifics of the FCN component of F-ConvNet for the KITTI and SUN-RGBD datasets are respectively given in Tab.X and Tab.XI.</figDesc><table><row><cell>Name</cell><cell>Kernel size/Filter no./Striding/Padding</cell></row><row><cell>Block1</cell><cell>3×128 / 128 / 1 / 1</cell></row><row><cell>Block2</cell><cell>3×128 / 128 / 2 / 1 3×128 / 128 / 1 / 1</cell></row><row><cell>Block3</cell><cell>3×128 / 256 / 2 / 1 3×256 / 256 / 1 / 1</cell></row><row><cell>Block4</cell><cell>3×256 / 512 / 2 / 1 3×512 / 512 / 1 / 1</cell></row><row><cell>Deconv2</cell><cell>1×128 / 256 / 1 / 0</cell></row><row><cell>Deconv3</cell><cell>2×256 / 256 / 2 / 0</cell></row><row><cell>Deconv4</cell><cell>4×512 / 256 / 4 / 0</cell></row><row><cell>Merge conv2</cell><cell>1×256 / 128 / 1 / 0</cell></row><row><cell>Merge conv3</cell><cell>1×512 / 256 / 1 / 0</cell></row><row><cell>Merge conv4</cell><cell>1×1024 / 512 / 1 / 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE X :</head><label>X</label><figDesc>Layer specifics of the FCN component of F-ConvNet for KITTI.</figDesc><table><row><cell>Name</cell><cell>Kernel size/Filter no./Striding/Padding</cell></row><row><cell>Block1</cell><cell>3×64 / 64 / 1 / 1</cell></row><row><cell>Block2</cell><cell>3×64 / 128 / 2 / 1 3×128 / 128 / 1 / 1</cell></row><row><cell>Block3</cell><cell>3×128 / 256 / 2 / 1 3×256 / 256 / 1 / 1</cell></row><row><cell>Block4</cell><cell>3×256 / 512 / 2 / 1 3×512 / 512 / 1 / 1</cell></row><row><cell>Block5</cell><cell>3×512 / 512 / 2 / 1 3×512 / 512 / 1 / 1</cell></row><row><cell>Deconv2</cell><cell>1×128 / 256 / 1 / 0</cell></row><row><cell>Deconv3</cell><cell>2×256 / 256 / 2 / 0</cell></row><row><cell>Deconv4</cell><cell>4×512 / 256 / 4 / 0</cell></row><row><cell>Deconv5</cell><cell>8×512 / 256 / 8 / 0</cell></row><row><cell>Merge conv2</cell><cell>1×256 / 128 / 1 / 0</cell></row><row><cell>Merge conv3</cell><cell>1×512 / 256 / 1 / 0</cell></row><row><cell>Merge conv4</cell><cell>1×1024 / 512 / 1 / 0</cell></row><row><cell>Merge conv5</cell><cell>1×1024 / 512 / 1 / 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XI :</head><label>XI</label><figDesc>Layer specifics of the FCN component of F-ConvNet for SUN-RGBD.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China. Email:</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems<address><addrLine>AnnArbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1513" to="1518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pointpillars: Encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05784</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Amodal detection of 3d objects: Inferring 3d bounding boxes from 2d ones in rgb-depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5762" to="5770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Single multifeature detector for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00238</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ipod: Intensive point-based object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05276</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04244</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5420" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A unified multiscale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1525" to="1533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4622" to="4630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d object detection with latent support surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="937" to="946" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
