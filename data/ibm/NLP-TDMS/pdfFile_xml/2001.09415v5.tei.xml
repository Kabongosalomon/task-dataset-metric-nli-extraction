<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DUMA: Reading Comprehension with Transposition Thinking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DUMA: Reading Comprehension with Transposition Thinking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-choice Machine Reading Comprehension (MRC) requires model to decide the correct answer from a set of answer options when given a passage and a question. Thus in addition to a powerful Pre-trained Language Model (PrLM) as encoder, multi-choice MRC especially relies on a matching network design which is supposed to effectively capture the relationships among the triplet of passage, question and answers. While the newer and more powerful PrLMs have shown their mightiness even without the support from a matching network, we propose a new DUal Multi-head Co-Attention (DUMA) model, which is inspired by human's transposition thinking process solving the multi-choice MRC problem: respectively considering each other's focus from the standpoint of passage and question. The proposed DUMA has been shown effective and is capable of generally promoting PrLMs. Our proposed method is evaluated on two benchmark multi-choice MRC tasks, DREAM and RACE, showing that in terms of powerful PrLMs, DUMA can still boost the model to reach new state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Machine Reading Comprehension has been a heated topic and challenging problem, and various datasets and models have been proposed in recent years <ref type="bibr" target="#b28">(Trischler et al. 2017;</ref><ref type="bibr" target="#b36">Zhang et al. 2018a;</ref><ref type="bibr" target="#b16">Nguyen et al. 2016;</ref><ref type="bibr" target="#b20">Rajpurkar et al. 2016;</ref><ref type="bibr" target="#b7">Hermann et al. 2015;</ref><ref type="bibr" target="#b24">Sun et al. 2019a;</ref><ref type="bibr" target="#b13">Lai et al. 2017;</ref><ref type="bibr" target="#b37">Zhang et al. 2018b;</ref><ref type="bibr" target="#b40">Zhu et al. 2018b;</ref><ref type="bibr" target="#b38">Zhang et al. 2020b;</ref><ref type="bibr" target="#b2">Bhargav et al. 2020;</ref><ref type="bibr" target="#b9">Hu et al. 2019)</ref>. For the tasks of MRC, given passage and question, the task can be categorized as generative and selective according to its answer style <ref type="bibr" target="#b1">(Baradaran, Ghiasi, and Amirkhani 2020)</ref>. Generative tasks require the model to generate answers according to the passage and question, not limited to spans of the passage, while selective tasks give model several candidate answers Woman: Has Tom moved to the downtown? Man: No. He is still living in the country.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Where does Tom live?</head><p>In the city. In the countryside. In the downtown. to select the best one. Multi-choice MRC is a typical task in selective type, xwhich is the focus of this paper. <ref type="figure" target="#fig_0">Figure 1</ref> shows one example of DREAM dataset <ref type="bibr" target="#b24">(Sun et al. 2019a</ref>), whose task is to select the best answer among three candidates given particular passage and question.</p><p>The kernel method for a model to solve MRC problem is a two-level hierarchical process, 1) representation encoding which is done by an encoder such as PrLM; and 2) capturing the relationship among the triplet of passage, question and answer which has to be carefully handled by various matching networks such as OCN <ref type="bibr" target="#b21">(Ran et al. 2019)</ref> and DCMN <ref type="bibr" target="#b35">(Zhang et al. 2020a)</ref>. With the development of PrLMs, matching network design tends to become more complicated for more effective improvements. <ref type="table">Table 1</ref> shows that as the newer variant of the PrLM such as ALBERT <ref type="bibr" target="#b14">(Lan et al. 2020</ref>) has shown its powerfulness even without the support from a proper matching network, in the meantime, the previous models 1 <ref type="bibr" target="#b21">(Ran et al. 2019;</ref><ref type="bibr" target="#b12">Kim and Fung 2020;</ref><ref type="bibr" target="#b35">Zhang et al. 2020a</ref>) either brings very limited improvements or even cause drop on the PrLM's <ref type="bibr" target="#b5">(Devlin et al. 2018;</ref><ref type="bibr" target="#b34">Yang et al. 2019;</ref><ref type="bibr" target="#b14">Lan et al. 2020;</ref><ref type="bibr" target="#b4">Clark et al. 2020)</ref> performance, which motivates us to develop an more effective mechanism to support the powerful enough PrLMs. Instead of designing more complicated matching network patterns, we choose a going-back-to-the-basic way to have obtained inspiration from human experience on solving MRC problems, which intuitively is to first 1) quickly read through the overall content of passage, question and answer options to build up a global impression, followed by a transposition thinking process: 2) based on dedicated information from question and answer options, re-considerate details of the passage and collect supporting evidences for question and answer options, 3) based on dedicated information from passage, re-considerate the question and answer options to decide the correct option and exclude wrong options. When humans are re-reading the passage, they tend to extract key information according to their impression of question and answer options, and it is the same when rereading question and answer options. It can be regarded as a bi-directional process in terms of transposition thinking pattern, and we adopt an attention inside network design to simulate this procedure, whose details are shown in the following Section Model.</p><p>Since attention mechanism was proposed <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref> originally for Neural Machine Translation, it has been widely used in MRC tasks to model the relationships between passage and question, and effectively enhances nearly all kinds of tasks <ref type="bibr" target="#b22">(Seo et al. 2017;</ref><ref type="bibr" target="#b37">Zhang et al. 2018b</ref><ref type="bibr" target="#b35">Zhang et al. , 2020a</ref>. Attention mechanism computes relationships of each word representation in one sequence to a target word representation in another sequence and aggregates them to form a final representation, which is commonly named as passage-to-question attention or questionto-passage attention.</p><p>Transformer <ref type="bibr" target="#b30">(Vaswani et al. 2017</ref>) uses self-attention mechanism to represent dependencies and relationships of different positions in one single sequence, which is an effective method to obtain representations of sentences for global encoding. Since <ref type="bibr" target="#b18">(Radford et al. 2018;</ref><ref type="bibr" target="#b5">Devlin et al. 2018)</ref> use it to improve the structure of PrLMs <ref type="bibr" target="#b17">(Peters et al. 2018</ref>), many kinds of PrLMs has been proposed to constantly refresh records of all kinds of tasks <ref type="bibr" target="#b14">Lan et al. 2020)</ref>. For PrLMs, the more layer and bigger hidden size they use, the better performance they achieve. Benefited from large-scale unlabeled training data and multiple stacked layers, PrLMs are able to encode sentences into very deep and precise representations. Moreover, <ref type="bibr" target="#b14">(Lan et al. 2020</ref>) reveals the importance of generalization for models, that is parameter sharing among layers can efficiently improve the performance. However, training a LM has been a time and labor consuming work, which usually needs amounts of engineering works to explore parameter settings. The bigger the model is, the more resource it consumes and the harder it can be implemented. Moreover, despite the great success they achieve in different tasks, we find that for MRC tasks, using self-attention of the Transformer to model sequences is far from enough. No matter how deep the structure is, it suffers from the nature of self-attention, which is only drawing a global relationship, while for MRC tasks the passage and the question are remarkable different in contents and literal structures and the relationship between them necessarily needs to be carefully considered. However, previous models <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015;</ref><ref type="bibr" target="#b22">Seo et al. 2017;</ref><ref type="bibr" target="#b35">Zhang et al. 2020a</ref>) only obtain limited improvement when applied on the top of PrLMs even though they use very complicated structure.</p><p>Rather than seeking a complicated matching network pattern, we are inspired by the human thinking experience solving MRC problems and put forward a new network design named as DUal Multi-head Co-Attention (DUMA) to sufficiently capture relationships among passage, question and answer options for multi-choice MRC, as a result it may effectively improve the performance when cooperating with newer and more powerful PrLMs. Our model is based on the Multi-head Attention module, which is the kernel module of Transformer. Similar to BiDAF <ref type="bibr" target="#b22">(Seo et al. 2017</ref>) and DCMN <ref type="bibr" target="#b35">(Zhang et al. 2020a)</ref>, we use the bi-directional way to obtain sufficient modeling of relationships. The contributions can be summarized as:</p><p>1) For multi-choice MRC tasks, we investigate effects of previous models over Pre-trained Language Models.</p><p>2) We propose a new DUal Multi-head Co-Attention (DUMA) model which well simulates the procedure human solving MRC tasks, and show its effectiveness and superiority to previous models through extensive experiments.</p><p>3) We have reached new state-of-the-art on two benchmark multi-choice MRC tasks, DREAM and RACE. <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref> first propose attention mechanism for Neural Machine Translation. The jointly learning of alignment and translation effectively improves the performance. Since then, attention model has been introduced to all kinds of Natural Language Processing tasks and various of architectures has been proposed <ref type="bibr" target="#b29">(Tu et al. 2020;</ref><ref type="bibr" target="#b3">Chen et al. 2019;</ref><ref type="bibr" target="#b6">Gao et al. 2019;</ref><ref type="bibr" target="#b33">Yan et al. 2019)</ref>. <ref type="bibr" target="#b22">(Seo et al. 2017</ref>) uses a multi-stage architecture to hierarchically model representation of the passage, and uses a bi-directional attention flow. These works are before PrLMs was proposed, and are able to model the representations well on the top of traditional encoder such as Long Short-Term Memory (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber 1997)</ref>. In fact, the experimental results show that they can still improve the representations of PrLMs, but the improvements are suboptimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Based on PrLMs, <ref type="bibr" target="#b21">(Ran et al. 2019)</ref> propose a method to model relationship and interaction among answer options to the benefit of distinguishing them. (Kim and Fung 2020) ensemble a model which learns to select the wrong answer. <ref type="bibr" target="#b35">(Zhang et al. 2020a</ref>) propose a sentence selection method to select more important sentences from passage to improve the matching representations, and considers interactions among answers for multi-choice MRC tasks. Even though the matching network design becomes more complicated, it cannot fully exploit powerful PrLMs and even cause drop on performance when applied on newer PrLMs 2 .</p><p>In a word, when applied on the top of PrLMs, previous models are not effective enough to improve the performance by a large margin. Thus inspired by the experience human solving MRC problems we design a new model which can effectively utilize well-modeled representations of PrLMs for even better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Definition</head><p>Multi-choice MRC tasks have to handle a triplet of passage P , question Q and answer A. When given the passage and question, the model is required to make a correct answer. The passage consists of multiple sentences, and its content can be dialogue, story, news and so on, depending on the domain of the dataset. The questions and corresponding answers are single sentences, which are usually much shorter than the passage. The target of multi-choice MRC is to select the correct answer from the candidate answer set A = {A 1 , ..., A t } for a given passage and question pair &lt; P, Q &gt;, where t is the number of candidate answers. Formally, the model needs to learn a probability distribution function F (A 1 , A 2 , ..., A t |P, Q). <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the overall architecture of our model. An encoder takes text input to form a global sequence representation, which is similar to human reading through the whole content for the first time to obtain an overall impression, and a decoder is to perform the answer prediction which is similar to human aggregating all the information to select the correct answer option. Our proposed Dual Multi-head Co-Attention (DUMA) layer is between the encoder and the decoder, which simulates human transposition thinking process to capture relationships of key information from passage, question and answer options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>To encode input tokens into representations, we take PrLMs as the encoder. To get global contextualized representation, for each different candidate answer, we concatenate it with its corresponding passage and question to form one sequence and then feed it into the encoder. Let P = [p 1 , p 2 , ..., p m ], Q = [q 1 , q 2 , ..., q n ], A = [a 1 , a 2 , ..., a k ] respectively denote the sequences of passage, question and a candidate answer, where p i , q i , a i are tokens. The adopted encoder with encoding function Enc(·) takes the concatenation of P , Q and A as input, namely E = Enc(P ⊕ Q ⊕ A). The encoding output E has a form [e 1 , e 2 , ..., e m+n+k ], where e i is a vector of fixed dimension d model that represents the respective token.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual Multi-head Co-Attention</head><p>We use our proposed Dual Multi-head Co-Attention module to calculate attention representations of passage and question-answer. <ref type="figure" target="#fig_1">Figure 2</ref> shows the details of our proposed DUMA, which may be stacked as k layers. The following formula takes k = 1 for simplicity. Our model is based on the Multi-head Attention module <ref type="bibr" target="#b30">(Vaswani et al. 2017</ref> , where e p i , e qa j denote the i-th and j-th token representation of passage and question-answer respectively and l p , l qa are the length. Then we calculate the attention representations in a bi-directional way, that is, take 1) E P as Query, E QA as Key and Value, and 2) E QA as Query, E P as Key and Value.</p><formula xml:id="formula_0">Attention(E P , E QA , E QA ) = softmax( E P (E QA ) T √ d k )E QA headi = Attention(E P W Q i , E QA W K i , E QA W V i ) MHA(E P , E QA , E QA ) =Concat(head1, ..., head h )W O MHA1 = MHA(E P , E QA , E QA ) MHA2 = MHA(E QA , E P , E P ) DUMA(E P , E QA ) = Fuse(MHA1, MHA2) (1) where W Q i ∈ R d model ×dq , W K i ∈ R d model ×d k , W V i ∈ R d model ×dv , W O i ∈ R hdv×d model are parameter matrices, d q , d k ,</formula><p>d v denote the dimension of Query vectors, Key vectors and Value vectors, h denotes the number of heads, M HA(·) denotes Multi-head Attention and DU M A(·) denotes our Dual Multi-head Co-Attention. The F use(·, ·) function first uses mean pooling to pool the sequence outputs of M HA(·), and then aggregates the two pooled outputs through a fusing method. In Subsection Investigation of Fusing Method, we investigate three fusing methods, namely element-wise multiplication, element-wise summation and concatenation.</p><p>As shown in the <ref type="figure" target="#fig_1">Figure 2</ref>, the left part of DUMA calculates question-answer-aware passage representation, which simulates human re-reading details in the passage with impression of question and answer, and the right part calculates passage-aware question-answer representation, which simulates re-considering the question-answer with deeper understanding of the passage. The F use(·, ·) function means fusing all the key information before deciding which is the best answer option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder</head><p>Our model decoder takes the outputs of DUMA and computes the probability distribution over answer options. Let A i denote the i-th answer option, O i ∈ R l denote the output of i-th &lt; P, Q, A i &gt; triplet, and A r denote the correct answer option, the loss function is computed as:</p><formula xml:id="formula_1">O i = DUMA(E P , E QAi ) L(A r |P, Q) = −log exp(W T O r ) s i=1 exp(W T O i )</formula><p>where W ∈ R l is a learnable parameter and s denotes the number of candidate answer options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Our proposed method is evaluated on two benchmark multichoice MRC tasks, DREAM and RACE.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>Our model takes ALBERT xxlarge as encoder, and use k = 2 layers of DUMA. We make the left and right part of DUMA and all the layers share parameters. Using the PrLM, our model training is done through a fine-tuning way for both tasks.</p><p>Our codes are written based on Transformers 3 , and results of ALBERT <ref type="bibr" target="#b14">(Lan et al. 2020)</ref>, ELECTRA  and BERT <ref type="bibr" target="#b5">(Devlin et al. 2018</ref>) models as baselines are our re-running unless otherwise specified.</p><p>For DREAM dataset, the learning rate is 1e-5, batch size is 8 and the warmup steps are 100. We train the model for 2 epochs in 4 hours. For RACE dataset, the learning rate is 1e-5, the batch size is 8 and the warmup steps are 1000. We train the model for 3 epochs in 2 days. For each dataset, we  use FP16 training from Apex 4 for accelerating the training process. We train the models on eight nVidia P40 GPUs. In the following Section Analysis Studies, for other re-running or re-implementation including PrLM baselines and PrLM plus other models for comparison, we use the same learning rate, warmup steps and batch size as mentioned above.</p><p>We choose the result on dev set that has stopped increasing for three checkpoints (382 steps for DREAM and 3000 steps for RACE). To obtain stable results, we run experiments 5 times with different random seeds and select the median as the ultimate performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Tables 3, 4 and 5 show the experimental results. Megatron-BERT <ref type="bibr" target="#b23">(Shoeybi et al. 2019</ref>) is a variant of BERT <ref type="bibr" target="#b5">(Devlin et al. 2018</ref>) which has 8.3 billion parameters and is nearly 40 times bigger than the largest size of ALBERT, so usually it is very hard applied in practice with present common computation power and its results are not strictly comparable to our ALBERT+DUMA. Except for this, our model both achieves state-of-the-art performance on RACE leaderboard 5 and DREAM leaderboard 6 , and it can be further improved with multi-task learning method MMM <ref type="bibr" target="#b31">(Wan 2020;</ref><ref type="bibr" target="#b10">Jin et al. 2020</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis Studies</head><p>We perform ablation experiments on the DREAM dataset to investigate key features of our proposed DUMA, such as attention modeling ability, structural simplicity, bi-directional setting and low coupling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Vanilla Self-attention and Transformer Block</head><p>We investigate whether the improvements are simply caused by the increase of parameters. Thus we conduct the experiments of ALBERT plus vanilla Multi-head Self-attention <ref type="bibr" target="#b30">(Vaswani et al. 2017)</ref>, whose inputs Q, K, V are all concatenation of passage, question and answer. Results shown in <ref type="table" target="#tab_9">Table 6</ref> indicate the effectiveness of our bi-directional coattention model design.</p><p>Moreover, we observe that the original Transformer Block (TB) <ref type="bibr" target="#b30">(Vaswani et al. 2017)</ref> consists not only Multi-head Attention module but also Layer Normalization (LN) and   <ref type="table" target="#tab_9">Table 6</ref> indicate TB-DUMA has no obvious difference with our DUMA in modeling relationships. However, our proposed DUMA holds more brief structure and equally effective performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Related Models</head><p>We compare our attention model with several representative works, which have been discussed in Section Related Works. Soft Attention <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2015)</ref> and BiDAF <ref type="bibr" target="#b22">(Seo et al. 2017</ref>) are originally based on traditional encoder such as LSTM <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber 1997)</ref>, and DCMN <ref type="bibr" target="#b35">(Zhang et al. 2020a</ref>), OCN <ref type="bibr" target="#b21">(Ran et al. 2019)</ref>, WAE <ref type="bibr" target="#b12">(Kim and Fung 2020)</ref> are based on BERT <ref type="bibr" target="#b5">(Devlin et al. 2018)</ref>. For fair comparison with Soft Attention, we simply use it to replace the attention score computing in our model. <ref type="table" target="#tab_10">Table 7</ref> compares the effectiveness of various model designs, and our proposed DUMA outperforms all other models. The performance of Soft Attention is much lower than our DUMA, which indicates the DUMA's similarity in structure with ALBERT (both use Multi-head Attention) makes it better to utilize information from encoded representation. Even though BiDAF has been a successful attention model since a long time ago, it is suboptimal for PrLMs. WAE uses an ensemble model design with nearly twice sized parameters as our model. DCMN adopts a much more complicated model structure design for better matching, but the result with ALBERT and ELECTRA is not satisfactory, which indicates it may be specially optimized for specific PrLM, while our DUMA achieves the absolutely highest accuracy with a intuitive structure design. In fact, our DUMA has nice generalization ability because it also works well with many kinds of PrLMs.   <ref type="table">Table 9</ref>: Comparison of number of parameters among different models. The models are same as listed in <ref type="table" target="#tab_10">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Investigation of Fusing Method</head><p>We investigate different implementations of fusing function from equation <ref type="formula">(1)</ref>, namely element-wise multiplication, element-wise summation and concatenation. The results are shown in <ref type="table" target="#tab_12">Table 8</ref>. We see that concatenation is optimal because it retains the matching information and lets network learn to fuse them dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Parameters</head><p>We compare number of parameters among different models in <ref type="table">Table 9</ref>. BiDAF requires the least model enlargement, however it is far less effective than our model. Besides, our model enlargement is far less than DCMN. In a word, our DUMA can obtain the best performance while requiring a little model enlargement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of DUMA Layers</head><p>We stack 2 layers of our DUMA, that is to make passage and question-answer interact more than once to obtain deeper representations. Besides, we make different layers share parameters, which is the same as ALBERT. <ref type="figure">Figure 3(a)</ref> shows the results. We can see that as the number of layers increases the performance fluctuates, and too   many layers even lead to slight drop. It is much like when human solving MRC tasks, excessive thinking and hesitation may make them misunderstand the meaning of some information. For the network with current number of parameters, it shows that interacting twice is enough to capture the key information, and stacking too many layers may disorder the well-modeled representations and make the model harder trained. Note that PrLMs <ref type="bibr" target="#b5">(Devlin et al. 2018;</ref><ref type="bibr" target="#b15">Liu et al. 2019;</ref><ref type="bibr" target="#b14">Lan et al. 2020</ref>) stacks Transformer Blocks (described in Subsection Comparison with Vanilla Self-attention and Transformer Block) instead of Multi-head Attention modules, which raises a doubt that it is the lack of LN and FFN that makes the DUMA improper for stacking deeper network. Thus we further conduct an experiment with TB-DUMA (the same as described in Subsection Comparison with Vanilla Self-attention and Transformer Block). Experimental results in <ref type="table" target="#tab_5">Table 3</ref>(b) show the same performance trend as the original DUMA, which again verifies the effectiveness of our DUMA design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Bi-direction</head><p>As figured out by <ref type="bibr" target="#b35">(Zhang et al. 2020a</ref>), bi-directional matching is a very important feature for sufficiently modeling the relationship between passage and question. To investigate the effect, we perform experiments on two settings, namely P-to-Q only and Q-to-P only. In other words, we respectively remove the right part and left part of our DUMA.   <ref type="table" target="#tab_3">Table 12</ref>: Results with and without self-attention on DREAM dataset. "SA" means self-attention and "CA" means co-attention. "SA+CA" means straightforwardly using CA to replace SA in ALBERT.</p><p>model the improvement is only 2.06% at most. The setting of bi-direction effectively improves the performance, which reveals its efficiency for modeling the relationship and agrees to the conclusion of <ref type="bibr" target="#b35">(Zhang et al. 2020a</ref>). Also it is the same as our intuitive understanding that all the passage, question and answer options should be deliberated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cooperation with PrLMs</head><p>Though the proposed DUMA is supposed to enhance stateof-the-art PrLM like ALBERT and ELECTRA, we claim that it is generally effective for less advanced models. Thus we simply replace the adopted ALBERT by its early variant BERT to examine the effectiveness of DUMA. <ref type="table" target="#tab_16">Table 11</ref> shows the results. We see that our model can be easily transferred to other PrLMs, thus it can be seemed as an effective module for modeling relationships among passage, question and answer for Multi-choice MRC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Self-attention</head><p>Our overall architecture can be split into two steps from the view of attention, of which the first is self-attention (AL-BERT) and the second is co-attention (DUMA). To examine whether the structure can be further simplified, that is only using co-attention, we straightforwardly change all of the Multi-head Self-attention of ALBERT model to our Dual Multi-head Co-attention, while still using its pre-trained parameters. The results are shown in <ref type="table" target="#tab_3">Table 12</ref>, showing that putting co-attention directly into ALBERT model may lead to much poorer performance compared to the original AL-BERT and our ALBERT+DUMA integration way. To conclude, a better way for modeling is our PrLM plus DUMA model, which is to firstly build a global relationship using self-attention of the well trained encoder and then further enhance the relationship between passage and question-answer and distill more matching information using co-attention. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Predictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>What will the man probably do?</p><p>Answer options 1) Ask for a three-day leave.</p><p>2) Go out with his friend.</p><p>3) Watch films at home. √ ALBERT +BiDAF +Sf Att +DCMN +DUMA Prediction 1) 1) 2) 3) √ <ref type="table" target="#tab_5">Table 13</ref>: Predictions of different models which are same as in <ref type="table" target="#tab_10">Table 7</ref>. "Sf Att" means Soft Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we simulates human transposition thinking experience when solving MRC problems and propose a novel DUal Multi-head Co-Attention (DUMA) to model the relationships among passage, question and answer for multichoice MRC tasks, which is able to cooperate with popular large-scale Pre-trained Language Models and brings effective performance improvements. Besides, we investigate previous attention mechanisms or matching networks applied on the top of PrLMs, and our model is shown as optimal through extensive experiments, which achieves the best performance with an intuitive motivated structure design. Our proposed DUMA enhancement has been verified effective on two benchmark multi-choice MRC tasks, DREAM and RACE, which achieves new state-of-the-art over strong PrLM baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of DREAM dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overall architecture. Our proposed DUMA is between the Encoder and Decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Different numbers of DUMA layers on DREAM dataset. (b) Different numbers of TB-DUMA layers on DREAM dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>+0.7) * 73.1(+2.1) 75.8(+4.8) * XLNet large 80.1 80.9(+0.8) 81.8(+1.7) 82.8(+2.7) *</figDesc><table><row><cell></cell><cell>+OCN</cell><cell>+WAE</cell><cell>+DCMN</cell></row><row><cell cols="2">BERT large 71.7(ALBERT xxlarge 71.0 87.2(+0.6)</cell><cell cols="2">87.3(+0.7) 85.7(-0.9)</cell></row><row><cell>86.6</cell><cell></cell><cell></cell></row><row><cell>ELECTRA large</cell><cell>86.3(+0.2)</cell><cell cols="2">86.9(+0.8) 84.9(-1.2)</cell></row><row><cell>86.1</cell><cell></cell><cell></cell></row><row><cell cols="4">Table 1: Improvements of several prior models for represen-</cell></row><row><cell cols="4">tative PrLMs (sorted by releasing time) on RACE dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). The proposed DUMA reuses the architecture of Multi-head Attention, while for the inputs, K and V are the same but Q is another sequence representation (Note this Q here denotes Query from the original paper, different from previous Q in this paper. And K, V are Key, Value respectively). We first separate the output representation from Encoder to obtain E P = [e p 1 , e p 2 , ..., e p lp ] and E QA = [e qa 1 , e qa 2 , ..., e qa lqa ]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>shows their data statistics, which indicates RACE is a large-scale dataset covering a broad range of domains, and DREAM is a small dataset presenting passage in a form of dialogue.</figDesc><table><row><cell>DREAM RACE</cell></row></table><note>DREAM DREAM (Sun et al. 2019a) is a dialogue-based dataset for multiple-choice reading comprehension, which is collected from English exams. Each dialogue as the given</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Statistical data of DREAM and RACE dataset.</figDesc><table><row><cell cols="4"># denotes the number. "Extractive" means the answers are</cell></row><row><cell cols="4">spans of the passage, and "Abstractive" means the answers</cell></row><row><cell>are not spans.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">dev test source</cell></row><row><cell>BERT large (Devlin et al. 2018)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">66.0 66.8</cell><cell></cell></row><row><cell>BERT large +WAE (Kim and Fung 2020)</cell><cell>-</cell><cell>69.0</cell><cell>leaderboard</cell></row><row><cell>XLNet large (Yang et al. 2019)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>72.0</cell><cell></cell></row><row><cell>RoBERTa large +MMM (Jin et al. 2020)</cell><cell cols="2">88.0 88.9</cell><cell></cell></row><row><cell>ALBERT xxlarge (Lan et al. 2020)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">89.2 88.5</cell><cell></cell></row><row><cell>ALBERT xxlarge +DUMA</cell><cell cols="2">89.9 90.5 91.8</cell><cell>our model</cell></row><row><cell>+multi-task learning(Wan 2020)</cell><cell>-</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Results on DREAM dataset. Results with multi-</cell></row><row><cell>task learning are reported by (Wan 2020).</cell></row><row><cell>passage has multiple corresponding questions and each</cell></row><row><cell>question has three candidate answers. The most important</cell></row><row><cell>feature of the dataset is that most of the questions are non-</cell></row><row><cell>extractive and need reasoning from more than one sentence,</cell></row><row><cell>so the dataset is small but still challenging.</cell></row><row><cell>RACE RACE (Lai et al. 2017) is a large dataset collected</cell></row><row><cell>from middle and high school English exams. Most of the</cell></row><row><cell>questions also need reasoning, and domains of passages are</cell></row><row><cell>diversified, ranging from news, story to ads.</cell></row></table><note>Evaluation For multi-choice MRC tasks, the evaluation criteria is accu- racy, acc = N + /N , where N + denotes the number of ex- amples the model selects the correct answer, and N denotes the number of the whole evaluation examples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results on RACE dataset.</figDesc><table><row><cell>model</cell><cell>dev</cell><cell>test (M/H)</cell></row><row><cell cols="2">ALBERT xxlarge 87.4</cell><cell>86.6(89.0/85.5)</cell></row><row><cell cols="3">ALBERT xxlarge 88.1(+0.7) 88.0(90.9/86.7)(+1.4) +DUMA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison with ALBERT baseline on RACE dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison among vanilla Multi-head Selfattention, DUMA and TB-DUMA on DREAM dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Comparison among different models on DREAM</cell></row><row><cell>dataset.</cell></row><row><cell>application and great success of TB for global encoding (De-</cell></row><row><cell>vlin et al. 2018; Liu et al. 2019; Lan et al. 2020), we inves-</cell></row><row><cell>tigate whether the Transformer Block better model the co-</cell></row><row><cell>attention relationships than Multi-head Attention using TB-</cell></row><row><cell>based DUMA (TB-DUMA). Experimental results shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Comparison among different implementation of the fusing method on DREAM dataset. The last three rows are our DUMA applying three kinds of implementations.</figDesc><table><row><cell>model</cell><cell>para. num.</cell></row><row><cell>ALBERT base</cell><cell>11.7M</cell></row><row><cell></cell><cell>13.5M (+1.8M) (+15.4%)</cell></row><row><cell>+Soft Attention(2015)</cell><cell></cell></row><row><cell></cell><cell>12.0M (+0.3M) (+2.6%)</cell></row><row><cell>+BiDAF(2017)</cell><cell></cell></row><row><cell></cell><cell>14.8M (+3.1M) (+26.5%)</cell></row><row><cell>+OCN(2019)</cell><cell></cell></row><row><cell></cell><cell>23.4M (+11.7M) (+100%)</cell></row><row><cell>+WAE(2020)</cell><cell></cell></row><row><cell></cell><cell>19.4M (+7.7M) (+65.8%)</cell></row><row><cell>+DCMN(2020a)</cell><cell></cell></row><row><cell></cell><cell>13.5M (+1.8M) (+15.4%)</cell></row><row><cell>+DUMA</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Bi-directional vs. uni-directional attentions on DREAM dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10</head><label>10</label><figDesc>shows the results. We see that for bi-directional model, the overall improvement is 2.84%, while for uni-directional +3.64) 64.03 (+2.49) 64.43 (+3.07)</figDesc><table><row><cell>model</cell><cell>dev</cell><cell>test</cell><cell>avg</cell></row><row><cell cols="2">ALBERT base 64.51</cell><cell>64.43</cell><cell>64.47</cell></row><row><cell></cell><cell cols="3">67.06 (+2.55) 67.56 (+3.13) 67.31 (+2.84)</cell></row><row><cell>+DUMA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT base</cell><cell>61.18</cell><cell>61.54</cell><cell>61.36</cell></row><row><cell></cell><cell>64.82 (</cell><cell></cell><cell></cell></row><row><cell>+DUMA</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Results using BERT as encoder on DREAM dataset. Results of BERT base are our re-running.</figDesc><table><row><cell>model</cell><cell>dev</cell><cell>test</cell></row><row><cell>ALBERT base (SA)</cell><cell cols="2">64.51 64.43</cell></row><row><cell cols="3">ALBERT base (SA) + DUMA (CA) 67.06 67.56</cell></row><row><cell>ALBERT base (SA+CA)</cell><cell cols="2">41.18 40.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13</head><label>13</label><figDesc>shows a hard example which needs to capture important relationships and matching information. Benefited from well-modeled relationship representations, DUMA can better distill important matching information between passage and question-answer.</figDesc><table><row><cell></cell><cell>Woman: So, you have three days off, what</cell></row><row><cell>Passage</cell><cell>are you going to do? Man: Well, I probably will rent some movi-</cell></row><row><cell></cell><cell>es with my friend bob.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/NVIDIA/apex 5 http://www.qizhexie.com/data/RACE leaderboard 6 https://dataset.org/dream/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The results of ALBERT+DCMN are our re-running of the official codes which we obtained through personal communication with its authors.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR 2015</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baradaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Amirkhani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01582</idno>
		<title level="m">A Survey on Machine Reading Comprehension Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translucent Answer Predictions in Multi-Hop Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gliozzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7700" to="7707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional spatial attention model for reading comprehension with multiple-choice questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6276" to="6283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating distractors for reading comprehension questions from real examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6423" to="6430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Read+ verify: Machine reading comprehension with unanswerable questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6529" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MMM: Multi-stage Multi-task Learning for Multichoice Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">UnifiedQA: Crossing format boundaries with a single QA system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00700</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to Classify the Wrong Answers for Multiple Choice Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Student Abstract</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding Comprehension Dataset From Examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Option Comparison Network for Multiple-choice Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03033</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional Attention Flow for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1534" />
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="217" to="231" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving Machine Reading Comprehension with General Reading Strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Matching Network for Multiple Choice Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-range Reasoning for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09074</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">NewsQA: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Select, Answer and Explain: Interpretable Multi-Hop Reading Comprehension over Multiple Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9073" to="9080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-task Learning with Multi-head Attention for Multi-choice Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04992</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Co-Matching Model for Multi-choice Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A deep cascade model for multi-document reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7354" to="7361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DCMN+: Dual Co-Matching Network for Multichoice Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Effective Character-augmented Word Embedding for Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing -7th CCF International Conference</title>
		<imprint>
			<publisher>NLPCC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling Multi-turn Conversation with Deep Utterance Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SG-Net: Syntax-Guided Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9636" to="9643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical Attention Flow for Multiple-Choice Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lingke: A Fine-grained Multi-turn Chatbot for Customer Service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 27th International Conference on Computational Linguistics: System Demonstrations (COLING DEMO</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
