<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Make One-Shot Video Object Segmentation Efficient Again</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
							<email>tim.meinhardt@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
							<email>leal.taixe@tum.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Make One-Shot Video Object Segmentation Efficient Again</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video object segmentation (VOS) describes the task of segmenting a set of objects in each frame of a video. In the semi-supervised setting, the first mask of each object is provided at test time. Following the one-shot principle, fine-tuning VOS methods train a segmentation model separately on each given object mask. However, recently the VOS community has deemed such a test time optimization and its impact on the test runtime as unfeasible. To mitigate the inefficiencies of previous fine-tuning approaches, we present efficient One-Shot Video Object Segmentation (e-OSVOS). In contrast to most VOS approaches, e-OSVOS decouples the object detection task and predicts only local segmentation masks by applying a modified version of Mask R-CNN. The one-shot test runtime and performance are optimized without a laborious and handcrafted hyperparameter search. To this end, we meta learn the model initialization and learning rates for the test time optimization. To achieve optimal learning behavior, we predict individual learning rates at a neuron level. Furthermore, we apply an online adaptation to address the common performance degradation throughout a sequence by continuously fine-tuning the model on previous mask predictions supported by a frame-to-frame bounding box propagation. e-OSVOS provides state-of-the-art results on DAVIS 2016, DAVIS 2017, and YouTube-VOS for one-shot fine-tuning methods while reducing the test runtime substantially. Code is available at https://github.com/dvl-tum/e-osvos. 10 −2 10 −1 10 0 10 1 Frames per second [Hz] 55 60 65 70 75 80 J [%] PReMVOS OnAVOS OSVOS-S MVOS OSVOS CINM FEELVOS MHP-VOS RANet RGMP FAVOS STM e-OSVOS-10 e-OSVOS-50 e-OSVOS-50-OnA e-OSVOS-100-OnA One-shot fine-tuning Other methods</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video object segmentation (VOS) describes a two-class (foreground-background) pixel-level classification task on each frame of a given video sequence. Multiple objects are discriminated by predicting individual foreground-background pixel masks. In this work, we address a variant of VOS which is semi-supervised at test time. To this end, the ground truth foreground-background segmentation mask of the first frame is provided for each object. Machine learning methods that tackle semi-supervised VOS are categorized by their utilization of the provided object ground truth masks.</p><p>We focus on fine-tuning methods <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b41">39,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b35">33]</ref>, which exploit the transfer learning capabilities of neural networks and follow a multi-step training procedure: (i) pre-training steps: learn general image and segmentation features from training the model on images and video sequences , and (ii) fine-tuning: one-shot test time optimization which enables the model to learn foreground-background characteristics specific to each object and video sequence. While elegant through their simplicity, fine-tuning methods face important shortcomings: (i) pre-training is fixed and not optimized for the subsequent fine-tuning, (ii) the hyperparameters of the test time optimization are often excessively handcrafted and fail to generalize between datasets. The common existing fine-tuning setups <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b23">21]</ref> are inefficient and suffer from a high test runtime with as many as 1000 training iterations per segmented object. As a consequence, recent methods refrain from such optimization at test time and 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2012.01866v1 [cs.CV] 3 Dec 2020 <ref type="figure">Figure 1</ref>: Performance versus runtime comparison of modern video object segmentation (VOS) approaches on the DAVIS 2017 validation set. We only show methods with publicly available runtime information. Our e-OSVOS approach demonstrates the relevance of fine-tuning for VOS and its inherent flexibility as we apply the same meta learned optimization for a varying number of iterations and with online adaptation (OnA).</p><p>instead opt for solutions such as template matching <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b15">13]</ref> and mask propagation <ref type="bibr" target="#b11">[9,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b28">26,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b46">44]</ref> for semi-supervised VOS.</p><p>In this work, we revisit the concept of one-shot fine-tuning for VOS, and show how to leverage the power of meta learning to overcome the aforementioned issues. To this end, we propose three key design choices which make one-shot fine-tuning for VOS efficient again:</p><p>Learning the Model Initialization The common pre-training <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b35">33</ref>] yields a segmentation model not specifically optimized for the subsequent fine-tuning task and requires an unlearning of potential false positive objects. Therefore, we propose to meta learn the pre-training step, i.e., we learn the best initialization of the segmentation model for a subsequent fine-tuning to any object.</p><p>Learning Neuron-Level Learning Rates We replace the laborious and handcrafted hyperparameter search from <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b35">33]</ref> and additionally optimize learning rates for each neuron of the model. In contrast to a single learning rate for the entire model <ref type="bibr" target="#b3">[1]</ref> or millions for all of its parameters <ref type="bibr" target="#b41">[39]</ref>, this allows for an ideal balance between individual learning behavior and additional trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization of Model with Object Detection</head><p>To account for the foreground-background pixel imbalance and the challenging object discrimination by individual fine-tuning, previous fine-tuning methods <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b35">33]</ref> rely on additional mask proposal or bounding box prediction methods. In contrast, we directly fine-tune Mask R-CNN <ref type="bibr" target="#b13">[11]</ref> with its separate end-to-end trainable object detection head which limits mask predictions to local object bounding boxes. This leads to our efficient one-shot video object segmentation (e-OSVOS) approach, which achieves state-of-the-art segmentation performance on the DAVIS-2016, DAVIS-2017, and YouTube-VOS benchmarks compared to all previous fine-tuning methods, at a much lower test runtime, see <ref type="figure">Figure 1</ref>. Overall, our results combat the negative preconceptions with respect to fine-tuning as a principle for semi-supervised VOS, and are intended to motivate future research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>We categorize VOS methods by their application of one-shot fine-tuning for semi-supervised VOS.</p><p>Without Fine-Tuning Several methods <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b36">34</ref>] pose VOS as the task of pixel retrieval in the learned embedding space. After the embedding learning, no fine-tuning is necessary during the inference -pixels are simply their respective nearest neighbors in the learned embedding space <ref type="bibr" target="#b9">[7,</ref><ref type="bibr" target="#b15">13]</ref> or used as a guide to the segmentation network <ref type="bibr" target="#b36">[34]</ref>. Other methods propagate segmentation masks using optical flow or point trajectories <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b37">35,</ref><ref type="bibr" target="#b38">36]</ref> or segment, propagate and combine object parts <ref type="bibr" target="#b11">[9]</ref>. The authors of <ref type="bibr" target="#b26">[24]</ref> propagate and decode segmentation masks based on the first-and query-frame embeddings. STM <ref type="bibr" target="#b33">[31]</ref> leverages a memory network to capture the information of the object in the past frames which is then decoded to predict the current frame mask. They achieve state-of-the-art performance but fail to capture small objects and require a large GPU memory for sequences with many objects.</p><p>With Fine-Tuning The concept of fine-tuning for semi-supervised VOS was first introduced in OSVOS <ref type="bibr" target="#b8">[6]</ref>. This family of methods fine-tunes a pre-trained segmentation model to the first frame ground truth mask of a given object and predicts segmentation masks for the remaining video frames. OnAVOS <ref type="bibr" target="#b35">[33]</ref> extends this approach by adapting the target appearance model online on consecutive frames using heuristics-based fine-tuning policies. While conceptually elegant, the aforementioned methods have no notion of individual objects, shapes, or motion consistency. To remedy this issue, OSVOS-S <ref type="bibr" target="#b24">[22]</ref> and PReMVOS <ref type="bibr" target="#b22">[20]</ref> leverage object detection and instance segmentation methods (e.g., MaskRCNN <ref type="bibr" target="#b13">[11]</ref>) during the inference as additional object guidance cues. This approach is akin to the tracking-by-detection paradigm, commonly followed in the multi-object tracking community. Fine-tuning methods for VOS all share one major drawback -the online fine-tuning process requires extensive manual hyperparameter search and so far numerous training iterations during the inference (up to 1000 in the original OSVOS <ref type="bibr" target="#b8">[6]</ref> method). Hence, recent methods refrain from such optimization at test time due to its impact on the runtime.</p><p>Towards Efficient Fine-Tuning Ideally, we would like to learn an appearance model and perform as-few-as-possible training steps to during the inference. One viable approach consists of posing the video object segmentation task as a meta learning problem and optimizing the fine-tuning policies (e.g., generic model initialization, learning rates, and the number of fine-tuning iterations). The first attempt in this direction, MVOS <ref type="bibr" target="#b41">[39]</ref>, proposes to learn the initialization and learning rates per model parameter. However, this approach is impractical for modern large-scale detection/segmentation networks. In this paper, we revisit the concept of meta learning for VOS and propose several critical design choices which yield state-of-the-art results and vastly outperform MVOS <ref type="bibr" target="#b41">[39]</ref> and other fine-tuning methods.</p><p>Meta Learning for Few-Shot Learning. Previous works have addressed analogous issues for image classification. The authors of MAML <ref type="bibr" target="#b12">[10]</ref> propose to learn the model initialization for an optimal subsequent fine-tuning at test time. Such initialization is supposed to benefit the fine-tuning beyond the traditional transfer learning which merely internalizes the training data. The MAML++ <ref type="bibr" target="#b3">[1]</ref> and Meta-SGD <ref type="bibr" target="#b18">[16]</ref> approaches suggest several improvements to MAML and compliment the model initialization by learning the optimal learning rate. However, both approaches limit their potential by optimizing only a single global learning rate for the entire model. The authors of <ref type="bibr" target="#b47">[45]</ref> conduct an analysis of the meta learning for few-shot scenarios problem and address the memorization problem with a specifically tailored loss function. Other approaches, such as <ref type="bibr" target="#b31">[29]</ref>, suggest to not only predict the learning rate but apply a parameterized model to predict the entire update step. However, these approaches so far are limited in their applicability to large-scale neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">One-Shot Fine-Tuning for Video Object Segmentation</head><p>For a given video sequence n with I n image frames {x n i : 0 ≤ i &lt; I n } and K n objects, video object segmentation (VOS) predicts individual object masks y n,k i of all frames x n i . In the case of semi-supervised VOS, the ground truth maskŷ n,k 0 of a single frame is provided at test time for each object. For simplicity, we assume that the given frame always corresponds to the first frame i = 0 of the video. However, potentially a video might contain multiple objects entering the sequence at different frames. The common approach for one-shot fine-tuning of a segmentation model f and its parameters θ f follows the three-step optimization pipeline presented in <ref type="bibr" target="#b8">[6]</ref>: (i) Base network: Learn general object features by training the feature extractor backbone of f on a large scale image recognition challenge, e.g. ImageNet <ref type="bibr" target="#b32">[30]</ref>. (ii) Parent network: Train f on a segmentation dataset, e.g., DAVIS-17 training set <ref type="bibr" target="#b30">[28]</ref>, to learn the foreground-background segmentation problem. (iii) Fine-tuning: Learn object and sequence-specific features by separately fine-tuning the parent network to each object of a given video sequence. It should be noted that one-shot learning by nature runs full-batch updates, hence iteration and epoch are often used interchangeably. For a sequence n, the fine-tuning yields K n separately trained models f n,k with parameters θ n,k f . The final object masks y n,k i = f n,k (x n i ) are obtained from the maximum over the predicted pixel probabilities over all objects K n . The steps (ii) and (iii) minimize the segmentation loss L seg (D, θ f ), e.g., binary cross-entropy, of the model f on a given training dataset D. For clarity, we omit the sequence and object indices on f and θ f in future references and refer to a problem solved by an optimization g as:</p><formula xml:id="formula_0">θ g f = argmin θ f with g L seg (D, θ f )<label>(1)</label></formula><p>Such optimization is defined by several hyperparameters, including, the model initialization, number of training iterations and the type of stochastic gradient descent (SGD) method as well as its learning rate(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Efficient One-Shot Video Object Segmentation</head><p>We describe the key design choices of e-OSVOS, namely, the model choice, meta learning of the finetuning optimization, and two additional test-time modifications to further enhance our performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Optimization of Model with Object Detection</head><p>Fine-tuning a fully-convolutional model for VOS suffers from two major issues: (i) the imbalance between foreground and background pixels and (ii) the challenging object discrimination by individual fine-tuning. Typically, the latter requires an unlearning of potential false positive pixels.Several finetuning approaches <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b35">33]</ref> tackle these issues by including separate mask proposal or bounding box prediction methods. We propose to directly fine-tune Mask R-CNN <ref type="bibr" target="#b13">[11]</ref> which decouples the object detection and requires demanding pixel-wise segmentation only to bounding boxes.</p><p>Mask R-CNN consists of a feature extraction backbone, a Region Proposal Network (RPN) and two network heads, namely, the bounding box object detection and mask segmentation heads. The RPN produces potential bounding box candidates, also known as proposals, on the intermediate feature representation provided by the backbone. The box detection head predicts the object class and regresses the final bounding boxes for each proposal. Finally, the segmentation head provides object masks for each object class and bounding box. The segmentation loss mentioned in Section 2 corresponds to the multi-task Mask R-CNN loss:</p><formula xml:id="formula_1">L seg = L RP N + L box + L mask .</formula><p>We adapt Mask R-CNN for the VOS task by replacing the pixel-wise cross-entropy L mask loss with the Lovász-Softmax <ref type="bibr">[5]</ref> loss. The Lovász-Softmax loss directly optimizes the intersectionover-union and demonstrates superior performance in our one-shot fine-tuning setting. In contrast to commonly applied batch normalization, group normalization <ref type="bibr" target="#b40">[38]</ref> allows for fine-tuning even on single sample (frame) batches. Therefore, we replace all normalization layers of the backbone with group normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Meta Learning the One-Shot Test Time Optimization</head><p>As outlined by <ref type="bibr" target="#b31">[29]</ref>, meta learning is of particular interest for semi-supervised or few-shot learning scenarios. In this work, we extend this idea from image classification to VOS and meta learn steps (ii) and (iii) of the optimization pipeline from Section 2. To this end, we learn differentiable components of the test time optimization, specifically, the model initialization and SGD learning rate(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Meta Tasks</head><p>In order to meta learn the optimization, we formulate the VOS fine-tuning problem as a meta task. A task represents the fine-tuning optimization on a single object of a video sequence. Given a set of N unique video sequences each with K n objects, we define the corresponding taskset T = {T n,k : 0 ≤ k &lt; K n |0 ≤ n &lt; N } with T n,k = {D n,k train , D n,k test }. As illustrated in <ref type="figure">Figure 2a</ref>, an individual task is created by splitting each sequence into a training and test dataset consisting of disjoint sets of video frames. The goal of task T n,k is to minimize the test loss L seg (D n,k test , f ) of the model f . The datasets D n,k train = {x n 0 ,ŷ n,k 0 } and D n,k test = {{x n i ,ŷ n,k i } : 1 ≤ i &lt; I n } include the first and all consecutive frames, respectively. We train e-OSVOS on the T train such that the fine-tuning optimization on any D n,k train yields optimal results on the corresponding D n,k test . This involves two optimizations, namely, the inner fine-tuning and outer meta optimization. As for all machine learning methods, the final generalization to the test taskset T test is paramount. In future references of the datasets D train and D test , we again omit the sequence and object indices n, k.             <ref type="figure">Figure 2</ref>: The test time optimization g of e-OSVOS is meta learned on a VOS taskset structured as in (a). Each task represents a video sequence with its frames split into training D n train and test D n test datasets. The optimization g depicted in (b) consists of the model initialization and a set of learning rates applied with vanilla stochastic gradient descent. Both of which are meta learned by backpropagation of the final test loss L seg (D test , θ T f ).</p><formula xml:id="formula_2">d o p 7 + 7 t H x x W j o 5 b R m W a s i Z V Q u l O S A w T X L I m c h S s k 2 p G k l C w d j i 6 m / n t M d O G K / m I k 5 Q F C R l I H n N K 0 E q t 3 j h S a P q V q l f z 5 n B X i V + Q K h R o 9 C t f v U j R L G E S q S D G d H 0 v x S A n G j k V b F r u Z Y a l h I 7 I g H U t l S R h J s j n 1 0 7 d c 6 t E b q y 0 L Y n u X P 0 9 k Z P E m E k S 2 s 6 E 4 N A s e z P x P 6 + b Y X w T 5 F y m G T J J F 4 v i T L i o 3 N n r b s Q 1 o y g m l h C q u b 3 V p U O i C U U b U N m G 4 C + / v E p a l z X f q / k P V 9 X 6 b R F H C U 7 h D C 7 A h 2 u o w z 0 0 o A k U n u A Z X u</formula><formula xml:id="formula_3">d o p 7 + 7 t H x x W j o 5 b R m W a s i Z V Q u l O S A w T X L I m c h S s k 2 p G k l C w d j i 6 m / n t M d O G K / m I k 5 Q F C R l I H n N K 0 E q t 3 j h S a P q V q l f z 5 n B X i V + Q K h R o 9 C t f v U j R L G E S q S D G d H 0 v x S A n G j k V b F r u Z Y a l h I 7 I g H U t l S R h J s j n 1 0 7 d c 6 t E b q y 0 L Y n u X P 0 9 k Z P E m E k S 2 s 6 E 4 N A s e z P x P 6 + b Y X w T 5 F y m G T J J F 4 v i T L i o 3 N n r b s Q 1 o y g m l h C q u b 3 V p U O i C U U b U N m G 4 C + / v E p a l z X f q / k P V 9 X 6 b R F H C U 7 h D C 7 A h 2 u o w z 0 0 o A k U n u A Z X u</formula><formula xml:id="formula_4">d o p 7 + 7 t H x x W j o 5 b R m W a s i Z V Q u l O S A w T X L I m c h S s k 2 p G k l C w d j i 6 m / n t M d O G K / m I k 5 Q F C R l I H n N K 0 E q t 3 j h S a P q V q l f z 5 n B X i V + Q K h R o 9 C t f v U j R L G E S q S D G d H 0 v x S A n G j k V b F r u Z Y a l h I 7 I g H U t l S R h J s j n 1 0 7 d c 6 t E b q y 0 L Y n u X P 0 9 k Z P E m E k S 2 s 6 E 4 N A s e z P x P 6 + b Y X w T 5 F y m G T J J F 4 v i T L i o 3 N n r b s Q 1 o y g m l h C q u b 3 V p U O i C U U b U N m G 4 C + / v E p a l z X f q / k P V 9 X 6 b R F H C U 7 h D C 7 A h 2 u o w z 0 0 o A k U n u A Z X u</formula><formula xml:id="formula_5">d o p 7 + 7 t H x x W j o 5 b R m W a s i Z V Q u l O S A w T X L I m c h S s k 2 p G k l C w d j i 6 m / n t M d O G K / m I k 5 Q F C R l I H n N K 0 E q t 3 j h S a P q V q l f z 5 n B X i V + Q K h R o 9 C t f v U j R L G E S q S D G d H 0 v x S A n G j k V b F r u Z Y a l h I 7 I g H U t l S R h J s j n 1 0 7 d c 6 t E b q y 0 L Y n u X P 0 9 k Z P E m E k S 2 s 6 E 4 N A s e z P x P 6 + b Y X w T 5 F y m G T J J F 4 v i T L i o 3 N n r b s Q 1 o y g m l h C q u b 3 V p U O i C U U b U N m G 4 C + / v E p a l z X f q / k P V 9 X 6 b R F H C U 7 h D C 7 A h 2 u o w z 0 0 o A k U n u A Z X u H N U c 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B y 2 + P Q g = = &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N</formula><formula xml:id="formula_6">Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N 0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N</formula><formula xml:id="formula_7">Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N 0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N</formula><formula xml:id="formula_8">Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N 0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N</formula><formula xml:id="formula_9">Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N</formula><formula xml:id="formula_10">Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N</formula><formula xml:id="formula_11">Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N</formula><formula xml:id="formula_12">Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Meta Optimization</head><p>In analogy to How to train your MAML <ref type="bibr" target="#b3">[1]</ref>, our test time optimization consists of a vanilla SGD with two trainable components, namely, the initialization of the model f and learning rates λ which are applied for a fixed number of iterations. We refer to the trainable parameters of such an optimization g with θ g . Learning a single task involves the following bi-level optimization problem for θ g and θ f :</p><formula xml:id="formula_13">θ * g = argmin θg L seg (D test , θ g f ),<label>(2)</label></formula><formula xml:id="formula_14">s.t. θ g f = argmin θ f with g L seg (D train , θ f ).<label>(3)</label></formula><p>The outer optimization in Equation <ref type="formula" target="#formula_13">(2)</ref> is handcrafted and performed on batches of tasks from T train . The bi-level optimization aims to maximize the generalization from a given training to its corresponding test dataset. In practice, one step of Equation <ref type="formula" target="#formula_13">(2)</ref> includes multiple steps in Equation <ref type="formula" target="#formula_14">(3)</ref>. This corresponds to fine-tuning the model f for multiple iterations on the first frame D train . The optimization g is trained by Backpropagation Through Time (BPTT) of the test loss after T training iterations:</p><formula xml:id="formula_15">L BP T T =L seg (D test , θ T f ),<label>(4)</label></formula><p>with θ t+1</p><formula xml:id="formula_16">f = g(∇ θ t f L seg (D train , θ t f ), θ t f ).<label>(5)</label></formula><p>As illustrated in <ref type="figure">Figure 2b</ref>, g connects the computational graph of each iteration over time. The optimization applies a gradient descent step with respect to D train and updates f . To this end, the optimization receives the current model parameters θ t f and their gradients ∇ θ t f L seg (D train , θ t f ). After T updates, the optimization itself is updated to minimize L BP T T with respect to the updated model f . As Equation (5) already requires the computation of model parameter gradients, the outer backpropagation of L BP T T introduces second order derivatives. To reduce the computational effort, these can be omitted which is equivalent to ignoring the dashed edges of the graph in <ref type="figure">Figure 2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Learning the Segmentation Model Initialization</head><p>Meta learning the model initialization for a subsequent optimization (fine-tuning) yields superior performance compared to classic transfer learning approaches (parent network training). The initialization not only internalizes the data of the tasks, but benefits the subsequent fine-tuning step. Previous works <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b41">39]</ref>, have applied this successfully to few-shot image classification. For semisupervised VOS, meta learning provides a model initialization θ 0 f for the fine-tuning optimization in Equation <ref type="bibr">(5)</ref>. Such an initialization avoids biases for specific objects and eases the individual fine-tuning to each object significantly. In addition to the classic overfitting to T train , meta learning is prone to zero-shot collapsing, also called the memorization problem <ref type="bibr" target="#b47">[45]</ref>. For image classification, this is avoided by randomly shuffling the class labels for each training task. For multi-object VOS we tackle the issue of zero-shot collapsing by separating objects of the same sequence to multiple tasks. The first two example tasks in <ref type="figure">Figure 2a</ref> demonstrate the necessity of a one-shot optimization for segmenting different objects given the same input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Learning Neuron-Level Learning Rates</head><p>The optimization g performs Equation <ref type="formula" target="#formula_16">(5)</ref> with a vanilla SGD step and updates the segmentation model by applying a set of meta learned learning rates λ for a fixed number of iterations. The entire set of trainable optimization parameters is denoted as θ g = {θ 0 f , λ}. Previous meta learning for few-shot approaches applied learning rates for different parameter hierarchy levels, from a single global learning rate for the entire model in <ref type="bibr" target="#b18">[16]</ref>, to learning rates for all model parameters θ f in MVOS <ref type="bibr" target="#b41">[39]</ref>. The latter is unfeasible for many modern state-of-the-art segmentation networks as it effectively doubles the number of trainable optimization parameters (|θ g | ≈ 2|θ 0 f |). Therefore, we propose an ideal balance between individual learning behavior and additional trainable parameters by optimizing a set of learning rates at the neuron level. A common linear neural network layer consists of multiple neurons, or kernels for convolution layers, where each neuron applies a weight tensor and corresponding scalar bias. We predict a pair of learning rates for each neuron of the model f , i.e., a single rate for each weight tensor and bias scalar. The amount of additional trainable parameters is neglectable for modern segmentation models as their total number of parameters typically exceeds 10 7 . In Algorithm 1 of the supplementary, we illustrate the full e-OSVOS training pipeline for a given VOS taskset T train .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Online Adaption and Bounding Box Propagation</head><p>By nature, fine-tuning methods are prone to overfit on the given single frame dataset D n,k train = {x n 0 ,ŷ n,k 0 }. For sequences with changing object appearance or new similar objects entering the scene, such overfitting often results in degrading recognition performance or drifting of the segmentation mask. However, e-OSVOS incorporates two test time techniques to overcome those problems.</p><p>Online adaptation Inspired by <ref type="bibr" target="#b35">[33]</ref>, we apply an online adaptation (OnA) which continuously finetunes the segmentation model on both the given first frame ground truth and past mask predictions. First, we fine-tune the model for T iterations only on the first frame which yields θ T f and then continue the fine-tuning every I OnA frames for T OnA additional iterations on the combined online dataset D n,k train = {x n 0 ,ŷ n,k 0 } ∪ {x n i , y n,k i }. In contrast to <ref type="bibr" target="#b35">[33]</ref>, our efficient test time optimization allows for a reset of the model before every additional fine-tuning to the first-frame model state θ T f . Such a reset avoids the accumulation of false positive pixels wrongly considered as ground truth. Our learned optimization g generalizes to such an online adaptation without any additional meta learning.</p><p>Bounding Box Propagation In analogy to <ref type="bibr" target="#b6">[4]</ref>, we extend the RPN proposals with the detected object boxes of the previous frame. To account for the changing position of the object, we augment the previous boxes with random spatial transformations. Starting with the first frame ground truth boxes, the frame-to-frame propagation facilitates the tracking of each object over the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We demonstrate the applicability of e-OSVOS on three semi-supervised VOS benchmarks, namely, DAVIS 2016 <ref type="bibr" target="#b27">[25]</ref>, DAVIS 2017 <ref type="bibr" target="#b30">[28]</ref>, and YouTube-VOS <ref type="bibr" target="#b43">[41]</ref>. The tasksets T for training and evaluation of e-OSVOS are constructed from the corresponding training, validation, and test video sequences of each benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>DAVIS 2016 The DAVIS 2016 <ref type="bibr" target="#b27">[25]</ref> benchmark consists of a training and validation set with 30 and 20 single object video sequences, respectively. Every sequence is captured at 24 frames per second (FPS) and semi-supervision is achieved by providing the respective first frame object mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS 2017</head><p>The DAVIS 2017 <ref type="bibr" target="#b30">[28]</ref> benchmark extends DAVIS-16 with 100 additional sequences including dedicated test-dev and test sets. The validation, test-dev, and test set each consist of 30 sequences. The extended train set contains the remaining 60 video sequences. Furthermore, DAVIS 2017 contains a mix of single and multi-object sequences with varying image resolutions.</p><p>YouTube-VOS Our largest benchmark, YouTube-VOS <ref type="bibr" target="#b43">[41]</ref>, consists of 4453 video sequences including dedicated test and validation sets with 508 and 474 sequences, respectively. As DAVIS 2017, this benchmark contains single and multi-object sequences in multiple resolutions but provides segmentation ground truth only at 6 FPS. In general, <ref type="bibr" target="#b43">[41]</ref> requires stronger tracking capabilities as objects enter in the middle of the sequence or leave and reenter the frame entirely.</p><p>Evaluation Metrics We evaluate the standard VOS metrics defined by <ref type="bibr" target="#b27">[25]</ref>. For the intersection over union (IoU )between predicted and ground truth masks, also known as Jaccard index J in %, we evaluate the mean as well as decay over the sequence. Furthermore, we report the mean contour accuracy F in %, the mean combination metric J &amp;F in % and the frames per second (FPS) in Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For all experiments, we apply a Mask R-CNN with ResNet50 <ref type="bibr" target="#b14">[12]</ref> and FPN <ref type="bibr" target="#b20">[18]</ref> pre-trained on the COCO <ref type="bibr" target="#b19">[17]</ref> segmentation dataset. In order to optimize the learning rates and model initialization jointly without overfitting, we follow previous VOS approaches such as <ref type="bibr" target="#b36">[34,</ref><ref type="bibr" target="#b25">23]</ref> and train e-OSVOS on YouTube-VOS combined with DAVIS 2017. To improve generalization, we construct training tasks T n,k ∈ T train by randomly sampling a single frame from a sequence and augmenting it with spatial and color transformations for each train and test dataset. Furthermore, for both DAVIS datasets, we fine-tune the meta learning of the model initialization for each dataset while keeping the previously learned learning rates fixed. For the outer optimization we apply RAdam <ref type="bibr" target="#b21">[19]</ref> with a fixed learning rate β, as shown in Algorithm 1 of the supplementary, on batches of 4 training tasks each distributed to a Quadro RTX 6000 GPU for a total of 4 days. To limit the computational effort, we ignore second order derivatives and fine-tune for T = 5 BPTT iterations. The learning rates are clamped to be non-negative after each meta update.</p><p>The online adaptation (OnA) is applied every I OnA = 5 steps for T OnA = 10 iterations. To further boost inner-sequence generalization, we apply spatial random transformations as in <ref type="bibr" target="#b8">[6]</ref> during the initial fine-tuning but not for the online adaptation. While the iterations are fixed to T = 5 during the meta learning e-OSVOS generalizes to varying numbers of iterations and the online adaptation without any further learning. To indicate different versions of e-OSVOS, we denote the application of online adaptation and the number of initial fine-tuning iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We demonstrate the effect of the individual e-OSVOS components on the DAVIS 2017 validation set in <ref type="table" target="#tab_0">Table 1</ref>. Both the parent and meta training utilize the combined dataset of YouTube-VOS and DAVIS 2017. For a fair comparison between a varying number of fine-tuning iterations, we refrained from any spatial random transformations at test time. The first row shows a handcrafted equivalent of the e-OSVOS test time optimization for which we apply a grid search to find the optimal global fine-tuning learning rate. Note, this baseline is not representative for state-of-the-art fine-tuning VOS approaches as we omitted any additional handcrafted test time improvements <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b24">22,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b35">33]</ref>, such as, layer-wise learning rates, learning rate scheduling, contour snapping. The handcrafted approach is inferior to meta learning the initialization and a single global learning rate even for substantially more iterations. The neuron-level learning rates and additional modifications to the Mask R-CNN motivated in Section 3 both yield substantial segmentation performance gains. While the improvement from bounding box propagation is comparatively small, it only adds an insignificant amount of additional runtime. The marginal improvement from e-OSVOS-50 to e-OSVOS-100 motivates the application of an online adaption to combat overfitting to the first frame and degrading performance over the course of the sequence. In <ref type="figure" target="#fig_12">Figure 3</ref>, we further demonstrate the efficiency of e-OSVOS on the DAVIS  Frames per second <ref type="bibr">[Hz]</ref> 2017 validation set. The meta learning enables large gains in segmentation performance after only a few fine-tuning iterations without suffering from low frames per second rates. With an increasing number of iterations, the actual inference time of the sequence becomes neglectable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Benchmark Evaluation</head><p>We present state-of-the-art VOS results for fine-tuning methods on DAVIS 2016 and 2017 in <ref type="table" target="#tab_1">Table 2</ref> and for YouTube-VOS in <ref type="table" target="#tab_2">Table 3</ref>. We focus our evaluation on fine-tuning, hence separating the results of methods without fine-tuning (FT). The overall state-of-the art method STM <ref type="bibr" target="#b33">[31]</ref>, which does not leverage fine-tuning, currently surpasses all existing approaches in terms of performance and runtime. Nevertheless, we want to motivate fine-tuning as a concept applicable to further boost results of methods like STM without harming its efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS 2016 and 2017</head><p>In terms of the important J metric, we outperform all previous one-shot finetuning approaches on the validation set while reducing the runtime multiple orders of magnitude. It is important to note, that unlike our approach all previous fine-tuning methods rely on post-processing or an ensemble of models to achieve optimal results. We even surpass PReMVOS <ref type="bibr" target="#b22">[20]</ref>, the longtime state-of-the-art VOS method, with a much simpler and more efficient fine-tuning approach. PReMVOS applies an additional contour snapping, as in <ref type="bibr" target="#b8">[6]</ref>, which explains its superiority in terms of contour accuracy F. On the test-dev set all methods achieve substantially worse results in all metrics compared to the validation set. This is due to more sequences with challenging identity preservation scenarios. We do not achieve state-of-the-art results for fine-tuning methods on the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The supplementary material complements our work with the training algorithm of our efficient One-Shot Video Object Segmentation (e-OSVOS) and additional implementation as well as training details. Furthermore, we provide a more detailed comparison to PremVOS, a state-of-the-art fine-tuning method, including selected visual results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Implementation Details</head><p>In Algorithm 1 we provide a structured overview of the meta learning algorithm for the e-OSVOS test time optimization. The optimization is defined by its trainable parameters θ g = {θ 0 f , λ} consisting of the model initialization and neuron-level learning rates. Given a taskset T train of training tasks we sample batches of tasks, fine-tune a model for T iterations, and update θ g to achieve an optimal generalization to the respective test sets D n,k test of sequence n and object k. For the sake of completeness and to facilitate the reproduction of our results, we provide additional implementation details to Section 4.2 of the main paper. We do not apply regularization, such as weight decay or dropout, neither during the meta training nor at test time. To improve segmentation results, we double the default RoIAlign <ref type="bibr" target="#b13">[11]</ref> pooling window size to 28. As the YouTube-VOS <ref type="bibr" target="#b43">[41]</ref> validation dataset does not provide publicly available ground truth, we extract 100 sequences of the training set to monitor our meta learning progress.   <ref type="figure">Figure 1</ref>: Visualization of our e-OSVOS-50-OnA results on the blackswan, kite-surf, shooting and india sequences from the DAVIS 2017 validation set. We illustrate the output of the Mask R-CNN <ref type="bibr" target="#b13">[11]</ref> mask head as well as the corresponding object detection bounding box predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DAVIS 2017 Sequence Analysis</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we provide a per sequence comparison between our e-OSVOS-100-OnA and PRe-MVOS <ref type="bibr" target="#b22">[20]</ref> on the DAVIS 2017 validation set. The e-OSVOS-50-OnA variant applies T = 100 first frame fine-tuning and subsequently T OnA = 10 iterations on past mask predictions every fifth frame (I OnA = 5). PReMVOS applies up to 1000 iterations and achieves state-of-the-art results with an ensemble of additional methods including a re-identification and contour snapping module. Nevertheless, we surpass the results of PReMVOS on many sequences while running orders of magnitude faster (0.29 vs. 0.01 frames per second). In <ref type="figure">Figure 1</ref>, we present visual results on four sequences for which we achieve worse results compared to PReMVOS. Due to the RoIAlign feature pooling in the Mask R-CNN <ref type="bibr" target="#b13">[11]</ref> architecture and without an additional contour snapping, we fail to produce highly detailed masks as observable on the blackswan sequence (first row). Furthermore, the kite-surf (second row) and india (fourth row) sequences demonstrate that our e-OSVOS approach is likely to yield false positive bounding box detections once an object is not visible anymore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N 0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N 0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N 0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N 0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o c g i A j p D j k x J r 5 k P W C 1 B B + y V G n k = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X Y B E 8 l U Q E P R a 9 e K x g P 6 A N Z b P Z t G s 3 u 2 F 3 U i i h / 8 G L B 0 W 8 + n + 8 + W / c t j l o 6 4 O B x 3 s z z M w L U 8 E N e t 6 3 s 7 a + s b m 1 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>H N U c 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B y 2 + P Q g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o c g i A j p D j k x J r 5 k P W C 1 B B + y V G n k = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X Y B E 8 l U Q E P R a 9 e K x g P 6 A N Z b P Z t G s 3 u 2 F 3 U i i h / 8 G L B 0 W 8 + n + 8 + W / c t j l o 6 4 O B x 3 s z z M w L U 8 E N e t 6 3 s 7 a + s b m 1 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>H N U c 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B y 2 + P Q g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o c g i A j p D j k x J r 5 k P W C 1 B B + y V G n k = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X Y B E 8 l U Q E P R a 9 e K x g P 6 A N Z b P Z t G s 3 u 2 F 3 U i i h / 8 G L B 0 W 8 + n + 8 + W / c t j l o 6 4 O B x 3 s z z M w L U 8 E N e t 6 3 s 7 a + s b m 1 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>H N U c 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B y 2 + P Q g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o c g i A j p D j k x J r 5 k P W C 1 B B + y V G n k = " &gt; A A A B 7 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X Y B E 8 l U Q E P R a 9 e K x g P 6 A N Z b P Z t G s 3 u 2 F 3 U i i h / 8 G L B 0 W 8 + n + 8 + W / c t j l o 6 4 O B x 3 s z z M w L U 8 E N e t 6 3 s 7 a + s b m 1 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; . . . &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U 8 C i 1 Y / p U R L 1 p A w G e g D U X N N 6 h U = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I U I 9 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y f R F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 j q w M M y 8 Y d + b M J X C o O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S N k m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l 7 4 S T u 7 n f e e L a i E Q 9 4 j T l Q U x H S k S C U b S S 3 x 8 m a A b V m l t 3 F y D r x C t I D Q q 0 B t U v m 2 N Z z B U y S Y 3 p e W 6 K Q U 4 1 C i b 5 r N L P D E 8 p m 9 A R 7 1 m q a M x N k C + W n Z E L q w x J l G j 7 F J K F + j u R 0 9 i Y a R z a y Z j i 2 K x 6 c / E / r 5 d h d B P k Q q U Z c s W W H 0 W Z J J i Q + e V k K D R n K K e W U K a F 3 Z W w M d W U o e 2 n Y k v w V k 9 e J + 2 r u u f W v Y f r W v O 2 q K M M Z 3 A O l + B B A 5 p w D y 3 w g Y G A Z 3 i F N 0 c 5 L 8 6 7 8 7 E c L T l F 5 h T + w P n 8 A f F o j s I = &lt; / l a t e x i t &gt; (a) Meta Taskset f &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 4 H L E b h r 7 T E t a e h B b 4 y g F Y y u i V 8 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q 7 K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G N n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y v 1 2 z y O I p z B O V y C B z W o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A y v G M 6 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 4 H L E b h r 7 T E t a e h B b 4 y g F Y y u i V 8 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W yx W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q 7 K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W YU D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G N n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y v 1 2 z y O I p z B O V y C B z W o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A y v G M 6 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 4 H L E b h r 7 T E t a e h B b 4 y g F Y y u i V 8 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q 7 K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G N n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y v 1 2 z y O I p z B O V y C B z W o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A y v G M 6 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 4 H L E b h r 7 T E t a e h B b 4 y g F Y y u i V 8 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q 7 K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W Y U D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G N n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y v 1 2 z y O I p z B O V y C B z W o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A y v G M 6 g = = &lt; / l a t e x i t &gt; f &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 4 H L E b h r 7 T E t a e h B b 4 y g F Y y u i V 8 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q 7 K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W YU D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G N n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y v 1 2 z y O I p z B O V y C B z W o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A y v G M 6 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 4 H L E b h r 7 T E t a e h B b 4 y g F Y y u i V 8 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q 7 K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W YU D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G N n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y v 1 2 z y O I p z B O V y C B z W o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A y v G M 6 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 4 H L E b h r 7 T E t a e h B b 4 y g F Y y u i V 8 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q 7 K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W YU D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G N n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y v 1 2 z y O I p z B O V y C B z W o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A y v G M 6 g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 4 H L E b h r 7 T E t a e h B b 4 y g F Y y u i V 8 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p G Q 7 K F b f q L k D W i Z e T C u R o D M p f / W H M 0 g i l Y Y J q 3 f P c x P g Z V Y Y z g b N S P 9 W YU D a h I + x Z K m m E 2 s 8 W h 8 7 I h V W G J I y V L W n I Q v 0 9 k d F I 6 2 k U 2 M 6 I m r F e 9 e b i f 1 4 v N e G N n 3 G Z p A Y l W y 4 K U 0 F M T O Z f k y F X y I y Y W k K Z 4 v Z W w s Z U U W Z s N i U b g r f 6 8 j p p X 1 U 9 t + o 1 r y v 1 2 z y O I p z B O V y C B z W o w z 0 0 o A U M E J 7 h F d 6 c R + f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A y v G M 6 g = = &lt; / l a t e x i t &gt; ✓ t f &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H U A b W O y R I G a l j o / k 2 c t 8 2 Z O W A S w = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 G S y C q 5 K I o M u i G 5 c V 7 A O a G C b T S T t 0 k g k z N 0 K J X f g r b l w o 4 t b f c O f f O G m z 0 N Y D w x z O u Z c 5 c 8 J U c A 2 O 8 2 0 t L a + s r q 1 X N q q b W 9 s 7 u / b e f l v L T F H W o l J I 1 Q 2 J Z o I n r A U c B O u m i p E 4 F K w T j q 4 L v / P A l O Y y u Y N x y v y Y D B I e c U r A S I F 9 6 I V S 9 P U 4 N h f 2 Y M i A B N E 9 B H b N q T t T 4 E X i l q S G S j Q D + 8 v r S 5 r F L A E q i N Y 9 1 0 n B z 4 k C T g W b V L 1 M s 5 T Q E R m w n q E J i Z n 2 8 2 n + C T 4 x S h 9 H U p m T A J 6 q v z d y E u s i o p m M C Q z 1 v F e I / 3 m 9 D K J L P + d J m g F L 6 O y h K B M Y J C 7 K w H 2 u G A U x N o R Q x U 1 W T I d E E Q q m s q o p w Z 3 / 8 i J p n 9 V d p + 7 e n t c a V 2 U d F X S E j t E p c t E F a q A b 1 E Q t R N E j e k a v 6 M 1 6 s l 6 s d + t j N r p k l T s H 6 A + s z x 9 L S 5 Z D &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H U A b W O y R I G a l j o / k 2 c t 8 2 Z O W A S w = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 G S y C q 5 K I o M u i G 5 c V 7 A O a G C b T S T t 0 k g k z N 0 K J X f g r b l w o 4 t b f c O f f O G m z 0 N Y D w x z O u Z c 5 c 8 J U c A 2 O 8 2 0 t L a + s r q 1 X N q q b W 9 s 7 u / b e f l v L T F H W o l J I 1 Q 2 J Z o I n r A U c B O u m i p E 4 F K w T j q 4 L v / P A l O Y y u Y N x y v y Y D B I e c U r A S I F 9 6 I V S 9 P U 4 N h f 2 Y M i A B N E 9 B H b N q T t T 4 E X i l q S G S j Q D + 8 v r S 5 r F L A E q i N Y 9 1 0 n B z 4 k C T g W b V L 1 M s 5 T Q E R m w n q E J i Z n 2 8 2 n + C T 4 x S h 9 H U p m T A J 6 q v z d y E u s i o p m M C Q z 1 v F e I / 3 m 9 D K J L P + d J m g F L 6 O y h K B M Y J C 7 K w H 2 u G A U x N o R Q x U 1 W T I d E E Q q m s q o p w Z 3 / 8 i J p n 9 V d p + 7 e n t c a V 2 U d F X S E j t E p c t E F a q A b 1 E Q t R N E j e k a v 6 M 1 6 s l 6 s d + t j N r p k l T s H 6 A + s z x 9 L S 5 Z D &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H U A b W O y R I G a l j o / k 2 c t 8 2 Z O W A S w = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 G S y C q 5 K I o M u i G 5 c V 7 A O a G C b T S T t 0 k g k z N 0 K J X f g r b l w o 4 t b f c O f f O G m z 0 N Y D w x z O u Z c 5 c 8 J U c A 2 O 8 2 0 t L a + s r q 1 X N q q b W 9 s 7 u / b e f l v L T F H W o l J I 1 Q 2 J Z o I n r A U c B O u m i p E 4 F K w T j q 4 L v / P A l O Y y u Y N x y v y Y D B I e c U r A S I F 9 6 I V S 9 P U 4 N h f 2 Y M i A B N E 9 B H b N q T t T 4 E X i l q S G S j Q D + 8 v r S 5 r F L A E q i N Y 9 1 0 n B z 4 k C T g W b V L 1 M s 5 T Q E R m w n q E J i Z n 2 8 2 n + C T 4 x S h 9 H U p m T A J 6 q v z d y E u s i o p m M C Q z 1 v F e I / 3 m 9 D K J L P + d J m g F L 6 O y h K B M Y J C 7 K w H 2 u G A U x N o R Q x U 1 W T I d E E Q q m s q o p w Z 3 / 8 i J p n 9 V d p + 7 e n t c a V 2 U d F X S E j t E p c t E F a q A b 1 E Q t R N E j e k a v 6 M 1 6 s l 6 s d + t j N r p k l T s H 6 A + s z x 9 L S 5 Z D &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H U A b W O y R I G a l j o / k 2 c t 8 2 Z O W A S w = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 G S y C q 5 K I o M u i G 5 c V 7 A O a G C b T S T t 0 k g k z N 0 K J X f g r b l w o 4 t b f c O f f O G m z 0 N Y D w x z O u Z c 5 c 8 J U c A 2 O 8 2 0 t L a + s r q 1 X N q q b W 9 s 7 u / b e f l v L T F H W o l J I 1 Q 2 J Z o I n r A U c B O u m i p E 4 F K w T j q 4 L v / P A l O Y y u Y N x y v y Y D B I e c U r A S I F 9 6 I V S 9 P U 4 N h f 2 Y M i A B N E 9 B H b N q T t T 4 E X i l q S G S j Q D + 8 v r S 5 r F L A E q i N Y 9 1 0 n B z 4 k C T g W b V L 1 M s 5 T Q E R m w n q E J i Z n 2 8 2 n + C T 4 x S h 9 H U p m T A J 6 q v z d y E u s i o p m M C Q z 1 v F e I / 3 m 9 D K J L P + d J m g F L 6 O y h K B M Y J C 7 K w H 2 u G A U x N o R Q x U 1 W T I d E E Q q m s q o p w Z 3 / 8 i J p n 9 V d p + 7 e n t c a V 2 U d F X S E j t E p c t E F a q A b 1 E Q t R N E j e k a v 6 M 1 6 s l 6 s d + t j N r p k l T s H 6 A + s z x 9 L S 5 Z D &lt; / l a t e x i t &gt; g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E i J K / E 3 t r R L z 9 T A m n U y a j Z 6 C u P w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R q U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A z H W M 6 w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E i J K / E 3 t r R L z 9 T A m n U y a j Z 6 C u P w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R q U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A z H W M 6 w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E i J K / E 3 t r R L z 9 T A m n U y a j Z 6 C u P w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R q U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A z H W M 6 w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E i J K / E 3 t r R L z 9 T A m n U y a j Z 6 C u P w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R q U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A z H W M 6 w = = &lt; / l a t e x i t &gt; g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E i J K / E 3 t r R L z 9 T A m n U y a j Z 6 C u P w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R q U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A z H W M 6 w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E i J K / E 3 t r R L z 9 T A m n U y a j Z 6 C u P w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R q U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A z H W M 6 w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E i J K / E 3 t r R L z 9 T A m n U y a j Z 6 C u P w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R q U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A z H W M 6 w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E i J K / E 3 t r R L z 9 T A m n U y a j Z 6 C u P w = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p O R q U K 2 7 V X Y C s E y 8 n F c j R G J S / + s O Y p R F K w w T V u u e 5 i f E z q g x n A m e l f q o x o W x C R 9 i z V N I I t Z 8 t D p 2 R C 6 s M S R g r W 9 K Q h f p 7 I q O R 1 t M o s J 0 R N W O 9 6 s 3 F / 7 x e a s I b P + M y S Q 1 K t l w U p o K Y m M y / J k O u k B k x t Y Q y x e 2 t h I 2 p o s z Y b E o 2 B G / 1 5 X X S v q p 6 b t V r X l f q t 3 k c R T i D c 7 g E D 2 p Q h 3 t o Q A s Y I D z D K 7 w 5 j 8 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A z H W M 6 w = = &lt; / l a t e x i t &gt; (b) Meta Optimization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 3 :</head><label>3</label><figDesc>We evaluate e-OSVOS for increasing number of initial fine-tuning iterations T on the DAVIS 2017 validation set. The first iterations yield the largest performance gains while still running at comparatively large frames per second rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Algorithm 1 : 1 while not done do 2 3 forall T n,k do 4 for t ← 0 to T − 1 do 5 6 L</head><label>1123456</label><figDesc>Meta learning the e-OSVOS test time optimization Input: Optimization g with model initialization and neuron-level learning rates θg = {θ 0 f , λ}. Meta learning rate β. Fine-tuning iterations T . Data: Taskset Ttrain Output: θ * g Sample tasks T n,k = {D n,k train , D n,k test } ∼ Ttrain Update n,k BP T T = Lseg(D n,k test , θ T f )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of each e-OSVOS component on the DAVIS 2017 validation set. The first row represents a handcrafted equivalent of our test time optimization. We present performance gains componentwise for 10 fine-tuning iterations and iteration-wise for our final e-OSVOS version.</figDesc><table><row><cell>Method</cell><cell>Iterations (T ) 10</cell><cell>J &amp;F ↑ 33.6</cell></row><row><cell>Mask R-CNN</cell><cell>50</cell><cell>39.7</cell></row><row><cell>+ parent training + single LR search</cell><cell>100</cell><cell>41.6</cell></row><row><cell></cell><cell>1000</cell><cell>42.7</cell></row><row><cell>Mask R-CNN</cell><cell></cell><cell></cell></row><row><cell>+ Learn model initialization and single LR</cell><cell>10</cell><cell>64.4 + 30.8</cell></row><row><cell>+ Learn neuron level learning rates</cell><cell>10</cell><cell>67.2 + 2.8</cell></row><row><cell>+ Group normalization + Lovász-Softmax</cell><cell>10</cell><cell>69.4 + 2.2</cell></row><row><cell>+ Bounding box propagation (e-OSVOS)</cell><cell>10</cell><cell>69.9 + 0.5</cell></row><row><cell>+ Online adaption (e-OSVOS-OnA)</cell><cell>10</cell><cell>71.2 + 1.3</cell></row><row><cell>e-OSVOS-T</cell><cell>50 100</cell><cell>71.3 71.2</cell></row><row><cell>e-OSVOS-T -OnA</cell><cell>50 100</cell><cell>73.7 74.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>VOS performance evaluation on the DAVIS 2016 and 2017 benchmarks. We categorize methods by their application of fine-tuning (FT) and post-processing (PP) of the predicted masks and label methods with an ensemble of models with †. The table is ordered by J &amp;F on DAVIS 2017 validation. The evaluation metrics are detailed in Sec. 4.1. If not publicly available we adopted the runtime (FPS) from<ref type="bibr" target="#b5">[3]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">DAVIS 2016 -validation</cell><cell></cell><cell cols="3">DAVIS 2017 -validation</cell><cell></cell><cell cols="2">DAVIS 2017 -test-dev</cell></row><row><cell>Method FAVOS [9] RGMP [24]</cell><cell cols="13">FT PP J ↑ J Decay ↓ F ↑ FPS ↑ J ↑ J Decay ↓ F ↑ J &amp;F ↑ J ↑ J Decay ↓ F ↑ J &amp;F ↑ 4.5 79.5 0.56 54.6 14.1 61.8 58.2 42.9 18.1 44.2 43.6 × 82.4 81.5 10.9 82.0 7.70 64.8 18.9 68.6 66.7 51.3 34.3 54.4 52.8</cell></row><row><cell>RVOS [24]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.5</cell><cell>24.9</cell><cell>63.6</cell><cell>60.6</cell><cell>47.9</cell><cell>35.7</cell><cell>52.6</cell><cell>50.3</cell></row><row><cell>MetaVOS [3]</cell><cell></cell><cell>81.5</cell><cell>5.0</cell><cell>82.7</cell><cell>4.0</cell><cell>63.9</cell><cell>14.4</cell><cell>70.7</cell><cell>67.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RANet [37]</cell><cell></cell><cell>86.6</cell><cell>7.4</cell><cell>87.6</cell><cell>30.3</cell><cell>63.2</cell><cell>18.6</cell><cell>68.2</cell><cell>65.7</cell><cell>53.4</cell><cell>21.9</cell><cell>57.3</cell><cell>55.4</cell></row><row><cell>FEELVOS [34]</cell><cell></cell><cell>81.1</cell><cell>13.7</cell><cell>82.2</cell><cell>2.22</cell><cell>69.1</cell><cell>17.5</cell><cell>74.0</cell><cell>71.5</cell><cell>55.1</cell><cell>29.8</cell><cell>60.4</cell><cell>57.8</cell></row><row><cell>MHP-VOS [42]</cell><cell></cell><cell>87.6</cell><cell>6.9</cell><cell>89.5</cell><cell>0.01</cell><cell>73.4</cell><cell>17.8</cell><cell>78.9</cell><cell>76.1</cell><cell>66.4</cell><cell>18.0</cell><cell>72.7</cell><cell>69.5</cell></row><row><cell>STM [31]</cell><cell></cell><cell>88.7</cell><cell>5.0</cell><cell>90.1</cell><cell>6.25</cell><cell>79.2</cell><cell>8.0</cell><cell>84.3</cell><cell>81.7</cell><cell>69.3</cell><cell>16.9</cell><cell>75.2</cell><cell>72.2</cell></row><row><cell cols="2">CINM  † [2] Lucid [15] MVOS [39] OSVOS [6] OSVOS-S  † [22] OnAVOS [33] PReMVOS  † [20] e-OSVOS-10 e-OSVOS-50 e-OSVOS-50-OnA e-OSVOS-100-OnA × × × × × × × × × × ×</cell><cell>× 83.4 × 83.9 × 83.3 × 79.8 × 85.6 × 86.1 84.9 85.1 85.5 85.9 86.6</cell><cell>12.3 9.1 -14.9 5.5 5.2 8.8 5.0 5.0 5.2 4.5</cell><cell cols="2">85.0 82.0 0.005 0.01 84.1 4.0 80.6 0.11 87.5 0.22 84.9 0.08 88.6 0.01 84.8 5.3 85.8 1.64 85.9 0.35 87.0 0.29</cell><cell>67.2 -56.3 56.6 64.7 61.6 73.9 69.2 70.7 73.0 74.4</cell><cell>24.6 --26.1 15.1 27.9 16.2 18.5 18.6 13.6 13.0</cell><cell>74.4 -62.1 63.9 71.3 69.1 81.8 74.6 75.9 78.3 80.0</cell><cell>70.7 -59.2 60.3 68.0 65.3 77.8 71.9 73.3 75.6 77.2</cell><cell>64.5 63.4 -47.0 52.9 49.9 67.5 --60.9 -</cell><cell>20.0 19.5 -19.2 24.1 23.0 21.7 --22.1 -</cell><cell>70.5 69.9 -54.8 62.1 55.7 75.8 --68.6 -</cell><cell>67.5 66.6 -50.9 57.5 52.8 71.6 --64.8 -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>VOS performance evaluated on the YouTube-VOS validation set. This benchmark additionally evaluates the performance on completely unseen object classes. Results of other methods are copied from<ref type="bibr" target="#b33">[31]</ref>.</figDesc><table><row><cell cols="2">Make One-Shot Video Object Segmentation</cell></row><row><cell cols="2">Efficient Again</cell></row><row><cell cols="2">Supplementary Material</cell></row><row><cell>Tim Meinhardt</cell><cell>Laura Leal-Taixé</cell></row><row><cell>Technical University of Munich</cell><cell>Technical University of Munich</cell></row><row><cell>tim.meinhardt@tum.de</cell><cell>leal.taixe@tum.de</cell></row></table><note>YouTube-VOS -validation Method FT PP Overall ↑ J Seen ↑ F Seen ↑ J Unseen ↑ F Unseen ↑</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>VOS performance comparison per sequence on the DAVIS 2017 validation benchmark between e-OSVOS and PReMVOS<ref type="bibr" target="#b22">[20]</ref> which resembles the state-of-the-art for fine-tuning methods. We present the mean Intersection over Union J averaged over the number of objects indicated in parentheses.77.1 95.8 60.9 80.8 92.6 97.5 97.1 94.1 79.8 94 88.1 88.2 94.9 88.8 85.6 83.9 54.5 83.1 44.5 60.4 86.8 78 80.7 53.2 30.4 92.4 69.6 82.6 77 75.6 e-OSVOS-100-OnA 73.4 93.1 55.2 82.1 89.7 96 96.1 92.4 86.5 94 89.2 87.9 93 88.1 85.5 81.3 54 81.2 35.9 59.8 84.8 78.2 79.3 69.7 44.8 91.7 68.3 80.7 72.1 83.1</figDesc><table><row><cell>Method</cell><cell>Bike-Packing (2)</cell><cell>Blackswan</cell><cell>Bmx-Trees (2)</cell><cell>Breakdance</cell><cell>Camel</cell><cell>Car-Roundabout</cell><cell>Car-Shadow</cell><cell>Cows</cell><cell>Dance-Twirl</cell><cell>Dog</cell><cell>Dogs-Jump (3)</cell><cell>Drift-Chicane</cell><cell>Drift-Straight</cell><cell>Goat</cell><cell>Gold-Fish (5)</cell><cell>Horsejump-High (2)</cell><cell>India (3)</cell><cell>Judo (2)</cell><cell>Kite-Surf (3)</cell><cell>Lab-Coat (5)</cell><cell>Libby</cell><cell>Loading (3)</cell><cell>Mbike-Trick (2)</cell><cell>Motocross-Jump (2)</cell><cell>Paragliding-Launch (3)</cell><cell>Parkour</cell><cell>Pigs (3)</cell><cell>Scooter-Black (2)</cell><cell>Shooting (3)</cell><cell>Soapbox (3)</cell></row><row><cell>PReMVOS [20]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Update θg ← θg − β∇ θg T j L n,k BP T T 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was funded by the Humboldt Foundation through the Sofja Kovalevskaja Award.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>We believe that the simple concept of fine-tuning a model to a specific object is incredibly powerful. With our work, we hope to inspire researchers to continue with that paradigm, now that we can properly train it to achieve state-of-the-art results. Looking at the impact that these tools can have for society, one can see extremely positive things such as the realization of social robots that could help the elderly in their daily chores.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authors are asked to include a section in their submissions discussing the broader impact of their work, including possible societal consequences, both, positive and negative.</p><p>Many methods for video object segmentation or multiple object tracking rely on appearance models of objects. In this work, we have shown that one can rely on the simple but elegant solution of fine-tuning of a model as a way to build appearance models.</p><p>Semi-supervised video object segmentation is often used to automatize video editing, e.g., to remove one object from a video. While it is clear that more automatic methods would have a positive impact in reducing the manual work needed to perform such video edits, there is also potential to misuse such technology. One could imagine the creation of fake videos, where objects are taken out or put on the scene to create out-of-context content that might lead viewers to misinterpret the situation. Nonetheless, we believe the technology is still in an early stage and far from being able to create fake content without substantial knowledge and manual work. Therefore, we believe that, for this particular task, the benefits outweigh the potential harms of the technology.</p><p>Appearance models are also key towards tackling multi-object tracking and segmentation, important for applications such as robotics. For example, social robots are often tasked with following one specific person, hence the robot has to learn fast an on the fly the appearance of the specific person that it has to follow. This can be extended to multiple people tracking, where each model would be fine-tuned to a specific person on the scene. Segmentation of an object of interest becomes also key for robotic tasks such as grasping or any object-robot interaction. But multi-object tracking and video object segmentation also have a dark side, with applications such as illegal surveillance. We want to note, that our method does not make use of any kind of identifying characteristic of a person (if the person would be our object to follow and segment). Therefore, we believe our technology does not directly contribute nor promote these kinds of misuses.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">test-dev set. However, our approach still demonstrates the potential of fine-tuning as we surpass most none-fine-tuning methods without applying any post-processing or an ensemble of methods</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the more challenging YouTube-VOS dataset, our approach yields overall better results compared to all previous fine-tuning methods. In particular, PReMVOS suffers from inferior performance on unseen object classes. This indicates that our meta learned initialization provides a superior fine-tuning initialization which is less prone to overfitting. It should be noted that some methods were evaluated on an earlier version of the YouTube-VOS benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Youtube</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vos</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>which causes slight variations in the final results</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">We first motivate our model choice to be a modified Mask R-CNN instead of a fully convolutional segmentation model. Furthermore, we meta learn the model initialization and a set of neuron-level learning rates. e-OSVOS works in addition to common test-time techniques which mitigate performance degradation, such as online adaptation with continuous finetuning and a bounding box propagation. We demonstrate the best performance amongst fine-tuning methods</title>
		<imprint/>
	</monogr>
	<note>Conclusion This works demonstrates the application of meta learning to VOS fine-tuning and makes one-shot video object segmentation efficient again. and aspire to reignite research in this promising approach to semi-supervised VOS</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How to train your MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higher-order spatio-temporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meta learning deep visual words for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harkirat</forename><surname>Singh Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2019 Workshop on Machine Learning for Autonomous Driving</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tracking without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Machine Learning, ICML&apos;17. JMLR.org</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Conf. on Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<idno>978- 1-7281-3293-8</idno>
	</analytic>
	<monogr>
		<title level="m">32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2019); Conference Location</title>
		<meeting><address><addrLine>Piscataway, NJ; Long Beach, CA, USA; Date</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
		</imprint>
	</monogr>
	<note>IEEE Conf. on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno>2019. 9</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><forename type="middle">Li</forename><surname>Meta-Sgd</surname></persName>
		</author>
		<title level="m">Learning to Learn Quickly for Few-Shot Learning. arXiv.org</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>2020. 7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Learning Representations</title>
		<meeting>the Eighth International Conference on Learning Representations</meeting>
		<imprint/>
	</monogr>
	<note>ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seoung</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The 2017 DAVIS challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>abs/1704.00675</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Ning Xu Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rvos: End-to-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised video object segmentation with super-trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Online Meta Adaptation for Fast Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maojun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1809.00461</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1809.03327</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Mhp-vos: Multiple hypotheses propagation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daizong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Metalearning without memorization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1912" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
