<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-Scale Object Detection in the Wild from Imbalanced Multi-Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junran</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Bu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large-Scale Object Detection in the Wild from Imbalanced Multi-Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training with more data has always been the most stable and effective way of improving performance in deep learning era. As the largest object detection dataset so far, Open Images brings great opportunities and challenges for object detection in general and sophisticated scenarios. However, owing to its semi-automatic collecting and labeling pipeline to deal with the huge data scale, Open Images dataset suffers from label-related problems that objects may explicitly or implicitly have multiple labels and the label distribution is extremely imbalanced. In this work, we quantitatively analyze these label problems and provide a simple but effective solution. We design a concurrent softmax to handle the multi-label problems in object detection and propose a softsampling methods with hybrid training scheduler to deal with the label imbalance. Overall, our method yields a dramatic improvement of 3.34 points, leading to the best single model with 60.90 mAP on the public object detection test set of Open Images. And our ensembling result achieves 67.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data is playing a primary and decisive role in deep learning. With the advent of ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>, deep neural network <ref type="bibr" target="#b14">[15]</ref> becomes well exploited for the first time, and an unimaginable number of works in deep learning sprung up. Some recent works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39]</ref> also prove that larger quantities of data with labels of low quality(like hashtags) could surpass the state-of-the-art methods by a large margin. Throughout the history of deep learning, it could be easily learned that the development of an area is closely related to the data.</p><p>In the past years, great progresses have also been achieved in the field of object detection. Some generic object detection datasets with annotations of high quality like Pascal VOC <ref type="bibr" target="#b8">[9]</ref> and MS COCO <ref type="bibr" target="#b20">[21]</ref> greatly boost the development of object detection, giving birth to plenty of amazing methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref>. However, these datasets are quite small in today's view, and begin to limit the advancement of object detection area to some degree. Attempts are frequently made to focus on atomic problems on these datasets instead of exploring object detection in harder scenarios.</p><p>Recently, Open Images dataset is published in terms of 1.7 million images with 12.4 million boxes annotated of 500 categories. This unseals the limits of data-hungry methods and may stimulate research to bring object detection to more general and sophisticated situations. However, accurately annotating data of such scale is labor intensive that manual labeling is almost infeasible. The annotating procedure of Open Images dataset is completed with strong assistance of deep learning that candidate labels are generated by models and verified by humans. This inevitably weakens the quality of labels because of the uncertainty of models and the knowledge limitation of human individuals, which leads to several major problems.</p><p>Objects in Open Image dataset may explicitly or implicitly have multiple labels, which differs from the traditional object detection. The object classes in Open Images form a hierarchy that most objects may hold a leaf label and all the corresponding parent labels. However, due to the annotation quality, there are cases that objects are only labeled as parent classes and leaf classes are absent. Apart from hierarchical labels, objects in Open Images dataset may also hold several leaf classes like car and toy. Another annoying case is that objects of similar classes are frequently anno-  <ref type="figure">Figure 1</ref>: Example of multi-label annotations in Open Images dataset. (a)(b) are cases that objects are explicitly annotated with multiple labels. In (a), a car toy is labeled as car and toy at the same time. In (b), an apple is hierarchically labeled as apple and fruit. (c)(d) are cases that objects implicitly have multiple labels. In (c), apples are only labeled as fruit. In (d), flashlights are always randomly labeled as torch or flashlight. tated as each other in both training and validation set, for example, torch and flashlight, as shown in 1.</p><p>As images of Open Images dataset are collected from open source in the wild, the label distribution is extremely imbalanced that both very frequent and infrequent classes are included. Hence, methods for balancing label distribution are requested to be applied to train detectors better. Nevertheless, earlier methods like over-sampling tends to impose over-fitting on infrequent categories and fail to fully use the data of frequent categories.</p><p>In this work, we engage in solving these major problems in large-scale object detection. We design a concurrent softmax to deal with explicitly and implicitly multilabel problem. We propose a soft-balance method together with a hybrid training scheduler to mitigate the over-fitting on infrequent categories and better exploit data of frequent categories. Our methods yield a total gain of 3.34 points, leading to a 60.90 mAP single model result on the public test-challenge set of Open Images, which is 5.09 points higher than the single model result of the first place method <ref type="bibr" target="#b0">[1]</ref> on the public test-challenge set last year. More importantly, our overall system achieves a 67.17 mAP, which is 4.29 points higher than their ensembled results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Object Detection. Generic object detection is a fundamental and challenging problem in computer vision, and plenty of works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref> in this area come out in recent years. Faster-RCNN <ref type="bibr" target="#b28">[29]</ref> first proposes an end-to-end two-stage framework for object detection and lays a strong foundation for successive methods. In <ref type="bibr" target="#b6">[7]</ref>, deformable convolution is proposed to adaptively sample input features to deal with objects of various scales and shapes. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref> utilize dilated convolutions to enlarge the effective receptive fields of detectors to better recognize objects of large scale. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> focus on predicting boxes of higher quality to accommodate the COCO metric.</p><p>However, most works are still exploring in datasets like Pascal VOC and MS COCO, which are small by modern standards. Only a few works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b9">10]</ref> have been proposed to deal with large-scale object detection dataset like Open Images <ref type="bibr" target="#b15">[16]</ref>. Wu et al. <ref type="bibr" target="#b37">[38]</ref> proposes a soft boxsampling method to cope with the partial label problem in large scale dataset. In <ref type="bibr" target="#b24">[25]</ref>, a part-aware sampling method is designed to capture the relations between entity and parts to help recognize part objects. Multi-Label Recognition. There have been many amazing attempts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b4">5]</ref> to solve the multilabel classification problem from different aspects. One simple and intuitive approach <ref type="bibr" target="#b1">[2]</ref> is to transform the multilabel classification into multiple binary classification problems and fuse the results, but this neglects relationships between labels. Some works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> embed dependencies among labels with deep learning to improve the performance of multi-label recognition. In <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b4">5]</ref>, graph structures are utilized to model the label dependencies. Gong et al. <ref type="bibr" target="#b10">[11]</ref> uses a ranking based learning strategy and reweights losses of different labels to achieve better accuracy. Wang et al. <ref type="bibr" target="#b33">[34]</ref> proposes a CNN-RNN framework to embed labels into latent space to capture the correlation between them. Imbalanced Label Distribution. There have been many efforts on handling long-tailed label distribution through data based resampling strategies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b23">24]</ref> or loss-based methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>. In <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref>, class-aware sampling is applied that each mini-batch is filled as uniform as possible with respect to different classes. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref> expand samples of minor classes through synthesizing new data. Dhruv et al. <ref type="bibr" target="#b23">[24]</ref> computes a replication factor for each image based on the distribution of hashtags and duplicates images the prescribed number of times.</p><p>As for loss-based methods, loss weights are assigned to samples of different classes to match the imbalanced label distribution. In <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>, samples are re-weighted by inverse class frequency, while Yin et al. <ref type="bibr" target="#b5">[6]</ref> calculates the effective number of each class to re-balance the loss. In OHEM <ref type="bibr" target="#b31">[32]</ref> and focal loss <ref type="bibr" target="#b19">[20]</ref>, difficulty of samples are evaluated in term of losses and hard samples are assigned higher loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Setting</head><p>Open Images dataset is currently the largest released object detection dataset to the best of our knowledge. It contains 12 million annotated instances for 500 categories on 1.7 million images. Considering its large size, it is unfeasible to manually annotate such huge number of images on 500 categories Owing to its scale size and annotation styles, we argue that there are three major problems regarding this kind of dataset, apart from the missing annotation problem which has been discussed in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b24">25]</ref>. Objects may explicitly have multiple labels. As objects in physical world always contain rich categorical attributes on different levels, the 500 categories in Open Images dataset form a class hierarchy of 5 levels, with 443 of the nodes as leaf classes and 57 of the nodes as parent classes. It is likely that some objects hold multiple labels including leaf labels and parent labels, like apple and fruit as shown in <ref type="figure">Figure 1b</ref>. Another case is that an object could easily have multiple leaf labels. For instance, a car-toy is labeled as toy and car at the same time as shown in <ref type="figure">Figure 1a</ref>. This happens frequently in dataset and all leaf labels are requested to be predicted during evaluation. Different from previous single-label object detection, how to deal with multiple labels is one of the crucial factors for object detection in this dataset. Objects may implicitly have multiple labels. Other than the explicit multi-label problem, there is also the implicit multi-label problem caused by the limited and inconsistent knowledge of human annotators. There remain many pairs of leaf categories that are hard to distinguish, and labels of these pairs are mixed up randomly. We analyze the proportion that an object of a leaf class is labeled as another, and find that there are at least 115 pairs of severely confused categories(confusion ratio ≥ 0.1). We display the top-55 confused pairs in <ref type="figure" target="#fig_1">Figure 3a</ref>, and find many categories heavily confused. For instance, nearly 65% of the torches are labeled as flashlight and 50% of the leopards are labeled as cheetah.</p><p>Besides, labels of leaf and parent classes are always not complete so that a large amount of objects are only annotated with parent labels without leaf label. As shown in <ref type="figure">Figure 1c</ref>, an apple is sometimes labeled as apple and sometimes labeled as fruit without the leaf annotation. We demonstrate the ratio of leaf annotations and parent annotations lacking leaf annotations in <ref type="figure" target="#fig_1">Figure 3b</ref>. This implicit coexistence phenomenon also happens frequently and needs to be taken into consideration, otherwise the detectors may learn the false signals.</p><p>Imbalanced label distribution. To build such a huge dataset, images are collected from open source in the wild. As on can expect, Open Images dataset suffers from extremely imbalanced label distribution that both infrequent and very frequent categories are included. As shown in <ref type="figure">Figure 2</ref>, the most frequent category owns nearly 30k times the training images of the most infrequent category. Naive re-balance strategy, such as widely used class-aware sampling <ref type="bibr" target="#b9">[10]</ref> which uniformly samples training images of different categories could not cope with such extreme imbalance, and may lead to two consequences: 1) For frequent categories, they are not trained sufficiently, for the reason that most of their training samples have never been seen and are wasted. 2) For infrequent categories, the excessive over-sampling on them may cause severe over-fitting and degrade the generalizability of recognition on these classes.</p><p>Once adopting the class-aware sampling, category like person is extremely undersampled that 99.13% of the instances are neglected, while category like pressure cooker is immensely oversampled that each instance is seen for 252 more times averagely within an epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this part, we explore methods to deal with the label related problems in large scale object detection. First, we design a concurrent softmax to handle both the explicit and implicit multi-label issues jointly. Second, we propose a soft-balance sampling together with a hybrid training scheduler to deal with the extremely imbalanced label distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-Label Object Detection</head><p>As one of the most widely used loss function in deep learning, the form of softmax loss about a bounding box b is presented as follows:</p><formula xml:id="formula_0">L cls (b) = − C i=1 y i log(σ i ), with σ i = e zi C j=1 e zj ,<label>(1)</label></formula><p>where z i denotes the response of the i-th class, y i denotes the label and C means the number of categories. It behaves well in single label recognition where y i = 1. However, things are different when it comes to multi-label recognition.</p><p>In the conventional object detection training scheme, each bounding box is assigned only one label during training ignoring other ground-truth labels. If we force to assign all the m(m ≥ 1) ground-truth labels that belongs to K = {k | k ∈ C, y k = 1} to bounding box during training, scores of multiple labels would restrain each other. When computing the gradient of each ground-truth label, it looks  like below:</p><formula xml:id="formula_1">∂L cls ∂z i = mσ i − 1, if i ∈ K; mσ i , if i / ∈ K.<label>(2)</label></formula><p>When mσ i &gt; 1 for i ∈ K, z i is optimized to become lower even if i is one of the ground-truth labels, which is the wrong optimization direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Concurrent Softmax</head><p>The concurrent softmax is designed to help solve the problem of recognizing objects with multiple labels in object detection. During training, the concurrent softmax loss of a predicted box b is presented as follows:</p><formula xml:id="formula_2">L * cls (b) = − C i=1 y i log σ * i , with σ * i = e zi C j=1 (1 − y j )(1 − r ij )e zj + e zi ,<label>(3)</label></formula><p>where y i denotes the label of class i regarding the box b, and r ij denotes the concurrent rate of class i to class j. And output of concurrent softmax during training is defined as:</p><formula xml:id="formula_3">∂L * cls ∂z i =    σ * i − 1, if i ∈ K; j∈K (1 − r ij )σ * i , if i / ∈ K.<label>(4)</label></formula><p>Unlike in softmax that responses of the ground-truth categories are suppressing all the others, we remove the suppression effects between explicitly coexisting categories in concurrent softmax. For instance, a bounding box is assigned multiple ground-truth labels K = {k | k ∈ C, y k = 1} during training. When computing the score of class i ∈ K, influences of all the other ground-truth classes j ∈ K \ {i} are neglected because of the (1 − y j ) term, and the score of each correct class is boosted. This avoids the unnecessary large losses due to the multi-label problem, and the gradients could focus on more valuable knowledge.</p><p>Apart from the explicit co-existence cases, there are still implicit concurrent relationships remain to be settled. We define a concurrent rate r ij as the probability that an object of class i is labeled as class j. The r ij is calculated based on the class annotations of training set and <ref type="figure" target="#fig_1">Figure 3a</ref> shows the concurrent rates of confusion pairs. For hierarchical relationships, r ij is set 0 when i is leaf node with j as its parent, and vice versa. With the (1 − r ij ) term, suppression effects between confusing pairs are weakened.</p><p>The influence of multi-label object detection is also prominent during inference. Different from the conventional multi-label recognition tasks, the evaluation metric of object detection is mean average precision(mAP). For each category, detection results of all images are firstly collected and ranked by scores to form a precision-recall curve, and the average area of precision-recall curve is defined as the mAP. In this way, the absolute value of box score matters, because it may influence the rank of predicted box over the entire dataset. Thus we also apply the concurrent softmax during inference, and present it as follows:</p><formula xml:id="formula_4">σ † i = e zi C j=1 (1 − r ij )e zj ,<label>(5)</label></formula><p>where we abandon the (1−y j ) term and keep the concurrent rate term. Scores of categories in a hierarchy and scores of similar categories would not suppress each other, and are boosted effectively, which is desirable in object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Compared with BCE loss</head><p>BCE is always a popular solution to mutl-label recognition, but it does not work well on multi-label detection task. We argue that sigmoid function fails to normalize scores and declines the suppression effect between categories which is desired when evaluated with mAP metric. We have tried BCE loss and focal loss, but it turns out that they yield much worse result even than the original softmax cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Soft-balance Sampling with Hybrid Training</head><p>As detailedly illuminated in 3, Open Images dataset suffers from severely imbalanced label distribution. We denote by C the number of categories, N the number of total training images, and n i the number of images containing objects of the i-th class. Conventionally, images are sampled in sequence without replacement for training in each epoch, and the original probability P o of class i being sampled is denoted as P o (i) = ni N , which may greatly degrade the recognition capability of model for infrequent classes. A widely used technique, i.e., class-aware sampling mentioned in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref> is a naive solution to handle the class imbalance problem, in which categories are sampled uniformly in each batch. The class-aware sampling probability P a of class i becomes P a (i) = 1</p><p>C . Yet this may cause heavy over-fitting on infrequent categories and insufficient training on frequent categories as aforementioned.</p><p>To alleviate the problems above, we firstly adjust the sampling probability based on number of samples, which we call soft-balance sampling. We first define the P n (i) = ni C i=j nj as the approximation of non-balance sampling probability P o (i) for convenience. Then the sampling prob-ability of class-aware balance can be reformulated as:</p><formula xml:id="formula_5">P a (i) = 1 C = 1 CP n (i) P n (i) = αP n (i),<label>(6)</label></formula><p>where the α = 1 CPn(i) can be regarded as a balance factor that is inversely proportional to the number of categories and the original sampling probability.</p><p>To reconcile the frequent and infrequent categories, we introduce soft-balance sampling by adjusting the balance factor with a new hyper-parameter λ:</p><formula xml:id="formula_6">P s (i) = α λ P n (i) = P a (i) λ P n (i) (1−λ) .<label>(7)</label></formula><p>Note that λ = 0 corresponds to non-balance sampling and λ = 1 corresponds to class-aware balance. The normalized probability is:</p><formula xml:id="formula_7">P * s (i) = P s (i) C j=1 P s (j) .<label>(8)</label></formula><p>This sampling strategy guarantees more sufficient training on dominate categories and decreases the excessive sampling frequency of infrequent categories. Even with the soft-balance method, there are still many samples of the frequent categories that are not sampled. Thus we propose a hybrid training scheduler to further mitigate this problem. We firstly train detector using the conventional strategy, which is sampling training images in sequence without replacement, and the equivalent sampling probability is P o . Then we finetune the model with softbalance strategy to cover categories with very few samples. This hybrid training schema exploits the effectiveness of pretrained model for object detection task from Open Images itself rather than ImageNet. It ensures that all the images have been seen during training, and endows the model with a better generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>To analyze the proposed concurrent softmax loss and soft-balance with hybrid training, we conduct experiment on Open Images challenge 2019 dataset. As an object detection dataset in the wild, it contains 1.7 million images with 12.4 million boxes of 500 categories in its challenge split. The scale of training images is 15 times of the MS COCO <ref type="bibr" target="#b20">[21]</ref> and 3 times of the second largest object detection dataset Object365 <ref type="bibr" target="#b29">[30]</ref>.</p><p>Considering the huge size of Open Images dataset, we split a mini Open Images dataset for our ablation study. The mini Open Images dataset contains 115K training images and 5K validation images named as mini-train and mini-val.</p><p>All the images are sampled from Open Images challenge 2019 dataset with the ratio of each category unchanged. Final results on full-val and public test-challenge in Open Images challenge 2019 dataset are also reported. We follow the metric used in Open Images challenge which is a variant mAP at IoU 0.5, as all false positives not existing in image-level labels 1 are ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We train our detector with ResNet-50 backbone armed with FPN. For the network configuration, we follow the setting mentioned in Detectron. We use SGD with momentum 0.9 and weight decay 0.0001 to optimize parameters. The initial learning rate is set to 0.00125 × batch size, and then decreased by a factor of 10 at 4 and 6 epoch for 1× schedule which has total 7 epochs. The input images are scaled so that the length of the shorter edge is 800 and the longer side is limited to 1333. Horizontal flipping is used as data augmentation and sync-BN is adopted to speed up the convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Concurrent Softmax</head><p>We explore the influence of concurrent softmax in training and testing stage respectively in this ablation study. All models are trained with mini-train and evaluated on minival. The impacts of concurrent softmax during training. <ref type="table">Table 1</ref> shows the results of the proposed concurrent softmax compared with the vanilla softmax and other existing methods during training stage. Concurrent softmax could outperform softmax by 1.13 points with class-aware sampling and 0.98 points with non-balance sampling. It is also found that sigmoid with BCE and focal loss behaves poorly in this case. We guess that they are incompatible with the mAP metric in object detection as mentioned in 4.1.2. Our method also outperforms dist-CE loss <ref type="bibr" target="#b23">[24]</ref> and Co-BCE loss <ref type="bibr" target="#b0">[1]</ref>. The impacts of concurrent softmax during testing. We also show results to demonstrate the effectiveness of concurrent softmax in testing stage in <ref type="table" target="#tab_2">Table 2</ref>. Solely applying concurrent softmax brings 0.36 mAP improvement during inference, while applying it in both training and testing stage yields 1.50 points improvement totally. This also proves the fact that suppression effects between leaf and parent categories or confusing categories are harming the performance of object detection in Open Images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Soft-balance Sampling</head><p>Results. <ref type="table" target="#tab_3">Table 3</ref> presents the results of the proposed softbalance sampling and other balance method. As Open Images is a long-tailed dataset, many categories have few sam- <ref type="table">Table 1</ref>: The comparison of different loss functions method. Models are trained in mini-train and evaluated on mini-val.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Type</head><p>Balance mAP Focal Loss <ref type="bibr" target="#b19">[20]</ref> 50.18 BCE Loss 54.29 Co-BCE Loss <ref type="bibr" target="#b0">[1]</ref> 55.74 dist-CE Loss <ref type="bibr" target="#b23">[24]</ref> 55   Class-aware sampling simply samples all categories data uniformly at random, it remedy the data imbalance problem to a great extent and boost the performance to 55.45. The effective number <ref type="bibr" target="#b5">[6]</ref> is used to re-weight the classification loss with the purpose of harmonizing the gradient contribution from different categories. Comparing to the non-balance method, the effective number improves the results by 7.56 points, but is worse than the class-aware sampling. We argue that it is because the balance strategy applied on data level is more efficient than that of loss level. Soft-balance with hyper-parameter λ allows us to transfer from non-balance (λ = 0) to class-aware sampling (λ = 1). Thus, we can find a point at which sufficient infrequent cat- egories data could be sampled to train the model and the over-fitting problem does not happen yet. The soft-balance with λ = 0.7 outperforms the class-aware sampling by 1.59 points.</p><p>The impacts of soft-balance during training. To investigate why soft-balance is better, we show the training curves of soft-balance with different λ in <ref type="figure">Figure 4</ref>. We can learn that the small λ = 0.3 is hard to achieve a good performance due to the data imbalance problem. But too large λ = 1.5 also fails on accomplishing the best performance comparing with the relatively smaller λ setting. Note that the mAP of λ = 1.5 is much higher than that of λ = 0.5 in the first learning rate stage (before epoch 4), but this situation reverses in subsequent train progress. This comparison proves that λ = 1.5 provides more sufficient rare categories data to train the model and achieve better performance in the beginning, however, it run into a severe over-fitting in the convergence stage. The results of λ = 1.0 and λ = 0.7 validate this rules again.</p><p>The impacts of soft-balance among categories. We further study the performance of λ = 0.0, λ = 0.7, and λ = 1.0 among categories in <ref type="figure" target="#fig_5">Figure 5</ref>, in which the challenge validation results of 500 categories are arranged from large to few by their number of images. As shown in <ref type="figure" target="#fig_5">Figure 5a</ref>, the λ = 1.0 (orange line) outperforms the λ = 0.0 (blue line) on the later half categories which have few image samples. Although λ = 1.0 solves the data insufficiency of infrequent categories, it under-samples the frequent categories and causes the performance dropping on the former half categories. <ref type="figure" target="#fig_5">Figure 5b</ref> shows that λ = 0.7 (orange line) alleviates the excessive under-sampling of the major categories comparing to λ = 1.0 (blue line). On the other hand, it mitigates the over-fitting problem of infrequent categories. Therefore, the performance of λ = 0.7 is almost always better than λ = 1.0 on the full category space. <ref type="table" target="#tab_4">Table 4</ref> summarizes the results of ResNeXt152 on Open Images Challenge dataset trained with different training scheduler. For the non-balance setting, the more epochs the model trained, the better performance the model achieves. And training a model from scratch yields better results than finetuning from ImageNet pretrained model. These observations match similar conclusion in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Hybrid Training Scheduler</head><p>While class-aware sampling significantly boosts the performance by 8.62 points using the ImageNet pretraining in 7 epochs setting, it suffers from over-fitting problem, as the mAP of model trained with 14 epochs is lower than that of 7 epochs. And frequent categories are still intensely under-sampled even applying the balance sampling. With hybrid training, class-aware sampling can achieve better performance in both non-balance ImageNet pretraining and non-balance scratch pretraining setting. Note that these improvements are not caused by more training epochs, because longer training schedule will decreases the performance if with only ImageNet pretraining. By further using soft-balance strategy, the hybrid training with non-balance scratch is improved from 65.92 to 67.09 mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Extension Results on Test-challenge Set</head><p>With the proposed concurrent softmax, soft-balance and hybrid training scheduler, we achieve 67.17 mAP and 4.29 points absolute improvement compared to the first place en-    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Ensemble Public Test 2018 1st <ref type="bibr" target="#b0">[1]</ref> 55.81 Ours 60.90 2018 1st <ref type="bibr" target="#b0">[1]</ref> 62.88 2018 2nd <ref type="bibr" target="#b9">[10]</ref> 62 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we investigate the multi-label problem and the imbalanced label distribution problem in large-scale object detection dataset , and introduce a simple but powerful solution. We propose the concurrent softmax function to deal with explicit and implicit multi-label problem in both training and testing stage. Our soft-balance method together with hybrid training scheduler could effectively deal with the extremely imbalanced label distribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Implicit multi-label problem caused by confused categories and absence of leaf classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 Figure 4 :</head><label>54</label><figDesc>Training curves of the proposed soft-balance sampling. Soft-balance with λ = 0.7 achieves the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Non-balance (blue) versus Class-aware Sampling (orange) sorted by the number of images for most frequent 100 categories (left) and most infrequent 100 categories (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Class-aware Sampling (blue) versus Soft-balance with λ = 0.7 (orange) sorted by the number of images for most frequent 100 categories (left) and most infrequent 100 categories (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The comparison of sampling strategy among categories. (Best viewed on high-resolution display)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The imbalance magnitude of Open Images and MS COCO dataset. Imbalance magnitude means the number of the images of the largest category divided by the smallest. (best viewed on high-resolution display)</figDesc><table><row><cell cols="2">10 1 10 2 10 3 Imbalance magnitude 10 4</cell><cell>Open Images MS COCO</cell></row><row><cell cols="2">10 0</cell><cell>Pressure cooker Torch Spatula Winter melon Screwdriver Ring binder Wrench Flashlight Toaster Measuring cup Crutch Light switch Beaker Oboe Cricket ball Salt and pepper shakers Training bench Common fig Slow cooker Binoculars Treadmill Serving tray Dumbbell Tick Stationary bicycle Briefcase Adhesive tape Stretcher Punching bag Envelope Squash Nail Mango Personal care Alarm clock Turtle Food processor Towel Paper towel Power plugs and sockets Blender Pretzel Burrito Artichoke Digital clock Mixer Drinking straw Guacamole Harpsichord Snowmobile Willow Cutting board Porcupine Popcorn Harp Croissant Printer Shower Dice Canary Telephone Lynx Blue jay Pomegranate Peach Toilet paper Dog bed Snowplow Submarine sandwich Seahorse Coffeemaker Racket Flute Centipede Ruler Cake stand Bagel Dagger Plumbing fixture Radish Pear Segway Rugby ball Filing cabinet Cabbage Kitchen knife Picnic basket Scissors Zucchini Pitcher Bow and arrow Wood-burning stove Frying pan Potato Asparagus Raccoon Seat belt Pineapple Bathroom cabinet Golf ball Washing machine Swim cap Golf cart Horn Limousine Ambulance Belt Corded phone Taco Honeycomb Sewing machine Pancake Bear Tiara Kite Stop sign Ostrich Hot dog Bidet Turkey Organ Bell pepper Waffle Bat Tennis ball Coconut Sword Shellfish Ant Jug Window blind Doughnut Lobster Goldfish Watermelon Woodpecker Fire hydrant Tart Light bulb Suitcase Barge Infant bed Shotgun Ceiling fan Missile Grapefruit Starfish Microwave oven Raven Beehive Alpaca Trombone Cucumber Kangaroo Jet ski Hamster Kettle Gas stove Chopsticks Scoreboard Wok Broccoli Reptile Fox Brown bear Otter Teapot Oyster Shark Polar bear Sombrero Oven Jaguar Loveseat Plastic bag Rhinoceros Refrigerator Snowman Bathtub Crown Door handle Ladybug Handgun Cheetah Gondola Snowboard Stool Volleyball Carrot Mule Table tennis racket Mechanical fan Knife Computer mouse Snail Zebra Shrimp Rocket Candy Barrel Billiard table Sushi Earrings Lemon Sock Fireplace Caterpillar Leopard Mouse Backpack Whale Crocodile Pasta Banana Jellyfish Roller skates Sea turtle Glove Grape Crab Miniskirt Ladder Camel Nightstand Invertebrate Cannon Sandwich Tablet computer Lantern Pig Tennis racket Trumpet Antelope Saxophone Accordion Wall clock Toilet Cupboard Dolphin Bust Harbor seal Whiteboard Wheelchair Orange Penguin Clock Egg Giraffe Sea lion Luggage and bags Lavender Marine mammal Skirt Human foot Cookie Teddy bear Pen Waste container Fork Spoon Tiger Home appliance Hamburger Parachute Tank Headphones Bull Tea Baseball bat Apple Goat Dinosaur Skateboard French fries Sofa bed Aircraft Chest of drawers studio couch Mirror Kitchen appliance Sparrow Tap Lion Rabbit Snake Sheep Lily Tripod Candle Brassiere Coin Lifejacket Piano Pillow Strawberry Pizza Office supplies Tomato Violin Sandal Tin can Tortoise Muffin Cello Owl Dragonfly Platter Swan Ski Frog Chicken Parrot Boot Taxi Watch Sink Lighthouse Bowl Ice cream Eagle Falcon Musical keyboard Skull High heels Vase Lamp Sunflower Saucer Mug Drawer Weapon Baseball glove Surfboard Spider Moths and butterflies Pumpkin Marine invertebrates Bread Elephant Juice Rifle Traffic light Mushroom Convenience store Squirrel Lizard Seafood Coffee Handbag Beetle Kitchen &amp; dining room table Cowboy hat Scarf Cart Plate Canoe Deer Football helmet Monkey Countertop Necklace Box Carnivore Bronze sculpture Paddle Maple Helicopter Watercraft Cocktail Curtain Salad Goose Television Porch Human beard Traffic sign Coffee table Bee Fedora Cattle Bed Cabinetry Christmas tree Fountain Balloon Couch Computer monitor Computer keyboard Umbrella Bookcase Doll Swimwear Billboard Ball Tent Coat Bench Swimming pool Cake Castle Coffee cup Mobile phone Sun hat Football Stairs Beer Duck Trousers Wine Wine glass Butterfly Office building Bicycle helmet Camera Shirt Vegetable Musical instrument Fruit Open Images category Vehicle registration plate Van Rose Fish Laptop Picture frame Shelf Tableware Insect Sports uniform Shorts Motorcycle Horse Goggles Bus Flowerpot Helmet Tie Book Truck Drum Human ear Flag Hat Dessert Houseplant Animal Desk Train Palm tree Street light Bottle Airplane Cat Jacket Door Furniture Sunglasses Bicycle wheel Vehicle Drink Poster Human leg Toy Bicycle Guitar Sculpture Bird Microphone Dog Skyscraper Tower Human hand Human eye Human mouth Boat Chair Human nose Land vehicle Tire Jeans Dress Human arm Glasses Suit Table Boy Human head Human hair House Window Flower Wheel Car Girl Building Footwear Person Woman Tree Human face Man</cell></row><row><cell cols="3">Mixer | Food processor Blender | Food processor Filing cabinet | Chest of drawers Food processor | Mixer Serving tray | Platter Wood-burning stove | Fireplace Tiara | Crown Toilet paper | Paper towel Knife | Kitchen knife Food processor | Blender Jug | Pitcher Loveseat | studio couch Harpsichord | Piano Mechanical fan | Ceiling fan Doughnut | Bagel Sword | Dagger Measuring cup | Beaker Shotgun | Rifle Hamster | Mouse Grapefruit | Orange Harbor seal | Sea lion Dagger | Knife Confusion pairs Torch | Flashlight Lynx | Cat Beehive | Honeycomb Kettle | Teapot Coffee cup | Mug Sea lion | Harbor seal Teapot | Kettle Billiard table | Table studio couch | Sofa bed Ceiling fan | Mechanical fan Cheetah | Jaguar Pitcher | Jug Bagel | Doughnut Paper towel | Toilet paper Dagger | Sword Tortoise | Sea turtle Honeycomb | Beehive Platter | Plate Ambulance | Van Sofa bed | studio couch Jaguar | Cheetah Falcon | Eagle Eagle | Falcon Antelope | Deer Kitchen knife | Knife Leopard | Jaguar Leopard | Cheetah Cheetah | Leopard Sea turtle | Tortoise Jaguar | Leopard Spider | Insect Mug | Coffee cup Figure 2: Pressure cooker | Slow cooker 10 0 10 0 10 1 10 1 10 2 10 2 Number of instances 10 3 10 3 Source Confusion</cell></row><row><cell cols="3">(a) We select the top-55 confused category pairs and show their</cell></row><row><cell cols="3">concurrent rates.</cell></row><row><cell></cell><cell cols="2">10 6</cell><cell>Parents Parents w/o Children</cell></row><row><cell>Number of instances</cell><cell cols="2">10 2 10 3 10 4 10 5</cell></row><row><cell></cell><cell cols="2">10 1</cell></row><row><cell></cell><cell cols="2">10 0</cell><cell>Personal care Invertebrate Telephone Squash Reptile Turtle Animal Watercraft Carnivore Plumbing fixture Aircraft Racket Bear Shellfish Trousers Vehicle Moths and butterflies Furniture Office supplies Marine mammal Weapon Home appliance Musical instrument Land vehicle Insect Glove Luggage and bags Seafood Sandwich Clock Tableware Helmet Skirt Person Fruit Vegetable Ball Building Hat Bed Kitchen appliance Parents Marine invertebrates Bird Couch Drink Dessert Beetle Traffic sign Toy Fish Sculpture Boat Table Flower Car Tree Footwear</cell></row><row><cell cols="3">(b) We show the ratio of parent annotations without leaf label</cell></row><row><cell cols="3">and total parent annotations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The effectiveness of concurrent softmax during testing. Models are trained in mini-train and evaluated on mini-val.</figDesc><table><row><cell>Train Method</cell><cell>Test Method</cell><cell>mAP</cell></row><row><cell>Softmax</cell><cell>Softmax</cell><cell>55.45</cell></row><row><cell>Softmax</cell><cell cols="2">Concurrent Softmax 55.77</cell></row><row><cell>Concurrent Softmax</cell><cell>Softmax</cell><cell>56.58</cell></row><row><cell cols="3">Concurrent Softmax Concurrent Softmax 56.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The comparison of different sampling methods. Models are trained in mini-train and evaluated on mini-val.</figDesc><table><row><cell>Methods</cell><cell>λ</cell><cell>mAP</cell></row><row><cell>Non-balance</cell><cell>-</cell><cell>38.16</cell></row><row><cell>Class-aware Sampling [10]</cell><cell>-</cell><cell>55.45</cell></row><row><cell>Effective Number [6]</cell><cell>-</cell><cell>45.72</cell></row><row><cell></cell><cell cols="2">0.3 50.69</cell></row><row><cell></cell><cell cols="2">0.5 56.19</cell></row><row><cell>Soft-balance</cell><cell cols="2">0.7 57.04</cell></row><row><cell></cell><cell cols="2">1.0 55.45</cell></row><row><cell></cell><cell cols="2">1.5 52.41</cell></row><row><cell cols="3">ples, so that non-balance training only achieves 38.16 mAP.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The effect of training scheduler. The λ of the softbalance is set to 0.7. Non-balance I14 denotes the model of epoch 14 trained with non-balance strategy from Im-ageNet pretrain. Non-balance S20 denotes the model of epoch 20 trained with non-balance strategy from scratch. Soft-balance * means that concurrent softmax is adopted in both training and testing stage. Models are trained on fulltrain and evaluated on full-val.</figDesc><table><row><cell>Method</cell><cell>Pretrain</cell><cell cols="2">Epochs mAP</cell></row><row><cell></cell><cell>ImageNet</cell><cell>7</cell><cell>56.06</cell></row><row><cell></cell><cell>ImageNet</cell><cell>11</cell><cell>59.12</cell></row><row><cell>Non-balance</cell><cell>ImageNet</cell><cell>14</cell><cell>59.85</cell></row><row><cell></cell><cell>ImageNet</cell><cell>16</cell><cell>59.95</cell></row><row><cell></cell><cell>Scratch</cell><cell>20</cell><cell>60.70</cell></row><row><cell></cell><cell>ImageNet</cell><cell>7</cell><cell>64.68</cell></row><row><cell>Class-aware</cell><cell>ImageNet</cell><cell>14</cell><cell>62.85</cell></row><row><cell>Sampling</cell><cell>Non-balance I14</cell><cell>14+7</cell><cell>65.60</cell></row><row><cell></cell><cell>Non-balance S20</cell><cell>20+7</cell><cell>65.92</cell></row><row><cell>Soft-balance</cell><cell>Non-balance S20</cell><cell>20+7</cell><cell>67.09</cell></row><row><cell>Soft-balance  *</cell><cell>Non-balance S20</cell><cell>20+7</cell><cell>68.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results with bells and whistles on Open Images public test-challenge set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>try on the public test-challenge set last year, as detailed inTable 5. We train a ResNeXt-152 FPN with multi-scale training and testing as our baseline which achieves 53.88 mAP. After using class-aware balance, the performance is boosted to 57.56. With the help of proposed concurrent softmax, the model achieves 58.60 mAP. The soft-balance and the hybrid training scheduler lead to mAP gains of 1.26 and 1.04 points, respectively. By further using other tricks including data augmentation, loss function search, and heavier head, we achieve a best single model with a mAP of 62.34. We use ResNeXt-101, ResNeXt-152, and EfficientNet-B7 with various tricks for model ensembling. The final mAP on Open Images public test-challenge set is 67.17.</figDesc><table><row><cell></cell><cell>.16</cell></row><row><cell>2018 3rd</cell><cell>61.70</cell></row><row><cell>Ours</cell><cell>67.17</cell></row><row><cell>Baseline (ResNeXt-152)</cell><cell>53.88</cell></row><row><cell>+Class-aware Sampling</cell><cell>57.56</cell></row><row><cell>+Concurrent Softmax Loss</cell><cell>58.60</cell></row><row><cell>+Soft-balance</cell><cell>59.86</cell></row><row><cell>+Hybrid Training Scheduler</cell><cell>60.90</cell></row><row><cell>+Other Tricks</cell><cell>62.34</cell></row><row><cell>+Ensemble</cell><cell>67.17</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In Open Images dataset, image-level labels consist of verified-exist labels and verified-not-exist labels. The unverified categories are ignored</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work was supported in part by the Major Project for New Generation of AI (No.2018AAA0100400), the National Natural Science Foundation of China (No.61836014, No.61761146004, No.61773375, No.61602481), the Key R&amp;D Program of Shandong Province (Major Scientific and Technological Innovation Project) (NO.2019JZZY010119), and CAS-AIR. We also thank Changbao Wang, Cunjun Yu, Guoliang Cao and Buyu Li for their precious discussion and help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pfdet: 2nd place solution to open images challenge 2018 object detection track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Niitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00778</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning multi-label scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Matthew R Boutell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhao-Min Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision(ICCV)</title>
		<meeting>the IEEE international conference on computer vision(ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Solution for large-scale hierarchical object detection datasets with incomplete annotation and data imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ti</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06208</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Alexander Toshev, and Sergey Ioffe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4894</idno>
	</analytic>
	<monogr>
		<title level="m">Deep convolutional ranking for multilabel image annotation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Rethinking imagenet pre-training. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning structured inference neural networks with label relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Tong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2960" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional graphical lasso for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2977" to="2986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-label image classification with a probabilistic label enhancement model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01892</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision(ICCV)</title>
		<meeting>the IEEE international conference on computer vision(ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision(ECCV)</title>
		<meeting>the European conference on computer vision(ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7363" to="7372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sampling techniques for large-scale object detection from sparsely annotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Niitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6510" to="6518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection with long-tail distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="864" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pod: Practical object detection with scalesensitive network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junran</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9607" to="9616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8430" to="8439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning graph structure for multi-label image classification via clique generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4100" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive low-rank multi-label active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1336" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06986</idno>
		<title level="m">Soft sampling for robust object detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multilabel image classification with regional latent semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2801" to="2813" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
