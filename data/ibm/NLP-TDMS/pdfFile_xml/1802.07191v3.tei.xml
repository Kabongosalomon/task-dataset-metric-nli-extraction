<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Architecture Search with Bayesian Optimisation and Optimal Transport</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
							<email>kandasamy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country>Petuum Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
							<email>willie@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country>Petuum Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
							<email>schneide@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country>Petuum Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
							<email>bapoczos@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country>Petuum Inc</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<email>epxing@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country>Petuum Inc</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Architecture Search with Bayesian Optimisation and Optimal Transport</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function f which is only accessible via point evaluations. It is typically used in settings where f is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network architectures. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.</p><p>While there are several approaches to BO, those based on Gaussian processes (GP) <ref type="bibr" target="#b34">[35]</ref> are most common in the BO literature. In its most unadorned form, a BO algorithm operates sequentially, starting at time 0 with a GP prior for f ; at time t, it incorporates results of evaluations from 1, . . . , t−1 in the form of a posterior for f . It then uses this posterior to construct an acquisition function ϕ t , where ϕ t (x) is a measure of the value of evaluating f at x at time t if our goal is to maximise f .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many real world problems, we are required to sequentially evaluate a noisy black-box function f with the goal of finding its optimum in some domain X . Typically, each evaluation is expensive in such applications, and we need to keep the number of evaluations to a minimum. Bayesian optimisation (BO) refers to an approach for global optimisation that is popularly used in such settings. It uses Bayesian models for f to infer function values at unexplored regions and guide the selection of points for future evaluations. BO has been successfully applied for many optimisation problems in optimal policy search, industrial design, and scientific experimentation. That said, the quintessential use case for BO in machine learning is model selection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40]</ref>. For instance, consider selecting the regularisation parameter λ and kernel bandwidth h for an SVM. We can set this up as a zeroth order optimisation problem where our domain is a two dimensional space of (λ, h) values, and each function evaluation trains the SVM on a training set, and computes the accuracy on a validation set. The goal is to find the model, i.e. hyper-parameters, with the highest validation accuracy.</p><p>The majority of the BO literature has focused on settings where the domain X is either Euclidean or categorical. This suffices for many tasks, such as the SVM example above. However, with recent successes in deep learning, neural networks are increasingly becoming the method of choice for many machine learning applications. A number of recent work have designed novel neural network architectures to significantly outperform the previous state of the art <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. This motivates studying model selection over the space of neural architectures to optimise for generalisation performance. A critical challenge in this endeavour is that evaluating a network via train and validation procedures is very expensive. This paper proposes a BO framework for this problem. These two steps are fairly straightforward in conventional domains. For example, in Euclidean spaces, we can use one of many popular kernels such as Gaussian, Laplacian, or Matérn; we can maximise ϕ t via off the shelf branch-and-bound or gradient based methods. However, when each x ∈ X is a neural network architecture, this is not the case. Hence, our challenges in this work are two-fold. First, we need to quantify (dis)similarity between two networks. Intuitively, in <ref type="figure" target="#fig_1">Fig. 1, network 1a</ref> is more similar to network 1b, than it is to 1c. Secondly, we need to be able to traverse the space of such networks to optimise the acquisition function. Our main contributions are as follows. 1. We develop a (pseudo-)distance for neural network architectures called OTMANN (Optimal Transport Metrics for Architectures of Neural Networks) that can be computed efficiently via an optimal transport program. 2. We develop a BO framework for optimising functions on neural network architectures called NASBOT (Neural Architecture Search with Bayesian Optimisation and Optimal Transport). This includes an evolutionary algorithm to optimise the acquisition function. <ref type="bibr" target="#b2">3</ref>. Empirically, we demonstrate that NASBOT outperforms other baselines on model selection tasks for multi-layer perceptrons (MLP) and convolutional neural networks (CNN). Our python implementations of OTMANN and NASBOT are available at github.com/kirthevasank/nasbot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work:</head><p>Recently, there has been a surge of interest in methods for neural architecture search <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref>. We discuss them in detail in the Appendix due to space constraints. Broadly, they fall into two categories, based on either evolutionary algorithms (EA) or reinforcement learning (RL). EA provide a simple mechanism to explore the space of architectures by making a sequence of changes to networks that have already been evaluated. However, as we will discuss later, they are not ideally suited for optimising functions that are expensive to evaluate. While RL methods have seen recent success, architecture search is in essence an optimisation problemfind the network with the lowest validation error. There is no explicit need to maintain a notion of state and solve credit assignment <ref type="bibr" target="#b42">[43]</ref>. Since RL is a fundamentally more difficult problem than optimisation <ref type="bibr" target="#b15">[16]</ref>, these approaches need to try a very large number of architectures to find the optimum. This is not desirable, especially in computationally constrained settings.</p><p>None of the above methods have been designed with a focus on the expense of evaluating a neural network, with an emphasis on being judicious in selecting which architecture to try next. Bayesian optimisation (BO) uses introspective Bayesian models to carefully determine future evaluations and is well suited for expensive evaluations. BO usually consumes more computation to determine future points than other methods, but this pays dividends when the evaluations are very expensive. While there has been some work on BO for architecture search <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>, they have only been applied to optimise feed forward structures, e.g. <ref type="figure" target="#fig_1">Fig. 1a</ref>, but not Figs. 1b, 1c. We compare NASBOT to one such method and demonstrate that feed forward structures are inadequate for many problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Set Up</head><p>Our goal is to maximise a function f defined on a space X of neural network architectures. When we evaluate f at x ∈ X , we obtain a possibly noisy observation y of f (x). In the context of architecture search, f is the performance on a validation set after x is trained on the training set. If x = argmax X f (x) is the optimal architecture, and x t is the architecture evaluated at time t, we want f (x ) − max t≤n f (x t ) to vanish fast as the number of evaluations n → ∞. We begin with a review of BO and then present a graph theoretic formalism for neural network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A brief review of Gaussian Process based Bayesian Optimisation</head><p>A GP is a random process defined on some domain X , and is characterised by a mean function µ : X → R and a (covariance) kernel κ : X 2 → R. Given n observations D n = {(x i , y i )} n i=1 , where  <ref type="figure" target="#fig_1">Figure 1</ref>: An illustration of some CNN architectures. In each layer, i: indexes the layer, followed by the label (e.g conv3), and then the number of units (e.g. number of filters). The input and output layers are pink while the decision (softmax) layers are green. From Section 3: The layer mass is denoted in parentheses. The following are the normalised and unnormalised distances d,d . All self distances are 0,</p><formula xml:id="formula_0">i.e. d(G, G) =d(G, G) = 0. Unnor- malised: d(a, b) = 175.1, d(a, c) = 1479.3, d(b, c) = 1621.4. Normalised: d(a, b) = 0.0286,d(a, c) = 0.2395, d(b, c) = 0.2625. x i ∈ X , y i = f (x i ) + i ∈ R, and i ∼ N (0, η 2 )</formula><p>, the posterior process f |D n is also a GP with mean µ n and covariance</p><formula xml:id="formula_1">κ n . Denote Y ∈ R n with Y i = y i , k, k ∈ R n with k i = κ(x, x i ), k i = κ(x , x i ), and K ∈ R n×n with K i,j = κ(x i , x j ).</formula><p>Then, µ n , κ n can be computed via,</p><formula xml:id="formula_2">µ n (x) = k (K + η 2 I) −1 Y, κ n (x, x ) = κ(x, x ) − k (K + η 2 I) −1 k .<label>(1)</label></formula><p>For more background on GPs, we refer readers to Rasmussen and Williams <ref type="bibr" target="#b34">[35]</ref>. When tasked with optimising a function f over a domain X , BO models f as a sample from a GP. At time t, we have already evaluated f at points</p><formula xml:id="formula_3">{x i } t−1 i=1 and obtained observations {y i } t−1 i=1 .</formula><p>To determine the next point for evaluation x t , we first use the posterior GP to define an acquisition function ϕ t : X → R, which measures the utility of evaluating f at any x ∈ X according to the posterior. We then maximise the acquisition x t = argmax X ϕ t (x), and evaluate f at x t . The expected improvement acquisition <ref type="bibr" target="#b30">[31]</ref>,</p><formula xml:id="formula_4">ϕ t (x) = E max{0, f (x) − τ t−1 } {(x i , y i )} t−1 i=1 ,<label>(2)</label></formula><p>measures the expected improvement over the current maximum value according to the posterior GP.</p><p>Here τ t−1 = argmax i≤t−1 f (x i ) denotes the current best value. This expectation can be computed in closed form for GPs. We use EI in this work, but the ideas apply just as well to other acquisitions <ref type="bibr" target="#b2">[3]</ref>.</p><p>GP/BO in the context of architecture search: Intuitively, κ(x, x ) is a measure of similarity between x and x . If κ(x, x ) is large, then f (x) and f (x ) are highly correlated. Hence, the GP effectively imposes a smoothness condition on f : X → R; i.e. since networks a and b in <ref type="figure" target="#fig_1">Fig. 1</ref> are similar, they are likely to have similar cross validation performance. In BO, when selecting the next point, we balance between exploitation, choosing points that we believe will have high f value, and exploration, choosing points that we do not know much about so that we do not get stuck at a bad optimum. For example, if we have already evaluated f (a), then exploration incentivises us to choose c over b since we can reasonably gauge f (b) from f (a). On the other hand, if f (a) has high value, then exploitation incentivises choosing b, as it is more likely to be the optimum than c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Mathematical Formalism for Neural Networks</head><p>Our formalism will view a neural network as a graph whose vertices are the layers of the network. We will use the CNNs in <ref type="figure" target="#fig_1">Fig. 1</ref> to illustrate the concepts. A neural network G = (L, E) is defined by a set of layers L and directed edges E. An edge (u, v) ∈ E is a ordered pair of layers. In <ref type="figure" target="#fig_1">Fig. 1</ref>, the layers are depicted by rectangles and the edges by arrows. A layer u ∈ L is equipped with a layer label (u) which denotes the type of operations performed at the layer. For instance, in <ref type="figure" target="#fig_1">Fig. 1a</ref>, (1) = conv3, (5) = max-pool denote a 3 × 3 convolution and a max-pooling operation. The attribute u denotes the number of computational units in a layer. In <ref type="figure" target="#fig_1">Fig. 1b</ref>, u(5) = 32 and u(7) = 16 are the number of convolutional filters and fully connected nodes.</p><p>In addition, each network has decision layers which are used to obtain the predictions of the network. For a classification task, the decision layers perform softmax operations and output the probabilities an input datum belongs to each class. For regression, the decision layers perform linear combinations of the outputs of the previous layers and output a single scalar. All networks have at least one decision layer. When a network has multiple decision layers, we average the output of each decision layer to obtain the final output. The decision layers are shown in green in <ref type="figure" target="#fig_1">Fig. 1</ref>. Finally, every network has a unique input layer u ip and output layer u op with labels (u ip ) = ip and (u op ) = op. It is instructive to think of the role of u ip as feeding a data point to the network and the role of u op as averaging the results of the decision layers. The input and output layers are shown in pink in <ref type="figure" target="#fig_1">Fig. 1</ref>. We refer to all layers that are not input, output or decision layers as processing layers.</p><p>The directed edges are to be interpreted as follows. The output of each layer is fed to each of its children; so both layers 2 and 3 in <ref type="figure" target="#fig_1">Fig. 1b</ref> take the output of layer 1 as input. When a layer has multiple parents, the inputs are concatenated; so layer 5 sees an input of 16 + 16 filtered channels coming in from layers 3 and 4. Finally, we mention that neural networks are also characterised by the values of the weights/parameters between layers. In architecture search, we typically do not consider these weights. Instead, an algorithm will (somewhat ideally) assume access to an optimisation oracle that can minimise the loss function on the training set and find the optimal weights. We next describe a distance d : X 2 → R + for neural architectures. Recall that our eventual goal is a kernel for the GP; given a distance d, we will aim for κ(x, x ) = e −βd(x,x ) p , where β, p ∈ R + , as the kernel. Many popular kernels take this form. For e.g. when X ⊂ R n and d is the L 2 norm, p = 1, 2 correspond to the Laplacian and Gaussian kernels respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The OTMANN Distance</head><p>To motivate this distance, note that the performance of a neural network is determined by the amount of computation at each layer, the types of these operations, and how the layers are connected. A meaningful distance should account for these factors. To that end, OTMANN is defined as the minimum of a matching scheme which attempts to match the computation at the layers of one network to the layers of the other. We incur penalties for matching layers with different types of operations or those at structurally different positions. We will find a matching that minimises these penalties, and the total penalty at the minimum will give rise to a distance. We first describe two concepts, layer masses and path lengths, which we will use to define OTMANN.</p><p>Layer masses: The layer masses m : L → R + will be the quantity that we match between the layers of two networks when comparing them. m(u) quantifies the significance of layer u. For processing layers, m(u) will represent the amount of computation carried out by layer u and is computed via the product of u(u) and the number of incoming units. For example, in <ref type="figure" target="#fig_1">Fig. 1b</ref>, m(5) = 32 × (16 + 16) as there are 16 filtered channels each coming from layers 3 and 4 respectively. As there is no computation at the input and output layers, we cannot define the layer mass directly as we did for the processing layers. Therefore, we use m(u ip ) = m(u op ) = ζ u∈PL m(u) where PL denotes the set of processing layers, and ζ ∈ (0, 1) is a parameter to be determined. Intuitively, we are using an amount of mass that is proportional to the amount of computation in the processing layers. Similarly, the decision layers occupy a significant role in the architecture as they directly influence the output. While there is computation being performed at these layers, this might be problem dependent -there is more computation performed at the softmax layer in a 10 class classification problem than in a 2 class problem. Furthermore, we found that setting the layer mass for decisions layers based on computation underestimates their contribution to the network. Following the same intuition as we did for the input/output layers, we assign an amount of mass proportional to the mass in the processing layers. Since the outputs of the decision layers are averaged, we distribute the mass among all decision layers; that is, if DL are decision layers, ∀ u ∈ DL, m(u) = ζ |DL| u∈PL m(u). In all our experiments, we use ζ = 0.1. In <ref type="figure" target="#fig_1">Fig. 1</ref>, the layer masses for each layer are shown in parantheses.</p><p>Path lengths from/to u ip /u op : In a neural network G, a path from u to v is a sequence of layers u 1 , . . . , u s where u 1 = u, u s = v and (u i , u i+1 ) ∈ E for all i ≤ s − 1. The length of this path is the number of hops from one node to another in order to get from u to v. For example, in <ref type="figure" target="#fig_1">Fig. 1c</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13</ref>) is a path from layer 2 to 13 of length 3. Let the shortest (longest) path length from u to v be the smallest (largest) number of hops from one node to another among all paths from u to v. Additionally, define the random walk path length as the expected number of hops to get from u to v, if, from any layer we hop to one of its children chosen uniformly at random. For example, in <ref type="figure" target="#fig_1">Fig. 1c</ref>, the shortest, longest and random walk path lengths from layer 1 to layer 14 are 5, 7, and 5.67 respectively. For any u ∈ L, let δ sp op (u), δ lp op (u), δ rw op (u) denote the length of the shortest, longest and random walk paths from u to the output u op . Similarly, let δ sp ip (u), δ lp ip (u), δ rw ip (u) denote the corresponding lengths conv3 conv5 max-pool avg-pool fc conv3 for walks from the input u ip to u. As the layers of a neural network can be topologically ordered 1 , the above path lengths are well defined and finite. Further, for any s ∈ {sp,lp,rw} and t ∈ {ip,op}, δ s t (u) can be computed for all u ∈ L, in O(|E|) time (see Appendix A.3 for details).</p><formula xml:id="formula_5">0 0.2 ∞ ∞ ∞ conv5 0.2 0 ∞ ∞ ∞ max-pool ∞ ∞ 0 0.25 ∞ avg-pool ∞ ∞ 0.25 0 ∞ fc ∞ ∞ ∞ ∞ 0</formula><p>We are now ready to describe OTMANN. Given two networks G 1 = (L 1 , E 1 ), G 2 = (L 2 , E 2 ) with n 1 , n 2 layers respectively, we will attempt to match the layer masses in both networks. We let Z ∈ R n1×n2 + be such that Z(i, j) denotes the amount of mass matched between layer i ∈ G 1 and j ∈ G 2 . The OTMANN distance is computed by solving the following optimisation problem.</p><formula xml:id="formula_6">minimise Z φ lmm (Z) + φ nas (Z) + ν str φ str (Z) (3) subject to j∈L2 Z ij ≤ m(i), i∈L1 Z ij ≤ m(j), ∀i, j</formula><p>The label mismatch term φ lmm , penalises matching masses that have different labels, while the structural term φ str penalises matching masses at structurally different positions with respect to each other. If we choose not to match any mass in either network, we incur a non-assignment penalty φ nas . ν str &gt; 0 determines the trade-off between the structural and other terms. The inequality constraints ensure that we do not over assign the masses in a layer. We now describe φ lmm , φ nas , and φ str .</p><p>Label mismatch penalty φ lmm : We begin with a label penalty matrix M ∈ R L×L where L is the number of all label types and M (x, y) denotes the penalty for transporting a unit mass from a layer with label x to a layer with label y. We then construct a matrix C lmm ∈ R n1×n2 with C lmm (i, j) = M ( (i), (j)) corresponding to the mislabel cost for matching unit mass from each layer i ∈ L 1 to each layer j ∈ L 2 . We then set φ lmm (Z) = Z, C lmm = i∈L1,j∈L2 Z(i, j)C(i, j) to be the sum of all matchings from L 1 to L 2 weighted by the label penalty terms. This matrix M , illustrated in <ref type="table" target="#tab_2">Table 1</ref>, is a parameter that needs to be specified for OTMANN. They can be specified with an intuitive understanding of the functionality of the layers; e.g. many values in M are ∞, while for similar layers, we choose a value less than 1.</p><p>Non-assignment penalty φ nas : We set this to be the amount of mass that is unassigned in both networks, i.e. φ nas (Z) = i∈L1 m(i) − j∈L2 Z ij + j∈L2 m(j) − i∈L1 Z ij . This essentially implies that the cost for not assigning unit mass is 1. The costs in <ref type="table" target="#tab_2">Table 1</ref> are defined relative to this. For similar layers x, y, M (x, y) 1 and for disparate layers M (x, y) 1. That is, we would rather match conv3 to conv5 than not assign it, provided the structural penalty for doing so is small; conversely, we would rather not assign a conv3, than assign it to fc. This also explains why we did not use a trade-off parameter like ν str for φ lmm and φ nas -it is simple to specify reasonable values for M (x, y) from an understanding of their functionality.</p><p>Structural penalty φ str : We define a matrix C str ∈ R n1×n2 where C str (i, j) is small if layers i ∈ L 1 and j ∈ L 2 are at structurally similar positions in their respective networks. We then set φ str (Z) = Z, C str . For i ∈ L 1 , j ∈ L 2 , we let C str (i, j) = 1 <ref type="bibr" target="#b5">6</ref> s∈{sp, lp, rw} t∈{ip,op} |δ s t (i) − δ s t (j)| be the average of all path length differences, where δ s t are the path lengths defined previously. We define φ str in terms of the shortest/longest/random-walk path lengths from/to the input/output, because they capture various notions of information flow in a neural network; a layer's input is influenced by the paths the data takes before reaching the layer and its output influences all layers it passes through before reaching the decision layers. If the path lengths are similar for two layers, they are likely to be at similar structural positions. Further, this form allows us to solve (3) efficiently via an OT program and prove distance properties about the solution. If we need to compute pairwise distances for several networks, as is the case in BO, the path lengths can be pre-computed in O(|E|) time, and used to construct C str for two networks at the moment of computing the distance between them.</p><p>This completes the description of our matching program. In Appendix A, we prove that (3) can be formulated as an Optimal Transport (OT) program <ref type="bibr" target="#b46">[47]</ref>. OT is a well studied problem with several efficient solvers <ref type="bibr" target="#b32">[33]</ref>. Our theorem below, shows that the solution of (3) is a distance. </p><formula xml:id="formula_7">Theorem 1. Let d(G 1 , G 2 ) be the solution of (3) for networks G 1 , G 2 . Under mild regularity condi- tions on M , d(·, ·) is a pseudo-distance. That is, for all networks G 1 , G 2 , G 3 , it satisfies, d(G 1 , G 2 ) ≥ 0, d(G 1 , G 2 ) = d(G 2 , G 1 ), d(G 1 , G 1 ) = 0 and d(G 1 , G 3 ) ≤ d(G 1 , G 2 ) + d(G 2 , G 3 ).</formula><p>For what follows, defined(G 1 ,</p><formula xml:id="formula_8">G 2 ) = d(G 1 , G 2 )/(tm(G 1 )+tm(G 2 )) where tm(G i ) = u∈Li m(u)</formula><p>is the total mass of a network. Note thatd ≤ 1. Whiled does not satisfy the triangle inequality, it provides a useful measure of dissimilarity normalised by the amount of computation. Our experience suggests that d puts more emphasis on the amount of computation at the layers over structure and vice versa ford. Therefore, it is prudent to combine both quantities in any downstream application. The caption in <ref type="figure" target="#fig_1">Fig. 1</ref> gives d,d values for the examples in that figure when ν str = 0.5.</p><p>We conclude this section with a couple of remarks. First, OTMANN shares similarities with Wasserstein (earth mover's) distances which also have an OT formulation. However, it is not a Wasserstein distance itself-in particular, the supports of the masses and the cost matrices change depending on the two networks being compared. Second, while there has been prior work for defining various distances and kernels on graphs, we cannot use them in BO because neural networks have additional complex properties in addition to graphical structure, such as the type of operations performed at each layer, the number of neurons, etc. The above work either define the distance/kernel between vertices or assume the same vertex (layer) set <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49]</ref>, none of which apply in our setting. While some methods do allow different vertex sets <ref type="bibr" target="#b47">[48]</ref>, they cannot handle layer masses and layer similarities. Moreover, the computation of the above distances are more expensive than OTMANN. Hence, these methods cannot be directly plugged into BO framework for architecture search.</p><p>In Appendix A, we provide additional material on OTMANN. This includes the proof of Theorem 1, a discussion on some design choices, and implementation details such as the computation of the path lengths. Moreover, we provide illustrations to demonstrate that OTMANN is a meaningful distance for architecture search. For example, a t-SNE embedding places similar architectures close to each other. Further, scatter plots showing the validation error vs distance on real datasets demonstrate that networks with small distance tend to perform similarly on the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NASBOT</head><p>We now describe NASBOT, our BO algorithm for neural architecture search. Recall that in order to realise the BO scheme outlined in Section 2.1, we need to specify (a) a kernel κ for neural architectures and (b) a method to optimise the acquisition ϕ t over these architectures. Due to space constraints, we will only describe the key ideas and defer all details to Appendix B.</p><p>As described previously, we will use a negative exponentiated distance for κ. Precisely, κ = αe −βd +ᾱd −βd , where d,d are the OTMANN distance and its normalised version. We mention that while this has the form of popular kernels, we do not know yet if it is in fact a kernel. In our experiments, we did not encounter an instance where the eigenvalues of the kernel matrix were negative. In any case, there are several methods to circumvent this issue in kernel methods <ref type="bibr" target="#b41">[42]</ref>.</p><p>We use an evolutionary algorithm (EA) approach to optimise the acquisition function <ref type="bibr" target="#b1">(2)</ref>. For this, we begin with an initial pool of networks and evaluate the acquisition ϕ t on those networks. Then we generate a set of N mut mutations of this pool as follows. First, we stochastically select N mut candidates from the set of networks already evaluated such that those with higher ϕ t values are more likely to be selected than those with lower values. Then we modify each candidate, to produce a new architecture. These modifications, described in <ref type="table" target="#tab_3">Table 2</ref>  increasing or decreasing the number of computational units in a layer, by adding or deleting layers, or by changing the connectivity of existing layers. Finally, we evaluate the acquisition on this N mut mutations, add it to the initial pool, and repeat for the prescribed number of steps. While EA works fine for cheap functions, such as the acquisition ϕ t which is analytically available, it is not suitable when evaluations are expensive, such as training a neural network. This is because EA selects points for future evaluations that are already close to points that have been evaluated, and is hence inefficient at exploring the space. In our experiments, we compare NASBOT to the same EA scheme used to optimise the acquisition and demonstrate the former outperforms the latter.</p><p>We conclude this section by observing that this framework for NASBOT/OTMANN has additional flexibility to what has been described. If one wishes to tune over drop-out probabilities, regularisation penalties and batch normalisation at each layer, they can be treated as part of the layer label, via an augmented label penalty matrix M which accounts for these considerations. If one wishes to jointly tune other scalar hyper-parameters (e.g. learning rate), they can use an existing kernel for euclidean spaces and define the GP over the joint architecture + hyper-parameter space via a product kernel. BO methods for early stopping in iterative training procedures <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b21">22]</ref> can be easily incorporated by defining a fidelity space. Using a line of work in scalable GPs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b49">50]</ref>, one can apply our methods to challenging problems which might require trying a very large number (∼100K) of architectures. These extensions will enable deploying NASBOT in large scale settings, but are tangential to our goal of introducing a BO method for architecture search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Methods: We compare NASBOT to the following baselines. RAND: random search; EA (Evolutionary algorithm): the same EA procedure described above. TreeBO <ref type="bibr" target="#b14">[15]</ref>: a BO method which only searches over feed forward structures. Random search is a natural baseline to compare optimisation methods. However, unlike in Euclidean spaces, there is no natural way to randomly explore the space of architectures. Our RAND implementation, operates in exactly the same way as NASBOT, except that the EA procedure is fed a random sample from Unif(0, 1) instead of the GP acquisition each time it evaluates an architecture. Hence, RAND is effectively picking a random network from the same space explored by NASBOT; neither method has an unfair advantage because it considers a different space. While there are other methods for architecture search, their implementations are highly nontrivial and are not made available.</p><p>Datasets: We use the following datasets: blog feedback <ref type="bibr" target="#b3">[4]</ref>, indoor location <ref type="bibr" target="#b45">[46]</ref>, slice localisation <ref type="bibr" target="#b10">[11]</ref>, naval propulsion <ref type="bibr" target="#b4">[5]</ref>, protein tertiary structure <ref type="bibr" target="#b33">[34]</ref>, news popularity <ref type="bibr" target="#b6">[7]</ref>, Cifar10 <ref type="bibr" target="#b23">[24]</ref>. The first six are regression problems for which we use MLPs. The last is a classification task on images for which we use CNNs.  Experimental Set up: Each method is executed in an asynchronously parallel set up of 2-4 GPUs, That is, it can evaluate multiple models in parallel, with each model on a single GPU. When the evaluation of one model finishes, the methods can incorporate the result and immediately re-deploy the next job without waiting for the others to finish. For the blog, indoor, slice, naval and protein datasets we use 2 GeForce GTX 970 (4GB) GPUs and a computational budget of 8 hours for each method. For the news popularity dataset we use 4 GeForce GTX 980 (6GB) GPUs with a budget of 6 hours and for Cifar10 we use 4 K80 (12GB) GPUs with a budget of 10 hours. For the regression datasets, we train each model with stochastic gradient descent (SGD) with a fixed step size of 10 −5 , a batch size of 256 for 20K batch iterations. For Cifar10, we start with a step size of 10 −2 , and reduce it gradually. We train in batches of 32 images for 60K batch iterations. The methods evaluate between 70-120 networks depending on the size of the networks chosen and the number of GPUs.</p><p>Results: <ref type="figure" target="#fig_0">Fig. 2</ref> plots the best validation score for each method against time. In <ref type="table" target="#tab_5">Table 3</ref>, we present the results on the test set with the best model chosen on the basis of validation set performance. On the Cifar10 dataset, we also trained the best models for longer (150K iterations). These results are in the last column of <ref type="table" target="#tab_5">Table 3</ref>. We see that NASBOT is the most consistent of all methods. The average time taken by NASBOT to determine the next architecture to evaluate was 46.13s. For RAND, EA, and TreeBO this was 26.43s, 0.19s, and 7.83s respectively. The time taken to train and validate models was on the order of 10-40 minutes depending on the model size. <ref type="figure" target="#fig_0">Fig. 2</ref> includes this time taken to determine the next point. Like many BO algorithms, while NASBOT's selection criterion is time consuming, it pays off when evaluations are expensive. In Appendices B and C, we provide additional details on the experiment set up and conduct synthetic ablation studies by holding out different components of the NASBOT framework. We also illustrate some of the best architectures found-on many datasets, common features were long skip connections and multiple decision layers.</p><p>Finally, we note that while our Cifar10 experiments fall short of the current state of the art <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b52">53]</ref>, the amount of computation in these work is several orders of magnitude more than ours (both the computation invested to train a single model and the number of models trained). Further, they use constrained spaces specialised for CNNs, while NASBOT is deployed in a very general model space.</p><p>We believe that our results can also be improved by employing enhanced training techniques such as image whitening, image flipping, drop out, etc. For example, using our training procedure on the VGG-19 architecture <ref type="bibr" target="#b36">[37]</ref> yielded a test set error of 0.1018 after 150K iterations. However, VGG-19 is known to do significantly better on Cifar10. That said, we believe our results are encouraging and lay out the premise for BO for neural architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We described NASBOT, a BO framework for neural architecture search. NASBOT finds better architectures for MLPs and CNNs more efficiently than other baselines on several datasets. A key contribution of this work is the efficiently computable OTMANN distance for neural network architectures, which may be of independent interest as it might find applications outside of BO. Our code for NASBOT and OTMANN will be made available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Details on OTMANN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Optimal Transport Reformulation</head><p>We begin with a review optimal transport. Throughout this section, ·, · denotes the Frobenius dot product. 1 n , 0 n ∈ R n denote a vector of ones and zeros respectively.</p><p>A review of Optimal Transport <ref type="bibr" target="#b46">[47]</ref>: Let</p><formula xml:id="formula_9">y 1 ∈ R n1 + , y 2 ∈ R n2 + be such that 1 n1 y 1 = 1 n2 y 2 . Let C ∈ R n1×n2 + .</formula><p>The following optimisation problem,</p><formula xml:id="formula_10">minimise Z Z, C (4) subject to Z &gt; 0, Z1 n2 = y 1 , Z 1 n1 = y 2 .</formula><p>is called an optimal transport program. One interpretation of this set up is that y 1 denotes the supplies at n 1 warehouses, y 2 denotes the demands at n 2 retail stores, C ij denotes the cost of transporting a unit mass of supplies from warehouse i to store j and Z ij denotes the mass of material transported from i to j. The program attempts to find transportation plan which minimises the total cost of transportation Z, C .</p><p>OT formulation of <ref type="formula">(3)</ref>: We now describe the OT formulation of the OTMANN distance. In addition to providing an efficient way to solve (3), the OT formulation will allow us to prove the metric properties of the solution. When computing the distance between</p><formula xml:id="formula_11">G 1 , G 2 , for i = 1, 2, let tm(G i ) = u∈Li m(u) denote the total mass in G i , andn i = n i + 1 where n i = |L i |. y 1 = [{ m(u)} u∈L1 , tm(G 2 )]</formula><p>∈ Rn 1 will be the supplies in our OT problem, and y 2 = [{ m(u)} u∈L2 , tm(G 1 )] ∈ Rn 2 will be the demands. To define the cost matrix, we augment the mislabel and structural penalty matrices C lmm , C str with an additional row and column of zeros;</p><formula xml:id="formula_12">i.e. C lmm = [C lmm 0 n1 ; 0 n2 ] ∈ Rn 1×n2 ; C str is defined similarly. Let C nas = [0 n1,n2 1 n1 ; 1 n2 0] ∈ Rn 1×n2 . We will show that (3) is equivalent to the following OT program. minimise Z Z , C<label>(5)</label></formula><p>subject to Z 1n 2 = y 1 , Z 1n 1 = y 2 .</p><p>One interpretation of (5) is that the last row/column appended to the cost matrices serve as a nonassignment layer and that the cost for transporting unit mass to this layer from all other layers is 1.</p><p>The costs for mislabelling was defined relative to this non-assignment cost. The costs for similar layers is much smaller than 1; therefore, the optimiser is incentivised to transport mass among similar layers rather than not assign it provided that the structural penalty is not too large. Correspondingly, the cost for very disparate layers is much larger so that we would never match, say, a convolutional layer with a pooling layer. In fact, the ∞'s in <ref type="table" target="#tab_2">Table 1</ref> can be replaced by any value larger than 2 and the solution will be the same. The following theorem shows that <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_12">(5)</ref> are equivalent.</p><p>Theorem 2. Problems <ref type="formula">(3)</ref> and <ref type="formula" target="#formula_12">(5)</ref> are equivalent, in that they both have the same minimum and we can recover the solution of one from the other.</p><p>Proof. We will show that there exists a bijection between feasible points in both problems with the same value for the objective. First let Z ∈ R n1×n2 be a feasible point for <ref type="bibr" target="#b2">(3)</ref>. Let Z ∈ Rn 1×n2 be such that its first n 1 × n 2 block is Z and,</p><formula xml:id="formula_13">Zn 1j = n1 i=1 Z ij , Z in2 = n2 j=1 Z ij , and Zn 1,n2 = ij Z ij . Then, for all i ≤ n 1 , j Z ij = m(j) and j Z n1j Z ij = j m(j) − ij Z ij + Zn 1,n2 = tm(G 2 )</formula><p>. We then have, Z 1n 2 = y 1 Similarly, we can show Z 1n 1 = y 2 . Therefore, Z is feasible for <ref type="bibr" target="#b4">(5)</ref>. We see that the objectives are equal via simple calculations, Z , C = Z , C lmm + C str + Z , C nas (6)</p><formula xml:id="formula_14">= Z, C lmm + C str + n2 j=1 Z ij + n1 i=1 Z ij = Z, C lmm + Z, C str + i∈L1 m(i) − j∈L2 Z ij + j∈L2 m(j) − i∈L1 Z ij .</formula><p>The converse also follows via a straightforward argument. For given Z that is feasible for <ref type="formula" target="#formula_12">(5)</ref>, we let Z be the first n 1 × n 2 block. By the equality constraints and non-negativity of Z , Z is feasible for <ref type="bibr" target="#b2">(3)</ref>. By reversing the argument in <ref type="bibr" target="#b5">(6)</ref> we see that the objectives are also equal.  <ref type="figure">Figure 3</ref>: An example of 2 CNNs which have d =d = 0 distance. The OT solution matches the mass in each layer in the network on the left to the layer horizontally opposite to it on the right with 0 cost. For layer 2 on the left, its mass is mapped to layers 2 and 3 on the left. However, while the descriptor of these networks is different, their functional behaviour is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Distance Properties of OTMANN</head><p>The following theorem shows that the solution of (3) is a pseudo-distance. This is a formal version of Theorem 1 in the main text.</p><p>Theorem 3. Assume that the mislabel cost matrix M satisfies the triangle inequality; i.e. for all labels x, y, z we have</p><formula xml:id="formula_15">M (x, z) ≤ M (x, y) + M (y, z). Let d(G 1 , G 2 ) be the solution of (3) for networks G 1 , G 2 . Then d(·, ·) is a pseudo-distance. That is, for all networks G 1 , G 2 , G 3 , it satisfies, d(G 1 , G 2 ) &gt; 0, d(G 1 , G 2 ) = d(G 2 , G 1 ), d(G 1 , G 1 ) = 0 and d(G 1 , G 3 ) ≤ d(G 1 , G 2 ) + d(G 2 , G 3 ).</formula><p>Some remarks are in order. First, observe that while d(·, ·) is a pseudo-distance, it is not a distance; i.e.</p><formula xml:id="formula_16">d(G 1 , G 2 ) = 0 G 1 = G 2 .</formula><p>For example, while the networks in <ref type="figure">Figure 3</ref> have different descriptors according to our formalism in Section 2.2, their distance is 0. However, it is not hard to see that their functionality is the same -in both cases, the output of layer 1 is passed through 16 conv3 filters and then fed to a layer with 32 conv3 filters -and hence, this property is desirable in this example. It is not yet clear however, if the topology induced by our metric equates two functionally dissimilar networks. We leave it to future work to study equivalence classes induced by the OTMANN distance. Second, despite the OT formulation, this is not a Wasserstein distance. In particular, the supports of the masses and the cost matrices change depending on the two networks being compared.</p><p>Proof of Theorem 3. We will use the OT formulation <ref type="bibr" target="#b4">(5)</ref> in this proof. The first three properties are straightforward. Non-negativity follows from non-negativity of Z , C in <ref type="bibr" target="#b4">(5)</ref>. It is symmetric since the cost matrix for d(G 2 , G 1 ) is C if the cost matrix for d(G 1 , G 2 ) is C and Z , C = Z , C for all Z . We also have d(G 1 , G 1 ) = 0 since, then, C has a zero diagonal.</p><p>To prove the triangle inequality, we will use a gluing lemma, similar to what is used in the proof of Wasserstein distances <ref type="bibr" target="#b32">[33]</ref>. Let G 1 , G 2 , G 3 be given and m 1 , m 2 , m 3 be their total masses. Let the solutions to d(G 1 , G 2 ) and d(G 2 , G 3 ) be P ∈ Rn 1×n2 and Q ∈ Rn 2×n3 respectively. When solving <ref type="bibr" target="#b4">(5)</ref>, we see that adding extra mass to the non-assignment layers does not change the objective, as an optimiser can transport mass between the two layers with 0 cost. Hence, we can assume w.l.o.g that (5) was solved with</p><formula xml:id="formula_17">y i = { m(u)} u∈Li , j∈{1,2,3} tm(G j ) − tm(G i ) ∈ Rn i for i = 1, 2, 3, when computing the distances d(G 1 , G 2 ), d(G 1 , G 3 ), d(G 2 , G 3 );</formula><p>i.e. the total mass was m 1 + m 2 + m 3 for all three pairs. We can similarly assume that P, Q account for this extra mass, i.e. Pn 1n2 and Qn 2n3 have been increased by m 3 and m 1 respectively from their solutions in <ref type="bibr" target="#b4">(5)</ref>.</p><p>To apply the gluing lemma, let S = P diag(1/y 2 )Q ∈ Rn 1×n3 , where diag(1/y 2 ) is a diagonal matrix whose (j, j) th element is 1/(y 2 ) j (note y 2 &gt; 0). We see that S is feasible for (5) when computing d(G 1 , G 3 ),</p><formula xml:id="formula_18">R1n 3 = P diag(1/y 2 )Q1n 3 = P diag(1/y 2 )y 2 = P 1n 2 = y 1 .</formula><p>Similarly, R 1n 1 = y 3 . Now, let U , V , W be the cost matrices C in (5) when computing d(G 1 , G 2 ), d(G 2 , G 3 ), and d(G 1 , G 3 ) respectively. We will use the following technical lemma whose proof is given below.</p><formula xml:id="formula_19">Lemma 4. For all i ∈ L 1 , j ∈ L 2 , k ∈ L 3 , we have W ik ≤ U ij + V jk .</formula><p>Applying Lemma 4 yields the triangle inequality.</p><formula xml:id="formula_20">d(G 1 , G 3 ) ≤ R, W = i∈L1,k∈L3 W ik j∈L2 P ij Q jk (y 2 ) j ≤ i,j,k (U ij + V jk ) P ij Q jk (y 2 ) j = ij U ij P ij (y 2 ) j k Q jk + jk V jk Q jk (y 2 ) j k P ij = ij U ij P ij + jk V jk Q jk = d(G 1 , G 2 ) + d(G 2 , G 3 )</formula><p>The first step uses the fact that d <ref type="figure" target="#fig_1">(G 1 , G 3 )</ref> is the minimum of all feasible solutions and the third step uses Lemma 4. The fourth step rearranges terms and the fifth step uses P 1n 1 = Q1n 3 = y 2 .</p><p>Proof of Lemma 4. Let W = W lmm + W str + W nas be the decomposition into the label mismatch, structural and non-assignment parts of the cost matrices; define similar quantities U lmm , U str , U nas , V lmm , V str , V nas for U , V . Noting a ≤ b+c and d ≤ e+f implies a+d ≤ b+e+c+f , it is sufficient to show the triangle inquality for each component individually. For the label mismatch term, (W lmm ) ik ≤ (U lmm ) ij + (V lmm ) jk follows directly from the conditions on M by setting x = (i), y = (j), z = (k), where i, j, k are indexing in L 1 , L 2 , L 3 respectively.</p><p>For the non-assignment terms, when (W nas ) ik = 0 the claim is true trivially. (W nas ) ik = 1, either when (i =n 1 , k ≤ n 3 ) or (i ≤ n 1 , k =n 3 ). In the former case, when j ≤ n 2 , (U nas ) jk = 1 and when j =n 2 , (V nas )n 2 = 1 as k ≤ n 3 . We therefore have, (W nas ) ik = (U nas ) ij + (V nas ) jk = 1. A similar argument shows equality for the (i ≤ n 1 , k =n 3 ) case as well.</p><p>Finally, for the structural terms we note that W str can be written as W str = t W (t) as can U (t) , T (t) .</p><p>Here t indexes over the choices for the types of distances considered, i.e. t ∈ {sp, lp, rw} × {ip, op}. It is sufficient to show (W (t) ) ik ≤ (U (t) ) ij + (T (t) ) jk . This inequality takes the form,</p><formula xml:id="formula_21">|δ (t) 1i − δ (t) 3k | ≤ |δ (t) 1i − δ (t) 2j | + |δ (t) 2j − δ (t) 3k |.</formula><p>Where δ (t) g refers to distance type t in network g for layer s. The above is simply the triangle inequality for real numbers. This concludes the proof of Lemma 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation &amp; Design Choices</head><p>Masses on the decision &amp; input/output layers: It is natural to ask why one needs to model the mass in the decision and input/output layers. For example, a seemingly natural choice is to use 0 for these layers. Using 0 mass, is a reasonable strategy if we were to allow only one decision layer. However, when there are multiple decision layers, consider comparing the following two networks: the first has a feed forward MLP with non-linear layers, the second is the same network but with an additional linear decision layer u, with one edge from u ip to u and an edge from u to u op . This latter models the function as a linear + non-linear term which might be suitable for some problems unlike modeling it only as a non-linear term. If we do not add layer masses for the input/output/decision layers, then the distance between both networks would be 0 -as there will be equal mass in the FF part for both networks and they can be matched with 0 cost.    For δ rw ip we make the following changes to Algorithm 1. In step 1, we set δ rw ip (u ip ) = 0, in step 3, we pop_first and ∆ in step 4 is computed using the parents. δ sp ip , δ lp ip are computed with the same procedure but by replacing the averaging with minimum or maximum as above.</p><p>Label Penalty Matrices: The label penalty matrices used in our NASBOT implementation, described below, satisfy the triangle inequality condition in Theorem 3.</p><p>CNNs: <ref type="table" target="#tab_8">Table 4</ref> shows the label penalty matrix M for used in our CNN experiments with labels conv3, conv5, conv7, max-pool, avg-pool, softmax, ip, op. convk denotes a k × k convolution while avg-pool and max-pool are pooling operations. In addition, we also use res3, res5, res7 layers which are inspired by ResNets. A resk uses 2 concatenated convk layers but the input to the first layer is added to the output of the second layer before the relu activation -See <ref type="figure" target="#fig_0">Figure 2</ref> in He et al. <ref type="bibr" target="#b11">[12]</ref>. The layer mass for resk layers is twice that of a convk layer. The costs for the res in the label penalty matrix is the same as the conv block. The cost between a resk and convj is M (resk, convj) = 0.9 × M (convk, convj) + 0.1 × 1; i.e. we are using a convex combination of the conv costs and the non-assignment cost. The intuition is that a resk is similar to convk block except for the residual addition.</p><p>MLPs: <ref type="table" target="#tab_9">Table 5</ref> shows the label penalty matrix M for used in our MLP experiments with labels relu, crelu, leaky-relu, softplus, elu, logistic, tanh, linear, ip, op. Here the first seven are common non-linear activations; relu, crelu, leaky-relu, softplus, elu rectifiers while logistic and tanh are sigmoidal activations.</p><p>Other details: Our implementation of OTMANN differs from what is described in the main text in two ways. First, in our CNN experiments, for a fc layer u, we use 0.1 × m(u) × #-incoming-channels as the mass, i.e. we multiply it by 0.1 from what is described in the main text. This is because, in the convolutional and pooling channels, each unit is an image where as in the fc layers each unit is a scalar. One could, in principle, account for the image sizes at the various layers when computing the layer masses, but this also has the added complication of depending on the size of the input image which varies from problem to problem. Our approach is simpler and yields reasonable results.</p><p>Secondly, we use a slightly different form for C str . First, for i ∈ L 1 , j ∈ L 2 , we let C all str (i, j) = 1 6 s∈{sp, lp, rw} t∈{ip,op} |δ s t (i) − δ s t (j)| be the average of all path length differences; i.e. C all str captures the path length differences when considering all layers. For CNNs, we similarly construct matrices C conv str , C pool str , C fc str , except they only consider the convolutional, pooling and fully connected layers respectively in the path lengths. For C conv str , the distances to the output (from the input) can be computed by zeroing outgoing (incoming) edges to layers that are not convolutional. We can similarly construct C pool str and C fc str only counting the pooling and fully connected layers. Our final cost matrix for the structural penalty is the average of these four matrices, C str = (C all str + C conv str + C pool str + C fc str )/4. For MLPs, we adopt a similar strategy by computing matrices C all str , C rec str , C sig str with all layers, only rectifiers, and only sigmoidal layers and let C str = (C all str + C rec str + C sig str )/3. The intuition is that by considering certain types of layers, we are accounting for different types of information flow due to different operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Some Illustrations of the OTMANN Distance</head><p>We illustrate that OTMANN computes reasonable distances on neural network architectures via a two-dimensional t-SNE visualisation <ref type="bibr" target="#b26">[27]</ref> of the network architectures based. Given a distance matrix between m objects, t-SNE embeds them in a d dimensional space so that objects with small distances are placed closer to those that have larger distances. <ref type="figure" target="#fig_4">Figure 4</ref> shows the t-SNE embedding using the OTMANN distance and its noramlised version. We have indexed 13 networks in both figures in a-n and displayed their architectures in <ref type="figure" target="#fig_6">Figure 5</ref>. Similar networks are placed close to each other indicating that OTMANN induces a meaningful topology among neural network architectures.</p><p>Next, we show that the distances induced by OTMANN are correlated with validation error performance. In <ref type="figure" target="#fig_7">Figure 6</ref> we provide the following scatter plot for networks trained in our experiments for the Indoor, Naval and Slice datasets. Each point in the figure is for pair of networks. The x-axis is the OTMANN distance between the pair and the y-axis is the difference in the validation error on the dataset. In each figure we used 300 networks giving rise to 45K pairwise points in each scatter plot. As the figure indicates, when the distance is small the difference in performance is close to 0. However, as the distance increases, the points are more scattered. Intuitively, one should expect that while networks that are far apart could perform similarly or differently, similar networks should perform similarly. Hence, OTMANN induces a useful topology in the space of architectures that is smooth for validaiton performance on real world datasets. This demonstrates that it can be incorporated in a BO framework to optimise a network based on its validation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation of NASBOT</head><p>Here, we describe our BO framework for NASBOT in full detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 The Kernel</head><p>As described in the main text, we use a negative exponentiated distance as our kernel. Precisely, we use,</p><formula xml:id="formula_22">κ(·, ·) = αe − i βid p i (·,·) +ᾱe − iβ idp i (·,·) .<label>(7)</label></formula><p>Here, d i ,d i , are the OTMANN distance and its normalised counterpart developed in Section 3, computed with different values for ν str ∈ {ν str,i } i . β i ,β i manage the relative contributions of d i ,d i , while (α,ᾱ) manage the contributions of each kernel in the sum. An ensemble approach of the above form, instead of trying to pick a single best value, ensures that NASBOT accounts for the different topologies induced by the different distances d i ,d i . In the experiments we report, we used {ν str,i } i = {0.1, 0.2, 0.4, 0.8}, p = 1 andp = 2. Our experience suggests that NASBOT was not particularly sensitive to these choices expect when we used only very large or only very small values in {ν str,i } i .</p><p>NASBOT, as described above has 11 hyper-parameters of its own; α,ᾱ, {(β i ,β i )} 4 i=1 and the GP noise variance η 2 . While maximising the GP marginal likelihood is a common approach to pick hyper-parameters, this might cause over-fitting when there are many of them. Further, as training large neural networks is typically expensive, we have to content with few observations for the GP      : Each point in the scatter plot indicates the log distance between two architectures (x axis) and the difference in the validation error (y axis), on the Indoor, Naval and Slice datasets. We used 300 networks, giving rise to ∼ 45K pairwise points. On all datasets, when the distance is small, so is the difference in the validation error. As the distance increases, there is more variance in the validation error difference. Intuitively, one should expect that while networks that are far apart could perform similarly or differently, networks with small distance should perform similarly.</p><p>in practical settings. Our solution is to start with a (uniform) prior over these hyper-parameters and sample hyper-parameter values from the posterior under the GP likelihood <ref type="bibr" target="#b39">[40]</ref>, which we found to be robust. While it is possible to treat ν str itself as a hyper-parameter of the kernel, this will require us to re-compute all pairwise distances of networks that have already been evaluated each time we change the hyper-parameters. On the other hand, with the above approach, we can compute and store distances for different ν str,i values whenever a new network is evaluated, and then compute the kernel cheaply for different values of α,ᾱ, {(β i ,β i )} i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Optimising the Acquisition</head><p>We use a evolutionary algorithm (EA) approach to optimise the acquisition function <ref type="bibr" target="#b1">(2)</ref>. We begin with an initial pool of networks and evaluate the acquisition ϕ t on those networks. Then we generate a set of N mut mutations of this pool as follows. First, we stochastically select N mut candidates from the set of networks already evaluated such that those with higher ϕ t values are more likely to be selected than those with lower values. Then we apply a mutation operator to each candidate, to produce a modified architecture. Finally, we evaluate the acquisition on this N mut mutations, add it to the initial pool, and repeat for the prescribed number of steps.</p><p>Mutation Operator: To describe the mutation operator, we will first define a library of modifications to a neural network. These modifications, described in <ref type="table" target="#tab_11">Table 6</ref>, might change the architecture either by increasing or decreasing the number of computational units in a layer, by adding or deleting layers, or by changing the connectivity of existing layers. They provide a simple mechanism to explore the space of architectures that are close to a given architecture. The one-step mutation operator takes a given network and applies one of the modifications in <ref type="table" target="#tab_11">Table 6</ref> picked at random to produce a new network. The k-step mutation operator takes a given network, and repeatedly applies the one-step operator k times -the new network will have undergone k changes from the original one. One can also define a compound operator, which picks the number of steps probabilistically. In our implementation of NASBOT, we used such a compound operator with probabilities (0.5, 0.25, 0.125, 0.075, 0.05); i.e. it chooses a one-step operator with probability 0.5, a 4-step operator with probability 0.075, etc. Typical implementations of EA in Euclidean spaces define the mutation operator via a Gaussian (or other) perturbation of a chosen candidate. It is instructive to think of the probabilities for each step in our scheme above as being analogous to the width of the Gaussian chosen for perturbation.</p><p>Sampling strategy: The sampling strategy for EA is as follows. Let {z i } i , where z i ∈ X be the points evaluated so far. We sample N mut new points from a distribution π where π(z i ) ∝ exp(g(z i )/σ). Here g is the function to be optimised (for NASBOT, ϕ t at time t). σ is the standard deviation of all previous evaluations. As the probability for large g values is higher, they are more likely to get selected. σ provides normalisation to account for different ranges of function values.  This modifier duplicates a random path in the network. Randomly pick a node u 1 and then pick one of its children u 2 randomly. Keep repeating to generate a path u 1 , u 2 , . . . , u k−1 , u k until you decide to stop randomly. Create duplicate layers u 2 , . . . ,ũ k−1 whereũ i = u i for i = 2, . . . , k − 1. Add these layers along with new edges (u 1 ,ũ 2 ), (ũ k−1 , u k ), and (ũ j ,ũ j+1 ) for j = 2, . . . , k − 2.</p><p>remove_layer Picks a layer at random and removes it. If this layer was the only child (parent) of any of its parents (children) u, then adds an edge from u (one of its parents) to one of its children (u).  Since our candidate selection scheme at each step favours networks that have high acquisition value, our EA scheme is more likely to search at regions that are known to have high acquisition. The stochasticity in this selection scheme and the fact that we could take multiple steps in the mutation operation ensures that we still sufficiently explore the space. Since an evaluation of ϕ t is cheap, we can use many EA steps to explore several architectures and optimise ϕ t .</p><p>Other details: The EA procedure is also initialised with the same initial pools in <ref type="bibr">Figures 20,</ref><ref type="bibr" target="#b20">21.</ref> In our NASBOT implementation, we increase the total number of EA evaluations n EA at rate O( √ t) where t is the current time step in NASBOT. We set N mut to be O( √ n EA ). Hence, initially we are only considering a small neighborhood around the initial pool, but as we proceed along BO, we expand to a larger region, and also spend more effort to optimise ϕ t .</p><p>Considerations when performing modifications: The modifications in <ref type="table" target="#tab_11">Table 6</ref> is straightforward in MLPs. But in CNNs one needs to ensure that the image sizes are the same when concatenating them as an input to a layer. This is because strides can shrink the size of the image. When we perform a modification we check if this condition is violated and if so, disallow that modification. When a skip modifier attempts to add a connection from a layer with a large image size to one with a smaller one, we add avg-pool layers at stride 2 so that the connection can be made (this can be seen, for e.g. in the second network in <ref type="figure">Fig. 8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Other Implementation Details</head><p>Initialisation: We initialise NASBOT (and other methods) with an initial pool of 10 networks. These networks are illustrated in <ref type="figure" target="#fig_0">Fig. 20</ref> for CNNs and <ref type="figure" target="#fig_0">Fig. 21</ref> for MLPs at the end of the document. These are the same networks used to initialise the EA procedure to optimise the acquisition. All initial networks have feed forward structure. For the CNNs, the first 3 networks have structure similar to the VGG nets <ref type="bibr" target="#b36">[37]</ref> and the remaining have blocked feed forward structures as in He et al. <ref type="bibr" target="#b11">[12]</ref>. We also use blocked structures for the MLPs with the layer labels decided arbitrarily.</p><p>Domain: For NASBOT, and other methods, we impose the following constraints on the search space. If the EA modifier (explained below) generates a network that violates these constraints, we simply skip it.</p><p>• Maximum number of layers: 60</p><p>• Maximum mass: 10 8</p><p>• Maximum in/out degree: 5</p><p>• Maximum number of edges: 200</p><p>• Maximum number of units per layer: 1024</p><p>• Minimum number of units per layer: 8</p><p>Layer types: We use the layer types detailed in Appendix A.3 for both CNNs and MLPs. For CNNs, all pooling operations are done at stride 2. For convolutional layers, we use either stride 1 or 2 (specified in the illustrations). For all layers in a CNN we use relu activations.</p><p>Parallel BO: We use a parallelised experimental set up where multiple models can be evaluated in parallel. We handle parallel BO via the hallucination technique in Ginsbourger et al. <ref type="bibr" target="#b9">[10]</ref>.</p><p>Finally, we emphasise that many of the above choices were made arbitrarily, and we were able to get NASBOT working efficiently with our first choice for these parameters/specifications. Note that many end-to-end systems require specification of such choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Addendum to Experiments</head><p>C.1 Baselines RAND: Our RAND implementation, operates in exactly the same way as NASBOT, except that the EA procedure (Sec. B.2) is fed a random sample from Unif(0, 1) instead of the GP acquisition each time it evaluates an architecture. That is, we follow the same schedule for n EA and N mut as we did for NASBOT . Hence RAND has the opportunity to explore the same space as NASBOT, but picks the next evaluation randomly from this space.</p><p>EA: This is as described in Appendix B except that we fix N mut = 10 all the time. In our experiments where we used a budget based on time, it was difficult to predict the total number of evaluations so as to set N mut in perhaps a more intelligent way.</p><p>TreeBO: As the implementation from Jenatton et al. <ref type="bibr" target="#b14">[15]</ref> was not made available, we wrote our own. It differs from the version described in the paper in a few ways. We do not tune for a regularisation penalty and step size as they do to keep it line with the rest of our experimental set up. We set the depth of the network to 60 as we allowed 60 layers for the other methods. We also check for the other constraints given in Appendix B before evaluating a network. The original paper uses a tree structured kernel which can allow for efficient inference with a large number of samples. For simplicity, we construct the entire kernel matrix and perform standard GP inference. The result of the inference is the same, and the number of GP samples was always below 120 in our experiments so a sophisticated procedure was not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Details on Training</head><p>In all methods, for each proposed network architecture, we trained the network on the train data set, and periodically evaluated its performance on the validation data set. For MLP experiments, we optimised network parameters using stochastic gradient descent with a fixed step size of 10 −5 and a batch size of 256 for 20,000 iterations. We computed the validation set MSE every 100 iterations; from this we returned the minimum MSE that was achieved. For CNN experiments, we optimised network parameters using stochastic gradient descent with a batch size of 32. We started with a learning rate of 0.01 and reduced it gradually. We also used batch normalisation and trained the model for 60, 000 batch iterations. We computed the validation set classification error every 4000 iterations; from this we returned the minimum classification error that was achieved.</p><p>After each method returned an optimal neural network architecture, we again trained each optimal network architecture on the train data set, periodically evaluated its performance on the validation data set, and finally computed the MSE or classification error on the test data set. For MLP experiments, we used the same optimisation procedure as above; we then computed the test set MSE at the iteration where the network achieved the minimum validation set MSE. For CNN experiments, we used the same optimisation procedure as above, except here the optimal network architecture was trained for 120,000 iterations; we then computed the test set classification error at the iteration where the network achieved the minimum validation set classification error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Optimal Network Architectures and Initial Pool</head><p>Here we illustrate and compare the optimal neural network architectures found by different methods. In <ref type="figure" target="#fig_1">Figures 8-11</ref>, we show some optimal network architectures found on the Cifar10 data by NASBOT, EA, RAND, and TreeBO, respectively. We also show some optimal network architectures found for these four methods on the Indoor data, in <ref type="figure" target="#fig_0">Figures 12-15</ref>, and on the Slice data, in <ref type="figure" target="#fig_1">Figures 16-19</ref>. A common feature among all optimal architectures found by NASBOT was the presence of long skip connections and multiple decision layers.</p><p>In <ref type="figure" target="#fig_0">Figure 21</ref>, we show the initial pool of MLP network architectures, and in <ref type="figure" target="#fig_0">Figure 20</ref>, we show the initial pool of CNN network architectures. On the Cifar10 dataset, VGG-19 was one of the networks in the initial pool. While all methods beat VGG-19 when trained for 24K iterations (the number of iterations we used when picking the model), TreeBO and RAND lose to VGG-19 (see Section 5 for details). This could be because the performance after shorter training periods may not exactly correlate with performance after longer training periods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Ablation Studies and Design Choices</head><p>We conduct experiments comparing the various design choices in NASBOT. Due to computational constraints, we carry them out on synthetic functions.</p><p>In <ref type="figure" target="#fig_12">Figure 7a</ref>, we compare NASBOT using only the normalised distance, only the unnormalised distance, and the combined kernel as in <ref type="bibr" target="#b6">(7)</ref>. While the individual distances performs well, the combined form outperforms both.</p><p>Next, we modify our EA procedure to optimise the acquisition. We execute NASBOT using only the EA modifiers which change the computational units (first four modifiers in <ref type="table" target="#tab_11">Table 6</ref>), then using the modifiers which only change the structure of the networks (bottom 5 in <ref type="table" target="#tab_11">Table 6</ref>), and finally using all 9 modifiers, as used in all our experiments. The combined version outperforms the first two.</p><p>Finally, we experiment with different choices for p andp in <ref type="bibr" target="#b6">(7)</ref>. As the figures indicate, the performance was not particularly sensitive to these choices.</p><p>Below we describe the three synthetic functions f 1 , f 2 , f 3 used in our synthetic experiments. f 3 applies for CNNs while f 1 , f 2 apply for MLPs. Here am denotes the average mass per layer, deg i is the average in degree the layers, deg o is the average out degree, δ is the shortest distance from u ip to u op , str is the average stride in CNNS, frac_conv3 is the fraction of layers that are conv3, frac_sigmoid is the fraction of layers that are sigmoidal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Discussion on Related Work</head><p>Historically, evolutionary (genetic) algorithms (EA) have been the most common method used for designing architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51]</ref>. EA techniques are popular as they provide a simple mechanism to explore the space of architectures by making a sequence of changes to networks that have already been evaluated. However, as we will discuss later, EA algorithms, while conceptually and computationally simple, are typically not best suited for optimising functions that are expensive to evaluate. A related line of work first sets up a search space for architectures via incremental modifications, and then explores this space via random exploration, MCTS, or A* search <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>. Comparison of p,p values (f3) p = 1,p = 1 p = 1,p = 2 p = 2,p = 1 p = 2,p = 2 (c)  <ref type="table" target="#tab_11">Table 6</ref>), modifiers which only change the structure of the networks (bottom 5 in <ref type="table" target="#tab_11">Table 6</ref>), and all 9 modifiers. (c): Comparison of NASBOT with different choices for p andp. In all figures, the x axis is the number of evaluations and the y axis is the negative maximum value (lower is better). All figures were produced by averaging over at least 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of evaluations</head><p>Some of the methods above can only optimise among feed forward structures, e.g. <ref type="figure" target="#fig_1">Fig. 1a</ref>, but cannot handle spaces with arbitrarily structured networks, e.g. <ref type="figure" target="#fig_1">Figs. 1b, 1c</ref>.</p><p>The most successful recent architecture search methods that can handle arbitrary structures have adopted reinforcement learning (RL) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref>. However, architecture search is in essence an optimisation problem -find the network with the highest function value. There is no explicit need to maintain a notion of state and solve the credit assignment problem in RL <ref type="bibr" target="#b42">[43]</ref>. Since RL is fundamentally more difficult than optimisation <ref type="bibr" target="#b15">[16]</ref>, these methods typically need to try a very large number of architectures to find the optimum. This is not desirable, especially in computationally constrained settings.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Cross validation results: In all figures, the x axis is time. The y axis is the mean squared error (MSE) in the first 6 figures and the classification error in the last. Lower is better in all cases. The title of each figure states the dataset and the number of parallel workers (GPUs). All figures were averaged over at least 5 independent runs of each method. Error bars indicate one standard error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Compute δ rw op (u) for all u ∈ L Require: G = (L, E), L is topologically sorted in S. 1: δ rw op (u op ) = 0, δ rw op (u) = nan ∀u = u op . 2: while S is not empty do 3: u ← pop_last(S) 4: ∆ ← {δ rw op (c) : c ∈ children(u)} 5: δ rw op (u) ← 1 + average(∆) 6: end while 7: Return δ rw op .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Computing path lengths δ t s :</head><label>s</label><figDesc>Algorithm 1 computes all path lengths in O(|E|) time. Note that topological sort of a connected digraph also takes O(|E|) time. The topological sorting ensures that δ rw op is always computed for the children in step 4. For δ sp op , δ lp op we would replace the averaging of ∆ in step 5 with the minimum and maximum of ∆ respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Two dimensional t-SNE embeddings of 100 randomly generated CNN architectures based on the OTMANN distance (top) and its normalised version (bottom). Some networks have been indexed a-n in the figures; these network architectures are illustrated inFigure 5. Networks that are similar are embedded close to each other indicating that the OTMANN induces a meaningful topology among neural network architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>#0</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Illustrations of the nextworks indexed a-n inFigure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6</head><label>6</label><figDesc>Figure 6: Each point in the scatter plot indicates the log distance between two architectures (x axis) and the difference in the validation error (y axis), on the Indoor, Naval and Slice datasets. We used 300 networks, giving rise to ∼ 45K pairwise points. On all datasets, when the distance is small, so is the difference in the validation error. As the distance increases, there is more variance in the validation error difference. Intuitively, one should expect that while networks that are far apart could perform similarly or differently, networks with small distance should perform similarly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>at random and decrease the number of units by 1/8. dec_en_masse First topologically order the networks, randomly pick 1/8 of the layers (in order) and decrease the number of units by 1/8. For networks with eight layers or fewer pick a 1/4 of the layers (instead of 1/8) and for those with four layers or fewer pick 1/2. inc_single Pick a layer at random and increase the number of units by 1/8. inc_en_masse Choose a large sub set of layers, as for dec_en_masse, and increase the number of units by 1/8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>dup_path</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>skip</head><label></label><figDesc>Randomly picks layersu, v where u is topologically before v and (u, v) / ∈ E. Add (u, v) to E.swap_label Randomly pick a layer and change its label. wedge_layer Randomly pick any edge (u, v) ∈ E. Create a new layer w with a random label (w). Remove (u, v) from E and add (u, w), (w, v). If applicable, set the number of units u(w) to be ( u(u) + u(v))/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>f 0</head><label>0</label><figDesc>= exp(−0.001 * |am − 1000|) + exp(−0.5 * |deg i − 5|) + exp(−0.5 * |deg o − 5|)+ exp(−0.1 * |δ − 5|) + exp(−0.1 * ||L| − 30|) + exp(−0.05 * ||E| − 100|) f 1 = f 0 + exp(−3 * |str − 1.5|) + exp(−0.3 * ||L| − 50|)+ exp(−0.001 * |am − 500|) + frac_conv3 f 2 = f 0 + exp(−0.001 * |am − 2000|) + exp(−0.1 * ||E| − 50|) + frac_sigmoid f 3 = f 0 + frac_sigmoid</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>We compare NASBOT for different design choices in our framework. (a): Comparison of NASBOT using only the normalised distance e −βd , only the unnormalised distance d −βd , and the combination e −βd +e −βd . (b): Comparison of NASBOT using only the EA modifiers which change the computational units (top 4 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Optimal network architectures found with TreeBO on Cifar10 data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>An example label mismatch cost matrix M . There is zero cost for matching identical layers, &lt; 1 cost for similar layers, and infinite cost for disparate layers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Descriptions of modifiers to transform one network to another. The first four change the number of units in the layers but do not change the architecture, while the last five change the architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, might change the architecture either by</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Blog Feedback, #workers = 2</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Indoor Location, #workers = 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Slice Localisation, #workers = 2</cell><cell>Naval Propulsion, #workers = 2</cell></row><row><cell></cell><cell></cell><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Cross Validation MSE</cell><cell>0.8 0.9 1 1.1</cell><cell></cell><cell></cell><cell></cell><cell>Cross Validation MSE</cell><cell cols="2">0.15 0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cross Validation MSE</cell><cell>0.6 0.7 0.8 0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cross Validation MSE</cell><cell>10 -2 10 -1</cell></row><row><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell></cell><cell>6</cell><cell>8</cell><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time (hours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Time (hours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time (hours)</cell><cell></cell><cell>Time (hours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Protein, #workers = 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">News, #workers = 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cifar10, #workers = 4</cell></row><row><cell></cell><cell cols="2">0.98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.17</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cross Validation MSE</cell><cell cols="2">0.86 0.88 0.9 0.92 0.94 0.96</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cross Validation MSE</cell><cell>0.8 0.9 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cross Validation Error</cell><cell>0.14 0.15 0.16 0.13</cell><cell></cell><cell></cell><cell></cell><cell>EA RAND TreeBO NASBOT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.12</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Time (hours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Time (hours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Time (hours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Blog (60K, 281)</cell><cell>Indoor (21K, 529)</cell><cell>Slice (54K, 385)</cell><cell>Naval (12K, 17)</cell><cell>Protein (46K, 9)</cell><cell>News (40K, 61)</cell><cell>Cifar10 (60K, 3K)</cell><cell>Cifar10 150K iters</cell></row><row><cell>RAND</cell><cell>0.780 ± 0.034</cell><cell>0.115 ±0.023</cell><cell>0.758 ± 0.041</cell><cell>0.0103 ± 0.002</cell><cell>0.948 ± 0.024</cell><cell>0.762 ±0.013</cell><cell>0.1342 ± 0.002</cell><cell>0.0914 ± 0.008</cell></row><row><cell>EA</cell><cell>0.806 ± 0.040</cell><cell>0.147 ± 0.010</cell><cell>0.733 ± 0.041</cell><cell>0.0079 ±0.004</cell><cell>1.010 ± 0.038</cell><cell>0.758 ±0.038</cell><cell>0.1411 ± 0.002</cell><cell>0.0915 ± 0.010</cell></row><row><cell>TreeBO</cell><cell>0.928 ± 0.053</cell><cell>0.168 ± 0.023</cell><cell>0.759 ± 0.079</cell><cell>0.0102 ± 0.002</cell><cell>0.998 ± 0.007</cell><cell>0.866 ± 0.085</cell><cell>0.1533 ± 0.004</cell><cell>0.1121 ± 0.004</cell></row><row><cell>NASBOT</cell><cell>0.731 ±0.029</cell><cell>0.117 ±0.008</cell><cell>0.615 ±0.044</cell><cell>0.0075 ±0.002</cell><cell>0.902 ±0.033</cell><cell>0.752 ±0.024</cell><cell>0.1209 ±0.003</cell><cell>0.0869 ±0.004</cell></row></table><note>gives the size and dimensionality of each dataset. For the first 6 datasets, we use a 0.6 − 0.2 − 0.2 train-validation-test split and normalised the input and output to have zero mean and unit variance. Hence, a constant predictor will have a mean squared error of approximately 1. For Cifar10 we use 40K for training and 10K each for validation and testing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The first row gives the number of samples N and the dimensionality D of each dataset in the form (N, D). The subsequent rows show the regression MSE or classification error (lower is better) on the test set for each method. The last column is for Cifar10 where we took the best models found by each method in 24K iterations and trained it for 120K iterations. When we trained the VGG-19 architecture using our training procedure, we got test errors 0.1718 (60K iterations) and 0.1018 (150K iterations).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The label mismatch cost matrix M we used in our CNN experiments. M (x, y) denotes the penalty for transporting a unit mass from a layer with label x to a layer with label y. The labels abbreviated are conv3, conv5, conv7, max-pool, avg-pool, fc, and softmax in order. A blank indicates ∞ cost. We have not shown the ip and op layers, but they are similar to the fc column, 0 in the diagonal and ∞ elsewhere.</figDesc><table><row><cell></cell><cell cols="5">re cr &lt;rec&gt; lg ta lin</cell></row><row><cell>re</cell><cell>0</cell><cell>.1</cell><cell>.1</cell><cell cols="2">.25 .25</cell></row><row><cell>cr</cell><cell>.1</cell><cell>0</cell><cell>.1</cell><cell cols="2">.25 .25</cell></row><row><cell>&lt;rec&gt;</cell><cell>.1</cell><cell>.1</cell><cell>0</cell><cell cols="2">.25 .25</cell></row><row><cell>lg</cell><cell cols="2">.25 .25</cell><cell>.25</cell><cell>0</cell><cell>.1</cell></row><row><cell>ta</cell><cell cols="2">.25 .25</cell><cell>.25</cell><cell>.1</cell><cell>0</cell></row><row><cell>lin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>The label mismatch cost matrix M we used in our MLP experiments. The labels abbreviated are relu, crelu, &lt;rec&gt;, logistic, tanh, and linear in order. &lt;rec&gt; is place-holder for any other rectifier such as leaky-relu, softplus, elu. A blank indicates ∞ cost. The design here was simple. Each label gets 0 cost with itself. A rectifier gets 0.1 cost with another rectifier and 0.25</figDesc><table /><note>with a sigmoid; vice versa for all sigmoids. The rest of the costs are infinity. We have not shown the ip and op, but they are similar to the lin column, 0 in the diagonal and ∞ elsewhere.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Descriptions of modifiers to transform one network to another. The first four change the number of units in the layers but do not change the architecture, while the last five change the architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Ablation study on Kernel Design (f1)Ablation study on EA modifiers (f2)</figDesc><table><row><cell></cell><cell>-4</cell><cell></cell><cell></cell><cell>Only Normalised</cell><cell></cell><cell>-2.5</cell><cell></cell><cell></cell><cell>Only Computational</cell><cell></cell><cell>-2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Negative maximum value</cell><cell>-6 -5.5 -5 -4.5</cell><cell></cell><cell></cell><cell>Only Unnormalised Combined</cell><cell>Negative maximum value</cell><cell>-4.5 -4 -3.5 -3</cell><cell></cell><cell></cell><cell>Only Structural Combined</cell><cell>Negative maximum value</cell><cell>-3.5 -3 -2.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Number of evaluations</cell><cell></cell><cell></cell><cell cols="3">Number of evaluations</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Optimal network architectures found with NASBOT on Cifar10 data.Figure 10: Optimal network architectures found with RAND on Cifar10 data.</figDesc><table><row><cell></cell><cell></cell><cell>17: conv3, 256 (32768) 20: conv3, 256 (65536) 23: conv3, 256 (65536)</cell><cell>7: max-pool (128) 9: conv3, 128 (16384) 11: conv3, 128 (16384) 13: conv3, 128 (16384) 15: max-pool (128) 18: conv3, 256 (32768) 21: conv3, 256 (65536) 25: max-pool (512) 27: conv3, 512 (262144) 29: conv3, 576 (294912) 31: conv3, 576 (331776) (198735) 8: conv3, 128 (16384) 10: max-pool (128) 12: max-pool (128) 14: conv3, 512 (65536) 16: conv3, 576 (294912) 19: conv3, 576 (331776) 22: max-pool (576) 24: fc, 128 (7372) 26: fc, 256 (3276) 28: fc, 512 (13107) 30: softmax (99367) 37: op 32: max-pool (576) 33: fc, 128 (7372) 34: fc, 256 (3276) 35: fc, 512 (13107) 36: softmax (99367)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0: ip (329217) 1: conv3, 64 (64) 2: conv3, 64 (4096) 3: avg-pool (64) 4: max-pool (64) 5: avg-pool (64) 6: conv3, 128 (8192) 7: avg-pool (64) 8: conv3, 128 (16384) 9: avg-pool (64) 11: max-pool (128) 12: avg-pool (64) 14: conv3, 144 (18432) 41: fc, 128 (7372) 15: conv3, 128 (18432) 16: conv3, 128 (16384) 17: conv3, 128 (16384) 18: max-pool 46: fc, 128 (13926) (128) 19: conv3, 256 (32768) 20: conv3, 256 (65536) 21: conv3, 256 (65536) 23: conv3, 256 (65536) 26: max-pool (256) 27: max-pool (256) 31: conv3, 512 (131072) 33: conv3, 512 (131072) 34: conv3, 512 (262144) 36: conv3, 512 22: conv3, 288 (73728) 25: conv3, 256 (73728) 29: max-pool (256) 30: conv3, 512 (131072) 35: conv3, 512 24: conv3, 256 (73728) 28: max-pool (256) 32: conv3, 512 (131072) (524288) (262144) 37: conv3, 512 (262144) 38: max-pool (512) 39: conv3, 512 (262144) 40: conv3, 512 (262144) 43: max-pool (1024) 42: res3 /2, 512 (262144) 44: fc, 512 (6553) 45: max-pool (512) 47: softmax (109739) 48: conv3, 128 (65536) 49: fc, 512 (6553) 55: op (329217) 50: fc, 128 (1638) 51: softmax (109739) 52: fc, 256 (3276) 53: fc, 512 (13107) 54: softmax (109739)</cell><cell>10: avg-pool (64) 13: avg-pool (64)</cell></row><row><cell>0: ip (131815)</cell><cell>0: ip (128156)</cell><cell cols="2">0: ip (95388) (49833) 0: ip</cell><cell cols="2">0: ip (86799) (104727) 0: ip</cell></row><row><cell>0: ip</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(328008) 1: conv3, 72 (72)</cell><cell>1: conv3, 64 (64)</cell><cell cols="2">1: conv3, 64 (64) 1: conv3, 63 (63)</cell><cell>1: conv3, 64 (64)</cell><cell>2: conv3, 64 (64) 1: conv3, 64 (64)</cell></row><row><cell>2: conv3, 72 (5184)</cell><cell>2: conv3, 64 (4096)</cell><cell cols="2">2: conv3, 64 (4096) 2: conv3, 56 (3528)</cell><cell>(4096) 3: conv3, 64</cell><cell>(4096) 4: conv3, 64 2: conv3, 64 (4096)</cell></row><row><cell>3: max-pool (72)</cell><cell>3: max-pool (64)</cell><cell cols="2">3: max-pool (64) 3: max-pool (56)</cell><cell>(64) 5: max-pool</cell><cell>(64) 6: max-pool 3: max-pool (64)</cell></row><row><cell>4: conv3, 144 (10368)</cell><cell>4: conv3, 128 (8192)</cell><cell cols="2">4: conv3, 128 (8192) 4: conv3, 112 (6272)</cell><cell>(8192) 7: conv3, 128</cell><cell>(8192) 8: conv3, 128 4: conv3, 128 (8192)</cell></row><row><cell>5: conv3, 162 (23328)</cell><cell>5: conv3, 128 (16384)</cell><cell cols="2">5: res5, 128 (16384) 5: conv3, 112 (12544)</cell><cell cols="2">(32768) 9: conv3, 128 5: conv3, 128 (16384)</cell></row><row><cell>6: max-pool (162)</cell><cell>6: max-pool (128)</cell><cell cols="2">6: conv5, 128 (16384) 6: max-pool (112)</cell><cell cols="2">(128) 10: max-pool 6: max-pool (128)</cell></row><row><cell>7: conv3, 144 (23328)</cell><cell>7: conv3, 128 (16384)</cell><cell cols="2">(16384) 7: conv3, 128 7: conv3, 112 (12544)</cell><cell cols="2">(16384) 11: conv3, 128 7: conv3, 128 (16384)</cell></row><row><cell>8: conv3, 144 (20736)</cell><cell>8: conv5, 128 (16384)</cell><cell cols="2">(128) 8: max-pool 8: conv3, 112 (12544)</cell><cell cols="2">(16384) 12: conv3, 128 8: conv3, 128 (16384)</cell></row><row><cell>9: conv3, 144 (20736)</cell><cell>9: conv3, 128 (16384)</cell><cell cols="2">(16384) 9: conv3, 128 9: max-pool (112)</cell><cell cols="2">(128) 13: max-pool 9: conv3, 128 (16384)</cell></row><row><cell>10: max-pool (144)</cell><cell>10: conv3, 128 (16384)</cell><cell cols="2">(16384) 10: conv3, 128 10: conv3, 256 (28672)</cell><cell cols="2">(32768) 14: conv3, 256 10: max-pool (128)</cell></row><row><cell cols="2">12: max-pool (256) 14: conv3, 512 (131072) 15: conv3, 512 (262144) 17: max-pool (512) 19: fc, 128 (6553) 21: softmax (29395) 24: op 13: avg-pool (256) 16: max-pool (768) 18: fc, 128 (9830) 20: fc, 256 (3276) 22: fc, 512 (13107) (58790) 23: softmax (29395) 11: conv3, 128 (16384) 12: max-pool (128) 13: conv3, 256 (32768) 14: conv3, 256 (65536) 15: conv3, 256 (65536) 16: conv3, 256 (65536) 17: max-pool (256) 18: conv3, 512 (131072) 19: conv3, 512 (262144) 20: conv3, 512 (262144) 21: conv3, 512 (262144) 22: max-pool (512) 24: fc, 256 (3276) 25: fc, 512 (13107) 26: softmax (128156) 27: op (128156) Figure 8: (65536) 11: conv3, 252 (36288) 12: conv3, 256 (64512) 13: conv3, 256 (65536) 14: max-pool (256) 15: conv3, 512 (131072) 16: conv5, 512 (262144) 17: conv3, 576 (294912) 18: conv3, 576 (331776) 19: max-pool (576) 20: fc, 144 (8294) 21: fc, 288 (4147) 22: fc, 504 (14515) 24: op (131815) 23: softmax (131815) 23: fc, 128 (6553)</cell><cell cols="2">11: conv3, 196 (50176) 11: conv3, 128 (16384) 12: max-pool (128) 13: conv3, 256 (32768) 14: conv3, 256 (65536) 15: conv3, 256 (65536) 16: max-pool (256) 17: conv3, 512 (131072) 18: conv3, 512 (262144) 19: conv3, 512 (262144) 20: max-pool (512) 21: fc, 128 (6553) 23: fc, 512 (13107) 24: softmax (95388) 25: op (95388) 12: max-pool (196) 13: conv3, 448 (87808) 14: conv3, 576 (258048) 15: max-pool (576) 16: fc, 144 (8294) 17: fc, 256 (3686) 18: fc, 512 (13107) 19: softmax (49833) 20: op (49833) 22: fc, 256 (3276)</cell><cell cols="2">11: conv3, 224 (28672) 15: conv3, 256 (65536) 16: max-pool (256) 17: conv3, 512 (131072) 18: res5, 512 (262144) 19: conv3, 512 (262144) 20: max-pool (512) 21: fc, 128 (6553) 22: fc, 256 (3276) 23: fc, 512 (13107) 24: softmax (86799) 25: op (86799) 12: conv3, 256 (57344) 13: conv3, 256 (65536) 14: max-pool (256) 15: conv5, 448 (114688) 16: conv3, 448 (200704) 17: conv3, 512 (229376) 18: conv3, 512 (262144) 19: max-pool (512) 20: fc, 128 (6553) 21: fc, 256 (3276) 22: softmax (104727) 23: op (104727)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A topological ordering is an ordering of the layers u1, . . . , u |L| such that u comes before v if (u, v) ∈ E.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknolwedgements</head><p>We would like to thank Guru Guruganesh and Dougal Sutherland for the insightful discussions. This research is partly funded by DOE grant DESC0011114, NSF grant IIS1563887, and the Darpa D3M program. KK is supported by a Facebook fellowship and a Siebel scholarship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<title level="m">Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Daniel</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">M</forename><surname>Cora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feedback prediction for blogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Buza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data analysis, machine learning and knowledge discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Machine learning approaches for improving condition-based maintenance of naval propulsion plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Coraddu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aessandro</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Savio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Figari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Institution of Mechanical Engineers</title>
		<meeting>the Institution of Mechanical Engineers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page" from="136" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavi</forename><surname>Gonzalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">Yang</forename><surname>Adanet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01097</idno>
		<title level="m">Adaptive structural learning of artificial neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A proactive intelligent decision support system for predicting the popularity of online news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelwin</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Vinagre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Cortez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Portuguese Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neuroevolution: from architectures to learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Floreano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dürr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Mattiussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="62" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A survey of graph edit distance. Pattern Analysis and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="113" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dealing with asynchronicity in parallel gaussian process based global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ginsbourger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janis</forename><surname>Janusevskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><forename type="middle">Le</forename><surname>Riche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ERCIM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2d image registration in ct images using radial image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Pölsterl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="607" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LION</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian optimization with tree-structured dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09512</idno>
		<title level="m">Contextual decision processes with low bellman rank are pac-learnable</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gaussian process bandit optimisation with multi-fidelity evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Junier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="992" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Junier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06288</idno>
		<title level="m">Multifidelity gaussian process bandit optimisation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The multi-fidelity multi-armed bandit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1777" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-fidelity Bayesian Optimisation with Continuous Approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirthevasan</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06240</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms with graph generation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Kitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="476" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast bayesian optimization of machine learning hyperparameters on large datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07079</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete input spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Risi Imre Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<title level="m">Progressive neural architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00436</idno>
		<title level="m">Hierarchical representations for efficient architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards automatically-tuned neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Automatic Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new algorithm for error-tolerant subgraph isomorphism detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Messmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="493" to="504" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Risto Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bala</forename><surname>Francon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00548</idno>
		<title level="m">Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian approach to global optimization and application to multiobjective and constrained problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Mockus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Mockus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08792</idno>
		<title level="m">Deeparchitect: Automatically designing and training deep architectures</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Computational Optimal Transport. Available online</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Physicochemical properties of protein tertiary structure data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Ps</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning. Adaptative computation and machine learning series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>University Press Group Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kernels and regularization on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sparse gaussian processes using pseudo-inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Practical Bayesian Optimization of Machine Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Active and Flexible Learning on Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael A</forename><surname>Osborne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4011</idno>
		<title level="m">Raiders of the lost architecture: Kernels for bayesian optimization in conditional parameter spaces</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ujiindoorloc: A new multi-building and multi-floor database for wlan fingerprint-based indoor localization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquín</forename><surname>Torres-Sospedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raúl</forename><surname>Montoliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adolfo</forename><surname>Martínez-Usó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">P</forename><surname>Avariento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauri</forename><surname>Arnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquín</forename><surname>Benedito-Bordonau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huerta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indoor Positioning and Indoor Navigation (IPIN), 2014 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph distances using graph union</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoubridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kraetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="701" to="704" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Kernel interpolation for scalable structured gaussian processes (kiss-gp)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Nickisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1775" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01513</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Genetic cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05552</idno>
		<title level="m">Practical network blocks design with q-learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page">ip</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">262144) #15 softplus</title>
	</analytic>
	<monogr>
		<title level="m">#14 softplus</title>
		<imprint>
			<date type="published" when="32768" />
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<title level="m">#19 tanh</title>
		<imprint>
			<biblScope unit="volume">576</biblScope>
		</imprint>
	</monogr>
	<note>73728) #20 relu, 128, (16384</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<title level="m">#21 tanh</title>
		<imprint>
			<date type="published" when="262144" />
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">253952) #19 logistic, 256, (192512) #13 tanh</title>
	</analytic>
	<monogr>
		<title level="m">#12 logistic</title>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<title level="m">#15 tanh</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>4096) #16 tanh</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Optimal network architectures found with NASBOTon Indoor data. #0 ip</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>232665) #1 relu, 128, (128) #2 relu, 256, (32768) #3 logistic, 512, (131072) #14 crelu, 512, (262144</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<title level="m">#4 logistic</title>
		<imprint>
			<date type="published" when="262144" />
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
	<note>262144) #5 elu</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<title level="m">#7 tanh</title>
		<imprint>
			<date type="published" when="294912" />
			<biblScope unit="volume">576</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<title level="m">#0 ip</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<title level="m">#2 relu</title>
		<imprint>
			<date type="published" when="12209" />
			<biblScope unit="volume">252</biblScope>
		</imprint>
	</monogr>
	<note>36288) #7 linear</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<title level="m">#4 tanh</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>8192) #8 op, 256, (12209) #0 ip</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<title level="m">10240) #7 softplus</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>#6 softplus</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Optimal network architectures found with EA on Indoor data. #0 ip</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>7795) #1 relu, 128, (128) #2 relu, 256, (32768) #7 linear</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">#10 logistic, 128, (8192) #11 logistic, 128, (16384) #12 logistic, 128, (16384) #13 leaky-relu</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>#7 logistic, 64, (4096) #8 logistic, 64, (4096) #9 logistic. 16384) #14 leaky-relu, 128, (16384) #15 leaky-relu, 128, (16384) #16 leaky-relu, 256, (32768) #17 leaky-relu, 256, (65536) #18 leaky-relu, 256, (65536) #20 tanh, 256, (65536) #21 tanh, 256, (65536) #22 tanh, 512, (131072) #23 tanh, 512, (262144) #24 tanh, 512, (262144) #25 linear, 512, (133952) #26 op, 512, (133952</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<title level="m">#0 ip</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<title level="m">#12 tanh</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>8192) #14 softplus, 256, (32768</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Optimal network architectures found with RAND on Indoor data. #0 ip</title>
		<imprint>
			<date type="published" when="19993" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>#1 relu, 256, (256) #2 logistic, 512, (131072) #3 elu, 56, (28672) #4 elu, 128, (7168</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<title level="m">#1 softplus</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>128) #2 linear, 256, (100) #3 op, 256, (100) #0 ip, 64, (632</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Optimal network architectures found with TreeBO on Indoor data. #0 ip</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>72512) #1 crelu, 128, (128) #2 crelu, 256, (32768) #11 linear, 512, (72512</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">229376) #10 logistic</title>
	</analytic>
	<monogr>
		<title level="m">#9 logistic</title>
		<imprint>
			<date type="published" when="229376" />
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">524288) #12 softplus</title>
	</analytic>
	<monogr>
		<title level="m">#11 softplus</title>
		<imprint>
			<date type="published" when="32768" />
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<title level="m">#13 tanh</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>4096) #14 tanh, 128, (8192) #15 crelu, 128, (16384) #16 logistic, 256, (32768</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Optimal network architectures found with NASBOTon Slice data. #0 ip</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>7795) #1 elu, 128, (128) #2 logistic, 256, (32768) #7 linear</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<title level="m">) #2 softplus</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>#1 softplus. 128) #3 softplus, 256, (32768) #4 softplus, 256, (32768) #9 crelu, 128, (32768</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<title level="m">#0 ip</title>
		<imprint>
			<date type="published" when="10664" />
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
				<title level="m">#9 op</title>
		<imprint>
			<date type="published" when="10664" />
			<biblScope unit="volume">256</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Optimal network architectures found with EA on Slice data</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="25088" />
		</imprint>
	</monogr>
	<note>#0 ip, 64, (11096) #1 leaky-relu, 112, (112) #2 leaky-relu</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<title level="m">#6 tanh</title>
		<imprint>
			<date type="published" when="11096" />
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>8192) #8 linear</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">#16 logistic, 512, (131072) #17 logistic</title>
	</analytic>
	<monogr>
		<title level="m">#23 tanh</title>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
	<note>393216) #18 softplus, 512, (262144) #19 softplus, 512, (262144) #20 softplus, 512, (262144) #22 tanh, 512, (262144</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<title level="m">#0 ip</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
	<note>40153) #1 relu</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<title level="m">#10 op</title>
		<imprint>
			<biblScope unit="volume">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Optimal network architectures found with RAND on Slice data. #0 ip</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>8179) #1 elu, 184, (184) #2 tanh, 202, (37168) #3 crelu, 220, (44440) #4 linear, 256, (8179) #5 op, 256, (8179) #0 ip, 64, (15704) #1 elu, 128, (128</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Optimal network architectures found with TreeBO on Slice data. 0: i p</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">57704</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
