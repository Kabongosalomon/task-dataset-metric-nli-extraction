<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IEEE Transactions on Circuits and Systems for Video Technology</orgName>
								<address>
									<addrLine>1 1`</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Digital Technology and Instrument</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">with the State Key Laboratory of Industrial Control Technology</orgName>
								<orgName type="department" key="dep2">Embedded System Engineering Research Center</orgName>
								<orgName type="department" key="dep3">Ministry of Education of China</orgName>
								<orgName type="institution" key="instit1">Zhejiang University</orgName>
								<orgName type="institution" key="instit2">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Laboratory for Network Multimedia Technologies</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Laboratory for Network Multimedia Technologies</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Laboratory for Network Multimedia Technologies</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-JPEG Deblocking</term>
					<term>Dual-domain Correction</term>
					<term>Dense Connection</term>
					<term>Pixel Labeling</term>
					<term>Dilated Convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Several dual-domain convolutional neural network-based methods show outstanding performance in reducing image compression artifacts. However, they are unable to handle color images as the compression processes for gray scale and color images are different. Moreover, these methods train a specific model for each compression quality, and they require multiple models to achieve different compression qualities. To address these problems, we proposed an implicit dual-domain convolutional network (IDCN) with a pixel position labeling map and quantization tables as inputs. We proposed an extractor-corrector framework-based <ref type="figure">IDCN (IDCN-f)</ref> was also developed to handle a wide range of compression qualities. Experiments for both objective and subjective evaluations on benchmark datasets show that IDCN is superior to state-of-the-art methods and IDCN-f exhibits excellent abilities to handle a wide range of compression qualities with little trade-off against performance; further, it demonstrates great potential for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>dual-domain correction unit (DCU) as the basic component to formulate the IDCN; the implicit dual-domain translation allows the IDCN to handle color images with discrete cosine transform (DCT)-domain priors. A flexible version of</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>OSSY image compression algorithms use the information redundancy of image patches to achieve a high compression ratio with desirable image qualities. The human eye is not sensitive to high-frequency information, and therefore, most lossy image compression methods are implemented by quantization or approximation on the frequency domain. However, as the compression ratio increases, the artifacts introduced by the severe degradation of high-frequency information significantly reduce the quality of visual experience. JPEG is a representative lossy compression standard and is used globally. JPEG compression standard divides the image into 8 × 8 patches and performs discrete cosine transform (DCT) on each patch; then, the DCT coefficients are quantized and encoded into bit streams.</p><p>Studies on artifact reduction (AR) in image compression have recently been conducted and validated based on JPEG compression standards. Traditional filter-based algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> focus on general image denoising and play a role in the AR problem. Machine learning-based methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> are more engaged in specific AR problems and present a promising performance in both subjective and objective evaluations. These methods learn nonlinear mapping from the compressed image to the original image. Chen et al. <ref type="bibr" target="#b3">[4]</ref> trained nonlinear reaction diffusion models for image denoising and JPEG deblocking. Inspired by SRCNN <ref type="bibr" target="#b6">[7]</ref>, Dong et al. <ref type="bibr" target="#b4">[5]</ref> first introduced a deep neural network (DNN) and built a four-layer full convolutional neural network (CNN) to solve the AR problem; it was called the ARCNN. Previous studies on cascaded CNNs (CAS-CNNs) <ref type="bibr" target="#b7">[8]</ref> and dual-domain multi-scale CNNs (DMCNNs) <ref type="bibr" target="#b9">[9]</ref> imported MCNNs and learned large-scale features to remove large-scale artifacts. Because JPEG compression artifacts are mainly caused by the lossy quantization of DCT coefficients, Liu et al. <ref type="bibr" target="#b10">[10]</ref> proposed a dual-domain image dictionary to recover the compressed images from both the pixel domain and the DCT domain. As DCT is a linear transformation that can be easily rewritten in a convolutional pattern, Guo et al. <ref type="bibr" target="#b12">[11]</ref> proposed a dual-domain convolutional network (DDCN) and improved its performance on gray-scale images.</p><p>Despite their high performance on gray-scale images, DCT domain-based methods typically suffer from handling color images for two main reasons. First, because the entire network is a highly nonlinear system, simple linear operations on input images would lead to unpredictable nonlinear outputs. In other Implicit Dual-domain Convolutional Network for Robust Color Image Compression Artifact Reduction Bolun Zheng, Yaowu Chen, and Xiang Tian, Fan Zhou, Xuesong Liu L words, using methods developed for gray-scale images to recover color images by restoring each color channel separately would lead to chromatic aberrations ( <ref type="figure" target="#fig_0">Fig. 1)</ref>. Second, because the compression algorithm for the luminance channel differs from that for the two chrominance channels, using a single model trained for the luminance channel to recover two chrominance channels would produce undesired results. However, for restoring color images compressed by JEPG, although pixel-domain learning methods can easily learn the correlations between each color channel, it is difficult to directly introduce the DCT-domain priors.</p><p>The mapping relationship between the compression quality factor and the quantization coefficient can be expressed as (50,100] <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">50]</ref> 200 50  <ref type="bibr" target="#b1">(2)</ref> where q is the compression quality factor, Q is the base quantization coefficient table, q Q is the quantization coefficient table with quality factor q, and  is the rounding operation. When q reaches a relatively low range, a slight change in q leads to a great change in q Q . Thus, the performance of recovering DCT coefficients based on the prior of quantization coefficients severely declines when q of the recovered image mismatches the q of the pre-trained model.</p><formula xml:id="formula_0">N () 5000 N qq q q q          ,<label>(1)</label></formula><p>To address these problems, we propose a method to reduce high-quality color image compression artifacts by developing an implicit dual-domain convolutional network (IDCN) to implicitly utilize both pixel-domain features and DCT-domain priors. The main contributions of our work are summarized as follows:</p><p>1) We propose a unified architecture called IDCN to learn dual-domain corrections for the reduction in color image compression artifacts. To overcome the limitations of DCT, IDCN directly estimates the DCT-domain losses without DCT to build a color-to-color network. 2) We propose an extractor-corrector framework to construct generalized residual connection and develop the dual-domain correction unit (DCU) based on this framework. 3) We introduce dilated convolution layers in the extractor of DCU to enlarge the receptive field, which shows great performance to remove wide-range distortions. 4) We propose a position labeling map to label the position of each pixel in the entire image and concatenate the position labeling map to the input image so that the network can utilize the position information to refine the compressed image. 5) We propose a flexible IDCN (IDCN-f) for robust color image compression AR. IDCN-f exhibits great performance in handling a wide range of compression qualities with one model weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Several CNN-based methods present promising potential to solve the image compression AR problem. In this section, we briefly review and discuss the methods relevant to this work.</p><p>Because the human eye is more sensitive to luminance than chrominance, early CNN-based methods focus on recovering the luminance channel of a JPEG compressed color image or only a gray-scale image. Dong et al. <ref type="bibr" target="#b4">[5]</ref> first proposed a deep learning based method (ARCNN) for image compression AR. ARCNN is composed of four convolution layers: functions of these layers can be defined as feature extraction, feature enhancement, mapping, and reconstruction. However, as the network goes deeper, the function of each convolution layer is increasingly fused. Guo et al. <ref type="bibr" target="#b12">[11]</ref> introduced a dual-domain learning architecture and built a 30-convolutional-layer network (DDCN) to utilize both the pixel-domain features and DCT-domain priors. DDCN consists of three branches: DCT-domain, pixel-domain, and aggregation, each containing 10 convolution layers. The DCT-domain branch replaces DCT and inverse DCT (iDCT) by equivalent convolution operations; quantization rectification is accomplished before the iDCT operation. Although dense DCT and iDCT convolution can utilize more redundant information from the surrounding image patches to correct the DCT coefficients of the 8 × 8 image patches, it may also mislead the corrections for those middle image patches between the adjacent 8 × 8 image patches. Inspired by the success of CNN for single-image super resolution (SR), Cavigelli et al. <ref type="bibr" target="#b7">[8]</ref> translated the AR problem to the SR problem and introduced stepped convolution and deconvolution <ref type="bibr" target="#b13">[12]</ref> layers (also known as up-sampling layers) to build a cascaded convolutional network (CAS-CNN). CAS-CNN introduced a multi-scale loss function to restore the compressed image in a different scale. Multi-scale loss can enlarge the gradients for the bottom layers to update the learnable weights and learn large-scale features to recover the details. However, because the proportion of larger-scale feature maps is too low, the benefits from the multi-scale learning are limited. Noticing the merits of DDCN and CAS-CNN, Zhang et al. <ref type="bibr" target="#b9">[9]</ref> proposed a DMCNN. They adopted the dual-domain framework and multi-scale loss and introduced dilated convolution layers to enlarge the receptive fields for the removal of banding effects. Not only is the DCT domain considered to formulate the dual domain learning network, wavelet domain is also introduced to build a dual domain architecture. Chen et al. <ref type="bibr" target="#b14">[13]</ref> proposed a dual pixel-wavelet domain deep CNN (DPW-SDNet) for soft decoding of JPEG-compressed images. DPW-SDNet made down-sampling and discrete wavelet transformation (DWT) on compressed image to get the input of pixel domain branch, and wavelet domain branch, and then, they synthesized the output of two branches to obtain a soft decoded image. Liu et al. <ref type="bibr" target="#b15">[14]</ref> combined the multi-scale method and DWT, proposing a multi-level wavelet CNN (MWCNN) for general image restoration. MWCNN used DWT to generate multi-scale feature maps, and it used inversed DWT (IWT) to up-sample the feature maps back to the origin scale. Recently, Zheng et al. <ref type="bibr" target="#b16">[15]</ref> identified the importance of the correlation of each color channel and constructed a scalable convolutional network (S-Net) to learn a nonlinear mapping for color JPEG compressed image restoration. Some CNN based image restoration methods also include the JPEG compression AR. Zhang et al. <ref type="bibr" target="#b17">[16]</ref> proposed a DnCNN to handle image denoising problem, which is also effective to handle SR and compression AR problems. Zhang et al. <ref type="bibr" target="#b18">[17]</ref> combined residual connection and dense connection, and proposed a residual dense network (RDN) for image restoration. Mao et al. <ref type="bibr" target="#b19">[18]</ref> introduced a series of symmetric skip connected encoder-decoder pairs, and they proposed RED-Net to restore noisy images. Tai et al. <ref type="bibr" target="#b20">[19]</ref> introduced short path, long path transmission, and gate unit, and they proposed a persistent memory network (MemNet) for image restoration.</p><p>Huang et al. <ref type="bibr" target="#b21">[20]</ref> proposed DenseNet to ensure maximum information flow between any layers within the same block and achieved state-of-the-art performance in high-level computer vision tasks. Following DenseNet, several low-level computer vision algorithms such as SRDenseNet <ref type="bibr" target="#b22">[21]</ref>, MemNet <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr">DCPDN [22]</ref>, and RDN <ref type="bibr" target="#b18">[17]</ref> introduced dense connections and achieved impressive performances in respective fields.</p><p>All these methods for AR showed significant improvement over conventional filter-based methods. However, none of them could effectively process the color images with DCT-domain priors. To address this problem, we propose an IDCN to represent the DCT-domain priors in the pixel-domain and fuse the corrections from both the DCT domain and the pixel domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. IMPLICIT DUAL-DOMAIN CONVOLUTION NETWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implicit Translation from DCT-domain to Pixel-domain</head><p>A major obstacle to applying DCT-domain-based methods to color image processing is that the compressing operation for luminance channels is completely different from that for chrominance channels. First, chrominance channels should be downsampled by a factor of 2 before DCT, whereas luminance channels do not need downsampling. Second, the quantization table for luminance channels is different from that for chrominance channels. Conventional DCT-domain-based methods directly recover the DCT coefficients, which is difficult to implement in color images. JPEG compression works on YCbCr mode images. The relationship between the YCbCr color space and the RGB color space can be expressed as </p><formula xml:id="formula_1">R Y Cr G Y Cb Cr B Y Cb                    , (3)</formula><p>where Y, Cb, and Cr denote the luminance channel and two chrominance channels, respectively, and R, G, and B denote the red, green, and blue channels, respectively. Therefore, we can easily calculate the RGB loss ,, </p><formula xml:id="formula_2">R G B  caused by JPEG compression ,,<label>( , , ) ( , , ) ( , , )</label></formula><formula xml:id="formula_3">R G B Y Cb Cr R G B R G B Y Y Y Cb Cb Cb R G B Cr Cr Cr                       , (4) where Y  , Cb  ,</formula><p>where   denotes the quantization loss.   can be expressed as</p><formula xml:id="formula_5">* q Q    ,<label>(6)</label></formula><p>where * denotes the element-wise multiplication and  is the relative quantization loss, which is an 8 × 8 matrix and satisfies 0.5 0.5</p><formula xml:id="formula_6">i i         .<label>(7)</label></formula><p>For an 8×8 image patch, the channel loss s  , { , , } s Y Cb Cr  in the YCbCr color space caused by the quantization of DCT coefficients can be written as , *</p><p>[0,7] , , ,</p><formula xml:id="formula_7">( ) ( ) , ,<label>, 77 ,, 00 ( )</label></formula><formula xml:id="formula_8">( ) ( ) ( , , , ) * ( , , ) ij s i j i j i j q ij q u v u v uv q ij Q u v Q f i j u v Q i j                       ,<label>( * )</label></formula><p>[0,7] ,, ( , , ) ( ) ( ) ( , , , ) ,</p><formula xml:id="formula_10">1 ( ) 0 8 2 ( ) 0 8 qq u v u v Q i j u v Q f i j u v u v uu uu                    ,<label>(9)</label></formula><p>(2 1) (2 1) ( , , , ) cos cos <ref type="bibr" target="#b17">16</ref> 16 <ref type="bibr" target="#b10">(10)</ref> where i and j denote the horizontal and vertical coordinates in the 8× 8 image patch and  denotes the iDCT operation.</p><formula xml:id="formula_11">i u j v f i j u v    ,</formula><p>Where s  ,  , and q Q are reshaped into a 1 × 1 × 64 vector, (8) can be rewritten as</p><formula xml:id="formula_12">* 11 * ( ) q s Q      ,<label>(11)</label></formula><formula xml:id="formula_13">* 1 1 8 , ( ) ( ) qq j i i j QQ    ,<label>(12)</label></formula><p>where *  is a 1 × 1 × 64 × 64 2D convolution kernel. Therefore, we can use a 2D convolution to translate the DCT-domain loss to the pixel-domain loss in the YCbCr color space so that the element-wise operation in (6) can be avoided; this will reduce the calculations by approximately 40%. If we ignore the loss from the downsampling operation on chrominance channels,</p><formula xml:id="formula_14">Y  , Cb  , and Cr  in (4) can be rewritten as * * ( ), {Y,Cb,Cr} q Q         .<label>(13)</label></formula><p>The quantization table of the Cb channel is the same as that of <ref type="bibr">Cb</ref>  , and Cr  from the compressed image is a highly nonlinear task, which can always be handled in the pixel-feature domain and the estimated losses calculated from Y  , Cb  , and Cr  are in the YCbCr color space. Although we can translate the losses into the RGB color space by simple linear operations, we would rather translate the estimated loss back to the pixel-feature domain for two main reasons. First, the estimation task is handled in the feature domain; this translation can maintain the integrity of the whole nonlinear operation so that we can easily introduce skip connections to construct a deeper network. Second, for a specific pixel location in the image, the estimated   is a 1 × 1 × 64 vector and it represents an 8 × 8 image patch, i.e., the   of the locations in the same 8 × 8 stepped patch would be close to each other and the blocking effect probably exists in   . This translation can help avoid the block effect of the deviation of   . Because the entire translation is operated from the pixel-feature domain to the pixel-feature domain and utilizes the prior from the DCT-domain, we call this translation an implicit dual-domain translation.</p><formula xml:id="formula_15">Cr; thus, Cr Cb C q q q Q Q Q  . Estimating Y  ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Position Labeling</head><p>Because most elements in the quantization table are different from each other and the loss of DCT coefficients at different positions lead to different losses in the RGB color space, the RGB color space loss caused by the quantization of different pixels in an 8 × 8 patch would probably be different from each other. We conducted an experiment on the DIV2K dataset to validate our assumption. As the chrominance channels are downsampled before DCT and quantization operations, we separated the luminance channel from the image and sliced the luminance channel into 8 × 8 patches in accordance with the JPEG image. Then, we calculated the standard deviation of the difference between the original image and the JPEG compressed image at different positions. The result is shown in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>. The standard deviations of the elements around the corners are larger than in the middle. To further support this conclusion, we increased the patch size to 16 and kept the moving step unchanged. The standard deviation map ( <ref type="figure" target="#fig_2">Fig. 2(b)</ref>) varies in cycles of 8 × 8, which is the same as the field of the DCT patch. We also conducted this experiment on other public image datasets ( <ref type="figure" target="#fig_2">Fig. 2(c)</ref>). Although the ranges of standard deviation are different, the regularity of distributions is close. If we expand this result to the entire RGB color space, the minimum period of the standard deviation should be 16 × 16.</p><p>Based on this conclusion, it is necessary to consider the position of the pixels. The deviation is introduced by quantization in the DCT-domain. Based on the randomness of the quantization operation, we can use a zero-mean Gaussian distribution to simply describe this deviation. Thus, the deviation distribution at a specific position could be defined as <ref type="bibr">[0,</ref><ref type="bibr" target="#b6">7]</ref> ,, (0, ) ,</p><formula xml:id="formula_16">i j i j ij     ,<label>(14)</label></formula><p>where ℕ denotes the Gaussian distribution and , ij  denotes the standard deviation at position <ref type="bibr">( , )</ref> ij. Inspired by FFDNet <ref type="bibr" target="#b23">[23]</ref>, we used the standard deviation to label the pixel position and we called this standard deviation map a "labeling map." The labeling map has the same size as the input image, and it is defined as</p><formula xml:id="formula_17">mod( , ),mod( , ) ( , ) , { , , } xy L x y R G B     ,<label>(15)</label></formula><p>where L denotes the labeling map and  denotes the minimum period of standard deviation in each axis. For color images, the channel size of the labeling map is 3 and 16   <ref type="figure">(Fig. 3</ref>). However, in practice, the above multi-channel labeling map did not work very well for training a single quality network. Too much unchanged labeling information may mislead the network to learn too much features from the labeling map rather than the image itself. To overcome the problem, we simplified the labeling map by</p><formula xml:id="formula_18">2 2 2 ( , ) ( , ) ( , ) ( , ) R G B L x y L x y L x y L x y    ,<label>(16)</label></formula><p>where L denotes the simplified labeling map. We used this simplified labeling map for training single-quality networks, C. Architecture <ref type="figure" target="#fig_4">Fig. 4</ref> illustrates the architecture of IDCN, which consists of three parts: feature encoder (FE), correction baseline (CB), and feature decoder (FD). We denote in I and out I as the input and output of IDCN. Specifically, based on the previous studies, the batch-normalization <ref type="bibr" target="#b24">[24]</ref> layer was not considered. FE takes two convolution layers to encode the input from the pixel domain to the pixel-feature domain and each convolution layer is linked to a ReLU <ref type="bibr" target="#b25">[25]</ref> layer. The first 5 × 5 convolution layer and the ReLU layer extract features 0 f from in I .</p><formula xml:id="formula_19">01 Max( ( ), 0) FE in fI  ,<label>(17)</label></formula><p>where <ref type="bibr">1 FE</ref> denotes the convolution operation of the first convolution layer of FE. The second 3 × 3 convolution layer and the ReLU layer then further enhance <ref type="bibr">0</ref> f to obtain 1 f for feature-domain corrections. <ref type="bibr" target="#b0">1</ref> 2 0</p><formula xml:id="formula_20">Max( ( ), 0) FE ff  ,<label>(18)</label></formula><p>where 2 FE denotes the convolution operation of the second convolution layer of FE. <ref type="bibr" target="#b0">1</ref> f is then used as an input to CB, which consists of a series of DCUs. Assuming that CB contains N DCUs, the output n F of the n-th DCU can be expressed as</p><formula xml:id="formula_21">, 1 ,<label>, 1 ,1 1 (</label></formula><formula xml:id="formula_22">) ( (...( ( ))...)) n CB n n CB n CB n CB F F f   ,<label>(19)</label></formula><p>where , CB n denotes the dual-domain correction operation of the n-th DCU. The details on DCU are provided in Section III.D. The corrected feature ( N F ) output by CB is then sent to FD.</p><p>The FD takes two convolution layers to translate N F to the final output out I . The first 3 × 3 convolution layer and the ReLU layer preprocess the corrected features ( N F ) to decodable</p><formula xml:id="formula_23">features ( d f ). 1 Max( ( ), 0) d FD N fF  ,<label>(20)</label></formula><p>where 1 FD denotes the convolution operation of the first convolution layer in FD. The second 5 × 5 convolution layer then translates d f to out I .</p><formula xml:id="formula_24">2 () out FD d If  ,<label>(21)</label></formula><p>where 2 FD denotes the convolution operation of the second convolution layer in FD. In our IDCN, have the same number of filters and 2 FD have three filters, as we output the color images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dual-domain Correction Unit</head><p>The DCU is designed based on residual learning and on the results presented in Section III.A. We compared several versions of residual connections <ref type="figure">(Fig. 5</ref>). <ref type="figure">Fig. 5 (a)</ref> shows the original residual connection in ResNet <ref type="bibr" target="#b26">[26]</ref>. SRResNet <ref type="bibr" target="#b27">[27]</ref> ( <ref type="figure">Fig. 5(b)</ref>) modified the original residual connection by removing the outside ReLU layer for single-image super resolution. The enhanced deep super-resolution network (EDSR) <ref type="bibr" target="#b28">[28]</ref>  <ref type="figure">(Fig. 5(c)</ref>) further improved the residual connection in SRResNet by removing the batch-normalization layers and concatenating the scaling layer to the residual branch to avoid gradient explosion. The residual branch can be regarded as an extractor-corrector framework. For example, in <ref type="figure">Fig. 5(c)</ref>, the first convolution layer and the ReLU layer can be regarded as a simple feature extractor and the second convolution layer and the scaling layer as a simple corrector.</p><p>In our DCU, we reconstructed the feature extractor by importing the dense block and the transition block. We also reconstructed the corrector by introducing dual correction branches, DCT-domain and pixel-domain, so that our DCU can make the corrections through both pixel and DCT domains. The structure of DCU is shown in <ref type="figure">Fig. 6</ref>. The two correction branches share the same feature extractor, shared feature extractor (SFE). Let </p><p>SFE consists of a dense block and a transition block. The dense block is a group of densely connected 3 × 3 convolution layers to extract high-dimensional features. The transition block is a 1 × 1 convolution layer with the ReLU activation layer to compress the high-dimensional features to low-dimensional features. We assumed that SFE contains L convolution layers and the growth is K. The number of filters of the first convolution layer in the dense block was set to 2K. Then, the number of filters of the remaining L-2 convolution layers was set to K. The outputs of the first L-1 convolution layers were concatenated together to be sent to the transition block. Moreover, we adopted the receptive field theory in DMCNN and introduced dilated convolution <ref type="bibr" target="#b29">[29]</ref> to enlarge the receptive field of feature extractors. For the first L-1 densely connected layers, the first half of the layers are normal convolution layers, (dilation rate is 1) and then, the dilation rate is increased by 1 for each stacked convolution layer.</p><p>In the DCT-domain branch, relative quantization loss estimating unit (REU) takes F  as input and uses a 3 × 3 convolution layer and quantization rectified unit (QRU) to estimate the relative quantization loss  in a specific channel of Y, Cr, and Cb. Here, QRU is a simple constraint layer to let satisfy <ref type="bibr" target="#b6">(7)</ref>. One REU estimates the loss from a channel of Y, Cb, and Cr. However, taking three REUs to estimate Y  , Cb  , and Cr  would introduce too many learnable parameters, which would increase the computation consumption and make the network difficult to converge. Because the Y channel is a major channel for the human visual sense and the Cb and Cr channels share the same quantization table, we use one REU, REU Y , to estimate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Network Settings. In the proposed IDCN, CB has N = 8 DCUs and each DCU has the same structure, K = 64 and L = 8. Except the second convolution layer in FD, all other convolution layers in FE and FD and all trainable convolution layers in two correction branches have B = 64 filters. In particular, the number of filters of the convolution layers in REU is fixed to 64 because the relative quantization losses of 64 DCT coefficients are output. Datasets and Metrics. The DIV2K <ref type="bibr" target="#b30">[30]</ref> dataset has been recently released for single-image SR. DIV2K consists of 1000 high-quality (2K resolution) images (800 training images, 100 validation images, and 100 test images). Because the test images were prepared for single-image SR competition and the ground truth has not been released, these images were not included in our experiment. We trained our models on 900 images including 800 training images and 100 validation images and conducted validation and testing on other three standard datasets: BSDS500 <ref type="bibr" target="#b31">[31]</ref>, LIVE1 <ref type="bibr" target="#b32">[32]</ref>, and WIN143 <ref type="bibr" target="#b16">[15]</ref>. We used half of 100 validation images of BSDS500 for validation during training. For testing, quantitative evaluations were conducted on 200 test images of BSDS500 (B200), 29 images of LIVE1, and 143 images of WIN143.</p><p>For quantitative evaluations, we used standard MATLAB library functions. The objective performance indicators peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) and PSNR-B were measured. Unlike most previous methods only considering the performance on the luminance channel, we measured PSNR on the full color image, and SSIM and PSNR-B are obtained by calculating the mean values of the results of the R, G, and B channels.</p><p>Training Settings. All model weights of the proposed IDCN were trained using the same training settings. We regarded the mean square error (MSE) loss as the loss function</p><formula xml:id="formula_26">2 1 1( , ) ( ) N i i MSE Y Y Y Y N    ,<label>(23)</label></formula><p>where Y is the ground truth, Ŷ is the output of network, and N is the batch size. We used Adam <ref type="bibr" target="#b33">[33]</ref> as our training optimizer with reference parameters. The learning rate was set to <ref type="bibr" target="#b3">4</ref> 10  for all layers. If the decrease in the validation loss is lower than 0.001 dB for five consecutive epochs, the learning rate is divided by 5. If the learning rate is lower than <ref type="bibr" target="#b5">6</ref> 10  , it is set to <ref type="bibr" target="#b5">6</ref> 10  and kept unchanged. If the learning rate reaches <ref type="bibr" target="#b5">6</ref> 10  and the decrease in the validation loss is lower than 0.001 dB for five consecutive epochs, the training is stopped. Referring to the settings in DMCNN, we randomly cropped 43 × 43 RGB patches with the batch size of 16 as the input and then enlarged the patch size to 96 × 96. The batch size is cut to 8 when the learning rate reaches <ref type="bibr" target="#b5">6</ref> 10  . For patches input with a size of 43 × 43, 5000 batches constitute an epoch and for those with a size of 96 × 96, 3000 batches constitute an epoch. We implemented the IDCN using the Keras framework with Tensorflow backend. Training IDCN takes approximately seven days on a K80 GPU server for 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Investigation</head><p>In this section, we investigate the effects of three optional items: position labeling (PL), multi-channel correction (MCC), and dilated convolution (DC). PL has three options: no position labeling (-N), simplified labeling synthesized by (16) (-SL), and multi-channel labeling synthesized by (15) (-ML). MCC has two options: full correction (-FC), and simple correction (-SC). DC has two options: constructing extractors with dilated convolutions (-DL), and without dilated convolution (-ND). Here, full correction refers to making corrections from Y, Cb, and Cr, and simple correction refers to making corrections from the luminance channel and the combined chrominance channel. <ref type="table" target="#tab_3">Table I</ref> lists the results of the ablation investigation of the effects of options on PL and MCC. Four networks have the same network settings; N = 3, K = 20, L = 6, and B = 64. This investigation was conducted on LIVE1 with q = 20 and the performance indicator is PSNR.</p><p>Using the -FC option may lead to worse results. Too many    subbranches in the DCT-domain correction branch cannot further improve the performance but may scatter the loss backpropagation. Using the -SC option instead can effectively simplify the network and make a significant improvement. Selecting the -SL option can also improve the performance by introducing simplified pixel position information to avoid misleading the training. Even though using the -DL option leads to little performance enhancement, enlarging the receptive of extractor could effectively help remove the wide range artifacts as shown in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Investigation of Dual-domain Learning</head><p>Analysis on network architecture. To investigate the effects of dual-domain learning, we developed three networks which are learned only in the pixel domain (IDCN-P), the pixel domain and only luminance channel in the DCT domain (IDCN-Y), and both the pixel domain and the DCT domain (IDCN-D). These three networks have the same settings as described in Section IV.B, and all of them have the same options, which are -SL, -SC and -D. <ref type="table" target="#tab_2">Table II</ref> provides the experimental results obtained on the LIVE1 dataset. It is evident that IDCN-Y is a little better than IDCN-P, and IDCN-D achieves the best performance on all PSNR and SSIM PSNR-B indicators.</p><p>Analysis on DCU. We investigated the effects of different subbranches in DCU. <ref type="figure" target="#fig_8">Fig. 9</ref> shows the correction maps from IDCN-D, which are calculated in the pixel domain, the DCT domain, the luminance channel in the DCT domain, and the combined chrominance channel in the DCT domain. The correction maps calculated in the DCT domain could output more smooth results, while several high-frequency artifacts remain in the pixel domain output. The correction maps calculated by the luminance channel and the combined chrominance channel are relatively closed such that the difference only exists around the edge positions. This phenomenon is caused by two reasons. First, the luminance channel and the combined chrominance channel have very closed structures, and therefore, they receive closed gradients in the training procedure, which finally lead to closed outputs. Second, to match the downsampling operation in the compressing procedure, the dilated convolution is introduced for estimating the loss in the combined chrominance channel, which will probably lead to a low-frequency output.</p><p>Analysis on DCT-domain learning. We measured the mean norms of DCT coefficients of different frequencies by </p><p>where fx and fy denote the horizontal and vertical coordinates in DCT coefficient map and , fx fy k s  denotes the DCT coefficient loss estimated by the k-th DCU. We visualized the mean norms of the estimated DCT coefficient losses of the luminance channel and the combined chrominance channel, and we compared them with the true DCT coefficient loss of the luminance channel and the two chrominance channels. The visualized results are shown in <ref type="figure">Fig. 8</ref>. We have three observations: (1) The estimated DCT coefficient losses are focused on a high-frequency area, which well matches the observation in the DCU analysis that the DCT domain could output more smooth results. (2) Because all the operations on   In general, the DCT-domain learning could statistically estimate the high-frequency components of DCT coefficient losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-Art Methods</head><p>We compare IDCN with the state-of-the-art CNN based methods on color image compression AR. Considering that most existing methods focus on gray scale image restoration, we reproduced several representative methods including ARCNN, MemNet, RDN, MWCNN, and DWP-SDNet for color image restoration to avoid the situation presented in <ref type="figure" target="#fig_0">Fig. 1</ref>. For fairness, all these methods are reproduced with the same training settings provided in original publications and trained on DIV2K (no augmentation). Owing to hardware limitations, we reproduced RDN by reducing the number of residual dense blocks (RDB) to 8, while it is 16 in the original paper. However, because the source codes of DDCN and DMCNN are not released and their DCT domain branches cannot be extended to handle color images, these two methods were not included in the comparison. We list the architecture details of the compared methods in <ref type="table" target="#tab_2">Table III</ref>. The comparison is conducted on an open release benchmark, including LIVE1, 200 testing images of BSDS500 (B200), and the WIN143 datasets. The quantitative results are presented in <ref type="table" target="#tab_3">Table IV</ref>. It is clear that IDCN exhibits a significant improvement on all datasets, compression qualities and metrics compared to those of the other state-of-the art methods. As q becomes lower, the gap is further increased. We also present the qualitative results in <ref type="figure" target="#fig_0">Fig. 10</ref>. The proposed method produces more defined edges, and more accurate colors whereas the block effects, blurrings, and color aberrations more or less exist in the results of other methods. Moreover, IDCN is more robust against wide-range color distortion, whereas the color band phenomenon cannot be totally removed by other methods. In general, IDCN generates more visually pleasing results.</p><p>To further demonstrate the effectiveness and superiority of proposed method, we also compare proposed method with the relevant methods on luminance channel image compression AR. However, even though some of them have released their codes and pre-trained model weights for handling luminance channel images, these codes and weights are obtained from different computing frameworks (e.g. Caffe, Matlab, Pytorch and Tensorflow, etc.) and datasets. For fairness, we reproduced these methods by following the settings in previous color image compression AR comparison. To make the proposed method could well match the gray scale image input, we made two modifications that recalculating the labeling map of luminance channel on DIV2K dataset, and removing the combined chrominance subbranch. The qualitative results are presented in <ref type="table" target="#tab_6">Table V</ref>. As it shown, the result of luminance component comparison is roughly the same as RGB channel comparison, that our IDCN surpasses all compared methods and achieves the state-of-the-art performance. Moreover, we measured the running times for the different methods. <ref type="figure" target="#fig_0">Fig. 11</ref> illustrates the performances and running times of different methods handling the LIVE1 dataset on a 2080Ti GPU. Owing to the downsampling operation, MWCNN and DWP-SDNet received excellent computational efficiencies. ARCNN, RDN, MemNet, S-Net, and our IDCN work on the original image resolution. Our IDCN has a running speed that is very similar to the simplified RDN, and it is a bit slower than MemNet and S-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Robust Compression Artifact Reduction</head><p>For image compression, especially in JPEG standards, the compression quality q is always known. We trained a flexible IDCN (IDCN-f) to handle the compressed images with a different q. Because the labeling map is generated by statistical priority for different q, it is not convenient to calculate all statistical priorities of different q values. We assume that IDCN-f is trained for compression qualities in <ref type="bibr">[ , ]</ref> N ql qh . We first calculate the labeling maps ql L and qh L of ql and qh, respectively, and the labeling map of q in <ref type="bibr">[ , ]</ref> N ql qh is generated by</p><formula xml:id="formula_28">( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) q qh ql q ql qh q L L L qh ql qh ql            .<label>(24)</label></formula><p>To cover the low compression quality, we trained IDCN-f in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">20]</ref> N . The quantitative results of IDCN-f on LIVE1 are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>.</p><p>IDCN-f achieved high-performance levels for all q values. Although there is a slight performance degradation compared to the specifically trained IDCN models, IDCN-f produces visual pleased outputs in a wide quality range. <ref type="figure">Fig. 8</ref> shows the performance curves of IDCN-f and other specifically trained IDCN models IDCN-5, IDCN-10 and IDCN-20 in the compression quality range of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">20]</ref> N . Here, IDCN-5, IDCN-10, and IDCN-20 refer to the IDCN model trained with compression qualities q = 5, 10, and 20, respectively. IDCN-f exhibits great robustness to handle variant compression qualities, whereas the specifically trained IDCN models cannot handle those mismatched compression qualities. <ref type="figure" target="#fig_0">Fig. 13</ref> shows the subjective comparison between IDCN-5, IDCN-10, IDCN-20, and IDCN-f. IDCN-5 obtained terrible results when handling larger compression qualities for both objective and subjective evaluations. Although IDCN-10 could obtain relatively better results than IDCN-20 and IDCN-5, the chromatic aberration and missing details remained. In contrast, IDCN-f could fix both chromatic aberrations and texture details for a wide range of compression qualities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this study, we proposed a novel network based on implicit dual-domain learning (IDCN) to reduce color image compression artifacts. We first analyzed the structure of the conventional residual connections for low-level computer vision tasks and presented the extractor-corrector framework for constructing more generalized residual connections. Then, we constructed a correction unit based on this framework and introduced dense connections for the extractor and dual-domain correction for the corrector. Unlike conventional dual-domain learning methods that introduce DCT to enter the DCT-domain, the losses in the DCT-domain were directly estimated from the extracted features without DCT. The DCU also benefits the larger receptive field from the dilated convolution layers to obtain superior results in low-compression quality images. We compared IDCN with other state-of-the-art methods on both widely used evaluation datasets and the expanded WIN143 dataset. The objective and subjective evaluations demonstrated the superiority of IDCN over the other state-of-the-art methods. Moreover, the flexible version of IDCN named IDCN-f exhibited excellent performance to handle variable compression qualities in both subjective and objective evaluations. Therefore, IDCN-f has a considerable potential for applications in the practical compression AR efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENT</head><p>This work was supported in part by the Fundamental Research Funds for the Central Universities. Xuesong Liu received the B.Sc. and Ph.D. degrees in sonar signal processing from Zhejiang University, Hangzhou, China, in 2010 and 2015, respectively. He is currently a Lecturer with Zhejiang University. His current research interests include sonar signal processing, parallel processing, and embedded system design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Comparison between methods for gray scale and color images. Here, ARCNN-Color refers to the ARCNN trained on color images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and Cr  denote the Y channel loss, Cb channel loss, and Cr channel loss caused by JPEG compression, respectively. The losses caused by JPEG compression are from the quantization operations on DCT coefficients. Considering  as an 8 × 8 DCT coefficient matrix and *  as the quantized result of  , we can define  as *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Standard deviation maps with q = 20. (a) Patch size is 8, WIN143. (b) Patch size is 16, WIN143. (c) Patch size is 16, 200 testing images of BSDS500. and the multi-channel labeling map for training a flexible network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 FE , 2 FE , and 1 FD</head><label>121</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Architecture of our proposed implicit dual-domain convolutional network (IDCN) (a) (b) (c) Fig. 5. Comparison of residual connections in original ResNet, SRResNet, and EDSR. (a) Original ResNet. (b) SRResNet. (c) EDSR. (a) (b) (c) (d) Fig. 3. Standard deviation maps with q = 20 on DIV2K dataset. (a) B channel. (b) G channel. (c) R channel. (d) Simplified labeling map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 nFFn</head><label>1</label><figDesc> and n F be the input and output of the n-th DCU. SFE takes 1 n  as the input and outputs the basic features F  for the DCT-domain branch to estimate DCT-domain correction DCT  and pixel-domain correction Pixel  .Then, we used a residual connection to connect 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>C.</head><label></label><figDesc>estimates the integrated loss from both Cr and Cb channels, the constraints based on<ref type="bibr" target="#b6">(7)</ref> do not work onC  ; thus, QRU is not included in C REU . Moreover, because the two chrominance channels are compressed in the DCT domain after the 2 × 2 downsampling operation, we introduced a 3 × 3 dilated convolution layer with a dilation rate of 2 for C REU , whereas Y REU is a normal 3 × 3 convolution layer. Then, the implicit translation units (ITUs) layer to fuse them to obtain the final pixel-feature domain correction of DCT-domain branch DCT  . In the pixel-domain branch, we used a 3 × 3 convolution layer as a simple corrector to obtain the pixel-feature domain correction of pixel-domain branch Pixel  To avoid gradient explosion, we introduced a scale layer in EDSR to scale both DCT  and Pixel  . The scaling rate was set to 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Architecture of our proposed dual-domain correction unit. 041 in WIN143 recovered by models with -DL option and -ND option. (a) Origin. (b) -ND. (c) -DL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Correction maps from dual domain, DCT domain, pixel domain, luminance channel and combined chrominance channel in DCT domain in IDCN-D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .Fig. 10 .</head><label>810</label><figDesc>Comparison between estimated DCT domain loss and true DCT domain loss. Subjective evaluation with compression quality q=10. From top to bottom: 140088 from B200, 005 from WIN143 and 29030 from B200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Performances and running times of different methods on LIVE1 dataset with q = 10. estimated DCT-domain loss are linear, considering the effect from the scale layer, the mean norms of the estimated DCT coefficient losses are statistically closed to the true losses around the high-frequency area. (3) The mean norms of the estimated DCT coefficient losses are around zero, which is caused by the implicit transformation itself, in that the mean absolute values of implicit transformation coefficients ( () q Q  ) of low frequencies are much smaller than those of high frequencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Subjective evaluation for robust compression artifact reduction. From top to bottom: plane from LIVE1 and q = 7, 024 from WIN143 and q = 10, 038 from WIN143 and q = 20. The best results are highlighted and the second-best results are underlined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">INVESTIGATION OF DUAL-DOMAIN LEARNING.</cell></row><row><cell></cell><cell>PSNR(dB)</cell><cell>SSIM</cell><cell>PSNR-B (dB)</cell></row><row><cell>IDCN-P</cell><cell>29.57</cell><cell>0.8741</cell><cell>29.53</cell></row><row><cell>IDCN-Y</cell><cell>29.64</cell><cell>0.8752</cell><cell>29.60</cell></row><row><cell>IDCN-D</cell><cell>29.69</cell><cell>0.8759</cell><cell>29.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I ABLATION</head><label>I</label><figDesc>INVESTIGATION OF PL, MCC AND DC OPTIONS.</figDesc><table><row><cell></cell><cell cols="2">Estimated Y loss</cell><cell></cell><cell cols="2">Estimated C loss</cell><cell></cell><cell>True Cb loss</cell><cell>True Cr loss</cell><cell>True Y loss</cell></row><row><cell cols="2">DC Option</cell><cell></cell><cell>PL Option</cell><cell></cell><cell cols="2">MCC Options</cell><cell>PSNR</cell></row><row><cell>-ND</cell><cell>-DL</cell><cell>-N</cell><cell>-SL</cell><cell>-ML</cell><cell>-FC</cell><cell>-SC</cell><cell>(dB)</cell></row><row><cell></cell><cell>√</cell><cell>√</cell><cell></cell><cell></cell><cell>√</cell><cell></cell><cell>29.51</cell></row><row><cell></cell><cell>√</cell><cell></cell><cell></cell><cell>√</cell><cell>√</cell><cell></cell><cell>29.55</cell></row><row><cell></cell><cell>√</cell><cell>√</cell><cell></cell><cell></cell><cell></cell><cell>√</cell><cell>29.59</cell></row><row><cell>√</cell><cell></cell><cell></cell><cell></cell><cell>√</cell><cell></cell><cell>√</cell><cell>29.66</cell></row><row><cell></cell><cell>√</cell><cell></cell><cell></cell><cell>√</cell><cell></cell><cell>√</cell><cell>29.69</cell></row><row><cell></cell><cell>√</cell><cell></cell><cell>√</cell><cell></cell><cell></cell><cell>√</cell><cell>29.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc>COMPARISON FOR HANDLING RGB IMAGES ON LIVE1, B200 AND WIN143. AVERAGE PSNR/SSIM /PSNR-B VALUES FOR COMPRESSION QUALITY q=10 AND 20. THE BEST RESULTS ARE HIGHLIGHTED AND THE SECOND BEST RESULTS ARE UNDERLINED.</figDesc><table><row><cell>Quality</cell><cell>Methods</cell><cell>PSNR</cell><cell>LIVE1 SSIM</cell><cell>PSNR-B</cell><cell>PSNR</cell><cell>B200 SSIM</cell><cell>PSNR-B</cell><cell>PSNR</cell><cell>WIN143 SSIM</cell><cell>PSNR-B</cell></row><row><cell></cell><cell>JPEG</cell><cell>28.06</cell><cell>0.8409</cell><cell>27.82</cell><cell>28.20</cell><cell>0.8483</cell><cell>27.90</cell><cell>29.47</cell><cell>0.8440</cell><cell>29.28</cell></row><row><cell></cell><cell>ARCNN</cell><cell>29.23</cell><cell>0.8659</cell><cell>29.24</cell><cell>29.36</cell><cell>0.8665</cell><cell>29.34</cell><cell>30.82</cell><cell>0.8776</cell><cell>30.92</cell></row><row><cell></cell><cell>DPW-SDNet</cell><cell>29.59</cell><cell>0.8744</cell><cell>29.55</cell><cell>29.67</cell><cell>0.8752</cell><cell>29.62</cell><cell>31.28</cell><cell>0.8866</cell><cell>31.31</cell></row><row><cell>20</cell><cell>MemNet MWCNN</cell><cell>29.76 29.80</cell><cell>0.8770 0.8769</cell><cell>29.75 29.78</cell><cell>29.80 29.85</cell><cell>0.8776 0.8789</cell><cell>29.78 29.82</cell><cell>31.47 31.55</cell><cell>0.8904 0.8916</cell><cell>31.56 31.63</cell></row><row><cell></cell><cell>RDN</cell><cell>29.84</cell><cell>0.8778</cell><cell>29.82</cell><cell>29.85</cell><cell>0.8779</cell><cell>29.83</cell><cell>31.54</cell><cell>0.8912</cell><cell>31.63</cell></row><row><cell></cell><cell>S-Net</cell><cell>29.81</cell><cell>0.8781</cell><cell>29.79</cell><cell>29.86</cell><cell>0.8782</cell><cell>29.82</cell><cell>31.47</cell><cell>0.8904</cell><cell>31.56</cell></row><row><cell></cell><cell>IDCN</cell><cell>30.04</cell><cell>0.8816</cell><cell>30.01</cell><cell>30.07</cell><cell>0.8816</cell><cell>30.02</cell><cell>31.82</cell><cell>0.8964</cell><cell>31.90</cell></row><row><cell></cell><cell>JPEG</cell><cell>25.69</cell><cell>0.7592</cell><cell>0.25.49</cell><cell>25.83</cell><cell>0.7584</cell><cell>25.58</cell><cell>27.08</cell><cell>0.7684</cell><cell>26.90</cell></row><row><cell></cell><cell>ARCNN</cell><cell>26.91</cell><cell>0.7946</cell><cell>26.92</cell><cell>27.02</cell><cell>0.7930</cell><cell>27.02</cell><cell>28.46</cell><cell>0.8207</cell><cell>28.57</cell></row><row><cell></cell><cell>DPW-SDNet</cell><cell>27.26</cell><cell>0.8036</cell><cell>27.28</cell><cell>27.39</cell><cell>0.8027</cell><cell>27.39</cell><cell>29.03</cell><cell>0.8326</cell><cell>29.13</cell></row><row><cell>10</cell><cell>MemNet MWCNN</cell><cell>27.33 27.45</cell><cell>0.8100 0.8083</cell><cell>27.34 27.44</cell><cell>27.46 27.52</cell><cell>0.8086 0.8069</cell><cell>27.46 27.52</cell><cell>29.04 29.25</cell><cell>0.8380 0.8375</cell><cell>29.15 29.34</cell></row><row><cell></cell><cell>RDN</cell><cell>27.47</cell><cell>0.8116</cell><cell>27.48</cell><cell>27.53</cell><cell>0.8096</cell><cell>27.53</cell><cell>29.19</cell><cell>0.8395</cell><cell>29.30</cell></row><row><cell></cell><cell>S-Net</cell><cell>27.35</cell><cell>0.8090</cell><cell>27.36</cell><cell>27.42</cell><cell>0.8066</cell><cell>27.43</cell><cell>28.95</cell><cell>0.8349</cell><cell>29.05</cell></row><row><cell></cell><cell>IDCN</cell><cell>27.63</cell><cell>0.8161</cell><cell>27.63</cell><cell>27.69</cell><cell>0.8136</cell><cell>27.69</cell><cell>29.45</cell><cell>0.8467</cell><cell>29.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III ARCHITECTURE</head><label>III</label><figDesc>DETAILS OF DIFFERENT METHODS.</figDesc><table><row><cell>Method</cell><cell>Layers</cell><cell>Parameters</cell></row><row><cell>ARCNN</cell><cell>5</cell><cell>117K</cell></row><row><cell>DWP-SDNet</cell><cell>40</cell><cell>1.3M</cell></row><row><cell>MemNet-M6R6</cell><cell>80</cell><cell>2.8M</cell></row><row><cell>MWCNN</cell><cell>24</cell><cell>15.4M</cell></row><row><cell>RDN(8 RDBs)</cell><cell>85</cell><cell>13.2M</cell></row><row><cell>S-Net</cell><cell>20</cell><cell>10.2M</cell></row><row><cell>IDCN</cell><cell>100</cell><cell>9.9M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V QUANTITATIVE</head><label>V</label><figDesc>COMPARISON FOR HANDLING LUMINANCE CHANNEL IMAGES ON LIVE1, B200 AND WIN143. AVERAGE PSNR/SSIM /PSNR-B VALUES FOR COMPRESSION QUALITY q=10 AND 20. THE BEST RESULTS ARE HIGHLIGHTED AND THE SECOND BEST RESULTS ARE UNDERLINED.</figDesc><table><row><cell>Quality</cell><cell>Methods</cell><cell>PSNR</cell><cell>LIVE1 SSIM</cell><cell>PSNR-B</cell><cell>PSNR</cell><cell>B200 SSIM</cell><cell>PSNR-B</cell><cell>PSNR</cell><cell>WIN143 SSIM</cell><cell>PSNR-B</cell></row><row><cell></cell><cell>JPEG</cell><cell>30.07</cell><cell>0.8683</cell><cell>29.64</cell><cell>30.04</cell><cell>0.8671</cell><cell>29.53</cell><cell>32.40</cell><cell>0.8924</cell><cell>31.79</cell></row><row><cell></cell><cell>ARCNN</cell><cell>31.41</cell><cell>0.8891</cell><cell>31.37</cell><cell>31.32</cell><cell>0.8872</cell><cell>31.26</cell><cell>33.80</cell><cell>0.9124</cell><cell>33.75</cell></row><row><cell></cell><cell>DPW-SDNet</cell><cell>31.69</cell><cell>0.8952</cell><cell>31.60</cell><cell>31.59</cell><cell>0.8933</cell><cell>31.49</cell><cell>34.12</cell><cell>0.9175</cell><cell>34.05</cell></row><row><cell>20</cell><cell>MemNet MWCNN</cell><cell>31.82 31.90</cell><cell>0.8970 0.8989</cell><cell>31.74 31.83</cell><cell>31.71 31.78</cell><cell>0.8950 0.8966</cell><cell>31.62 31.69</cell><cell>34.27 34.41</cell><cell>0.9190 0.9207</cell><cell>34.19 34.33</cell></row><row><cell></cell><cell>RDN</cell><cell>31.93</cell><cell>0.8991</cell><cell>31.85</cell><cell>31.81</cell><cell>0.8986</cell><cell>31.71</cell><cell>34.43</cell><cell>0.9207</cell><cell>34.34</cell></row><row><cell></cell><cell>S-Net</cell><cell>31.83</cell><cell>0.8975</cell><cell>31.76</cell><cell>31.71</cell><cell>0.8952</cell><cell>31.62</cell><cell>31.28</cell><cell>0.9192</cell><cell>34.20</cell></row><row><cell></cell><cell>IDCN</cell><cell>32.09</cell><cell>0.9006</cell><cell>32.00</cell><cell>31.95</cell><cell>0.8981</cell><cell>31.86</cell><cell>34.60</cell><cell>0.9220</cell><cell>34.51</cell></row><row><cell></cell><cell>JPEG</cell><cell>27.77</cell><cell>0.7905</cell><cell>27.38</cell><cell>27.79</cell><cell>0.7874</cell><cell>27.33</cell><cell>29.92</cell><cell>0.8248</cell><cell>29.37</cell></row><row><cell></cell><cell>ARCNN</cell><cell>29.11</cell><cell>0.8235</cell><cell>29.07</cell><cell>29.08</cell><cell>0.8209</cell><cell>29.03</cell><cell>31.44</cell><cell>0.8641</cell><cell>31.40</cell></row><row><cell></cell><cell>DPW-SDNet</cell><cell>29.40</cell><cell>0.8320</cell><cell>29.34</cell><cell>39.35</cell><cell>0.8292</cell><cell>29.28</cell><cell>31.84</cell><cell>0.8717</cell><cell>31.78</cell></row><row><cell>10</cell><cell>MemNet MWCNN</cell><cell>29.44 29.55</cell><cell>0.8327 0.8357</cell><cell>29.39 29.49</cell><cell>29.38 29.47</cell><cell>0.8295 0.8325</cell><cell>29.32 29.42</cell><cell>31.89 32.09</cell><cell>0.8727 0.8754</cell><cell>31.85 32.04</cell></row><row><cell></cell><cell>RDN</cell><cell>29.56</cell><cell>0.8359</cell><cell>29.51</cell><cell>29.47</cell><cell>0.8325</cell><cell>29.42</cell><cell>32.05</cell><cell>0.8751</cell><cell>32.01</cell></row><row><cell></cell><cell>S-Net</cell><cell>29.44</cell><cell>0.8325</cell><cell>29.39</cell><cell>29.38</cell><cell>0.8294</cell><cell>29.32</cell><cell>31.86</cell><cell>0.8718</cell><cell>31.82</cell></row><row><cell></cell><cell>IDCN</cell><cell>29.71</cell><cell>0.8384</cell><cell>29.66</cell><cell>29.61</cell><cell>0.8347</cell><cell>29.55</cell><cell>32.23</cell><cell>0.8773</cell><cell>32.19</cell></row></table><note>Fig. 12. Performance curves of JPEG, IDCN-f, IDCN-10, and IDCN-20. The averaged PSNR values are evaluated on LIVE1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Bolun Zheng received the B.Sc. degree from Zhejiang University in 2014 and now pursuing the Ph.D. degree in engineering instrument science and technology with Zhejiang University, Hangzhou, China. His current research interests include image processing, video processing and deep learning. Chen received the Ph.D. degree from Zhejiang University, Hangzhou, China, in 1998. He is currently a Professor and the Director of the Institute of Advanced Digital Technologies and Instrumentation, Zhejiang University. His current research interests include embedded system, multimedia system, and networking. Tian received B.Sc. and Ph.D. degrees in signal processing from Zhejiang University, Hangzhou, China, in 2001 and 2007, respectively. He is currently an Associate Professor at Zhejiang University. His research focuses on the fields of signal processing and video coding. Zhou received the B.Sc. and Ph.D. degrees from Zhejiang University, Hangzhou, China, in 2000 and 2006, respectively. He is currently an Associate Professor with Zhejiang University.</figDesc><table><row><cell cols="3">Yaowu Xiang Fan His</cell></row><row><cell cols="3">current research interests include sonar</cell></row><row><cell cols="3">signal processing and fieldprogrammable</cell></row><row><cell>gate</cell><cell>array-based</cell><cell>high-performance</cell></row><row><cell cols="2">computing.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pointwise shape-adaptive DCT for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reducing Artifacts in JPEG Decompression Via a Learned Dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="718" to="728" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conf. on Compute Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5261" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compression Artifacts Reduction by a Deep Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Int. Conf. Comput. Vis. (ICCV) 2015</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV) 2015</meeting>
		<imprint>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compression Artifacts Removal Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hradis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zemcik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Wscg</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europe Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">CAS-CNN: A deep convolutional neural network for image compression artifact suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting><address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="752" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DMCNN: Dual-Domain Multi-Scale Convolutional Neural Network for Compression Artifacts Removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Data-driven sparsity-based restoration of JPEG-compressed images in dual transform-pixel domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">IEEE Conf. on Compute Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5171" to="5178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building Dual-Domain Representations for Compression Artifacts Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Europe Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Deconvolution Network for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DPW-SDNet: Dual pixel-wavelet domain deep CNNs for soft decoding of JPEG-compressed images&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="711" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-level wavelet-CNN for image restoration&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="886" to="88609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">S-Net: a scalable convolutional neural network for JPEG compression artifact reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43037</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno>Arxiv:1812.10477</idno>
		<title level="m">Residual Dense Network for Image Restoration&apos;, Arxiv Preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image restoration using convolutional auto-encoders with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename></persName>
		</author>
		<idno>Arxiv:1606.08921</idno>
	</analytic>
	<monogr>
		<title level="j">Arxiv Preprint</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MemNet: A Persistent Memory Network for Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Int. Conf. Comput. Vis. (ICCV) 2017</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV) 2017</meeting>
		<imprint>
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D M</forename><surname>Laurens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Dense Skip Connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4809" to="4817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FFDNet: Toward a Fast and Flexible Solution for CNN based Image Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszá R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions&apos;, arXiv Preprint</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<title level="m">NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contour Detection and Hierarchical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">898</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual importance pooling for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of selected topics in signal processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
