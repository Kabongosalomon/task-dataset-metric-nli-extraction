<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">I n 2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
							<email>pramuditha.perera@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Abavisani</surname></persName>
							<email>mahdi.abavisani@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
							<email>vishal.m.patel@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<addrLine>94 Brett Road</addrLine>
									<postCode>08854</postCode>
									<settlement>Piscataway</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">I n 2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In unsupervised image-to-image translation, the goal is to learn the mapping between an input image and an output image using a set of unpaired training images. In this paper, we propose an extension of the unsupervised image-toimage translation problem to multiple input setting. Given a set of paired images from multiple modalities, a transformation is learned to translate the input into a specified domain. For this purpose, we introduce a Generative Adversarial Network (GAN) based framework along with a multi-modal generator structure and a new loss term, latent consistency loss. Through various experiments we show that leveraging multiple inputs generally improves the visual quality of the translated images. Moreover, we show that the proposed method outperforms current state-of-the-art unsupervised image-to-image translation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of unsupervised image-to-image translation has made promising strides with the advent of Generative Adversarial Networks (GAN) <ref type="bibr">[5]</ref> in recent years. Given an input from a particular domain, the goal of image-to-image translation is to transform the input onto a specified second domain. Recent works in image-to-image translation has successfully learned this transformation across various tasks including satellite images to map images, night images to day images, greyscale images to color images etc. <ref type="bibr" target="#b24">[29]</ref>, <ref type="bibr" target="#b7">[12]</ref>, <ref type="bibr" target="#b6">[11]</ref>  <ref type="bibr" target="#b21">[26]</ref>.</p><p>In this work, we propose an extension of the original problem from a single input image to multiple input images, called multi-image-to-image translation (I n 2I). Given semantically related multiple images across n number of different domains, the goal of I n 2I is to produce the corresponding image in a specified domain. For example, the traditional problem of translating a greyscale image onto the RGB domain can be extended into an I n 2I problem by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{NIR, Greyscale} to RGB Color Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-spectral Image to Visible Image Translation</head><p>Synthetic to Real Image Translation <ref type="figure">Figure 1</ref>: Traditional unsupervised image-to-image translation maps a single image onto a specified domain. In this work, we address multi-image-to-image translation (I n 2I), where a set of input images from different domains are mapped onto a specified domain. This figure illustrates three applications of the proposed method.</p><p>providing the near infrared (NIR) image of the same scene as an additional input. Now, the objective would be to use information present in greyscale and NIR domains to produce the corresponding output in the RGB domain as shown in <ref type="figure">Figure 1</ref>. In this paper, we study the problem of I n 2I in the more generic unsupervised setting and provide initial direction to solve the problem.</p><p>Image-to-image translation is a challenging problem. For a given input, there exists multiple possible representations in the specified second domain. Having multiple inputs from different image modalities reduces this ambiguity due to the presence of complimentary information. There-fore, as we show later in experimental results section, leveraging multiple input images leads to an output of higher perceptual quality.</p><p>Multiple input modalities can be incorporated naively by concatenating all available modalities as channels and feeding into an existing image-to-image translation algorithm. However, such an approach leads to unsatisfactory results (see supplementary material for experiments). Therefore, we argue that unsupervised multi-image-to-image translation should be treated as a unique problem. In this work our main contributions are three-fold: 1. We introduce the problem of unsupervised multi-imageto-image translation. We show that by leveraging multiple modalities one can produce a better output in the desired domain as compared to when only a single input modality is used. 2. A GAN-based scheme is proposed to combine information of multiple modalities to produce the corresponding output from the desired domain. We introduce a new latent consistency loss term, into the objective function. 3. We propose a generalization to the GAN generator network by introducing a multi-modal generator structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>To the best of our knowledge, I n 2I problem has not been previously addressed in the literature. In this section, we outline previous work related to the proposed method. Generative Adversarial Networks (GANs). The fundamental idea behind GAN introduced in [5], <ref type="bibr" target="#b14">[19]</ref> is to use two competing Fully Convolutional Networks (FCN <ref type="bibr" target="#b15">[20]</ref>), the generator and the discriminator, for generative tasks. Here, the generator FCN learns to generate images from the target domain and the discriminator tries to distinguish generated images from real images of the given domain. During training, two FCNs engage in a min-max game trying to outperform each other. Learning objective of this problem is collectively called as the adversarial loss <ref type="bibr" target="#b24">[29]</ref>. Many applications have since employed GANs for various image generation tasks with success <ref type="bibr" target="#b12">[17]</ref>, <ref type="bibr" target="#b3">[8]</ref>, <ref type="bibr" target="#b5">[10]</ref>, <ref type="bibr">[7]</ref>, <ref type="bibr" target="#b19">[24]</ref>, <ref type="bibr" target="#b23">[28]</ref>, <ref type="bibr" target="#b22">[27]</ref>.</p><p>In, <ref type="bibr" target="#b8">[13]</ref> GANs were studied in a conditional setting where a conditioning label is provided as the input to both the generator and the discriminator. Here, we limit our discussion on Conditional GANs (CGAN) to image-to-image generation tasks. The Pix2Pix framework introduced in [7] uses CGANs for supervised image-to-image translation. In their work, they showed successful transformations across multiple tasks when paired samples across the two domains are available during training. In <ref type="bibr" target="#b16">[21]</ref>, CGANs are used to generate real eye images from synthetic eye images. In order to learn an effective discriminator, <ref type="bibr" target="#b16">[21]</ref> proposes to maintain and use a history of generated images. The Co-GAN framework introduced in <ref type="bibr" target="#b7">[12]</ref>, maps images of two different domains onto a common representation to perform domain adaptation using a weight sharing FCN architecture. Unpaired image-to-image translation. Several recent methods have addressed the unsupervised image-to-image translation task when the input is a single image. Here, unlike in the supervised setting, paired samples across the two domains do not exist. In <ref type="bibr" target="#b24">[29]</ref>, image-to-image translation problem is tackled by having two generators and discriminators, one for each domain. In addition to the adversarial loss, a cycle consistency constraint is added to ensure that the semantic information is preserved in the translation. A similar rationale is adopted in DualGAN <ref type="bibr" target="#b21">[26]</ref> which has been developed independently of CycleGAN. In <ref type="bibr" target="#b6">[11]</ref>, the CoGAN framework was extended using GANs and variational autoencoders with the assumption of a common latent space between the domains. Image fusion. Although image fusion <ref type="bibr" target="#b9">[14]</ref> operates on multiple input images, we note that our task is very different from image fusion since the former does not involve a domain translation process. In image fusion tasks, multiple input modalities are combined in an informative latent space. This space is usually found by a derived multi-resolution transformation such as wavelets <ref type="bibr" target="#b11">[16]</ref>. In <ref type="bibr" target="#b10">[15]</ref> operating on deep networks, a latent space is used to re-generate outputs of multiple modalities. Motivated by this technique, we fuse mid-level deep features from each input domain in the proposed generator FCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Notation. In this paper, we use the following notations. Source domain and target domain are denoted by S and T , respectively. The latent space is denoted by Z. In the presence of multiple source domains, the set of source domains {S 1 , . . . , S n } are denoted collectively as S. A data sample drawn from an arbitrary domain X is denoted as x. The transformation between domains X and Y is denoted by the function f X→Y . The transformation between the domains X and the latent space Z is denoted by h X→Z . Overview. In conventional image-to-image translation, the objective is to translate images from an original domain S to a target domain T using a learned transformation f S→T (.). In the supervised setting of the problem, a set of image pairs {(s 1 , t 1 ), (s 2 , t 2 ), . . . , (s p , t p )} are given, where s i ∈ S and t i ∈ T are paired images from the two domains. Imageto-image translation task is less challenging in this scenario since the desired output for a given input is known ahead of time.</p><p>Similar to the supervised version of the problem, images from both target and source domains are provided in the unsupervised image-to-image translation problem. However, in this case, provided images of the two domains are not paired. In other words, for a given source image s i , the corresponding ground truth image t i is not provided. In the absence of image pairs from both domains, it is not possi-  <ref type="figure">Figure 2</ref>: Network structure used for unsupervised imageto-image translation. Top: CycleGAN, Bottom: Proposed method for I n 2I</p><formula xml:id="formula_0">T S D S D T f S→T D T f T→S T Z D S1 D S2 D Sn h T→Z h Z→T h S→Z h Z→S1 h Z→S2 h Z→Sn f S→T f T→S</formula><p>ble to optimize over a distance between the estimated output and the target. One possible option is to introduce an adversarial loss to facilitate reward if the generated image is from the same domain as the target domain. However, having an adversarial loss alone does not guarantee that the generated image will share semantics with the input. Therefore, to successfully solve this problem, additional constraints need to be imposed. In <ref type="bibr" target="#b24">[29]</ref>, such a solution is sought by enforcing the cycle consistency property. Here, an inverse transformation f T →S (.) is learned along with f S→T (.). Then, the cycle consistency ensures that the learned transformation yields a good approximation of the input s i by comparing s i with f T →S (f S→T (s i )). We develop our method based on the foundations of CycleGAN proposed in <ref type="bibr" target="#b24">[29]</ref>. Here, we briefly review the CycleGAN method and we will draw differences between CycleGAN and our method in succeeding sections. CycleGAN as shown in <ref type="figure">Figure 2</ref> (top), contains a forward transformation from source domain to target domain and a reverse transformation from target to source. Two discriminators D S and D T are used to asses whether a given input belongs to source or target, respectively. Multimodal Generator. The I n 2I problem accepts n inputs and translates them into a single output. Therefore, in contrast to CycleGAN, the proposed method deals with multiple inputs in the forward transformation and multiple outputs in the reverse transformation. In order to facilitate this operation, we propose a generalization of the generator structure for multiple inputs and outputs. The generic struc-  <ref type="figure">Figure 3</ref>: Multi-modal generator: generalization of the generator for multiple inputs and multiple outputs.</p><p>ture of the proposed generator is shown in <ref type="figure">Figure 3</ref>. In general, it is possible for the generator to have N inputs and M outputs. The generator treats each input modality independently and extracts features and fuses them prior to feeding them to the encoder. The encoder maps resultant features to a latent space. Operating on the latent space, M number of independent decoders generate M output images.</p><p>For the specific application of I n 2I, two generators are used for the forward and reverse transformations. When there are n input images, M is set to be equal to one during the forward transformation where the goal is to generate a single output image (N = n, M = 1). In the reverse transformation, a single input image is processed to generate n outputs thereby making N = 1 and M = n. Therefore, generator networks used in I n 2I are asymmetric in structure as shown in <ref type="figure">Figure 2</ref> (bottom).</p><p>The proposed method treats n inputs independently initially in the forward transformation and then extracted features are fused together. The fused feature is first transformed into a latent space Z as shown in in <ref type="figure">Figure 2</ref> (bottom) and then transformed into the target domain. In the reverse transformation, the single input is mapped back to the same latent space first. Then, the latent space representation is used to produce n outputs belonging to n source domains. In this formulation, n + 1 discriminators are used, one for each domain as opposed to CycleGAN. In addition, a latent space consistency loss is added to ensure that the same concept in all domains have a common latent space representation. Problem Formulation. Formally, given n number of input modalities S = {S 1 , S 2 , . . . , S n }, the objective is to learn a transformation f S→T (.). Here, we note that the input to the forward transformation is a set of images, where the output of the transformation is a single image. Similarly, the backward transformation f T →S (.) takes a single image input and produces n output images.</p><p>In order to approach the solution to this problem, first we view all input images and the desired output image as different representations of the same concept. Motivated by the techniques used in domain adaptation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr">[4]</ref>, <ref type="bibr" target="#b17">[22]</ref> we hypothesize the existence of a latent representation that can be derived using the provided representations. With this as-sumption, we treat our original problem as a series of subproblems where the requirement is to learn the transformation and the inverse transformation to the latent representation from each domain. If the latent representation is Z, we will attempt to learn transformations h I→Z and h Z→I ,</p><formula xml:id="formula_1">where I ∈ {S, T } and h I→Z = h −1 Z→I . With this formu- lation, the forward transform f S→T becomes f S→T (.) = h Z→T (h S→Z (.)) and the reverse transformation f T →S be- comes f T →S(.) = h Z→S (h T →Z (.)). Adversarial Loss.</formula><p>In order to learn transformation f S→T , we use an adversarial generator-discriminator pair</p><formula xml:id="formula_2">{f S→T (.), D T (.)} [5].</formula><p>Denoting the data distributions of domains S and T as P data (s) and P data (t), respectively, the generator function tries to learn the transformation f S→T . The discriminator is trained to differentiate real images from the target domain S from generated images f S→T (s). This procedure is captured in the adversarial loss as follows:</p><formula xml:id="formula_3">L GAN,S→T = E t∼p data (t) [log D T (t)] + E s∼p data (s) [1 − log D T (f S→T (s))].</formula><p>(1) Similarly, to learn f T →S we use a single generator f T →S . However, since there exists n input domains in total, we require n discriminators {D si }, where i = 1, 2, . . . , n, one for each domain. With this formulation, the total adversarial loss in backward transformation becomes a summation of n adversarial terms as follows:</p><formula xml:id="formula_4">L GAN,T →S = n i=1 E si∼p data (si) [log D Si (s i )] + n i=1 E t∼p data (t) [1 − log D Si (f T →Si (t))].</formula><p>(2) Latent Consistency Loss. As briefly discussed above, the adversarial loss only ensures that the generated image looks realistic in the target domain. Therefore, adversarial loss alone is inadequate to result in a transformation which preserves semantic information of the input. However, based on the assumption that both input and target domains share a common latent representation, it is possible to enforce a more strict constraint to ensure semantics between the input and the output are preserved. This is done by forcing the latent representation obtained during the forward transformation to be equal to the latent representation obtained during the reverse transformation for the same input.</p><p>More specifically, for a given input s, a set of latent representations h s→Z (s) are recorded. Then, this recorded vector is compared against the latent representation obtained during the reverse transformation h T →Z (f S→T (s)). The latent consistency loss in the forward transformation is defined as,</p><formula xml:id="formula_5">(3) L latent,S →T = E s∼p data (s) h S→Z (s) − h T →Z (f S→T (s)) 1 .</formula><p>Similarly, the latent consistency loss in the reverse transfor-mation is defined as,</p><formula xml:id="formula_6">(4) L latent,T →S = E t∼p data (t) h T →Z (t) − h S→Z (f T →S (t)) 1 . Cycle Consistency Loss.</formula><p>If the input and the transformed image do not share semantic information, it is impossible to regenerate the input using the transformed image. Therefore by forcing the learned transformation to have a valid inverse transform, it is further possible to force the generated image to share semantics with the input. Based on this rationale, in <ref type="bibr" target="#b24">[29]</ref> cycle consistency loss is introduced to ensure that the transformed image shares semantics with the input image. Since this argument is equally valid for the multi-input case, we adopt cycle consistency loss <ref type="bibr" target="#b24">[29]</ref> in our formulation. Proposed backward cycle consistency loss is similar to that of <ref type="bibr" target="#b24">[29]</ref> in definition. We define the reverse cycle consistency loss as:</p><formula xml:id="formula_7">(5) L cyc,T →S = E t∼p data (t) [ f S→T (f T →S (t)) − t 1 ].</formula><p>However, in comparison, the forward cycle consistency loss takes into account n inputs and compares the distance among the n reconstructions as opposed to <ref type="bibr" target="#b24">[29]</ref>. The forward cycle consistency loss is defined as,</p><formula xml:id="formula_8">(6) L cyc,s →T = E s∼p data (s) [ F T →s (F s→T (s)) − s 1 ].</formula><p>Cumulative Loss. The final objective function is the addition of all three losses introduced in this section. The cumulative loss L total is defined as follows:</p><formula xml:id="formula_9">(7) L total = L GAN,S→T + L GAN,T →S + λ 1 (L cyc,T →S + L cyc,∫ →T ) + λ 2 (L latent,S→T + L latent,T →S )</formula><p>, where, λ 1 and λ 2 are constants. Limiting Case. It is interesting to investigate the behavior of the proposed network in the limiting case when n = 1. In this case, both the number of input and output modalities of the network becomes one; i.e. N = 1 and M = 1. Therefore S becomes S in equations (1), (2), (5) and (6). In addition, with n = 1, summation in (2) reduces to a single statement. If we disregard the latent consistency loss by forcing λ 2 = 0, the total objective reduces to,</p><formula xml:id="formula_10">L total = E t∼p data (T ) [log D T (t)] + E s∼p data (S) [log D S (s)] + E s∼p data (s) [1 − log D T (f s→T (s))] + E t∼p data (T ) [1 − log D S (t)] + E t∼p data (t) [ f s→T (f T →s (t)) − t 1 ] + E s∼p data (s) [ F T →s (F s→T (s)) − s 1 ]</formula><p>. This reduced objective is identical to the total objective in CycleGAN. Therefore, in the limiting case when n = 1, the proposed method reduces to the cycleGAN formulation when the latent consistency loss is disregarded. Network Architecture. In this section, we describe the network architecture of the proposed Generator by considering the case where two input modalities are used; i,e when n = 2. The resulting two generators in this case is illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>. It should be noted that the Convolutional Neural Network (CNN) architectures used in both forward and reverse transformations here are in coherence with the generic structure shown in <ref type="figure">Figure 3</ref>. In principle, the generator can be based on any backbone architecture. In our work, we used ResNet [6] with nine resnet blocks as the backbone. In our proposed network, a CNN is used for each module in <ref type="figure">Figure 3</ref>. These CNNs are typically convolutions/transposed convolutions followed by nonlinearities, batch-normalization layers and possibly with skip connections.</p><p>Two input images (from the two input domains) are present as the input of the forward transformation. These images are subjected to two parallel CNNs to extract features from each modality. Then, the extracted features are fused to generate an intermediate feature representation. In our work, feature fusion is performed by concatenating feature maps of feature extraction stage and using a convolution operation to reduce the dimension. This feature is then subjected to a set of convolution operations to arrive at the latent space. Finally, the latent space representation is subjected to a series of CNNs with transposed convolution operations to generate a single output image (from the target domain).</p><p>During the backward transformation, a single input is present. A CNN with convolution operations is used to transform the input into the latent space. It should be noted that since there is only a single input, there is no notion of fusion in this case. Two parallel CNNs consisting of transposed convolutions branch out from the latent space to produce two outputs corresponding to domains S1 and S2.</p><p>This architecture can be extended n modalities. In this case, the core structure will be similar to that of <ref type="figure" target="#fig_1">Figure 4</ref> except that there will be n parallel branches instead of two at either ends of the network. For the discriminator networks we use PatchGANs proposed in <ref type="bibr">[7]</ref>. Please refer to the supplementary material for exact details of the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We test the proposed method on three publicly available multi-modal image datasets across three tasks against stateof-the-art unsupervised image-to-image translation meth-ods. The training was carried out adhering to principles of unsupervised learning. Even when ground truth images of the desired translation were available, they were not used during training. When available, ground truth images were used during testing to quantify the structural distortion introduced by each method through calculating PSNR and SSIM <ref type="bibr" target="#b20">[25]</ref> metrics. It should be noted that in this case, two disjoint image sets were used for training and testing.</p><p>As the benchmark for performance comparison, we use CycleGAN <ref type="bibr" target="#b24">[29]</ref> and UNIT <ref type="bibr" target="#b6">[11]</ref> frameworks. Since both of these methods are specifically designed for single inputs, we used all available image modalities, one at a time to produce the corresponding outputs. In the implementation of the proposed method, λ 1 and λ 2 in (7) are set equal to 10 and 1, respectively. Learning is performed using the Adam optimizer <ref type="bibr" target="#b4">[9]</ref> with a batch size of 1. Initial learning rates of generators and discriminators were set equal to 0.0002 and 0.0001, respectively. Training was conducted for 200 epochs, where learning rate was linearly decayed in the last 100 epochs. Image Colorization (EPFL NIR-VIS Dataset.) The EPFL NIR-VIS dataset <ref type="bibr" target="#b1">[2]</ref> includes 477 images in 9 categories captured in the RGB and the Near-infrared (NIR) image modalities across diverse scenes. Scenes included in this dataset are categorized as country, field, forest, indoor, mountain, old building, street, urban and water. We use this dataset to simulate the image colorization task. We generated greyscale images from the RGB visible images and use greyscale and NIR images as the input modalities with the aim of producing the corresponding RGB image. We randomly selected 50 images to be the test images and used the remaining images for training.</p><p>First we trained CycleGAN <ref type="bibr" target="#b24">[29]</ref> and UNIT <ref type="bibr" target="#b6">[11]</ref> models for each input modality independently. Then, the proposed method was used to train a model based on both input modalities. Obtained results for these cases are shown in <ref type="figure">Figure 5</ref>. Obtained PSNR and SSIM values for each method on the test data are tabulated in <ref type="table" target="#tab_1">Table 1</ref>. By inspection, CycleGAN operating on greyscale images were able to identify segments in the image but failed to assign correct colors. For example, in the first row, the tree is correctly segmented but with a wrong color. In comparison, CycleGAN with NIR images have resulted in a much better colorization. Since the amount of energy a color reflects depends on the wavelength of the color, a NIR signal contains some information about the color of the object. This could be the reason why NIR images have performed better colorization compared to greyscale. The same trend can be observed in the outputs of the UNIT method.</p><p>On the other hand, the proposed method has produced a colorization very similar to the ground truth. As an example we wish to draw the attention of the reader to the color of the tree and the field in the first row, colors of the building and the tree in the last row. It has also recorded a superior PSNR and SSIM values compared with the other baselines as shown in <ref type="table" target="#tab_1">Table 1</ref>. It should be noted that PSNR and SSIM values only reflect how well the structure of objects in images have been preserved. It is not meant to be an indication of how well colorization task has been carried out. Synthetic-to-Real Image Translation (Synthia and CityScapes Datasets). In this subsection, we experiment on generating real images using synthetic images. For this purpose, we use two datasets, Synthia <ref type="bibr" target="#b13">[18]</ref> and CityScapes [3], respectively as the source and the target domains. The Cityscapes dataset contains images taken across fifty urban cities during daytime. We use 1525 images from the validation set of the dataset to represent the target domain in the synthetic-to-real translation task. The Synthia dataset contains graphical simulations of an urban city. The scenes included in the dataset contain different weather conditions, lighting conditions and seasons. For our work, we only use the summer day light subset of the dataset which includes 901 images for training. The Synthia dataset provides RGB image intensities as well as the depth information of the scene. Hence, we use these as the two input modalities. Results of this experiment are shown in <ref type="figure">Figure 6</ref>. In this particular task, UNIT method has only changed the generic color scheme of the scene with incorrect association; for example note that skies look brown instead of blue in resulting images. In addition, objects in the scene continues to possess the characteristics of synthetic images. In contrast, Cy-cleGAN has attempted to convert appearance of synthetic images to real. However, in the process it has distorted the structure of objects. When only depth information is used, the cycleGAN method is unable to preserve the structure of objects in the scene. For example, lines along the roads have ended up being warped in the learned representation in <ref type="figure">Figure 6</ref>. The CycleGAN model based on the RGB im-ages preserves the overall structure to an extent. However, vital details are either missing or misleading. For example, pavements are missing from images shown in rows 2 and 3 in <ref type="figure">Figure 6</ref>. The absence of a shadow on the road in row 2, addition of clutter in the left pavement in row 3 and disappearance of the telephone pole in row 4 are some of the notable incoherences. Comparatively, fusion of both RGB and depth information has resulted in a more realistic translation. It should be noted that synthetic-to-real translation is a challenging problem in practice and when certain concepts were missing in either of source or target domains, the model found it difficult to learn such concepts. For example, training images from Cityscape did not have zebra crossings in any of the images. Therefore, the concept of zebra crossings is not learned well by the model as shown in row 1. Hyperspectral-to-Visible Image Translation.</p><p>The Freiburg Forest Dataset <ref type="bibr" target="#b18">[23]</ref> contains 230 training images and 136 test images of a forest scenery with images of different wavelengths. The types of images contained in the dataset include RGB, depth, NIR, EVI (Enhanced Vegetation Index) and combinations of aforementioned image types. In our experiments, we use this dataset to perform hyperspectral-to-real image translation where the depth and the NIR image modalities are used to recover the RGB images of the visual spectrum. We note that this task is relatively easier compared to the earlier two tasks due to the low diversity in scenes. All methods, except for UNIT, were able to generate realistic RGB images from the provided input modalities, as shown in <ref type="figure" target="#fig_3">Figure 7</ref>. The reason why UNIT fails in this task could be due to the low number of training image samples <ref type="bibr" target="#b6">[11]</ref>. In CycleGAN, visual domain image reconstruction solely based on the depth information has resulted in sub-standard images. For example, the road is missing in row 1 and the road takes a wrong shape in row 2 for the depth image-based CycleGAN output. When the NIR images are used as the input, the resulting CycleGAN output is more closer to the ground truth. But, in this case, there exists multiple missing regions, where pixels are painted in white, as shown in rows 1-3.</p><p>In terms of reconstructing finer details, the proposed method that utilizes both NIR and depth information have outperformed the other baselines. In comparison to the earlier task, all methods have less structural distortion as evident from <ref type="table" target="#tab_1">Table 1</ref>. However, even in this case, the proposed method has performed marginally better than the other baseline methods in terms of SSIM and PSNR performance.</p><p>Since the dataset has three modalities available, we ran an extra experiment by inputting all three modalities (Depth, NIR and EVI) into the proposed method. Results produced in this case were more closer to the ground truth </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>Target Outputs <ref type="figure">Figure 5</ref>: Qualitative results for the image colorization task. <ref type="figure">Figure 6</ref>: Qualitative results for the synthetic-to-real translation task.</p><formula xml:id="formula_11">I n 2I (Ours) UNIT (NIR) UNIT (Depth) CycleGAN (Depth) CycleGAN (NIR) Synthetic Visible Synthetic Depth</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs Outputs</head><p>in general. For example, water residues on the ground are produced in row 3 for this case, which was absent in all prior cases. The use of three modalities have improved the PSNR performance by more than 2.0 according as shown in <ref type="table" target="#tab_1">Table 1</ref>. Please refer to supplementary material for more results and analysis.</p><p>Ablation Study. The loss function proposed in (7) has three main components: the adversarial loss, latent consistency loss and cycle consistency loss. In this subsection, we carry out an ablation study on the Freiburg Forest Dataset to investigate the impact of each individual loss term. In this study, we considered three alternative loss functions: (a)  only adversarial loss, (b) addition of adversarial loss and the latent consistency loss and the (c) total loss. Obtained image constructions for each case for a set of sample images are shown in <ref type="figure">Figure 8</ref>. Images generated in case (a) are plausible forest images; but they are very different from the ground truth. This is because the adversarial loss doesn't take semantic information into account. Comparatively, images in case (b) shares more semantics with ground truth. For example, in row 1, some trees are generated at the right side of the road when compared with case (a). Further, image artifacts present at the left side in case (b) have disappeared in case (c). Addition of the cycle consistency loss in case (c), increases the coherence between the output and the ground truth even more compared with the earlier cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>Target Outputs <ref type="figure">Figure 8</ref>: Results of the ablation study carried out on the hyperspectral-to-visible translation task.</p><p>Impact of Multiple Inputs. Three experiments performed in this section are of different levels of difficulty. The hyperspectral-to-visible image translation task is the easiest task due to the low variance in the scenes in the dataset. In such scenarios, even a single modality is able to produce a reasonable translation. However, we note that introducing an additional modality has improved the performance. In comparison, the colorization task is more challenging due to the availability of diverse scenes. As a result, a single modality was not able to perform colorization satisfactorily. In this case, multi-image-to-image translation was able to induce a high improvement in terms of visual quality by using two informative input modalities. The final case, synthetic-to-real image translation, is very challenging. We note that the depth modality in this case is not very informative since it leads to image constructions of sub-standard quality. In comparison, the RGB synthetic image modality resulted in better translations. Using both modalities has improved the visual quality of the output. But this improvement was marginal as compared to the case of the colorization task. In summary, multiple modalities generally improve the visual quality of the output image; specially when the translation is more challenging. However, the amount of improvement introduced was dependent on the informativeness of the second modality. In fact, introducing a noisy modality for the sake of having multiple inputs would not contribute towards an improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduced multi-image-to-image translation problem. We proposed a multi-modal generator struc-ture and a GAN based framework as the initial direction to solve the problem. We tested the proposed method across three tasks against state-of-the-art unsupervised image-toimage translation methods. We showed that using multiple image modalities improves the visual quality of the output compared with results generated by the state-of-the-art methods. We analyzed the behavior of the proposed method in the limiting case and provided discussion as to when the use of multiple image modalities is most suited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplimetary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Detailed Network Architecture</head><p>In this section, we provide details about the architecture used in the generator network for the cases when two and three input modalities are used. In both cases there are two generators; one for the forward transformation and the other for the reverse transformation. These generators follow the structure shown in <ref type="figure">Figure 3</ref> in the main paper.</p><p>When two input modalities are available, the used generator structure is shown in <ref type="figure" target="#fig_1">Figure 4</ref> in the paper in modular form. Exact details of each of these modules are tabulated in <ref type="table" target="#tab_3">Table 2</ref>. Details of the reverse transformation network shown in <ref type="figure" target="#fig_1">Figure 4</ref> in the paper can be found in <ref type="table">Table 3</ref>. Here, Conv refers to a collection of convolution, instance normalization and relu layers. Deconv refers to a collection of transposed convolution, instance normalization and relu layers. Res refers to a Resnet block. Feature Extraction S1</p><p>Conv S11 Image 1 Conv S12 1 × 7 × 7 × 64 (1,0) Conv S12</p><p>Conv S11 Conv S13 1 × 3 × 3 × 128 (2,1) Conv S13</p><p>Conv S12 Res S11 1 × 3 × 3 × 256 (2,1) Res S11</p><p>Conv S13 Res S12 1 × 3 × 3 × 512 (1,0) Res S12</p><p>Res S11 Res S13 1 × 3 × 3 × 1024 (1,0) Res S13</p><p>Res S12 Res S14 1 × 3 × 3 × 1024 (1,0) Res S14</p><p>Res S13</p><formula xml:id="formula_12">Conv F 1 × 3 × 3 × 1024 (1,0) Feature Extraction S2 Conv S21 Image 2 Conv S22 1 × 7 × 7 × 64 (1,0) Conv S22 Conv S21 Conv S23 1 × 3 × 3 × 128 (2,1) Conv S23 Conv S22 Res S21 Input(NIR) Input (Greyscale) Target (RGB) Ours CycleGAN (NIR)</formula><p>CycleGAN (Grey) <ref type="bibr" target="#b24">[29]</ref> CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (NIR) <ref type="bibr" target="#b6">[11]</ref> UNIT (Grey) [11] <ref type="figure">Figure 9</ref>: Results corresponding to the image colorization task (EPFL NIR-VIS Dataset -country category). Colorization achieved by the proposed method is closer to the ground truth compared with the baseline methods. This is most evident in the color of tree leaves, grass and the sky.</p><formula xml:id="formula_13">Input(NIR) Input (Greyscale) Target (RGB) Ours CycleGAN (NIR)</formula><p>CycleGAN (Grey) <ref type="bibr" target="#b24">[29]</ref> CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (NIR) <ref type="bibr" target="#b6">[11]</ref> UNIT (Grey) [11] <ref type="figure">Figure 10</ref>: Results corresponding to the image colorization task (EPFL NIR-VIS Dataset -mountain category). Both Cycle-GAN(NIR) and UNIT(NIR) has produced results on par with the proposed method. However, valleys are colored in green in CycleGAN. UNIT has a dark gray shade imposed over the whole image. Comparatively the proposed method is more closer to the ground truth. CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (NIR) <ref type="bibr" target="#b6">[11]</ref> UNIT (Grey) [11] <ref type="figure">Figure 11</ref>: Results corresponding to the image colorization task (EPFL NIR-VIS Dataset -urban category). Color of sky, tree and the building of the proposed method's output is more closer to the ground truth compared to other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input(NIR) Input (Greyscale) Target (RGB) Ours CycleGAN (NIR)</head><p>CycleGAN (Grey) <ref type="bibr" target="#b24">[29]</ref> CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (NIR) <ref type="bibr" target="#b6">[11]</ref> UNIT (Grey) [11] <ref type="figure">Figure 12</ref>: Results corresponding to the image colorization task (EPFL NIR-VIS Dataset -water category). CycleGAN (NIR) has the closest result in terms of quality to the proposed method. However, water has a yellow shade mixed to it and the sky seems more blue compared to the ground truth. These differences are not present in our output. CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (RGB) <ref type="bibr" target="#b6">[11]</ref> UNIT (Depth) [11] <ref type="figure">Figure 13</ref>: Results corresponding to the Synthetic-to-Real translation task (sample 00400). CycleGAN (RGB) and Cycle-GAN (Concat) have distorted constructions. The pole color is inverted; distortions are visible on cars and buildings; building far away are mistaken as trees. UNIT(RGB) has only changed the color scheme of the output. Characteristics of artificial graphics are apparent (texture of the road for an example). Proposed method produces a more realistic construction comparatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input(RGB) Input (Depth) Ours CycleGAN (RGB)</head><p>CycleGAN (Depth) <ref type="bibr" target="#b24">[29]</ref> CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (RGB) <ref type="bibr" target="#b6">[11]</ref> UNIT (Depth) [11] <ref type="figure" target="#fig_1">Figure 14</ref>: Results corresponding to the Synthetic-to-Real translation task (sample 00600). In addition to points made in <ref type="figure">Figure 13</ref>, note that both CycleGAN (RGB) and CycleGAN (Concat) have erroneously generated clutter in the left pavement. CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (RGB) <ref type="bibr" target="#b6">[11]</ref> UNIT (Depth) [11] <ref type="figure">Figure 15</ref>: Results corresponding to the Synthetic-to-Real translation task (sample 00800). CycleGAN (RGB) output is similar to the output of the proposed method to an extent in this case. Note there are some distortions present in walls in either side of the street in CycleGAN (RGB) output image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input(RGB) Input (Depth) Ours CycleGAN (RGB)</head><p>CycleGAN (Depth) <ref type="bibr" target="#b24">[29]</ref> CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (RGB) <ref type="bibr" target="#b6">[11]</ref> UNIT (Depth) [11] <ref type="figure">Figure 16</ref>: Results corresponding to the Synthetic-to-Real translation task (sample 00700). Both CycleGAN (RGB) and CycleGAN (Concat) have failed to identify the shadow of the tree in their respective outputs. Compared to the output of the proposed method, both these outputs have considerable distortions on the walls at either side of the street. CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (NIR) <ref type="bibr" target="#b6">[11]</ref> UNIT (Depth) [11] <ref type="figure" target="#fig_3">Figure 17</ref>: Results corresponding to the hyperspectral-to-visible translation task (image b118-047). White color artifacts present in CycleGAN (NIR) is more apparent in CycleGAN (Concat). Image Fusion (CycleGAN) has produced an image with less artifacts. However, the proposed method produces the closest reconstruction to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input(NIR) Input (Depth) Target (RGB) Ours CycleGAN (NIR)</head><p>CycleGAN (Depth) <ref type="bibr" target="#b24">[29]</ref> CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (NIR) <ref type="bibr" target="#b6">[11]</ref> UNIT (Depth) [11] <ref type="figure">Figure 18</ref>: Results corresponding to the hyperspectral-to-visible translation task (image b197-61). CycleGAN (Concat) have produced a result on par with the proposed method while there are observable deficiencies in other outputs. However, in terms of colors (trees at the left and the grass), latter is more closer to the ground truth. CycleGAN (Concat) <ref type="bibr" target="#b24">[29]</ref> Image Fusion (CycleGAN) <ref type="bibr" target="#b24">[29]</ref> UNIT (NIR) <ref type="bibr" target="#b6">[11]</ref> UNIT (Depth) [11] <ref type="figure">Figure 19</ref>: Results corresponding to the hyperspectral-to-visible translation task (image b304-36). Both CycleGAN (Concat) and the proposed method produces results of similar quality for this image. Therefore, feature fusion and pixel-level fusion both have similar performances for this image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Generator architecture of for I n 2I when two input modalities are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results for the hyperspectral-to-visible translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>[3] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Learning Research, 17</head><label>17</label><figDesc>:59:1-59:35, 2016. 3 [5] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Ve-gas, NV, USA, June 27-30, 2016, pages 770-778, 2016. 5 [7] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Mean PSNR and SSIM values for image colorization and hyperspectral-to-real image translation tasks. Variance is indicated within the brackets.</figDesc><table><row><cell cols="2">EPFL NIR-VIS</cell><cell></cell><cell cols="3">Freiburg Forest Dataset</cell></row><row><cell>Method</cell><cell>PSNR</cell><cell>SSIM</cell><cell>Method</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>Ours</cell><cell>23.113</cell><cell>0.739</cell><cell>Ours</cell><cell>19.444</cell><cell>0.584</cell></row><row><cell>(NIR+Grey)</cell><cell>(9.147)</cell><cell>(0.008)</cell><cell>(NIR+Depth)</cell><cell>(7.059)</cell><cell>(0.012)</cell></row><row><cell>UNIT</cell><cell>8.324</cell><cell>0.041</cell><cell>UNIT</cell><cell>9.681</cell><cell>0.414</cell></row><row><cell>(Grey)</cell><cell>(2.219)</cell><cell>(0.018)</cell><cell>(Depth)</cell><cell>(1.490)</cell><cell>(0.007)</cell></row><row><cell>UNIT</cell><cell>15.331</cell><cell>0.544</cell><cell>UNIT</cell><cell>9.494</cell><cell>0.382</cell></row><row><cell>(NIR)</cell><cell>(9.088)</cell><cell>(0.012)</cell><cell>(NIR)</cell><cell>(0.868)</cell><cell>(0.004)</cell></row><row><cell>CycleGAN</cell><cell>8.438</cell><cell>0.056</cell><cell>CycleGAN</cell><cell>16.5945</cell><cell>0.525</cell></row><row><cell>(Grey)</cell><cell>(2.939)</cell><cell>(0.018)</cell><cell>(Depth)</cell><cell>(4.308)</cell><cell>(0.010)</cell></row><row><cell>CycleGAN</cell><cell>17.381</cell><cell>0.657</cell><cell>CycleGAN</cell><cell>18.574</cell><cell>0.552</cell></row><row><cell>(NIR)</cell><cell>(9.345)</cell><cell>(0.018)</cell><cell>(NIR)</cell><cell>(3.252)</cell><cell>(0.014)</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>Ours (All</cell><cell>21.65</cell><cell>0.600</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Inputs)</cell><cell>(2.302)</cell><cell>(0.0105)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Network details corresponding to the forward transformation generator for n = 2.</figDesc><table><row><cell>Layer</cell><cell>Input</cell><cell>output</cell><cell>Kernel</cell><cell>(stride,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>pad)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by US Office of Naval Research (ONR) Grant YIP N00014-16-1-3134.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We carried out a single experiment using three modalities using the hyperspectral-to-visible image translation task. We outline the network architecture in both the for- Res S15, Res S25</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity</head><p>Decoder S1</p><p>Res S15 Latent res S16 1 × 3 × 3 × 1024 (1,0) Res S16</p><p>Res S15</p><p>Res S16 res S18 1 × 3 × 3 × 1024 (1,0) Res S18</p><p>Res S17 Res S19 1 × 3 × 3 × 512 (1,0) Res S19</p><p>Res S18 Deconv S11</p><p>Deconv S11</p><p>Res S19 Deconv S12</p><p>Deconv S12</p><p>Deconv S11</p><p>Deconv S13</p><p>Deconv S13</p><p>Deconv S12</p><p>ward and the reverse transformations in <ref type="table">Tables 4 and 5</ref>, respectively. Feature Extraction S1</p><p>Conv S11 Image 1 Conv S12 1 × 7 × 7 × 64 (1,0) Conv S12</p><p>Conv S11 Conv S13 1 × 3 × 3 × 128 (2,1) Conv S13</p><p>Conv S12 Res S11 1 × 3 × 3 × 256 (2,1) Res S11</p><p>Conv S12 Res S12 1 × 3 × 3 × 512 (1,0) Res S12</p><p>Res S11 Res S13 1 × 3 × 3 × 1024 (1,0) Res S13</p><p>Res S12 Res S14</p><p>Res <ref type="formula">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity</head><p>Decoder S1</p><p>Res S15 Latent res S16 1 × 3 × 3 × 1024 (1,0) Res S16</p><p>Res S15 ResS17 1 × 3 × 3 × 1024 (1,0) Res S17</p><p>Res S16 res S18 1 × 3 × 3 × 1024 (1,0) Res S18</p><p>Res S17 Res S19 1 × 3 × 3 × 512 (1,0) Res S19</p><p>Res S18 Deconv S11</p><p>Deconv S11</p><p>Res S19 Deconv S12</p><p>Deconv S12</p><p>Deconv S11</p><p>Deconv S13</p><p>Deconv S13</p><p>Deconv S12</p><p>Output S1</p><p>Deconv S33</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deconv S32</head><p>Output S3 1 × 7 × 7 × 3 (1,0)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Additional Experimental Results</head><p>In this section, we present results reported in the main paper with a higher resolution. In addition, we present the following two additional baseline comparisons. 1. CycleGAN (Concat). Input of multiple modalities are concatenated as channels. Operating on the concatenated input, cycleGAN is used to find the relevant transformation. 2. Image Fusion. Input images are first fused using a wavelet-based image fusion technique. In particular, wavelet coefficients of each input modality is found independently using db4 wavelet. In the wavelet domain, coefficients are fused by taking the average over all modalities. Fused coefficients are transformed back to the image domain by taking inverse wavelet transformation. Then, Cy-cleGAN is operated on the fused image.</p><p>Results corresponding to the image colorization task are shown in <ref type="figure">Figures 9,10,11</ref> and 12. In all these cases, the proposed method yields more realistic colorization. In <ref type="figure">Figures  13,14,15 and 16</ref> results obtained for synthetic-to-real translation are shown. As described in the main paper, the pro-posed method has performed a translation of higher quality in this task as well. The third task, hyper-spectral-to-visual image translation, is the easier task among the three tasks. Therefore, CycleGAN (NIR), CycleGAN(Concat) and Image Fusion(CycleGAN) are able to produce results on par with the proposed method ( <ref type="figure">Figure 19</ref>). However, in images 17 and 18, the proposed method is able to produce images of distinguishable higher quality compared with the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Pixel-level Fusion vs Feature Fusion</head><p>In the proposed network, information of input modalities are fused at the beginning of the encoder sub-network. In principle, fusion can carried out as pixel-level fusion, feature fusion or decision fusion <ref type="bibr" target="#b9">[14]</ref>. Since the task in hand takes the form of image reconstruction, decision fusion is not applicable. In our method, we utilize the feature fusion technique where we first extract some feature maps from each modality and fuse them together using a convolution operation. In principle, it is also possible to use pixel-level fusion for this task. For example, pixel-level fusion can be performed by training a CycleGAN model where a concatenation of all n inputs are provided as the input to the network (CycleGAN (Concat) in our experiments).</p><p>However, when the input modalities are from incompatible domains, pixel-level fusion may result in incoherent reconstructions. In order to illustrate this, we direct readers' attention to CycleGAN (Concat) results shown in <ref type="figure">Fig</ref>  <ref type="figure">Figure 10</ref> much better than the fused version has. In this case, pixel-level fusion in fact has deteriorated the performance as compared to the original case. This trend can be observed across all three tasks. However, the performance of CycleGAN(Concat) is reasonable in most images (except for <ref type="figure">Figure 17</ref>), in the hyper-spectral-to-visible translation task. In this special case, pixel-level fusion has worked effectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multispectral SIFT for scene category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR11)</title>
		<meeting><address><addrLine>Colorado Springs</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">translation with conditional adversarial networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Image Fusion: Theories, Techniques and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Publishing Company</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Incorporated, 1st edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A wavelet-based image fusion tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pajares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cruz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Elsevier Journal of Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1855" to="1872" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2058" to="2065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep multispectral semantic scene understanding of forested environments using multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 International Symposium on Experimental Robotics (ISER 2016)</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
