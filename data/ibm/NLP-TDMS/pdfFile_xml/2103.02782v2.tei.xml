<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Boosting, Suppression, and Diversification for Fine-Grained Visual Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Song</surname></persName>
							<email>songjianwei@smail.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Yang</surname></persName>
							<email>yangry@nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Boosting, Suppression, and Diversification for Fine-Grained Visual Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning feature representation from discriminative local regions plays a key role in fine-grained visual classification. Employing attention mechanisms to extract part features has become a trend. However, there are two major limitations in these methods: First, they often focus on the most salient part while neglecting other inconspicuous but distinguishable parts. Second, they treat different part features in isolation while neglecting their relationships. To handle these limitations, we propose to locate multiple different distinguishable parts and explore their relationships in an explicit way. In this pursuit, we introduce two lightweight modules that can be easily plugged into existing convolutional neural networks. On one hand, we introduce a feature boosting and suppression module that boosts the most salient part of feature maps to obtain a part-specific representation and suppresses it to force the following network to mine other potential parts. On the other hand, we introduce a feature diversification module that learns semantically complementary information from the correlated part-specific representations. Our method does not need bounding boxes/part annotations and can be trained end-to-end. Extensive experimental results show that our method achieves state-of-the-art performances on several benchmark fine-grained datasets. Source code is available at https://github.com/chaomaer/FBSD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Fine-grained visual classification (FGVC) focuses on distinguishing subtle visual differences within a basic-level category, e.g., species of birds <ref type="bibr" target="#b0">[1]</ref> and dogs <ref type="bibr" target="#b1">[2]</ref>, and models of aircrafts <ref type="bibr" target="#b2">[3]</ref> and cars <ref type="bibr" target="#b3">[4]</ref>. Recently, convolutional neural networks (CNNs) have made great progress on many tasks, such as face recognition <ref type="bibr" target="#b4">[5]</ref>, automatic driving <ref type="bibr" target="#b5">[6]</ref>, Pedestrian re-identification <ref type="bibr" target="#b6">[7]</ref>, intelligent logistics in IOT and so on. However, traditional CNNs are not powerful enough to capture the subtle discriminative features due to the large intra-class and small inter-class variations as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, which makes FGVC still a challenging task. Therefore, how to make CNNs locate the distinguishable parts and learn discriminative features are important issues that need to be addressed. Early works <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b8">[9]</ref> [10] <ref type="bibr" target="#b10">[11]</ref> relied on predefined bounding boxes and part annotations to capture visual differences. However, collecting extra annotated information is labor-intensive and requires professional knowledge, which makes these methods less practical. Hence, researchers recently have focused more on weakly-supervised FGVC that only needs image labels as supervision. There are two paradigms towards this direction. One is based on part features, these methods <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b12">[13]</ref>  <ref type="bibr" target="#b13">[14]</ref> [15] <ref type="bibr" target="#b16">[16]</ref> are often composed of two different subnetworks. Specifically, a localization subnetwork with attention mechanisms is designed for locating discriminative parts and a classification subnetwork is followed for recognition. The dedicated loss functions are designed to optimize both subnetworks. The limitation of these methods is that it is difficult to optimize because of the specially designed attention modules and loss functions. The other is based on high-order information, these methods <ref type="bibr" target="#b17">[17]</ref>  <ref type="bibr" target="#b18">[18]</ref>  <ref type="bibr" target="#b19">[19]</ref> [20] <ref type="bibr" target="#b21">[21]</ref> argue that the first-order information is not sufficient to model the differences and instead use high-order information to encode the discrimination. The limitation of these methods is that it takes up a lot of GPU resources and has poor interpretability.</p><p>We propose feature boosting, suppression, and diversification towards both efficiency and interpretability. We argue that attention-based methods tend to focus on the most salient part, so other inconspicuous but distinguishable parts have no chance to stand out <ref type="bibr" target="#b22">[22]</ref>. However, the network will be forced to mine other potential parts when masking or suppressing the most salient part. Based on this simple and effective idea, we introduce a feature boosting and suppression module (FBSM), which highlights the most salient part of feature maps at the current stage to obtain a part-specific representation and suppresses it to force the following stage to mine other potential parts. By inserting FBSMs into the middle layers of CNNs, we can get multiple part-specific feature representations that are explicitly concentrated on different object parts.</p><p>Intuitively, individual part-specific feature representation neglects the knowledge from the entire object and may not see the forest for the trees. To eliminate the bias, we introduce a feature diversification module (FDM) to diversify each part-specific feature representation. Specifically, given a partspecific representation, we enhance it by aggregating complementary information discovered from other parts. Through modeling the part interaction with FDM, we make the partspecific feature representation more discriminative and rich.</p><p>Finally, we jointly optimize FBSM and FDM as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Our method does not need bounding boxes/part annotations and state-of-the-art performances are reported on several standard benchmark datasets. Moreover, our model is lightweight and easy to train as it does not involve the multicrop mechanism <ref type="bibr" target="#b11">[12]</ref> [23] <ref type="bibr" target="#b12">[13]</ref>.</p><p>Our contributions are summarized as follows:</p><p>• We propose a feature boosting and suppression module, which can explicitly force the network to focus on multiple discriminative parts. • We propose a feature diversification module, which can model part interaction and diversify each part-specific representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Below, we review the most representative methods related to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fine-Grained Feature Learning</head><p>Ding et al. <ref type="bibr" target="#b16">[16]</ref> proposed sparse selective sampling learning to obtain both discriminative and complementary regions. Sun et al. <ref type="bibr" target="#b24">[24]</ref> proposed a one-squeeze multi-excitation module to learn multiple parts, then applied a multi-attention multiclass constraint on these parts. Zhang et al. <ref type="bibr" target="#b25">[25]</ref> proposed to discover contrastive clues by comparing image pairs. Yang et al. <ref type="bibr" target="#b13">[14]</ref> introduced a navigator-teacher-scrutinizer network to obtain discriminative regions. Luo et al. <ref type="bibr" target="#b26">[26]</ref> proposed Cross-X learning to explore the relationships between different images and different layers. Gao et al. <ref type="bibr" target="#b27">[27]</ref> proposed to model channel interaction to capture subtle differences. Li et al. <ref type="bibr" target="#b19">[19]</ref> proposed to capture the discrimination by matrix square root normalization and introduced an iterative method for fast end-to-end training. Shi et al. <ref type="bibr" target="#b28">[28]</ref> removed confusable features from the distinguishable parts to facilitate fine-grained classification. He et al. <ref type="bibr" target="#b29">[29]</ref> proposed progressive attention to localize parts at different scales. Our method utilizes feature boosting and suppression to learn different part representations in an explicit way, which is significantly different from previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Fusion</head><p>FPN <ref type="bibr" target="#b30">[30]</ref> and SSD <ref type="bibr" target="#b31">[31]</ref> aggregating feature maps from different layers have achieved great success in the object detection field. However, they use element-wise addition as the aggregation operation, making the capabilities of these methods still limited. Wang el at. <ref type="bibr" target="#b32">[32]</ref> proposed a non-local operation that computes the response at a spatial position as a weighted sum of the features at all positions in the feature maps. SG-Net <ref type="bibr" target="#b33">[33]</ref> utilized the non-local operation to fuse feature maps from different layers. CIN <ref type="bibr" target="#b27">[27]</ref> adopted the non-local operation to mine semantically complementary information from different feature channels. Our FDM is similar with <ref type="bibr" target="#b33">[33]</ref> and <ref type="bibr" target="#b27">[27]</ref>, but there are essential differences: (1) SG-Net tends to explore positive correlations to capture long-range dependencies, while FDM tends to explore negative correlations to diversify the feature representation. (2) CIN mines complementary information along channel dimension whereas FDM along the spatial dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we will detail the proposed method. An overview of the framework is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Our model consists of two lightweight modules: (1) A feature boosting and suppression module (FBSM) aiming at learning multiple discriminative part-specific representations as different as possible. <ref type="formula" target="#formula_1">(2)</ref> A feature diversification module (FDM) aiming at modeling part interaction to enhance each part-specific representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Boosting and Suppression Module</head><p>Given feature maps X ∈ R C×W ×H from a specific layer, where C, W , H represents the number of channels, width, and height respectively. Inspired by <ref type="bibr" target="#b6">[7]</ref>, we simply split X evenly into k parts along width dimension and denote each striped parts as X (i) ∈ R C×(W/k)×H , i ∈ [1, k]. Then we employ a 1 × 1 convolution φ to explore the importance of each part:</p><formula xml:id="formula_0">A (i) = Relu(φ(X (i) )) ∈ R 1×(W/k)×H<label>(1)</label></formula><p>The nonlinear function Relu <ref type="bibr" target="#b34">[34]</ref> is applied to remove the negative activations. φ is shared among different striped parts and acts as a grader. We then take the average of A (i) as the importance factor b i for X (i) , i.e.,</p><formula xml:id="formula_1">b i = GAP(A (i) ) ∈ R<label>(2)</label></formula><p>where GAP denotes global average pooling. We use softmax</p><formula xml:id="formula_2">to normalize B = (b 1 , · · · , b k ) T : b i = exp(b i ) j∈[1,k] exp(b j )<label>(3)</label></formula><p>With the normalized importance factors B = (b 1 , · · · , b k ) T , the most salient part can be determined immediately. We then obtain the boosting feature X b by boosting the most salient part:</p><formula xml:id="formula_3">X b = X + α * (B ⊗ X)<label>(4)</label></formula><p>where α is a hyper-parameter, which controls the extent of boosting, ⊗ denotes element-wise multiplication. A convolutional layer h is applied on X b to get a part-specific representation X p :  By suppressing the most striped part, we can obtain the suppression feature X s :</p><formula xml:id="formula_4">X p = h(X b )<label>(5)</label></formula><formula xml:id="formula_5">X s = S ⊗ X<label>(6)</label></formula><formula xml:id="formula_6">s i = 1 − β, if b i = max(B) 1, otherwise<label>(7)</label></formula><p>where S = (s 1 , · · · , s k ) T , β is a hyper-parameter, which controls the extent of suppressing.</p><p>In short, the functionality of FBSM can be expressed as: FBSM(X) = (X p , X s ). Given feature maps X, FBSM outputs part-specific feature X p and potential feature maps X s . Since X s suppresses the most salient part in current stage, other potential parts will stand out after feeding X s into the following stage. A diagram of the FBSM is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Diversification Module</head><p>As learning discriminative and diverse feature plays a key role in FGVC <ref type="bibr" target="#b35">[35]</ref> [24] <ref type="bibr" target="#b23">[23]</ref>, we propose a feature diversification module, which enhances each part-specific feature by aggregating complementary information mined from other part-specific representations.</p><p>We first discuss how two part-specific features diversify each other with the pairwise complement module (PCM). A simple illustration of PCM is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Without loss of generality, we denote two different part-specific features as X p1 ∈ R C×W1H1 and X p2 ∈ R C×W2H2 , where C denotes the number of channels, W 1 H 1 and W 2 H 2 denote their spatial size respectively. We use subscript p i to denote that X pi focuses on the i th part of the object and will omit the subscript when there is no ambiguity. We denote the feature vector at each spatial position along channel dimension as a pixel, i.e.,</p><formula xml:id="formula_7">pixel(X, i) = (X 1,i , · · · , X C,i ) T<label>(8)</label></formula><p>We first calculate the similarities between pixels in X p1 and pixels in X p2 :</p><formula xml:id="formula_8">M = f (X p1 , X p2 ), f (X, Y ) = X T Y<label>(9)</label></formula><p>Here, we use inner product to compute the similarity. The element M i,j represents the similarity of the i th pixel of X p1 and the j th pixel of X p2 . The lower the similarity of two pixels is, the more complementary they are, so we adopt −M as the complementary matrix. Then we operate normalization on −M row-wise and column-wise respectively:</p><formula xml:id="formula_9">A p2 p1 = softmax(−M T ) ∈ [0, 1] W2H2×W1H1 (10) A p1 p2 = softmax(−M ) ∈ [0, 1] W1H1×W2H2<label>(11)</label></formula><p>where softmax is performed column-wise. Then we can get the complementary information:</p><formula xml:id="formula_10">Y p2 p1 = X p2 A p2 p1 ∈ R C×W1H1 (12) Y p1 p2 = X p1 A p1 p2 ∈ R C×W2H2<label>(13)</label></formula><p>where Y pj pi denotes the complementary information of X pi relative to X pj . It is worth noting that each pixel of Y p2 p1 can be written as:</p><formula xml:id="formula_11">pixel(Y p2 p1 , i) = j∈[1,W2H2] (A p2 p1 ) i,j * pixel(X p2 , j)<label>(14)</label></formula><p>i.e., each pixel of Y p2 p1 takes all pixels of X p2 as references, and the higher the complementarity between pixel(X p1 , i) and pixel(X p2 , j) is, the greater the contribution of pixel(X p2 , j) to pixel(Y p2 p1 , i) is. In this way, every pixel in these two part-specific features can mine semantically complementary information from each other. Now we discuss the general case. Formally, given a collection of part-specific features P = {X p1 , X p2 , X p3 · · · , X pn }, the complementary information of X pi is:</p><formula xml:id="formula_12">Y pi = Xp j ∈P ∧i =j Y pj pi<label>(15)</label></formula><p>where Y pj pi can be obtained by applying X pi and X pj on (9), (10), and <ref type="bibr" target="#b11">(12)</ref>. In practice, we can compute Y pj pi and Y pi pj simultaneously as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Then we get the enhanced part-specific feature:</p><formula xml:id="formula_13">Z pi = X pi + γ * Y pi<label>(16)</label></formula><p>where γ is a hyper-parameter, which controls the extent of diversification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Design</head><p>Our method can be easily implemented on various convolutional neural networks. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we take Resnet <ref type="bibr" target="#b36">[36]</ref> as an example. The feature extractor of Resnet has five stages and the spatial size of feature maps is halved after each stage. Considering that the deep layers have more semantic information, we plug FBSMs into the end of stage 3 , stage 4 , stage <ref type="bibr" target="#b4">5</ref> . The different part-specific representations generated by FBSMs are fed into FDM to diversify each representation. Our method is highly customizable, it can adapt to different granularities of classification by adjusting the number of FBSMs directly.</p><p>At training time, we compute the classification loss for each enhanced part-specific feature Z pi :</p><formula xml:id="formula_14">L i cls = −y T log(p i ), p i = softmax(cls i (Z pi ))<label>(17)</label></formula><p>where y is the ground-truth label of the input image and represented by one-hot vector, cls i is a classifier for the i th part, p i ∈ R N is the prediction score vector, N is the number of object categories. The final optimization objective is:</p><formula xml:id="formula_15">L = T i=1 L i cls<label>(18)</label></formula><p>where T = 3 is the number of enhanced part-specific features. At inference time, we take the average of prediction scores for all enhanced part-specific features as the final prediction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Baselines</head><p>We evaluate our model on four commonly used datasets: CUB-200-2011 <ref type="bibr" target="#b0">[1]</ref>, FGVC-Aircraft <ref type="bibr" target="#b2">[3]</ref>, Stanford Cars <ref type="bibr" target="#b3">[4]</ref>, Stanford Dogs <ref type="bibr" target="#b1">[2]</ref>. The details of each dataset can be found in <ref type="table" target="#tab_0">Table I</ref>. We compare our model with following baselines due to their state-of-the-art results. All baselines are listed as follows:</p><p>• Part-RCNN <ref type="bibr" target="#b7">[8]</ref>: proposes geometric constraints on mined semantic parts to normalize the pose.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We validate the performance of our method on Resnet50, Resnet101 <ref type="bibr" target="#b36">[36]</ref> and Densenet161 <ref type="bibr" target="#b40">[40]</ref>, which are all pretrained on the ImageNet dataset <ref type="bibr" target="#b41">[41]</ref>. We insert FBSMs at the end of stage 3 , stage 4 and stage 5 . During training, the input images are resized to 550 × 550 and randomly cropped to 448 × 448. We apply random horizontal flips to augment the trainset. During testing, the input images are resized to 550 × 550 and cropped from center into 448 × 448. We set hyper-parameters α = 0.5, β = 0.5 and γ = 1.</p><p>Our model is optimized by Stochastic Gradient Descent with the momentum of 0.9, epoch number of 200, weight decay of 0.00001, mini-batch of 20. The learning rate of the backbone layers is set to 0.002, and the newly added layers are set to 0.02. The learning rate is adjusted by cosine anneal scheduler <ref type="bibr" target="#b42">[42]</ref>. We use PyTorch to implement our experiments. More implementation details can be referred to our code https://github.com/chaomaer/FBSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State-of-the-Art</head><p>The top-1 classification accuracy on CUB-200-2011 <ref type="bibr" target="#b0">[1]</ref>, FGVC-Aircraft <ref type="bibr" target="#b2">[3]</ref>, Stanford Cars <ref type="bibr" target="#b3">[4]</ref> and Stanford Dogs <ref type="bibr" target="#b1">[2]</ref> datasets are reported in <ref type="table" target="#tab_0">Table II</ref>.</p><p>Results on CUB-200-2011: CUB-200-2011 is the most challenging benchmark in FGVC, our models based on Resnet50, Resnet101, and Densenet161 all achieve the best performances on this dataset. Compared with DeepLAC and Part-RCNN which use predefined bounding boxes/part annotations, our method is 9.0%, 7.7% higher than them. Compared with the two-stage methods: RA-CNN, NTS, MGE-CNN, S3N, and FDL, which all take the raw image as input at the first stage to explore informative regions and takes them as input at the second stage, our model is 4.0%, 1.8%, 0.8%, 0.8%, 0.7% higher than them respectively. ISQRT-COV and DTB-Net explore high-order information to capture the subtle differences, our method outperforms them by large margins. Compared with API-Net and Cross-X, which both take image pairs as input and model the discrimination by part interaction, our model gets 1.6% improvements. The accuracy of our method is 3.1% higher than MAMC, which formulates part mining into a metric learning problem. Compared with MA-CNN, CIN, and LIO, our method is 2.8%, 1.9%, 1.3% higher than them respectively. DCL spots discriminative parts by diving into the destructed image, our method surpasses it by 1.5%. Notably, our method based on Resnet50 outperforms almost all other methods based on Resnet101.</p><p>Results on FGVC-Aircraft: Our method gets competitive results on this dataset. Compare with RA-CNN and MA-CNN, our method exceeds them by large margins. With Resnet50 backbone, our model is 2.7%, 1.5%, 1.3%, 0.1%, 0.1% higher than ISQRT-COV, DTB-Net, NTS, Cross-X, and CIN respectively. LIO that enhances feature learning by modeling object structure obtains the same result as our model. Our model is 0.1%, 0.3%, 0.3%, 0.7% lower than S3N, DCL, API-Net and FDL. However, S3N and FDL are both two-stage methods whereas our method is one-stage and more efficient. DCL destructs the image to locate discriminative regions, but it is not easy to define what level of destruction is appropriate. API-Net needs to consider different pairwise image combinations and requires large computing resources.</p><p>Results on Stanford Cars: Our method equipped with Resnet101 gets the best result on this dataset. Our method exceeds RA-CNN and MA-CNN which use VGG <ref type="bibr" target="#b43">[43]</ref> as backbone by large margins. With Resnet50 backbone, our method is higher than ISQRT-COV, MAMC, NTS, MEG-CNN, DTB-Net, CIN, and FDL but lower than DCL, LIO, Cross-X, S3N, and API-Net. We suspect that features extracted from shadow layers (stage 3 &amp;stage 4 ) lack rich semantic information, which may cause degradation of recognition performance. When deepening the network and taking Resnet101 as the backbone, we obtain the best result of 95.0%.</p><p>Results on Stanford Dogs: Most previous methods do not report results on this dataset because of the computational complexity. Our method obtains a competitive result on this </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>We perform ablation studies to understand the contributions of each proposed module. We make experiments on four datasets with Resnet50 as the backbone. The results are reported in <ref type="table" target="#tab_0">Table III</ref>.</p><p>The effect of FBSM: To obtain multiple discriminative part-specific feature representations, we insert FBSMs at the end of stage 3 , stage 4 and stage 5 of Resnet50. With this module, the accuracy of Bird, Aircraft, Car, and Dog increased by 3.4%, 2.1%, 4.2%, and 6.4% respectively, which reflects the effectiveness of the FBSM.</p><p>The effect of FDM: When introducing FDM into our approach to model part interaction, the classification results on Bird, Aircraft, Car, and Dog datasets increased by 0.4%, 0.3%, 0.4%, and 0.7% respectively, which indicates the effectiveness of the FDM. It is worth noting that FDM does not introduce additional learning parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization</head><p>We visualize the activation maps taken from Resnet50 without FBSMs, with FBSMs, and with FDM on four benchmark datasets. Specifically, the activation map is obtained by averaging the activation values across the channel dimension given feature maps. As shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, for each raw image sampled from four datasets, the activation maps at the first to third columns correspond to the third to fifth stages of Resnet50 respectively. We can observe that the network tends to focus on the most salient part without FBSMs and is forced to mine different parts when equipped with FBSMs. Taking the bird as an example, without FBSMs, the features at different stages all focus on the swing. When there are FBSMs, the features in stage 3 focus on the swing, the features in stage 4 focus on the head, and the features in stage 5 focus on the tail. When introducing FDM, the features in all stages focus on the entire parts mined by different stages. The visualization experiments prove the capability of FBSMs for mining multiple different discriminative object parts and FDM for diversifying feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose to learn feature boosting, suppression, and diversification for fine-grained visual classification. Specifically, we introduce two lightweight modules: One is the feature boosting and suppression module which boosts the most salient part of the feature maps to obtain the part-specific feature and suppresses it to explicitly force following stages to mine other potential parts. The other is the feature diversification module which aggregates semantically complementary information from other object parts to each part-specific representation. The synergy between these two modules helps the network to learn more discriminative and diverse feature representations. Our method can be trained end-to-end and does not need bounding boxes/part annotations. The state-of-the-art results are obtained on several benchmark datasets and ablation studies further prove the effectiveness of each proposed module. In the future, we will investigate how to adaptively divide feature maps into suitable patches to boost and suppress, instead of simple striped parts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of large intra-class and small inter-class variations in FGVC. The images with large variations in each row belong to the same class. However, the images with small variations in each column belong to different classes. This situation is opposite to generic visual classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overview of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The diagram of the FBSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The diagram of the PCM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•</head><label></label><figDesc>DeepLAC<ref type="bibr" target="#b8">[9]</ref>: integrates part localization, part alignment, and classification in one deep neural network. • S3N<ref type="bibr" target="#b16">[16]</ref>: learns to mine discriminative and complementary parts to enhance the feature learning. • API-Net<ref type="bibr" target="#b25">[25]</ref>: proposes an attentive pairwise interaction network to identify differences by comparing image pairs. • NTS<ref type="bibr" target="#b13">[14]</ref>: guides region proposal network by forcing the consistency between informativeness of the regions and their probabilities being ground-truth class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of the activation maps at different stages without FBSMs, with FBSMs, and with FDM on four benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I FOUR</head><label>I</label><figDesc>FINE-GRAINED DATASETS COMMONLY USED IN FGVC.</figDesc><table><row><cell>Dataset</cell><cell>Name</cell><cell cols="3">#Class #Train #Test</cell></row><row><cell>CUB-200-2011</cell><cell>Bird</cell><cell>200</cell><cell>5994</cell><cell>5794</cell></row><row><cell>FGVC-Aircraft</cell><cell>Aircraft</cell><cell>100</cell><cell>6667</cell><cell>3333</cell></row><row><cell>Stanford Cars</cell><cell>Car</cell><cell>196</cell><cell>8144</cell><cell>8041</cell></row><row><cell>Stanford Dogs</cell><cell>Dog</cell><cell>120</cell><cell>12000</cell><cell>8580</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>WITH STATE-OF-THE-ART METHODS ON FOUR FINE-GRAINED BENCHMARK DATASETS. "-" MEANS THE RESULT IS NOT MENTIONED IN THE RELEVANT PAPER. DCL<ref type="bibr" target="#b37">[37]</ref>: learns to destruct and construct the image to acquire the expert knowledge. MAMC<ref type="bibr" target="#b24">[24]</ref>: applies the multi-attention multi-class constraint in a metric learning framework to mine parts. MA-CNN<ref type="bibr" target="#b35">[35]</ref>: makes part mining and fine-grained features learning in a mutual reinforced way. RA-CNN<ref type="bibr" target="#b11">[12]</ref>: learns discriminative region attention at multiple scales recursively. Cross-X<ref type="bibr" target="#b26">[26]</ref>: proposes to learn multi-scale feature representation between different layers and different images. CIN<ref type="bibr" target="#b27">[27]</ref>: models channel interaction to mine semantically complementary information. FDL<ref type="bibr" target="#b12">[13]</ref>: proposes to enhance discriminative region attention by filtration and distillation learning. LIO<ref type="bibr" target="#b39">[39]</ref>: proposes to model internal structure of the object to enhance feature learning.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell cols="5">1-Stage CUB-200-2011 FGVC-Aircraft Stanford Cars Stanford Dogs</cell></row><row><cell>DeepLAC</cell><cell>VGG</cell><cell>×</cell><cell>80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Part-RCNN</cell><cell>VGG</cell><cell>×</cell><cell>81.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RA-CNN</cell><cell>VGG</cell><cell>×</cell><cell>85.3</cell><cell>88.1</cell><cell>92.5</cell><cell>87.3</cell></row><row><cell>MA-CNN</cell><cell>VGG</cell><cell></cell><cell>86.5</cell><cell>89.9</cell><cell>92.8</cell><cell>-</cell></row><row><cell>MAMC</cell><cell>Resnet50</cell><cell></cell><cell>86.2</cell><cell>-</cell><cell>92.8</cell><cell>84.8</cell></row><row><cell>NTS</cell><cell>Resnet50</cell><cell>×</cell><cell>87.5</cell><cell>91.4</cell><cell>93.3</cell><cell>-</cell></row><row><cell>API-Net</cell><cell>Resnet50</cell><cell></cell><cell>87.7</cell><cell>93.0</cell><cell>94.8</cell><cell>88.3</cell></row><row><cell>Cross-X</cell><cell>Resnet50</cell><cell></cell><cell>87.7</cell><cell>92.6</cell><cell>94.5</cell><cell>88.9</cell></row><row><cell>DCL</cell><cell>Resnet50</cell><cell></cell><cell>87.8</cell><cell>93.0</cell><cell>94.5</cell><cell>-</cell></row><row><cell>DTB-Net</cell><cell>Resnet50</cell><cell></cell><cell>87.5</cell><cell>91.2</cell><cell>94.1</cell><cell>-</cell></row><row><cell>CIN</cell><cell>Resnet50</cell><cell></cell><cell>87.5</cell><cell>92.6</cell><cell>94.1</cell><cell>-</cell></row><row><cell>LIO</cell><cell>Resnet50</cell><cell></cell><cell>88.0</cell><cell>92.7</cell><cell>94.5</cell><cell>-</cell></row><row><cell>ISQRT-COV</cell><cell>Resnet50</cell><cell></cell><cell>88.1</cell><cell>90.0</cell><cell>92.8</cell><cell>-</cell></row><row><cell>MGE-CNN</cell><cell>Resnet50</cell><cell>×</cell><cell>88.5</cell><cell>-</cell><cell>93.9</cell><cell>-</cell></row><row><cell>S3N</cell><cell>Resnet50</cell><cell>×</cell><cell>88.5</cell><cell>92.8</cell><cell>94.7</cell><cell>-</cell></row><row><cell>FDL</cell><cell>Resnet50</cell><cell>×</cell><cell>88.6</cell><cell>93.4</cell><cell>94.3</cell><cell>85.0</cell></row><row><cell>Ours</cell><cell>Resnet50</cell><cell></cell><cell>89.3</cell><cell>92.7</cell><cell>94.4</cell><cell>88.2</cell></row><row><cell>MAMC</cell><cell>Resnet101</cell><cell></cell><cell>86.5</cell><cell>-</cell><cell>93.0</cell><cell>85.2</cell></row><row><cell>DTB-Net</cell><cell>Resnet101</cell><cell></cell><cell>88.1</cell><cell>91.6</cell><cell>94.5</cell><cell>-</cell></row><row><cell>CIN</cell><cell>Resnet101</cell><cell></cell><cell>88.1</cell><cell>92.8</cell><cell>94.5</cell><cell>-</cell></row><row><cell>API-Net</cell><cell>Resnet101</cell><cell></cell><cell>88.6</cell><cell>93.4</cell><cell>94.9</cell><cell>90.3</cell></row><row><cell>ISQRT-COV</cell><cell>Resnet101</cell><cell></cell><cell>88.7</cell><cell>91.4</cell><cell>93.3</cell><cell>-</cell></row><row><cell>MGE-CNN</cell><cell>Resnet101</cell><cell>×</cell><cell>89.4</cell><cell>-</cell><cell>93.6</cell><cell>-</cell></row><row><cell>Ours</cell><cell>Resnet101</cell><cell></cell><cell>89.5</cell><cell>93.1</cell><cell>95.0</cell><cell>89.4</cell></row><row><cell>FDL</cell><cell>Densenet161</cell><cell></cell><cell>89.1</cell><cell>91.3</cell><cell>94.0</cell><cell>84.9</cell></row><row><cell>Ours</cell><cell>Densenet161</cell><cell></cell><cell>89.8</cell><cell>93.2</cell><cell>94.5</cell><cell>88.1</cell></row><row><cell cols="4">DBT-Net [38]: performs bilinear transformation on each</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">semantically consistent channel group to model high-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>order information.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>• MGE-CNN [15]: learns a mixture of granularity-specific experts to capture granularity-specific parts.••••• ISQRT-COV [17]: utilizes bilinear information to model pairwise interaction.•••••</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDIES ON FOUR BENCHMARK DATASETS CNN, MAMC, and FDL by large margins. Compared with Cross-X and API-Net which take image pairs as input, our method does not need to consider how to design a non-trivial data sampler to sample inter-class and intra-class image pairs<ref type="bibr" target="#b44">[44]</ref>.In summary, due to the simplicity and effectiveness of our model, it scales well to all four benchmark datasets. With Resnet50 backbone, API-Net and Cross-X obtain the best result on Stanford Cars and Stanford Dogs respectively, but both get poor results on CUB-200-2011. FDL obtains the best result on FGVC-Aircraft but behaves inferiorly on Stanford Dogs. Our model achieves the best result on CUB-200-2011 and relatively competitive results on the other three datasets.</figDesc><table><row><cell>Methods</cell><cell cols="2">Bird Aircraft</cell><cell>Car</cell><cell>Dog</cell></row><row><cell>Resnet50</cell><cell>85.5</cell><cell>90.3</cell><cell cols="2">89.8 81.1</cell></row><row><cell>Resnet50+FBSM</cell><cell>88.9</cell><cell>92.4</cell><cell cols="2">94.0 87.5</cell></row><row><cell>Resnet50+FBSM+FDM</cell><cell>89.3</cell><cell>92.7</cell><cell cols="2">94.4 88.2</cell></row><row><cell>dataset and surpasses RA-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finegrained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="834" to="849" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Part-stacked cnn for finegrained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4476" to="4484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="438" to="454" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning a mixture of granularity-specific experts for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6598" to="6607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ser. ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ser. ICCV &apos;15<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning granularity-aware convolutional neural network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02788</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-grained recognition: Accounting for subtle differences between similar classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="834" to="850" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13130" to="13137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Channel interaction networks for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond the attention: Distinguish the discriminative and confusable features for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="601" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning rich part hierarchies with progressive attention networks for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="476" to="488" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="21" to="37" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Salience-guided cascaded suppression network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4277" to="4286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Look-into-object: Self-supervised structure modeling for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
