<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SpinalNet: Deep Neural Network with Gradual Input</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Dipu Kabir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moloud</forename><surname>Abdar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed</forename><surname>Mohammad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE;</roleName><forename type="first">Jafar</forename><surname>Jalali</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Abbas</forename><surname>Khosravi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Amir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE;</roleName><forename type="first">F</forename><surname>Atiya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE;</roleName><forename type="first">Saeid</forename><surname>Nahavandi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE.</roleName><forename type="first">Dipti</forename><surname>Srinivasan</surname></persName>
						</author>
						<title level="a" type="main">SpinalNet: Deep Neural Network with Gradual Input</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-DNN</term>
					<term>CNN</term>
					<term>AdaNet</term>
					<term>ResNet</term>
					<term>VGG</term>
					<term>Transfer Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past few years, deep neural networks (DNNs) have garnered remarkable success in a diverse range of real-world applications. However, DNNs consider a large number of inputs and consist of a large number of parameters, resulting in high computational demand. We study the human somatosensory system and propose the SpinalNet to achieve higher accuracy with less computational resources. In a typical neural network (NN) architecture, the hidden layers receive inputs in the first layer and then transfer the intermediate outcomes to the next layer. In the proposed SpinalNet, the structure of hidden layers allocates to three sectors: 1) Input row, 2) Intermediate row, and 3) output row. The intermediate row of the SpinalNet contains a few neurons. The role of input segmentation is in enabling each hidden layer to receive a part of the inputs and outputs of the previous layer. Therefore, the number of incoming weights in a hidden layer is significantly lower than traditional DNNs. As all layers of the SpinalNet directly contributes to the output row, the vanishing gradient problem does not exist. We also investigate the SpinalNet fullyconnected layer to several well-known DNN models and perform traditional learning and transfer learning. We observe significant error reductions with lower computational costs in most of the DNNs. We have also obtained the state-of-the-art (SOTA) performance for QMNIST, Kuzushiji-MNIST, EMNIST (Letters, Digits, and Balanced), STL-10, Bird225, Fruits 360, and Caltech-101 datasets. The scripts of the proposed SpinalNet are available with the following link: https://github.com/dipuk0506/SpinalNet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep Neural Networks (DNNs) have brought the state of the art performance in various scientific and engineering fields <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. DNNs usually have a large number of input features, as the consideration of more parameters usually improves the accuracy of the prediction. The size of the first hidden layer is critical. A small first hidden layer fails to propagate all input features properly while a large first hidden layer increases the number of weights drastically. Another limitation of the traditional DNNs is the vanishing gradient. When the number of layers is large, the gradient is high at parameters near H M Dipu Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali, Abbas Khosravi, and Saeid Nahavandi are with Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Australia. (Email: {hussain.kabir, mabdar, sjalali, abbas.khosravi, saeid.nahavandi}@deakin.edu.au) Amir F Atiya in with Cairo University. Email: amir@alumni.caltech.edu Dipti Srinivasan is with National University of Singapore. Email: dipti@nus.edu.sg Manuscript received -, 2020; accepted---. output, and the gradient becomes negligible at parameters near inputs. DNN training becomes difficult due to the vanishing gradient problem.</p><p>The human brain also receives a lot of information from our skin. Numerous tactile sensory neurons send the sense of touch, heat, vibration, etc. from different parts of our body. They can sense pressure, heat, vibrations, complex-textures, hardness, state of matter, etc. <ref type="bibr" target="#b4">[5]</ref>. Humans can have different touch sensitivity over time. Although the exact mechanism is unknown to humans, the current knowledge base states a tremendous function of our spinal cord neurons. The human spinal cord receives senses of touch from different locations in different parts of it. Multiple vertebrae can be connected to one internal organ too. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> presents simplified rough connections between human touch-sensors and the spinal cord. Researchers develop convolutional neural networks (CNN) by mimicking the functionality of the cats' visual cortex, and that brings a significant improvement in the accuracy of NNs <ref type="bibr" target="#b5">[6]</ref>. The miraculous spinal architecture of humans and the recent success of CNNs motivate us to develop a neural network with gradual inputs.</p><p>A well-known approach to reducing computation is pooling <ref type="bibr" target="#b6">[7]</ref>. However, pooling causes loss of information. Popular solutions to the vanishing gradient problem are ResNet and DenseNet. They allow shortcut connections over different layers. Therefore, the gradient remains high at neurons near the input <ref type="bibr" target="#b7">[8]</ref>. ResNets always provide better performance with increasing depth and can be as deep as thousands of layers. However, there is a slight marginal improvement in ResNet with increased depth. Moreover, very deep ResNets have a problem of diminishing feature reuse. Therefore, Sergey et al. proposed wide residual networks <ref type="bibr" target="#b8">[9]</ref> and achieved superior performance. Zifeng et al. also received superior performance with shallow and wide NNs <ref type="bibr" target="#b9">[10]</ref>. Gao et al. propose DenseNet, where all layers are connected <ref type="bibr" target="#b10">[11]</ref>. DenseNet training is faster and provides better performance in most situations due to two reasons i) all layers of DenseNet are connected, ii) they made dense-net narrower than the ResNet. When all layers are connected, the gradient and feature-reuse do not vanish over layers. However, as all layers are connected, an increment of the network size by one layer needs connections to that layer from all existing layers. Therefore, deep DenseNets are computationally expensive. Adaptive Structural Learning of Neural Networks(AdaNet) performs both connecting neurons and optimizing weights during the training. Consideration of all possible connections of a DNN is computationally intensive. The inauguration of a new neuron requires the consideration of connecting the neuron to other neurons. Therefore, AdaNet is suitable for shallow NNs <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Although DNNs have achieved SOTA performances in several fields, DNNs still suffers from a large computational overhead during training and execution <ref type="bibr" target="#b13">[14]</ref>. This paper proposes the SpinalNet, presented in <ref type="figure" target="#fig_0">Fig. 1</ref>(b) to improve performance with smaller computational overhead. The proposed structure with gradual and repetitive input capabilities enables NNs to achieve promising results with fewer parameters. We investigate the proposed SpinalNet as the fully connected layer of the VGG-5 network and receive SOTA performance in four MNIST datasets. We apply the transfer learning with Spinal-Net fully connected layer and receive SOTA performance in STL-10, Bird225, Fruits 360, and Caltech-101 datasets. We investigate SpinalNet and its variations in more than seventeen different datasets. We also receive promising results in other datasets with at least one variant of SpinalNet.</p><p>We organize the rest of the paper as follows: Section II presents the theoretical background of SpinalNet. The section discusses the similarity between the human spinal cord and the proposed SpinalNet, proves the universal approximation of the SpinalNet, and discuss the transfer learning. Section III reports the experimental results of SpinalNet with other competitors for solving regression and classification problems, The information of any touch or pain reaches the brain through the nerve plexus and the spinal cord. Nerve plexus is a network of intersecting nerves. Our spinal cord receives information gradually. Here, C5-C8 and T1 are vertebrae in the human skeleton <ref type="bibr" target="#b17">[18]</ref>.</p><p>Section IV presents the prospects of SpinalNet, and Section V is the concluding section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THEORETICAL BACKGROUND</head><p>Generally, there are two types of DNNs: convolutional and non-convolutional. The structure of non-convolutional NNs consist of fully connected input, hidden, and output layers. Deep CNNs contain convolutional layers and fully-connected layers. The number of parameters increases drastically due to the convolution and flattening procedures. The task of pooling is in reducing the number of network parameters <ref type="bibr" target="#b14">[15]</ref>. According to Vogt's work <ref type="bibr" target="#b15">[16]</ref>, there are two main points to the importance of pooling layers in DNN methods. Firstly, pooling declines the size of input for the next layer but it allows learning more mappings. Second, pooling assists in combining the output obtained from the prior layer on a large scale. However, pooling also causes the loss of information, as it considers only the aggregation of nearby values <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The information gathered from pooling layer is converted to a one-dimensional vector through flattening. At the end, fullyconnected layers compute the output from the one-dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Human Somatosensory System and the Spinal Cord</head><p>Although the exact mechanism of the human somatosensory system is not well understood, we find several similarities between the human spinal cord and the proposed neural network <ref type="bibr" target="#b18">[19]</ref>. Features we aimed to mimic are as follows:</p><p>1) Gradual input and nerve plexus.</p><p>2) Voluntary and involuntary actions.</p><p>3) Attention to pain intensity. Sensory neurons reach the spinal cord through a complex network, known as nerve plexus. <ref type="figure" target="#fig_1">Fig. 2</ref> represents a portion of nerve plexus. A single vertebra does not receive all of the information. The tactile sensory network covers millions of sensors. Furthermore, the human tactile system is more stable compared to the vision or the auditory systems, as there are a fewer number of touch-blind patients, compared to the number of blind patients. The nerve plexus network sends all tactile signals to the spinal cord gradually. Different locations of a spinal cord receive the pain of leg and the pain of hand <ref type="bibr" target="#b18">[19]</ref>. The neurons existing in the vertebral column are responsible for transferring the sense of touch to the brain and may take some actions. Our brain can control the spinal neurons to increase or decrease the pain intensity <ref type="bibr" target="#b19">[20]</ref>. Sensory neurons may also convey information to the lower-motor before getting instruction from the brain. This miraculous procedure is called involuntary or reflex movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed SpinalNet</head><p>The proposed SpinalNet has the following similarities with the above-mentioned features of the human spinal cord. 1) Gradual input 2) Local output and probable global influence 3) Weights reconfigured during training Similar to our spinal cord, the proposed SpinalNet takes inputs gradually and repetitively. Each layer of the SpinalNet contributes towards the local output (reflex). The SpinalNet also sends a modulated version of inputs towards the globaloutput (brain). The NN training process configures weights based on the training data, similar to our brain, configuring the spinal neurons for tuning the pain sensitivity of different sensories of our body. <ref type="figure" target="#fig_0">Fig. 1</ref> demonstrates the structure of the proposed SpinalNet. The network structure consists of an input row, an intermediate row, and an output row. The input is split and sent to the intermediate row of multiple hidden layers. In <ref type="figure" target="#fig_0">Fig. 1</ref>, the intermediate row, and the output row contain two neurons per each hidden layer. The number of output neurons per each hidden layer is equal to the number of outputs. The number of intermediate neurons can be changed according to the user. However, both the number of intermediate neurons and the number of inputs per layer is regularly kept small to reduce the number of multiplication. As typically the number of inputs and the intermediate hidden neurons per layer allocates a small amount, the network may have an under-fit shape. As a consequence, each layer receives inputs from the previous layer. Since the input is repeated, if one important feature of input does not impact on the output in one hidden layer, the feature may impact the output in another hidden layer. The intermediate row contains a nonlinear activation function and the output row contains the linear activation function. In <ref type="figure" target="#fig_0">Fig.  1(b)</ref>, the input values are split into three rows. These rows are assigned to the different hidden layers in a repeated manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Universal Approximation of the Proposed SpinalNet</head><p>Proposing a new NN architecture raises a question about its universal approximability <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Therefore, we prove the universal approximation. The traditional mathematical proof of the universal approximation theorem contains scholarly and esoteric equations. However, we aim to make the paper equation-free to attract general audiences. Therefore, we prove the universal approximation with the following approach. 1) Single hidden layer of a NN with large width is a universal approximator <ref type="bibr" target="#b22">[23]</ref>. 2) If we can prove that SpinalNet of a large number of layers is equivalent to the single hidden layer of a large width NN, the universal approximation is proved. <ref type="figure" target="#fig_2">Fig. 3</ref> presents how a simpler version of SpinalNet can be equivalent to a single hidden layer NN. In <ref type="figure" target="#fig_2">Fig. 3</ref>(a), a SpinalNet with two hidden layers (HLs), each layer containing two neurons is simplified. The neurons of the first layer contain the purely linear function. Therefore, the first layer gives only the weighted sum of x 1 to x 5 inputs. Outputs of each hidden neuron for the first hidden layer only go to a similar neuron of the second hidden layer. Cross connections and connections from the first hidden layer to output are disconnected by assigning zero weights. The second hidden layer receives the weighted summation from x 6 to x 10 . It also receives the weighted summation from x 1 to x 5 of the previous layer. Then, the neurons of this layer apply an activation function to the weighted sum of x 1 to x 10 . Therefore, these two layers are equivalent to a neural network of single HL, containing two hidden neurons, shown in <ref type="figure" target="#fig_2">Fig. 3(b)</ref>. A simplified version of SpinalNet with four HLs, containing two neurons in each layer is shown in <ref type="figure" target="#fig_2">Fig. 3(d)</ref>. Similarly, the SpinalNet in <ref type="figure" target="#fig_2">Fig.  3(d)</ref> is also equivalent to a NN with one HL, containing four neurons in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>.</p><p>Similarly, a deep SpinalNet can be equivalent to a NN of a single hidden layer, containing a large number of neurons. A NN with a single hidden layer and a large number of neurons achieves the universal approximation. Therefore, a deep SpinalNet also has universal approximability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transfer Learning</head><p>Transfer of learning is an efficient technique of using previously acquired knowledge and skills in novel problems. It is also similar to educating humans with a much broader syllabus to achieve competencies for an unpredictable future. One of the most efficient ideas of machine learning is the transfer learning(TL), which is similar to the transfer of learning in humans <ref type="bibr" target="#b23">[24]</ref>. DNNs requires adequate training samples for proper training. Insufficient training samples may result in poor performance. Transfer learning (TL) is an efficient DNN training technique where initial layers of DNN are pre-trained with a large dataset. The corresponding train dataset trains only a few final layers. As a result, the user can get a welltrained NN for the specific dataset of a small sample number, with lower computational overhead <ref type="bibr" target="#b24">[25]</ref>. The TL is gaining huge popularity these days due to exceptional performance and usability. Many researchers expect TL as the next driver of the commercial success of machine learning <ref type="bibr" target="#b25">[26]</ref>.</p><p>Several standard datasets contain insufficient training samples. CIFAR-100, Caltech-101, STL-10, etc. datasets are examples of such datasets. The traditional DNN training trains the entire DNN. According to the reported results, the traditional DNN training technique cannot achieve more than 90% accuracy on the CIFAR-100 dataset, where TL achieves more than 93% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head><p>In this section, we aim to verify the effectiveness of Spinal-Net for regression and classification problems. We apply different variants of the SpinalNet and receive SOTA performance in several datasets. The training procedure follows stochastic gradient descent (SGD) or Adam training technique. We upload training scripts to GitHub to help future researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Regression Dataset</head><p>Regression is a less popular topic among the researchers of NN compared to classification. There exist a large number of datasets and organized competition among various algorithms for the classification problem. Regregression lacks popular standard datasets and competitions. Therefore, we compare our SpinalNet with the PyTorch regression example, developed by Ben Phillips <ref type="bibr" target="#b26">[27]</ref>. The example considers a single input and a single output. Following the example, we apply the Adam algorithm <ref type="bibr" target="#b27">[28]</ref> for the optimization purpose. The loss function used in the experiments is the mean square error (MSE), the learning rate is equal to 0.01, and the number of the epoch is equal to 200. We changed the problem to eight variables and tried different combinations of variables with the same level of noise. Combinations are 1) summation of variables ( x), 2) sine of summation of variables (sin( x)), and 3) product of variables ( x), and 4) sine of product of variables (sin( x)). We record the MSE at 100 and 200 epochs. The default code <ref type="bibr" target="#b26">[27]</ref> shows the MSE of the last epoch, but our code shows the minimum MSE among the current and previous epochs. We segment inputs into two groups, each containing four inputs.</p><p>The number of hidden neurons in traditional NN is 300 <ref type="bibr" target="#b26">[27]</ref>. The number of hidden neurons in SpinalNet is also 300. The number of multiplication in traditional NN is 21700, and the number of multiplications in SpinalNet is 14000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification: Learning from Scratch</head><p>We train several existing networks and different variations of SpinalNet with random initialization on MNIST, Fashion-MNIST, KMNIST, QMNIST, EMNIST, CIFAR-10, and CIFAR-100 image classification datasets.</p><p>1) MNIST: The MNIST dataset is one of the most popular datasets for investigating image classification algorithms due to its simplicity and small size. We compare our SpinalNet with PyTorch CNN <ref type="bibr" target="#b28">[29]</ref>. A hidden layer of fifty neurons joins them with the output. The default CNN code provides 98.17% accuracy. We investigate the same NN with a SpinalNet fullyconnected (FC) layer. The FC layer consists of six sub-hidden layers, each layer contains eight neurons. That CNN with Spinal FC provides 98.44% accuracy. That structure brings more than a 48.5% reduction in multiplication and a 4% reduction in the activation functions on the fully-connected layer. The first segment in <ref type="table" target="#tab_0">Table II</ref> presents results on the MNIST dataset. The SpinalNet reduces the overall number of parameters and increases performance significantly.</p><p>As VGG models perform very well with the MNIST datasets, we incorporate the SpinalNet with the VGG-5 network <ref type="bibr" target="#b29">[30]</ref>. VGG-5 with the Spinal fully connected layer provides a near state-of-the-art performance. We receive 99.72% accuracy with VGG-5 (Spinal FC). According to our literature search, it is one of the top twenty reported accuracies. We perform the random rotation of 10 degrees and the random perspective PyTorch library functions to enhance data.</p><p>2) Fashion-MNIST: The Fashion-MNIST data is quite similar to MNIST. It also contains 28 × 28 grayscale images and the output contains 10 classes. Therefore, MNIST codes are executable to the Fashion-MNIST data without any modification. The same NN is applied to compute CNN and CNN (Spinal FC).</p><p>We receive 94.68% accuracy with VGG-5 (Spinal FC). The default VGG-5 provides 94.63% accuracy. Therefore, the Spinal FC provides better performance with a lower number of multiplications. We apply random rotation and random resized crops to enhance the data. According to our literature search, it is one of the top five reported results.</p><p>3) Kuzushiji-MNIST: Kuzushiji-MNIST or shortly, KM-NIST is a Japanese character recognition dataset. The data format of KMNIST is also the same as the MNIST data, and the same codes can be applied. <ref type="table" target="#tab_0">Table II</ref> presents the performance of codes. We receive superior performance by using the Spinal FC layer with CNN.</p><p>We receive 99.15% accuracy with VGG-5 (Spinal FC). It is a new SOTA for the Kuzushiji-MNIST dataset <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. We apply the random perspective and the random rotation to enhance the data. The data augmentation for VGG-5 and VGG-5 (Spinal FC) are the same. The default VGG-5 provides 98.94% accuracy. Therefore, the Spinal FC provides better performance with a lower number of multiplications. 4) QMNIST: QMNIST is a recently published English digit recognition dataset. QMNIST dataset contains fifty thousand test images. The dimensions of inputs and outputs are the same as the dimension of inputs and outputs of the MNIST dataset. Therefore, the same code is executable for the QMNIST dataset. The fourth segment of table II presents QMNIST results. The default PyTorch CNN provides 97.82% accuracy on the QMNIST data. CNN(Spinal FC) provides 97.97% and 98.07% accuracy for the spinal-layer size of eight and ten respectively.</p><p>The VGG-5 receives 99.66% accuracy. The VGG-5 (Spinal FC) receives 99.68% accuracy. According to our literature search, we have received SOTA performance for the QMNIST dataset. We apply the random perspective and random rotation functions to achieve these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) EMNIST:</head><p>The EMNIST dataset contains several handwritten character datasets. These datasets are derived from the NIST database and converted to a 28×28 pixel greyscale image format. The EMNIST(digits) dataset also has ten classes. The accuracy of NNs on the EMNIST (digit) data are available in the fifth segment of table II. The default PyTorch CNN provides 98.89% accuracy. The VGG-5 provides 99.81% accuracy and VGG-5 (Spinal FC) provides 99.82% accuracy. According to our literature search, we have received SOTA performance for the EMNIST (digits) dataset.</p><p>The default PyTorch CNN provides 87.57% accuracy for EMNIST (letters) dataset. The VGG-5 provides 95.86% accuracy and VGG-5 (Spinal FC) provides 95.88% accuracy. According to our literature search, we have received SOTA performance for the EMNIST (letters) dataset.</p><p>The default PyTorch CNN provides 79.61% accuracy for EMNIST (balanced) dataset. The VGG-5 provides 91.04% accuracy. VGG-5 (Spinal FC) provides 91.05% accuracy. According to our literature search, we have received SOTA performance for the EMNIST (balanced) dataset. We apply the random perspective and the random rotation functions to enhance all EMNIST datasets. 6) CIFAR-10 Dataset: The CIFAR-10 dataset is less predictable compared to the original MINST dataset <ref type="bibr" target="#b32">[33]</ref>. The PyTorch CNN example <ref type="bibr" target="#b33">[34]</ref> classifies the CIFAAR-10 data with 60% accuracy. A SpinalNet of 6 hidden layers, each layer containing 20 neurons achieves 62% accuracy for the same number of epochs. 5% lower error is achieved with a 4.8% multiplication reduction at the fully-connected layer.</p><p>The DenseNet code <ref type="bibr" target="#b34">[35]</ref> shared by Hasan et al. provides 77.79% accuracy after 35 epochs. The DenseNet fully connected layer has a 512 neuron hidden layer. We train a SpinalNet of 8 hidden layers, each containing 16 neurons. The number of hidden neurons becomes one-fourth and the number of multiplication is reduced by 87%. Still, we observe a 15% error reduction. Another SpinalNet has 8 hidden layers, each containing 64 neurons. The number of hidden neurons remains the same and the number of multiplication is reduced by 44.1%. The error is reduced by 18.7% compared to the previous DenseNet code <ref type="bibr" target="#b34">[35]</ref>.</p><p>The ResNet <ref type="bibr" target="#b35">[36]</ref> example of PyTorch provides 88.35% accuracy after 80 cycles. We achieve 88.65% accuracy after 80 cycles and 88.93% accuracy after 140 cycles with a SpinalNet of 4 hidden layers, each layer contains 16 neurons. However, we reduce the pooling and increase multiplication at the fullyconnected layer at the default ResNet code <ref type="bibr" target="#b35">[36]</ref>. However, the accuracy is lower with the standard ResNet blocks. The spinal fully-connected layer degrades the performance of ResNet-18 and ResNet-34 slightly.</p><p>A consistent improvement in the NN size and performance is achieved with the VGG network. As the VGG NN contains a large fully-connected layer, the SpinalNet can act as an optimized fully-connected layer. We investigate VGG-11, VGG-13, VGG-16, and VGG-19. VGG-19 provides the highest accuracy (91.40%) amongst the VGG networks and VGG SpinalNets. However, the ResNet-18 provides 91.98% accuracy, and adding a Spinal fully connected layer degrades the accuracy.</p><p>According to our results, the SpinalNet brings significant improvement on NNs, containing activation functions in the fully connected layer. ResNet has no activation on the fully connected layer. Therefore, the improvement of ResNet with a SpinalNet FC layer is not significant. 7) CIFAR-100 Dataset: ResNet and VGG networks bring promising results with CIFAR-10 data. Therefore, we apply them to the CIFAR-100 dataset. We receive significant improvement with the Spinal FC layer for different VGG networks. Among the VGG networks, VGG-16 provides the best performance. VGG-16 provides 63.20% accuracy alone, and 64.99% accuracy with the SpinalNet after 150 epoch of training. Moreover, the number of neurons in the fully connected layer is reduced to half with the Spinal FC layer. Moreover, the number of multiplication in the FC layer is reduced to 7%. The VGG-16 has two hidden layers of size 4096. Replacing them with four spinal layers of 512 size reduces the number of multiplication significantly. However, the Spinal fully-connected layer does not improve the performance of ResNet. Although the number of hidden neurons in the fully connected layer is increasing for ResNet, we are getting a lower performance. The probable reason can be a decrease in the gradient is the initial layers of ResNet due to additional layers in the SpinalNet.  www.paperswithcode.com/task/image-classification * * After the online appearance of the current paper, the performance of SpinalNet will be SOTA performance; unless someone reports a better performance earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification: Transfer Learning</head><p>While learning from the scratch, we observe that, although the number of hidden neurons in the fully connected layer is increasing, the ResNet with Spinal FC is getting a lower performance compared to the similar ResNet. The probable reason can be a decrease with the gradient in the initial layers of ResNet due to additional layers. Therefore, we apply the transfer learning technique to the pre-trained ResNet and VGG networks. We download pre-trained VGG-19 bn and Wide-Resnet-101 models from the Torchvision. These models are pre-trained on the Imagenet dataset. We apply these two models to the following datasets.</p><p>1) CIFAR-10: We train pre-trained VGG-19 bn and Wide-Resnet-101 models to the CIFAR-10 data. We investigate both of the traditional FC layers and Spinal FC layers. The SpinalNet achieves a significant improvement in the VGG network in terms of accuracy and the number of parameters. However, there is a slightly lower performance in the wide ResNet with the Spinal FC, compared to the original Wide-ResNet.</p><p>2) CIFAR-100: As usual, the SpinalNet performs well with the VGG network on the CIFAR-100 dataset. We receive a superior accuracy of 88.34% with a wide Spinal FC of 512 neurons with dropout on the Wide-ResNet-101. It is one of the top ten reported results on the CIFAR-100 dataset. Results are presented in the second segment of table III.</p><p>3) Caltech-101: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Highest Accuracy</head><p>Table IV presents SOTA performances for our investigated datasets. Combining SpinalNet with VGG-5 provides near SOTA or SOTA performance in MNIST datasets. MNIST SOTA models are usually a combined model and have the best performance for a specific dataset. We also obtain SOTA performance in five MNIST datasets. The prime reason for obtaining SOTA is not the proposed network. As these datasets are new, very few researchers investigated these data, and our result is the best among the reported results. We also apply transfer learning with a spinal fully connected layer and achieve SOTA performance in several color image datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROSPECTS OF SPINALNET</head><p>There are many rooms for further investigation and improvement of SpinalNet which can be mentioned in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Auto Dimension Reduction</head><p>Dimension reduction is a popular technique of reducing the number of inputs to a neural network without facing noticeable performance degradation <ref type="bibr" target="#b62">[63]</ref>. The input combination of the NN network may contain a large number of inter-related or irrelevant data. As the proposed SpinalNet takes input in every layer and there are fewer neurons per hidden layer than the total number of inputs, the SpinalNet may automatically discard irrelevant data. Moreover, the necessity for dimension reduction may decrease, as a large number of input features do not increase the computation greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Very Deep NN</head><p>The computation performed inside the proposed SpinalNet increases linearly with the increase in depth. The SpinalNet has outputs at every layer. Moreover, gradual training may enable us to increase SpinalNet depth gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spinal Hidden Layer</head><p>This paper presents the SpinalNet as an independent network and as the fully connected layer of a CNN. The Spinal-Net can also replace a wide hidden layer of a traditional NN. <ref type="figure" target="#fig_3">Fig. 4</ref> presents how a SpinalNet can replace a traditional hidden layer. The figure shows two inputs and one neuron per sub-layer. The number of inputs can be large and the input can be segmented into more than two segments. Similarly, one sub-layer may hold more than one neuron. The number of sub-layer can be more than or less than four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Better Accuracy and New Datasets</head><p>The SpinalNet may achieve higher accuracy with augmented datasets and its different structural variants. We apply Spinal-Net as fully connected layers on several other networks to achieve higher accuracies. Researchers may apply SpinalNet for different datasets, new applications [64]- <ref type="bibr" target="#b65">[66]</ref>, and combining with other networks in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. NN Ensemble and Voting</head><p>Recently Mo Kweon et al. perform ensemble and voting from two different VGG networks and ResNet to achieve better performance <ref type="bibr" target="#b29">[30]</ref>. Researchers may use different NN along within SpinalNet to get better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper aims to present the concept and structure of a novel DNN model called SpinalNet. The chordate nervous system has a unique way of connecting a large number of sensing information and taking local decisions. The main drawback of the recent proposed DNNs is in their computational intensiveness due to a large number of network inputs. Therefore, taking inputs gradually and considering local decisions such as similar to our spinal cord decreases computations. In this paper, we also present the effectiveness of SpinalNet on several well-known benchmark datasets leading to the improvement of the classification accuracy. Moreover, the SpinalNet is usually less computation extensive than its counterpart. Combining with VGG-5 and transfer learning, the SpinalNet has achieved SOTA performance in four datasets. As the future research direction, we will try to improve the accuracy of the proposed SpinalNet and then apply the improved SpinalNet to a wide range of real-world scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>We develop SpinalNet by mimicking the human somatosensory system to receive large input efficiently and to achieve better performance.(a) Half part of the human somatosensory system, presenting how our spinal cord receives sensory signals from our body. (b) Structure of the proposed SpinalNet. The proposed NN consists of the input row, the intermediate row, and the output row. The intermediate row contains multiple hidden layers. Each hidden layer receives a portion of the input. All layers except the first layer also receive outputs of the previous layer. The output layer adds the weighted outputs of all hidden neurons of the intermediate row. The user can also construct and train a SpinalNet for any arbitrary number of inputs, intermediate neurons, and outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>A portion of the human nerve plexus; known as the brachial plexus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The visual proof of the universal approximation of the SpinalNet. A simplified version of SpinalNet in (a) can act as a NN of a single hidden layer, drawn in (b). Similarly, a 4 layer SpinalNet in (d) can be equivalent to a NN of one hidden layer (HL), containing four neurons, shown in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Any traditional hidden layer can be converted to a spinal hidden layer. The traditional hidden layer in (a) is converted to a spinal hidden layer in (b). A spinal hidden layer has the structure of the proposed SpinalNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>BETWEEN TRADITIONAL FEED-FORWARD NN AND SPINALNET FOR REGRESSION DATASETS.The SpinalNet achieves a 35.5% reduction in the number of multiplications. The number of parameters in the traditional NN is 22.00k, and the number of parameters in SpinalNet is 14.30k. There are eight combinations for MSE comparisons, as shown inTable I. Superior performances are highlighted as bold characters. The SpinalNet performs better in six out of eight combinations.</figDesc><table><row><cell>Neural Network</cell><cell>Data</cell><cell></cell><cell cols="2">MSE (10 −3 Unit)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>100 Epoch</cell><cell>200 Epoch</cell></row><row><cell>Feed-forward NN</cell><cell>8 Var.</cell><cell>x</cell><cell>1.178</cell><cell>0.887</cell></row><row><cell>Two Hidden Layers</cell><cell cols="2">8 Var. sin( x)</cell><cell>1.918</cell><cell>1.086</cell></row><row><cell>200, 100 Neurons</cell><cell>8 Var.</cell><cell>x</cell><cell>3.875</cell><cell>3.875</cell></row><row><cell>[27]</cell><cell cols="2">8 Var. sin( x)</cell><cell>3.403</cell><cell>1.554</cell></row><row><cell>SpinalNet</cell><cell>8 Var.</cell><cell>x</cell><cell>1.007</cell><cell>0.855</cell></row><row><cell>6 Hidden Layers</cell><cell cols="2">8 Var. sin( x)</cell><cell>1.912</cell><cell>1.219</cell></row><row><cell>50 Neurons Each Layer</cell><cell>8 Var.</cell><cell>x</cell><cell>3.966</cell><cell>2.217</cell></row><row><cell>Half Input Each Layer</cell><cell cols="2">8 Var. sin( x)</cell><cell>0.910</cell><cell>0.910</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>OF THE SPINALNET AND SEVERAL POPULAR NETS ON DIFFERENT CLASSIFICATION DATASETS</figDesc><table><row><cell>Data</cell><cell>Model</cell><cell cols="4">Size of Fully Connected Layer Epoch Test Accuracy Error Reduction</cell><cell>Parameters</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>98.17%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>MNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>98.44%</cell><cell>14.8%</cell><cell>13.82k</cell></row><row><cell>[37]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>98.48%</cell><cell>16.9%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>100</cell><cell>99.72%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>100</cell><cell>99.72%</cell><cell>0.0%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>84.10%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>Fashion-MNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>85.98%</cell><cell>11.8%</cell><cell>13.82k</cell></row><row><cell>[38]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>86.61%</cell><cell>15.8%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>100</cell><cell>94.63%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>100</cell><cell>94.68%</cell><cell>0.9%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>84.48%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>Kuzushiji-MNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>87.94%</cell><cell>22.3%</cell><cell>13.82k</cell></row><row><cell>(KMNIST) [39]</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>88.25%</cell><cell>24.3%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>200</cell><cell>98.94%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>200</cell><cell>99.15%</cell><cell>19.8%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>97.82%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>QMNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>97.97%</cell><cell>6.9%</cell><cell>13.82k</cell></row><row><cell>[40]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>98.07%</cell><cell>11.5%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>100</cell><cell>99.66%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>100</cell><cell>99.68%</cell><cell>5.9%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>98.89%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>EMNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>99.12%</cell><cell>20.7%</cell><cell>13.82k</cell></row><row><cell>(Digits) [41]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>99.16%</cell><cell>24.3%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>50</cell><cell>99.81%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>50</cell><cell>99.82%</cell><cell>5.3%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>87.57%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>EMNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>90.07%</cell><cell>20.11%</cell><cell>13.82k</cell></row><row><cell>(Letters) [41]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>90.23%</cell><cell>21.4%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>200</cell><cell>95.86%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>200</cell><cell>95.88%</cell><cell>0.5%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [29]</cell><cell>1HL, 50 Neurons</cell><cell>8</cell><cell>79.61%</cell><cell>-</cell><cell>21.84k</cell></row><row><cell>EMNIST</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 8 Neurons Per Layer</cell><cell>8</cell><cell>82.77%</cell><cell>15.50%</cell><cell>13.82k</cell></row><row><cell>(Balanced) [41]</cell><cell>CNN(Spinal FC)</cell><cell>6HL, 10 Neurons Per Layer</cell><cell>8</cell><cell>83.21%</cell><cell>17.66%</cell><cell>16.05k</cell></row><row><cell></cell><cell>VGG-5 [30]</cell><cell>1HL, 512 Neurons</cell><cell>200</cell><cell>91.04%</cell><cell>-</cell><cell>3.646M</cell></row><row><cell></cell><cell>VGG-5 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>200</cell><cell>91.05%</cell><cell>0.1%</cell><cell>3.630M</cell></row><row><cell></cell><cell>CNN [34]</cell><cell>2HL, 120 and 84 Neurons</cell><cell>8</cell><cell>60.65%</cell><cell>-</cell><cell>62.01k</cell></row><row><cell>CIFAR-10</cell><cell>CNN (Spinal FC)</cell><cell>6HL, 20 Neurons Per Layer</cell><cell>8</cell><cell>62.37%</cell><cell>4.4%</cell><cell>30.20k</cell></row><row><cell>[42]</cell><cell>DenseNet [35]</cell><cell>1HL, 512 Neurons</cell><cell>35</cell><cell>77.79%</cell><cell>-</cell><cell>1.07M</cell></row><row><cell></cell><cell>DenseNet (Spinal FC)</cell><cell>8HL, 16 Neurons Per Layer</cell><cell>35</cell><cell>81.13%</cell><cell>15.0%</cell><cell>1.13M</cell></row><row><cell></cell><cell>DenseNet (Spinal FC)</cell><cell>8HL, 64 Neurons Per Layer</cell><cell>35</cell><cell>81.95%</cell><cell>18.7%</cell><cell>1.36M</cell></row><row><cell></cell><cell>ResNet-18 [8]</cell><cell>Only Output Layer</cell><cell>150</cell><cell>91.98%</cell><cell>-</cell><cell>3.14M</cell></row><row><cell></cell><cell>ResNet-18 (Spinal FC)</cell><cell>4HL, 20 Neurons Per Layer</cell><cell>150</cell><cell>91.42%</cell><cell>-7.0%</cell><cell>3.22M</cell></row><row><cell></cell><cell>ResNet-34 [8]</cell><cell>Only Output Layer</cell><cell>150</cell><cell>89.56%</cell><cell>-</cell><cell>5.67M</cell></row><row><cell></cell><cell>ResNet-34 (Spinal FC)</cell><cell>4HL, 256 Neurons Per Layer</cell><cell>150</cell><cell>89.88%</cell><cell>3.1%</cell><cell>5.75M</cell></row><row><cell></cell><cell>VGG-11 [43]</cell><cell>2HL, 4096 Neurons Each</cell><cell>35</cell><cell>86.68%</cell><cell>-</cell><cell>28.15M</cell></row><row><cell></cell><cell>VGG-11 (Spinal FC)</cell><cell>4HL, 1024 Neurons Each</cell><cell>35</cell><cell>87.08%</cell><cell>3.0%</cell><cell>9.35M</cell></row><row><cell></cell><cell>VGG-13 [43]</cell><cell>2HL, 4096 Neurons Each</cell><cell>35</cell><cell>87.79%</cell><cell>-</cell><cell>28.33M</cell></row><row><cell></cell><cell>VGG-13 (Spinal FC)</cell><cell>4HL, 1024 Neurons Each</cell><cell>35</cell><cell>89.16%</cell><cell>11.2%</cell><cell>95.31M</cell></row><row><cell></cell><cell>VGG-19 [43]</cell><cell>2HL, 4096 Neurons Each</cell><cell>200</cell><cell>90.75%</cell><cell>-</cell><cell>38.96M</cell></row><row><cell></cell><cell>VGG-19 (Spinal FC)</cell><cell>4HL, 512 Neurons Each</cell><cell>200</cell><cell>91.40%</cell><cell>7.0%</cell><cell>20.16M</cell></row><row><cell></cell><cell>ResNet-18 [8]</cell><cell>Only Output Layer</cell><cell>30</cell><cell>65.04%</cell><cell>-</cell><cell>3.16M</cell></row><row><cell>CIFAR-100</cell><cell>ResNet-18 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>30</cell><cell>63.60%</cell><cell>-4.1%</cell><cell>4.66M</cell></row><row><cell>[42]</cell><cell>ResNet-34 [8]</cell><cell>Only Output Layer</cell><cell>30</cell><cell>65.51%</cell><cell>-</cell><cell>5.69M</cell></row><row><cell></cell><cell>ResNet-34 (Spinal FC)</cell><cell>4HL, 128 Neurons Per Layer</cell><cell>30</cell><cell>63.32%</cell><cell>-6.3%</cell><cell>7.19M</cell></row><row><cell></cell><cell>VGG-11 [43]</cell><cell>2HL, 4096 Neurons Each</cell><cell>40</cell><cell>55.60%</cell><cell>-</cell><cell>28.52M</cell></row><row><cell></cell><cell>VGG-11 (Spinal FC)</cell><cell>4HL, 1024 Neurons Each</cell><cell>40</cell><cell>60.48%</cell><cell>11.0%</cell><cell>9.39M</cell></row><row><cell></cell><cell>VGG-13 [43]</cell><cell>2HL, 4096 Neurons Each</cell><cell>50</cell><cell>60.75%</cell><cell>-</cell><cell>28.70M</cell></row><row><cell></cell><cell>VGG-13 (Spinal FC)</cell><cell>4HL, 1024 Neurons Each</cell><cell>50</cell><cell>62.46%</cell><cell>4.4%</cell><cell>9.58M</cell></row><row><cell></cell><cell>VGG-16 [43]</cell><cell>2HL, 4096 Neurons Each</cell><cell>150</cell><cell>63.20%</cell><cell>-</cell><cell>34.02M</cell></row><row><cell></cell><cell>VGG-16 (Spinal FC)</cell><cell>4HL, 512 Neurons Each</cell><cell>150</cell><cell>64.99%</cell><cell>4.9%</cell><cell>14.89M</cell></row><row><cell></cell><cell>VGG-19 [43]</cell><cell>2HL, 4096 Neurons Each</cell><cell>150</cell><cell>62.05%</cell><cell>-</cell><cell>39.33M</cell></row><row><cell></cell><cell>VGG-19 (Spinal FC)</cell><cell>4HL, 512 Neurons Each</cell><cell>150</cell><cell>64.77%</cell><cell>7.2%</cell><cell>20.20M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III TRANSFER</head><label>III</label><figDesc>LEARNING PERFORMANCE OF THE SPINALNET AND SEVERAL POPULAR NETS ON DIFFERENT CLASSIFICATION DATASETS</figDesc><table><row><cell>Data</cell><cell>Model</cell><cell cols="2">Size of Fully Connected Layer</cell><cell>Epoch</cell><cell cols="3">Test Accuracy Error Reduction Parameters</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>95.91%</cell><cell>-</cell><cell>263.27M</cell></row><row><cell>CIFAR-10</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>96.00%</cell><cell>2.2%</cell><cell>198.26M</cell></row><row><cell>[42]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>50</cell><cell>98.22%</cell><cell>-</cell><cell>124.86M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell></cell><cell>4HL, 20 Neurons Per Layer</cell><cell>50</cell><cell>98.12%</cell><cell>-5.6%</cell><cell>124.92M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>79.22%</cell><cell>-</cell><cell>263.63M</cell></row><row><cell>CIFAR-100</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>79.56%</cell><cell>1.6%</cell><cell>198.63M</cell></row><row><cell>[42]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>50</cell><cell>87.15%</cell><cell>-</cell><cell>125.04M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell cols="2">4HL, 512 Neurons Per Layer</cell><cell>50</cell><cell>88.34%</cell><cell>9.26%</cell><cell>132.59M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>10</cell><cell>92.98%</cell><cell>-</cell><cell>263.64M</cell></row><row><cell>Caltech-101</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>10</cell><cell>93.16%</cell><cell>2.6%</cell><cell>198.63M</cell></row><row><cell>[44]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>10</cell><cell>97.11%</cell><cell>-</cell><cell>125.05M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell cols="2">4HL, 101 Neurons Per Layer</cell><cell>10</cell><cell>97.32%</cell><cell>7.27%</cell><cell>132.60M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>98.67%</cell><cell>-</cell><cell>264.15M</cell></row><row><cell>Bird225</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>99.02%</cell><cell>26.3%</cell><cell>199.14M</cell></row><row><cell>[45]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>25</cell><cell>99.38%</cell><cell>-</cell><cell>125.30M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell cols="2">4HL, 225 Neurons Per Layer</cell><cell>25</cell><cell>99.56%</cell><cell>11.1%</cell><cell>126.11M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>87.21%</cell><cell>-</cell><cell>264.03M</cell></row><row><cell>Stanford Cars</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>88.72%</cell><cell>13.4%</cell><cell>153.78M</cell></row><row><cell>[46]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>25</cell><cell>93.35%</cell><cell>-</cell><cell>125.24M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell cols="2">4HL, 196 Neurons Per Layer</cell><cell>25</cell><cell>93.35%</cell><cell>0%</cell><cell>132.98M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>94.80%</cell><cell>-</cell><cell>263.27M</cell></row><row><cell>SVHN</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>95.26%</cell><cell>18.28%</cell><cell>198.26M</cell></row><row><cell>[47]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>25</cell><cell>97.80%</cell><cell>-</cell><cell>124.86M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell></cell><cell>4HL, 20 Neurons Per Layer</cell><cell>25</cell><cell>97.87%</cell><cell>3.18%</cell><cell>124.92M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>90.28%</cell><cell>-</cell><cell>263.27M</cell></row><row><cell>CINIC-10</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>91.00%</cell><cell>8.00%</cell><cell>198.26M</cell></row><row><cell>[48]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>50</cell><cell>92.15%</cell><cell>-</cell><cell>124.86M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell></cell><cell>4HL, 20 Neurons Per Layer</cell><cell>50</cell><cell>93.60%</cell><cell>18.47%</cell><cell>124.92M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>10</cell><cell>95.44%</cell><cell>-</cell><cell>263.27M</cell></row><row><cell>STL-10</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>10</cell><cell>95.57%</cell><cell>2.9%</cell><cell>198.26M</cell></row><row><cell>[49]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>10</cell><cell>98.40%</cell><cell>-</cell><cell>124.86M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell></cell><cell>4HL, 20 Neurons Per Layer</cell><cell>10</cell><cell>98.66%</cell><cell>16.3%</cell><cell>124.92M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>25</cell><cell>95.20%</cell><cell>-</cell><cell>263.64M</cell></row><row><cell>Oxford 102</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>25</cell><cell>95.46%</cell><cell>32.5%</cell><cell>198.63M</cell></row><row><cell>Flower [50]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>50</cell><cell>99.39%</cell><cell>-</cell><cell>125.05M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell cols="2">4HL, 101 Neurons Per Layer</cell><cell>50</cell><cell>99.30%</cell><cell>-14.7%</cell><cell>125.32M</cell></row><row><cell></cell><cell>VGG-19 bn</cell><cell cols="2">2 HL, 4096 Neurons Per Layer</cell><cell>10</cell><cell>99.90%</cell><cell>-</cell><cell>263.76M</cell></row><row><cell>Fruits 360</cell><cell>VGG-19 bn (Spinal FC)</cell><cell cols="2">4HL, 1024 Neurons Per Layer</cell><cell>10</cell><cell>99.96%</cell><cell>60%</cell><cell>198.75M</cell></row><row><cell>[51]</cell><cell>Wide ResNet-101 2</cell><cell></cell><cell>0 Neurons</cell><cell>10</cell><cell>99.96%</cell><cell>-</cell><cell>125.11M</cell></row><row><cell cols="2">Wide ResNet-101 2 (Spinal FC)</cell><cell cols="2">4HL, 131 Neurons Per Layer</cell><cell>10</cell><cell>100%</cell><cell>100%</cell><cell>125.50M</cell></row><row><cell></cell><cell>TABLE IV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SOTA PERFORMANCES OF INVESTIGATED DATASETS IN JUNE 2020*</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data</cell><cell>Model Name</cell><cell></cell><cell>Accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNIST</cell><cell cols="2">Branching/Merging CNN [1]</cell><cell>99.84%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fashion-MNIST</cell><cell cols="2">PreAct-ResNet18 + FMix [52]</cell><cell>96.36%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kuzushiji-MNIST**</cell><cell>CAMNet3 [32]</cell><cell></cell><cell>99.05%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>QMNIST**</cell><cell>Deep Regularization [53]</cell><cell></cell><cell>99.67%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMNIST (Digits)**</cell><cell>DWT-DCT with KNN [54]</cell><cell></cell><cell>97.74%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EMNIST (Letters)**</cell><cell>TextCaps [55]</cell><cell></cell><cell>95.39%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">EMNIST (Balanced)** TextCaps [55]</cell><cell></cell><cell>90.46%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR-10</cell><cell>BiT-L (ResNet) [56]</cell><cell></cell><cell>99.00%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR-100</cell><cell>BiT-L (ResNet) [56]</cell><cell></cell><cell>93.51%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Caltech-101**</cell><cell>UL-Hopfield [57]</cell><cell></cell><cell>91.00%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Stanford Cars</cell><cell>DAT [58]</cell><cell></cell><cell>96.20%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Oxford 102 Flowers</cell><cell>BiT-L (ResNet) [56]</cell><cell></cell><cell>99.63%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>STL-10**</cell><cell>NAT-M4 [59]</cell><cell></cell><cell>97.90%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CINIC-10</cell><cell>NAT-M4 [59]</cell><cell></cell><cell>94.80%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SVHN</cell><cell>WideResNet-28-10 [60]</cell><cell></cell><cell>99.00%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>225 Bird Species**</cell><cell>Vgg-16 [61]</cell><cell></cell><cell>99.10%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fruits 360**</cell><cell>EfficientNet-B1 [62]</cell><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">*  According to the following website and our literature search:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The SpinalNet performs well with both of the VGG and the Wide-ResNet network on the Caltech-101 dataset. We receive SOTA performance for the Caltech-101 dataset. 4) Bird225: The SpinalNet performs well with both of the VGG and the Wide-ResNet network on the Bird225 dataset. We receive SOTA performance for the Bird225 dataset. Results are available as the fourth segment of table III. 5) Stanford Cars: Although SpinalNet performs well with the VGG model, the accuracy with the Wide-ResNet model is equal. Moreover, our results are not one of the top 20 results in the Stanford Car leaderboard. The probable reason for the poor performance is the high resolution of the image. Several other researchers are concentrating on certain parts of the image to get high accuracy.6) SVHN: Although we receive a slightly better result with the Spinal FC for both networks, our performance is not one of the top twenty performance. The probable reason for getting such poor performance is the training with a different kind of data. The most classes in the Imagenet dataset are not digits. Pre-training with different types of data may limit the performance.7) CINIC-10: We receive significant improvements in terms of accuracy for both VGG and Wide-ResNet neural networks. Our result with Wide-ResNet Spinal FC is one of the top five reported results. Detailed procedures of data augmentation and simulations are available in the GitHub code.8) STL-10: The SpinalNet performs well with both of the VGG and the Wide-ResNet network on the STL-10 dataset. We receive SOTA performance for the STL-10 dataset. 9) Oxford 102 Flower: We trained pre-trained VGG-19 bn and Wide-Resnet-101 to the Oxford 102 Flower data. We investigate both of the traditional FC layers and Spinal FC layers. The SpinalNet achieves a significant improvement in the VGG network in terms of accuracy and the number of parameters. There is a slightly lower performance in the wide ResNet. Our results with the Wide ResNet networks are among the top 5 accuracies.10) Fruits 360: Fruits 360 dataset is one of the easiest machine learning problems. Several programmers have released codes with more than 99.9% efficiency in Kaggle for that dataset. We achieve promising performance with the SpinalNet on the Fruit 360 dataset with both the VGG and the Wide-ResNet networks. The Wide-ResNet-101 with Spinal FC provides 100% accuracy. However, another paper has reported the same efficiency this year. 11) Other Investigated Datasets: We also investigate the Wide-ResNet101 with SpinalNet fully connected layers in 'Intel Image Classification' and '10 Monkey Species' datasets in Kaggle and receive accuracies of 93.77% and 99.26% respectively.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A branching and merging convolutional network with homogeneous filter capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Byerly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dear</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09136</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rmdl: Random multimodel deep learning for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Information System and Data Mining</title>
		<meeting>the 2nd International Conference on Information System and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Psychophysical dimensions of tactile perception of textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Haptics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face recognition: A convolutional neural-network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adanet: Adaptive structural learning of artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gonzalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Partial adversarial training for neural network-based uncertainty quantification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal autonomous driving through deep imitation learning and neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M J</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Kebria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1215" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A mathematical theory of deep convolutional neural networks for feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiatowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bölcskei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1845" to="1866" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An overview of deep learning and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vogt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="178" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple techniques make sense: Feature pooling and normalization for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1251" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vascularized brachial plexus allotransplantationan experimental study in brown norway and lewis rats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gorden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. C.-C</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transplantation</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="159" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spinal cord mechanisms of pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dickenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British journal of anaesthesia</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="16" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention modulates spinal cord responses to pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eippert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Finsterbusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Büchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1022" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Resnet with one-neuron hidden layers is a universal approximator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6169" to="6178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Universal approximation with quadratic deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="383" to="392" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Approximation with artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Csáji</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Transfertransfo: A transfer learning approach for neural network based conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08149</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Starcraft micromanagement with reinforcement learning and curriculum transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="84" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nuts and bolts of building ai applications using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Keynote Talk</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Regression with neural networks in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Phillips</surname></persName>
		</author>
		<ptr target="https://medium.com/@benjamin.phillips22/simple-regression-with-neural-networks-in-pytorch-313f06910379" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mnist handwritten digit recognition in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koehler</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mnist competition tensorflow kr group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kweon</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Training neural networks with local error signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nøkland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Eidnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06656</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Context-aware multipath networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tissera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kahatapitiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wijesinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rodrigo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11519</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="Available:pytorch.org/tutorials/beginner/blitz/cifar10tutorial.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Densenet implementation on cifar10 dataset using pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M K</forename><surname>Hasan</surname></persName>
		</author>
		<ptr target="https://github.com/SMKamrulHasan/DenseNet-using-PyTorch-CIFAR10" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">3.2.2 resnet cifar10-pytorch tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch-tutorial.readthedocs.io/en/latest/tutorial/chapter03intermediate/322cnnresnetcifar10/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>best of the web</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep learning for classical japanese literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01718</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cold case: The lost mnist digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="443" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Emnist: Extending mnist to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Caltech 101 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">225 bird species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerry</forename></persName>
		</author>
		<ptr target="https://www.kaggle.com/gpiosenka/100-bird-species" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Vehicle-detected stanford cars data classes folder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename></persName>
		</author>
		<ptr target="https://www.kaggle.com/sungtheillest/vehicledetected-stanford-cars-data-classes-folder" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The street view house numbers (svhn) dataset</title>
		<ptr target="http://ufldl.stanford.edu/housenumbers/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Darlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03505</idno>
		<title level="m">Cinic-10 is not imagenet or cifar-10</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fruit recognition from images using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mureşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oltean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Universitatis Sapientiae, Informatica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="42" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Understanding and enhancing mixed sample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prügel-Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep regularization and direct training of the inner layers of neural networks with kernel flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08335</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Handwritten digit and letter recognition using hybrid dwt-dct with knn and svm classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghadekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ingole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sonone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Textcaps: Handwritten character recognition with very small datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jayasundara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jayasekara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rajasegaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Large scale learning of general visual representations for transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised learning using pretrained cnn and associative memory bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Domain adaptive transfer learning with specialist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07056</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sreekumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05859</idno>
		<title level="m">Neural architecture transfer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">bird cnn vgg16 final</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Purohit</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/aditya276/bird-cnn-vgg16-99-accuracy-on-test-set" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automated fruit recognition using efficientnet and mixnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Di</forename><surname>Sipio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Di Ruscio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page">105326</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Active subspace-based dimension reduction for chemical kinetics applications with epistemic uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vohra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexanderian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combustion and Flame</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="152" to="161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural network-based uncertainty quantification: A survey of methodologies and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A unified approach for conventional zero-shot, generalized zero-shot, and few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5652" to="5667" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE intelligent vehicles symposium (IV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="286" to="291" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
