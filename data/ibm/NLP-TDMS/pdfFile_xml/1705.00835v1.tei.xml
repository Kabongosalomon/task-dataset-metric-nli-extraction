<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">INVESTIGATION OF DIFFERENT SKELETON FEATURES FOR CNN-BASED 3D ACTION RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Multimedia Research Lab</orgName>
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Multimedia Research Lab</orgName>
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
							<email>philipo@uow.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Multimedia Research Lab</orgName>
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
							<email>wanqing@uow.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Multimedia Research Lab</orgName>
								<orgName type="institution">University of Wollongong</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">INVESTIGATION OF DIFFERENT SKELETON FEATURES FOR CNN-BASED 3D ACTION RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Skeleton</term>
					<term>3D Action recognition</term>
					<term>Convo- lutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning techniques are being used in skeleton based action recognition tasks and outstanding performance has been reported. Compared with RNN based methods which tend to overemphasize temporal information, CNN-based approaches can jointly capture spatio-temporal information from texture color images encoded from skeleton sequences. There are several skeleton-based features that have proven effective in RNN-based and handcrafted-feature-based methods. However, it remains unknown whether they are suitable for CNN-based approaches. This paper proposes to encode five spatial skeleton features into images with different encoding methods. In addition, the performance implication of different joints used for feature extraction is studied. The proposed method achieved state-of-the-art performance on NTU RGB+D dataset for 3D human action analysis. An accuracy of 75.32% was achieved in Large Scale 3D Human Activity Analysis Challenge in Depth Videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recognition of human actions has recently attracted increased interest because of its applicability in systems such as humancomputer interaction, game control, and intelligent surveillance. With the development of cost-effective sensors such as Microsoft Kinect cameras, RGB-D-based recognition has almost become commonplace <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Among the three most common input streams (RGB, depth, and skeleton), RGB is the most popular and widely studied. However, it suffers the challenge of pose ambiguity due to the loss of 3D information. On the other hand, depth and skeleton which capture 3D information of human bodies inherently overcome this challenge.</p><p>Skeleton has the advantage of being invariant to viewpoints or appearances compared with depth, thus suffering less intra-class variance <ref type="bibr" target="#b4">[5]</ref>. Furthermore, learning over skeleton is simple because they are higher-level information *Corresponding author based on advanced pose estimation. The foregoing observations motivated the study of skeleton-based human action recognition in this paper.</p><p>The methods based on handcrafted skeleton features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> have the drawback of dataset dependency while methods based on deep learning techniques have achieved outstanding performance. Currently, there are mainly two ways of using deep learning techniques to capture the spatio-temporal information in skeleton sequences; Recurrent Neural Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). RNNs are adopted to capture temporal information from extracted spatial skeleton features. The performance relies much on the effectiveness of the extracted spatial skeleton features due to the sequential flow of information. Moreover, the temporal information can be easily overemphasized especially when the training data is insufficient, leading to overfitting <ref type="bibr" target="#b8">[9]</ref>.</p><p>In contrast, CNNs directly extract information from texture images which are encoded from skeleton sequences. Wang et al <ref type="bibr" target="#b8">[9]</ref> used Joint Trajectory Maps (JTM) to encode body joint trajectories (positions, motion directions, and motion magnitudes) of each time instance into HSV images. In the images, spatial information is represented by positions and the dynamics is represented by colors. Hou et al <ref type="bibr" target="#b9">[10]</ref> adopted Skeleton Optical Spectra (SOS) to encode dynamic spatial-temporal information. Li et al <ref type="bibr" target="#b10">[11]</ref> adopted joint distances as spatial features and a colorbar was used for colorencoding. In the images, textures of rows capture spatial information and textures of columns capture temporal information. Currently, the spatial features used for encoding are relatively simple (joints positions and pair-wise distances).</p><p>Following the CNN-based approach, this paper investigates encoding richer spatial features into texture color images, including features between two or more joints. Specifically, inspired by the work from Zhang et al <ref type="bibr" target="#b4">[5]</ref>, the encoding of the following five types of spatial features is studied: joint-joint distances (JJd), joint-joint orientations (JJo), jointjoint vectors (JJv), joint-line distances (JLd), line-line angles (LLa). Each kind of feature is encoded into images in two or more ways to further explore the spatio-temporal information. CNN is adopted to train and recognize corresponding actions and score fusion is used to make a final classification. The effectiveness of this kind of approach has been verified in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref>. The investigation is conducted on NTU RGB+D Dataset <ref type="bibr" target="#b11">[12]</ref> and achieves state-of-the-art performance.</p><p>The rest of the paper is organized as follows. Section 2 introduces the proposed method and, in Section 3, experimental results and discussions are described. The conclusion and future work are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHOD</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the proposed method consists of five main components, namely spatial feature extraction from input skeleton sequences, key feature selection, texture color image encoding from key features, CNN model training based on images, and the score fusion. There are five types of features extracted from all joint combinations including JJd, JJo, JJv, JLd and LLa. Key features of certain joint combinations are then chosen for color encoding. For each type of key features, there are multiple selection methods and encoding methods, resulting in a total of 13 types of images. CNN is trained on each kind of image, and the output scores of CNNs are fused into the final score for final recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature extraction</head><p>The spatial features studied in this paper include joint-joint distances, joint-joint vectors, joint-joint orientations, jointline distances and line-line angles which were introduced in <ref type="bibr" target="#b4">[5]</ref>. In this paper, every action is assumed to be performed by two subjects, the main subject and an auxiliary other. In cases where there is only one person in the sequence, a 'shadow subject' copied from main subject is adopted. Suppose each subject has n joints, then in each frame there will be N = 2 × n joints. Let p j = (x, y, z), j ∈ N denote the 3D coordinate (Euclidean space) of the j th joint in a frame. The five features at frame t are calculated as follows:</p><formula xml:id="formula_0">JJd t jk = ||p t j − p t k || (1) JJv t jk = p t j − p t k (2) JJo t jk = JJv t jk /JJd t jk (3) JLd t jkm = JJv t jk ⊗ JJv t jm /JJd t km (4) LLa t jkmn = arccos(JJo t jk JJo t mn )<label>(5)</label></formula><p>where j, k, m, n ∈ N are the joint indices, ⊗ is cross product and is dot product. Meanwhile, j = k in equations (1-4), j = m = k in equation <ref type="formula">(4)</ref>, and (i, k) = (m, n) in equation <ref type="bibr" target="#b4">(5)</ref>. In total, there are C 2 50 = 1225 dimensions of the JJd feature, 3 × 1225 = 3675 dimensions of JJv and JJo features. There are also 1225 lines, resulting in 1225 × 48 = 58800 dimensions of JLd feature and C 2 1225 = 749700 dimensions of LLa feature. The resulting high dimensional feature space is neither cost-effective nor robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature selection</head><p>Feature selection is conducted by selecting key joints and key lines to reduce the number of combinations. The selection follows the principle that selected features should contain as much information as possible and be invariant to viewpoints and actions. Based on the observation that the motions are mainly located on the ends of skeletons and are usually locally sensitive, three strategies are proposed to select key joints for joint-joint feature calculation.</p><p>Joint strategy one (JS1): only the relations of joints within the same subject are considered, resulting in 2 * C 2 25 = 600 dimensional JJd feature. JS2: twelve joints from each subject are used, resulting in C 2 24 = 276 dimensional JJd feature. The joints start from 'middle of the spine' and are all two-steps away from the others. JS3: eleven joints from each subject are used, resulting in C 2 22 = 231 dimensional JJd feature.</p><p>The joints start from 'base of the spine' and are all two-steps away from the others. Two strategies are used to select key lines. Line strategy one (LS1): adopting the method in <ref type="bibr" target="#b4">[5]</ref> to select 39 lines from the main subject, resulting in 897 dimensional JLd feature and 741 dimensional LLa feature. LS2: using joints selected via JS3 to generate lines, and for each line the joints within twostep distance from end joints are used to calculate JLd feature, resulting in 570 dimensional JLd feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Color encoding</head><p>Inspired by <ref type="bibr" target="#b10">[11]</ref>, color images are used to encode the spatial features to capture temporal information. Specifically, each column in the image represents spatial features in a frame, and each row represents the sequence of a specific feature. In this way, the textures represent the spatio-temporal information of skeleton sequences. Given a skeleton sequence with T frames, N -dimensional features (scalar/vector) are extracted for each frame. The following three methods are used to encode the N × T feature into a H(eight) × W (idth) sized color image (256 × 256 in this paper).</p><p>Encoding method one (EM1): for scalar features including JJd, JLd and LLa, the jet colorbar <ref type="bibr" target="#b10">[11]</ref> is adopted to encode RGB channels jointly. The RGB value of pixel at h th row and w th column is </p><formula xml:id="formula_1">RGB(h, w) = colorbar((f w h −min F h )/(max(F h −min F h ))<label>(6)</label></formula><formula xml:id="formula_2">RGB(h, w) = (f w h − minF h )/(max(F h − minF h ) (7)</formula><p>where f w h ∈ R 3 is the vector of h th feature at w th frame. Note that the operations are applied on each dimension. EM3: this method encodes RGB channels based on scalar features from both subjects. Specifically, red channel is encoded based on features of main subject, green channel is encoded using features of the auxiliary subject, and blue channel is encoded based on both features. The encoding method is formulated as follows:</p><formula xml:id="formula_3">R(h, w) = 1 − (f w h − min F h )/(max(F h − min F h ) G(h, w) = (v w h − min V h )/(max(V h − min V h ) B(h, w) = 4 × R × G<label>(8)</label></formula><p>where f, F and v, V represent features from main subject and other subject specifically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">CNN training and score fusion</head><p>In this paper, the Caffenet (a version of Alexnet <ref type="bibr" target="#b12">[13]</ref>) is adopted as the CNN model. The protocols used in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> are adopted to train the CNN models from scratch. Given a testing skeleton sequence, thirteen types of images are generated and each type of image is recognized with a trained CNN model. All the outputs (scores) of the CNN models are then fused into a final score by element-wise multiplication, which has been verified in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. The fusion is done as follows:</p><formula xml:id="formula_4">label = F max (v 1 • v 2 · · · v 12 • v 13 )<label>(9)</label></formula><p>where v are the score vectors, • is the element-wise multiplication, and F max (. ) is a function to find the index of the maximum element.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Implementation details</head><p>Joint coordinates are normalized in a way similar to the method in <ref type="bibr" target="#b11">[12]</ref>, where the spine lengths of the same subject in each frame (from 'base of the spine' to 'spine') are normalized to 1, and the other limb lengths are scaled in equal proportions. The scheme to select the main subject is adopted from <ref type="bibr" target="#b11">[12]</ref>, where the skeleton sequence having larger variations is set to be the main subject. Before selection, joint coordinates are translated from camera coordinate system to the body coordinate system, as described in <ref type="bibr" target="#b4">[5]</ref>. The spatial features are directly calculated from normalized skeleton data to reduce the deviation introduced by the coordinate transformation.</p><p>Caffe was adopted as the CNN platform and a Nvidia Titan X GPU was used to run the experiments. The CNNs were trained using stochastic gradient descent (SGD) for a total of 30000 iterations. The models were trained from scratch and the weights were initialized using Gaussian Filter. The multistep scheme was used to train the CNNs with step sizes as 10000, 18000, 24000, 28000 specifically. The learning rate was initially set to 0.01 and multiplied by 0.1 every epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENT RESULTS</head><p>The proposed method was evaluated on NTU RGB+D Dataset. Currently, NTU RGB+D Dataset <ref type="bibr" target="#b11">[12]</ref> is the largest dataset for action recognition. It has 56578 samples of 60 different actions classes, which are captured under 18 settings with different camera viewpoints and heights. The actions include single-subject cases and multi-subject interaction cases and are performed by 40 subjects aged between 10 and 35. This dataset is challenging and there are two types of protocols for evaluation of methods, cross-subject and cross-view. In this paper, the cross-view protocol is used. The effectiveness of different types of spatial features, different joint selection schemes were evaluated. The images in each row are generated from the same sample, and the images in each column are generated using the same method. The images within the same row represent the difference of methods, and the images within the same column represent the difference between action classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation of spatial features</head><p>The results of individual features and different encoding methods are listed in <ref type="table" target="#tab_1">Table 1</ref>, as well as results of score-  <ref type="bibr" target="#b7">[8]</ref> 52.76% Dynamic Skeletons <ref type="bibr" target="#b13">[14]</ref> 65.22% HBRNN <ref type="bibr" target="#b14">[15]</ref> 63.97% Deep RNN <ref type="bibr" target="#b11">[12]</ref> 64.09% Part-aware LSTM <ref type="bibr" target="#b11">[12]</ref> 70.27% ST-LSTM+Trust Gate <ref type="bibr" target="#b15">[16]</ref> 77.70% JTM <ref type="bibr" target="#b8">[9]</ref> 75.20% Geometric Features <ref type="bibr" target="#b4">[5]</ref> 82.39% STA-LSTM <ref type="bibr" target="#b16">[17]</ref> 81.20% Proposed Method 82.31% multiplication fusion. There are five features evaluated, each of which was evaluated with different feature (joint) selection methods and different encoding methods. The methods are denoted in the form 'feature selection method -encoding method', which have been described in Section 2.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, images generated from samples of different actions have discriminative textures. In addition, the spatial features are encoded into different textures by different methods.</p><p>From <ref type="table" target="#tab_1">Table 1</ref>, it can be seen that the JJv feature is the best joint-joint feature, based on the comparisons of single results and fused results. Moreover, JLd seems to be the best feature among the five types of features, which coincidences with the observations reported by <ref type="bibr" target="#b4">[5]</ref>. Among the three kinds of joint selection methods, JS3 generally works better than the other two. This observation suggests that some of the joints are noise with regard to this task, which is consistent with the above analysis.</p><p>From <ref type="table" target="#tab_2">Table 2</ref> the results indicate that, compared with methods based hand-crafted features and those based on deep learning (RNNs and CNNs), the proposed method achieved state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, a method for skeleton-based action recognition using CNNs is proposed. This method explored encoding different spatial features into texture color images and achieved state-of-the-art results on NTU RGB+D Dataset. The experimental results indicated the effectiveness of texture images when used as spatio-temporal information representation, and the effectiveness of joint selection strategies for robust and cost-efficient computation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The framework of the proposed method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Samples generated by the proposed method on NTU RGB+D Dataset. Six samples from different actions are visualized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where f w h is the value of the h th feature at w th frame, and</figDesc><table><row><cell>f w h = f N ×h/H , i.e. the features are resized to H  *  W using T ×w/W</cell></row><row><cell>bilinear interpolation. F h = {f 1 h , f 2 h , ..., f T h }, colorbar() is</cell></row><row><cell>a mapping function which maps [0, 1] to corresponding RGB</cell></row><row><cell>colors.</cell></row><row><cell>EM2: for vector features like JJo and JJv, RGB channels</cell></row><row><cell>are encoded based on XYZ values respectively as follows:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results of different features and encoding methods. : this method encodes the RGB channels based on JJd, JLd and LLa respectively. Note 2: this method is not used for final score fusion.</figDesc><table><row><cell>Feature</cell><cell>Method</cell><cell>Accuracy</cell><cell>Fused Accuracy</cell></row><row><cell></cell><cell>JS1-EM2</cell><cell>62.45%</cell><cell></cell></row><row><cell>JJv</cell><cell>JS2-EM2</cell><cell>65.12%</cell><cell>75.23%</cell></row><row><cell></cell><cell>JS3-EM2</cell><cell>69.02%</cell><cell></cell></row><row><cell></cell><cell>JS1-EM2</cell><cell>64.11%</cell><cell></cell></row><row><cell>JJo</cell><cell>JS2-EM2 2</cell><cell>55.14%</cell><cell>73.51%</cell></row><row><cell></cell><cell>JS3-EM2</cell><cell>63.30%</cell><cell></cell></row><row><cell></cell><cell>JS1-EM1</cell><cell>59.18%</cell><cell>82.31%</cell></row><row><cell>JJd</cell><cell>JS2-EM1</cell><cell>62.86%</cell><cell>73.01%</cell></row><row><cell></cell><cell>JS3-EM1</cell><cell>62.95%</cell><cell></cell></row><row><cell>JLd</cell><cell>LS1-EM3 LS2-EM1</cell><cell>63.08% 59.71%</cell><cell>76.20%</cell></row><row><cell>LLa</cell><cell>LS1-EM3</cell><cell>62.57%</cell><cell>62.57%</cell></row><row><cell>Com 1</cell><cell>JS1&amp;LS1-EM2</cell><cell>62.00%</cell><cell>62.00%</cell></row><row><cell>Note 1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Experimental results (accuracy) on NTU RGB+D</figDesc><table><row><cell>Dataset</cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Lie Group</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3D points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on CVPR Workshops (CVPRW)</title>
		<meeting>IEEE Conference on CVPR Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convnets-based action recognition from depth maps through virtual cameras and pseudocoloring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conference on Multimedia</title>
		<meeting>ACM Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Action recognition from depth maps using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phlip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="509" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scene flow to action map: A new representation for RGB-D based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On geometric features for skeleton-based action recognition using multilayer LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Chih</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on CVPR Workshops</title>
		<meeting>IEEE Conference on CVPR Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining mid-level features for action recognition based on effective skeleton representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanling</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Digital lmage Computing: Techniques and Applications (DlCTA)</title>
		<meeting>IEEE Conference on Digital lmage Computing: Techniques and Applications (DlCTA)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conference on Multimedia</title>
		<meeting>ACM Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skeleton optical spectra based action recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint distance maps based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuankun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint angles similarities and hog2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshed</forename><surname>Ohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on CVPR Workshops</title>
		<meeting>IEEE Conference on CVPR Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
