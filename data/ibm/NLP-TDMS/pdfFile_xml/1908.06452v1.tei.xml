<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Neural Network with Median Layers for Denoising Salt-and-Pepper Contaminations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Gueguen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinming</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">¶</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
						</author>
						<title level="a" type="main">Convolutional Neural Network with Median Layers for Denoising Salt-and-Pepper Contaminations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Median layer</term>
					<term>Deep neural network</term>
					<term>Salt-and- pepper noise</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a deep fully convolutional neural network with a new type of layer, named median layer, to restore images contaminated by the salt-and-pepper (s&amp;p) noise. A median layer simply performs median filtering on all feature channels. By adding this kind of layer into some widely used fully convolutional deep neural networks, we develop an end-to-end network that removes the extremely high-level s&amp;p noise without performing any non-trivial preprocessing tasks, which is different from all the existing literature in s&amp;p noise removal. Experiments show that inserting median layers into a simple fully-convolutional network with the L2 loss significantly boosts the signal-to-noise ratio. Quantitative comparisons testify that our network outperforms the state-of-the-art methods with a limited amount of training data. The source code has been released for public evaluation and use (https://github.com/llmpass/medianDenoise).</p><p>Index Terms-Median layer, Deep neural network, Salt-andpepper noise † L. Liang is with the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image denoising is a well-studied yet not well-solved problem <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, where the goal is to recover the underlying signal from its contaminated observation. The contaminations can be categorized into many different types according to their distributions and behaviors, e.g., additive (Gaussian) noise, shot (Poisson) noise, JPEG noise, etc. We focus on the salt-and-pepper (s&amp;p) noise, which is an impulse contamination to the image. In an image with the s&amp;p noise, pixels become maximal or minimal values with a predefined probability, which is called the noise level, i.e. the higher this value is, the more pixels will be contaminated. The s&amp;p noise is a special case of random-value impulse noise defined in <ref type="bibr" target="#b4">[4]</ref> and <ref type="bibr" target="#b5">[5]</ref>. For a given noise level p ∈ (0, 1), an s&amp;p contaminated image could be defined as</p><formula xml:id="formula_0">I(i, j) =      0,</formula><p>r 1 &lt; p and r 2 &lt; 0.5 255, r 1 &lt; p and r 2 ≥ 0.5, I(i, j), r 1 ≥ p <ref type="bibr" target="#b0">(1)</ref> where both r 1 and r 2 are 2 random values generated on each pixel, with the former one determining if a pixel will be contaminated or not and later one controlling if that the pixel will turn to be the maximal (salt) value or the minimal (pepper) value. From Equation 1, one observes that the s&amp;p noise is neither like the additive (Gaussian) noise, which can be <ref type="bibr">(</ref> fully separated from the signal <ref type="bibr" target="#b0">[1]</ref>; nor like the shot (Poisson) noise, which is signal dependent <ref type="bibr" target="#b5">[5]</ref>. It appears as the pure noise at the contaminated locations (called missing pixel in <ref type="bibr" target="#b5">[5]</ref>) and therefore erases all signals there. This fact prevents us to use any optimization method in a continuous space to recover the signal, since the gradients estimated from missing pixels are totally not reliable and they will further scatter to other unpolluted locations. Traditional ways to recover images from s&amp;p pollution all require nonlinear searches and mappings. The search step <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref> generally determines the locations of the contaminated pixels and the mapping step tries to give a feasible estimate at each contaminated pixel by weightedly averaging the similar pixel values around it. This set of filters are named switching filters. However, when the noise level is increasing, the search step becomes more and more unreliable. On the other hand, the signal estimation step will also be degraded by the high-level noise since the similarity estimation becomes intractable.</p><p>To alleviate this limitation, <ref type="bibr" target="#b4">[4]</ref> uses switching templates to avoid noise disturbance in the process of measuring similarity. Based on the similarities, they extract repairable information in non-local regions instead of local patches. This filter is named as non-local switching filter (NLSF). The method uses a trained Convolutional Neural Network to finally refine the signal recovered by NLSF. Therefore, NLSF is considered as a prepocessing step to the neural network. This method is a combination of traditional methods and the learning-based method.</p><p>Besides the usual learning-based ideas that train models to arXiv:1908.06452v1 [cs.CV] 18 Aug 2019 denoise using pairs of clean images and their noisy versions, the noise-to-noise method <ref type="bibr" target="#b5">[5]</ref> trains models only on noisy images. They discover that training without using clean images can achieve, sometimes even exceeds, the result obtained by training using ground truths. Following this paradigm, <ref type="bibr" target="#b5">[5]</ref> shows the ability to remove the random-valued impulse noise, which can be considered as a superset of s&amp;p noise. To deal with the gradient loss problem introduced by pure noisy pixels, they adopt an annealed version of the "L 0 loss" to replace the traditional L 2 loss. The loss function is gradually changing from L 2 to L 0 as the training progress. However, the speed of this annealing procedure must be carefully chosen (usually reducing the power of the norm on the loss function according to the number of iterations). When the prediction is far from the truth, the loss function must be closer to L 2 ; when the prediction is getting closer enough, L 0 becomes more favorable, since L 0 loss emphasis the number of different pixels, which leads the learning process to a detail amendment stage.</p><p>We introduce the use of local nonlinear search into the neural network without performing any pre-processing step and also avoid changing the loss function from L 2 loss to some other losses that are not easy to optimize. We resort to median filter <ref type="bibr" target="#b11">[11]</ref>, which is the first efficient method to denoise the salt-and-pepper noise. By incorporating the median-filter-like operations into deep neural networks, our method outperforms state-of-the-art methods. Details of our methodology as well as the model design can be found in Section II, evaluations are presented in Section III. Section IV is for conclusion of our work. We release our source code, training dataset and pretrained models at https://github.com/llmpass/medianDenoise for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>Median filter is a traditional nonlinear filter which is especially efficient for removing impulse noise. It replaces the pixel centered in a given window with the median of this window. As shown in <ref type="figure">Figure 1</ref>, applying median filter on a highly contaminated image (b) removes spikes and therefore greatly improves the signal to noise ratio. Applying a 5×5 median filter once ( <ref type="figure">Figure 1c</ref>) and twice <ref type="figure">(Figure 1d</ref>),respectively, removes about 50% and 90% noise. A natural idea is to repeatedly apply the median filter upon the image until all spikes are replaced by the median in a fixed-size local window. It does remove the noise, however, it fails to recover the signal. The Peak Signal to Noise Ratio (PSNR) increases in the first several iterations but drops finally as the image becomes blocky and blurry, see <ref type="figure">Figure 1g</ref>. This phenomena indicates that the median filter deviates the signal too much from its original shape, which is also the main reason why modern researchers abandon median filter in denoising s&amp;p noise.</p><p>In addition, the best PSNR value appears at different iterations of repeated median filtering when denoising different levels of noise. <ref type="figure" target="#fig_0">Figure 2</ref> shows the higher density the noise is, the more iterations of median filters are required.</p><p>Our basic idea is to keep the ability of spike removal from the traditional median filter but try to recover the degradations introduced by it. <ref type="figure">Figure 3</ref> illustrates a simple 1D synthetic example. We contaminate an evenly-sampled 1D sine function (dotted curves in <ref type="figure">Figure 3a</ref>) by 50%-level s&amp;p noise. After that, we tried to use different ways to recover the clean signal: 1) Repeated Median filters, see Here, all Median and Gaussian filters have the same window size that equals to 5 pixels. One may observe the third schema yields the best approximations (green curves) to the original sine function, no matter in the aspect of the signal shape or mean square errors (mse) between the smoothed curves (solid) and the true signal (dotted) curve. Using only Median filters creates plateau-like artifacts; using only Gaussian filters over smooth the noisy curves. By alternating Median and Gaussian filters, apparent plateau-like artifacts are washed out while the resulting curve still stays close to the true signal. The quickdropping mse values between smoothed curves generated by the third schema and the truth quantitatively support our observation.</p><p>We leverage these observations to design our deep neural network model for 2d image denoising. We replace the Gaussian filter, which is a fix-parameter smoothing filter, by a set of learnable convolution operations and thus design an end-to-end fully convolutional network with Median and other convolutions alternatingly appearing.</p><p>Instead of directly applying median filters on the images, we implement median filtering as a neural network operation and perform it on different feature channels. In this way, we essentially remove spikes in different feature spaces and then combine the de-spiked features to predict a better noise removed image. On one hand, the median filtering in the feature space acts just like the switch filters in the traditional methods <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b4">[4]</ref>; on the other hand, the de-spike ability introduced by median operations allow the gradients to pass through the non-noisy pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Median layer definition</head><p>Median filter is applied to each element of a feature channel in a moving window fashion. For example, an input image that consists of RGB channels, corresponds to 3 feature channels; a set of features generated after the convolution generally contains many number of channels. For each feature channel, we first extract a set of given size (3 × 3, 5 × 5, ...) size patches centered at each pixel. Then, we find the median of the sequence formed by all elements in that patch. We show a simple tensorflow/python implementation of this median filter layer in Listing 1. Here, parameter x is a channel of the input tensor and k denotes an integer kernel size. In practice, this median layer is applied on each feature channel and then we concatenate them to form a new set of features, e.g. median layer will be applied 64 times given a set of 64 feature channels generated by Convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network architecture</head><p>As shown in <ref type="figure" target="#fig_5">Figure 4a</ref>, our network is a fully convolutional network, so that no restrictions are posed on the size of the input. It starts with 2 consecutive median layers, which are then followed by a sequence of residual blocks and median layers. The last part of the network is just residual blocks without inserting median layers in between them. In practice, we only insert median layers into the first half of the sequence of residual blocks. The first part of the network is dedicated to remove noise from the image, the second half of the network is designed for recovering the signal.</p><p>We choose to generate 64 features per convolution layer and our residual block is designed as a skip connection over 2 64-convolutions, followed by batch normalization layers and nonlinear activations (relu in practice), as shown in <ref type="figure" target="#fig_5">Figure 4b</ref>.</p><p>As mentioned beforehand, we stick to use the simplest L 2 loss as our objective function. This loss is simply defined as the the mean square error of the estimated image and the ground truth image, as minimizing mse directly relates to increase of denoise metrics psnr. Details can be found in Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EVALUATION</head><p>We design several experiments to evaluate the properties of median layers (Section III-B) and performances of the proposed network (Section III-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training and testing setup</head><p>For fair comparisons, we train all models with the same data set described in <ref type="bibr" target="#b12">[12]</ref> that contains 91 different images, which is also employed in other works <ref type="bibr" target="#b4">[4]</ref>. Since our network is a fully convolutional network, the input size can be arbitrary. We first resize these 91 images to 200 × 200 and then we generate 70 × 70 patches from them as clean images. We degrade each patch by the s&amp;p noise with levels from 10% to 90% with a step equals to 10% as a sequence of noisy images. The    models are trained to learn a series of weights in layers that can transfer the input noisy image to the clean image.</p><p>To quantitatively compare the performance of different methods, we perform denoising on 3 sets of the images. The first set of image consist of some classical images in the image processing field (also used in <ref type="bibr" target="#b4">[4]</ref>); the second set is BSD300 <ref type="bibr" target="#b13">[13]</ref> (https://www2.eecs.berkeley.edu/ Research/Projects/CS/vision/grouping/segbench/BSDS300/ html/dataset/images.html); the third set is Kodak Image Dataset (http://r0k.us/graphics/kodak/), which has been widely used as the evaluation set <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[5]</ref>.</p><p>The metric considered in the comparison is Peak Signal to Noise Ratio (PSNR). It is defined by P SN R = 10 log 10 (</p><formula xml:id="formula_1">255 2 M SE ),<label>(2)</label></formula><p>where M SE is the mean-squared error between two M × N 8-bit images I 1 and I 2 , defined by</p><formula xml:id="formula_2">M SE = M,N [I 1 (m, n) − I 2 (m, n)] 2 M × N .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effects of the median layer</head><p>The first experiment is designed to show the effectiveness of the median layer. We trained several pairs of fully convolutional networks mainly consisting of residual or convolution-batchNorm-relu blocks, but one with median layers and one without them.</p><p>We train two sets of deep fully convolutional networks, the first set of networks are traditional ones that do not contain any median layers, the second set of networks are the counterparts of the first set with median layers inserted into them with the same strategy shown in <ref type="figure" target="#fig_5">Figure 4a</ref>, i.e. the first half of the network contains median layers, the second half does not. The networks in the first set consist of repeated blocks of convolution, batch normalization and activation or repeated residual blocks as shown in <ref type="figure" target="#fig_5">Figure 4b</ref>.     Losses in <ref type="figure">Figure 6</ref> shows how median layers boost the PSNR value of the network. Training losses of two networks with median layers inserted converge to a better minima comparing to the losses without the median layers. The "ConvRelu 16" network in <ref type="figure">Figure 6</ref> is a DnCnn <ref type="bibr" target="#b0">[1]</ref> style network, which consists of 16 stacked Convolution-BatchNormalization-Activation units. The "ResBlock 16" network in <ref type="figure">Figure 6</ref> is formed by simply replacing the Convolution-BatchNormalization-Activation units to residual blocks shown in <ref type="figure" target="#fig_5">Figure 4b</ref>. All convolution layers here generate 64 features.</p><p>PSNR comparisons of models with and without median layers inserted <ref type="table" target="#tab_2">(Table I)</ref> show the improvements of PSNR.</p><p>The PSNR values of models with median layers are usually 0.5db higher than the ones that do not have them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons to the state-of-the-arts</head><p>Both quantitative and qualitative comparisons to the stateof-the-arts are performed in this section.</p><p>1) Quantitative comparisons: We quantitatively compare our network in <ref type="figure" target="#fig_5">Figure 4a</ref> to several state-of-the-art methods. Baselines include 5 traditional methods: Decision-Based Algorithm (DBA) <ref type="bibr" target="#b14">[14]</ref>, Adaptive Switching Non-local Filter (NASNLM) <ref type="bibr" target="#b9">[9]</ref>, PARIGI <ref type="bibr" target="#b10">[10]</ref>, NLSF <ref type="bibr" target="#b4">[4]</ref> (prepocessing part of NLSF-CNN), NLSF-MLP (NLSF with multi-layer perception proposed in <ref type="bibr" target="#b15">[15]</ref>) and 2 most recent neural network based methods: NLSF-CNN <ref type="bibr" target="#b4">[4]</ref> and Noise2Noise <ref type="bibr" target="#b5">[5]</ref>, as shown in <ref type="table" target="#tab_2">Table II</ref>. Many methods here are designed for denoising s&amp;p noise with moderate levels, therefore, we choose to evaluate the methods under noise levels equal to 30%, 50% and 70%.</p><p>In addition, we also compare our method to DeepBoosting <ref type="bibr" target="#b7">[7]</ref> and Noise2Noise <ref type="bibr" target="#b5">[5]</ref> on Kodak image dataset, as shown in <ref type="table" target="#tab_2">Table III</ref>.</p><p>Our method outperforms most of the state-of-the-arts besides the pepper image. Comparing to current best baseline method Noise2Noise <ref type="bibr" target="#b5">[5]</ref>, PSNR values achieved by our model is about 1-2db higher in average and the severer the noise contamination, the comparably better our method performs.</p><p>2) Qualitative comparisons on extremely high-level noise: We further qualitatively compare our method to noise2noise method [5] on denoising extremely high-level s&amp;p noise (noise level equals to 90%). In <ref type="figure">Figure 7</ref>, we choose three images from BSD300 dataset, where different challenges can be found there:</p><p>• both sharp feature and smooth background exist in the first image ( <ref type="figure">Figure 7d</ref>); • pure black and white interphase pattern in the second image ( <ref type="figure">Figure 7h</ref>); • noise-like nature scene background <ref type="figure">(Figure 7l</ref>).</p><p>The left-most column in <ref type="figure">Figure 7</ref> shows the contaminated images, which are the noisy version of their counterparts in the right-most column. One may hardly see the contours of the original salient objects there, since 90% of pixels become either maximal or minimal values.</p><p>Our method performs consistently better than noise2noise on all of these challenges. In <ref type="figure">Figure 7b</ref>, noise2noise generates many small white blocky artifacts on the sky (red rectangle) and also blurs the sharp edges (blue rectangle) of the windows. Both of these 2 degradations are alleviated in our result shown in <ref type="figure">Figure 7c</ref>.</p><p>Recovering underlying signal with pure black and white interphase pattern from high-level s&amp;p noise contamination is a very difficult problem because both signal and noise are almost binary in each channel. The method may have a hard time to distinguish which pixel is contaminated. By comparing the results shown in <ref type="figure">Figure 7f</ref> (noise2noise) and <ref type="figure">Figure 7g</ref> (ours), one may observe that our method produces higher quality images.</p><p>Noise-like patterns are common to many nature scene images, for example, the grass and the feathers of the owl in <ref type="figure">Figure 7l</ref> and the leaves in the bridge image in <ref type="table" target="#tab_2">Table II</ref>. We observe that the image still looks noisy after being processed by noise2noise method, where apparent small blue and red dots stand out on the grass <ref type="figure">(Figure 7j</ref>). However, such dots are invisible in our result, as shown in <ref type="figure">Figure 7k</ref>.</p><p>IV. CONCLUSION In this paper, we show that incorporating the median filtering technique in the deep neural network helps achieving compelling results in denoising the s&amp;p noise, especially when the noise level is high. The ability of the median layer to denoise is also experimentally testified with increasing PSNR. Our work opens the door in adopting traditional lowlevel nonlinear signal processing techniques in deep neural networks. The methodology of inserting non-linear spatial layers may boost the performances of some well-known deep networks.</p><p>The median is the optimum point of a set of values under L 1 norm, which minimizes the sum of absolute deviations. This fact makes median layers act as a regularizer to the feature channels. Unlike the annealing procedure on the loss function adopted in <ref type="bibr" target="#b5">[5]</ref>, where the speed of evolving the loss from L 2 to L 0 must be carefully chosen to achieve the best result (with respect to the amount of noises), median layers is a more feasible way to control the quality of the extracted features. A single model can be trained to recover latent images with different levels of noise contaminations only using L 2 loss.</p><p>Spatial filtering have been invented and could be leveraged into convolutional neural networks to deal with images affected by non-linear noise. More study on the median placements could result in understanding its impact in the process. Mingqiang Wei received his Ph.D degree (2014) in Computer Science and Engineering from the Chinese University of Hong Kong (CUHK). He is an associate professor at the School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics (NUAA). Before joining NUAA, he served as an assistant professor at Hefei University of Technology, and a postdoctoral fellow at CUHK. His research interest is computer graphics with an emphasis on smart geometry processing. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Peak Signal to Noise Ratio trends with respect to the number of iterations of repeated 5 × 5 median filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3b-d; 2) Repeated Gaussian filters, see Figure 3e-f; 3) Alternating Median and Gaussian filters, see Figure 3h-j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Noisy signal (b) Median filtered (c) Median filter applied twice (d) Median filter applied 3 times (e) Gaussian filtered (f) Gaussian filter applied twice (g) Gaussian filter applied 3 times (h) Median filtered followed by Gaussian filtered (i) filters applied: Median, Gaussian and Median (j) filters applied: Median, Gaussian, Median and Gaussian Fig. 3: 1D signal denoising example using median filters and gaussian filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>d e f f i n d m e d i a n s ( x , k = 3 ) : p a t c h e s = t f . e x t r a c t i m a g e p a t c h e s ( x , k s i z e s = [ 1 , k , k , 1 ] , s t r i d e s = [ 1 , 1 , 1 , 1 ] , r a t e s = [ 1 , 1 , 1 , 1 ] , p a d d i n g = 'SAME ' ) m idx = i n t ( k * k / 2 + 1 ) t o p , = t f . nn . t o p k ( p a t c h e s , m idx , s o r t e d = T r u e ) median = t f . s l i c e ( t o p , [ 0 , 0 , 0 , m idx −1], [−1, −1, −1, 1 ] ) r e t u r n media Listing 1: Tensorflow implementation of Median layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Fully convolutional network with median layers in between residual blocks.(b) Our residual blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Our network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>W/ medians (32.16db) (d) W/O medians (28.70db) Denoise results with and without median layers on an image of BSD300, measured by PSNR (db). Training losses with and without median layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Xinming</head><label></label><figDesc>Wu received the Ph.D. degree in geophysics from the Colorado School of Mines, Golden, CO, USA, in 2016. He was a member of the Center for Wave Phenomena, Colorado School of Mines. He is currently a professor at School of earth and space sciences of University of Science and Technology China (USTC). Post-Doctoral Fellow with The University of Texas at Austin, Austin, TX, USA. His research interests include image processing, 3-D seismic interpretation, subsurface modeling, and geophysical inversion. Dr. Wu received the awards for the Best Paper in Geophysics in 2016 and the Best Student Poster Paper presented at the 2017 SEG Annual Meeting. Jing Qin is currently an assistant professor in the Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University. He received his PhD degree in Computer Science and Engineering from the Chinese University of Hong Kong in 2009. Dr Qin received the Champion in the 3rd Hong Kong Innovation Day and Innovation Awards Competition, Medical Image Analysis-MICCAI'17 Best Paper Award, the Best Paper Award in Medical Image Computing in International Conference on Medical Imaging and Augmented Reality 2016, the Hong Kong Medical and Health Device Industries Association Student Research Award in 2009 and Global Scholarship Program for Research Excellence (CNOOC Grants) from CUHK in 2008. He and his collaborators were nominated for outstanding paper award in International Simulation and Gaming Association 40th Annual Conference in 2009. Dr Qin's research interests include medical image processing, virtual/augmented reality for healthcare and medicine training, deep learning, visualization and human-computer interaction and health informatics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>PSNR (db) comparisons w/o Median layers on BSD300.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>PSNR (db) Comparisons with state-of-the-arts on a set of classic images and BSD300 image database. Best performances of every noise levels of different images are in bold.</figDesc><table><row><cell>Noise level</cell><cell cols="2">DeepBoosting [7] Noise2Noise [5]</cell><cell>Ours</cell></row><row><cell>30%</cell><cell>21.69</cell><cell>34.95</cell><cell>36.39</cell></row><row><cell>50%</cell><cell>19.50</cell><cell>32.27</cell><cell>34.35</cell></row><row><cell>70%</cell><cell>15.74</cell><cell>30.49</cell><cell>31.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc></figDesc><table /><note>PSNR (db) Comparisons with state-of-the-arts on Kodak image database. Best performances of every noise levels of different images are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Luming Liang is a senior researcher at Microsoft. Before working at Microsoft, Luming was a software engineer and then a data &amp; applied scientist at Uber and Microsoft, respectively. He received his BSc. and MSc. in the School of Information Science and Engineering from Central South University, China, in 2005 and 2008, respectively and his PhD in the Department of Electrical Engineering and Computer Science from Colorado School of Mines, USA in 2014. His primary research interest is finding shape correspondences and image analysis. Deng is a master candidate at Nanjing University of Aeronautics and Astronautics (NUAA), China. He received his Bachelor's degree in the University of Electronic Science and Technology of China. His research interests include deep learning, image processing and computer vision. Lionel Gueguen is a Senior Engineer at Uber, CO, USA since 2016, where he conducts research and engineering for information extraction from images. He had been an R&amp;D Scientist with Image Mining Labs of DigitalGlobe Inc. Lionel received the engineering degree in telecommunications and the M.S. degree in signal and image processing from the Ecole Nationale Superieure des Telecommunications de Bretagne, Brest, France, in 2004 and the Ph.D. degree in signal and image processing from the Ecole Nationale Superieure des Telecommunications, Paris, France, in 2007.</figDesc><table><row><cell>Sen</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for CNN based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">When image denoising meets high-level vision tasks: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="842" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<idno type="DOI">10.24963/ijcai.2018/117</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/117" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A convolutional neural networks denoising approach for salt and pepper noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Reng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noise2noise: Learning image restoration without clean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munkberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hasselgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2971" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully convolutional network with multi-step reinforcement learning for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep boosting for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision 2018 (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An efficient switching median filter based on local outlier factor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="551" to="554" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive switching non-local filter for the restoration of salt and pepper impulse-corrupted digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tairan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arabian Journal for Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3233" to="3246" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parigi: a patch-based approach to remove impulse-gaussian noise from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desolneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Process On Line</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="130" to="154" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast two-dimensional median filtering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="13" to="18" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int&apos;l Conf. Computer Vision</title>
		<meeting>8th Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new fast and efficient decisionbased algorithm for removal of high-density impulse noises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebenezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="189" to="192" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with bm3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>2012 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
