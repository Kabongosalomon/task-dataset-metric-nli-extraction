<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 ASSEMBLENET: SEARCHING FOR MULTI-STREAM NEURAL CONNECTIVITY IN VIDEO ARCHITECTURES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
							<email>mryoo@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
							<email>tanmingxing@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
							<email>anelia@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 ASSEMBLENET: SEARCHING FOR MULTI-STREAM NEURAL CONNECTIVITY IN VIDEO ARCHITECTURES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. Architectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning to represent videos is a challenging problem. Because a video contains spatio-temporal data, its representation is required to abstract both appearance and motion information. This is particularly important for tasks such as activity recognition, as understanding detailed semantic contents of the video is needed. Previously, researchers approached this challenge by designing a two-stream model for appearance and motion information respectively, combining them by late or intermediate fusion to obtain successful results: <ref type="bibr" target="#b20">Simonyan &amp; Zisserman (2014)</ref> <ref type="bibr">;</ref><ref type="bibr" target="#b6">Feichtenhofer et al. (2016b;</ref><ref type="bibr">a;</ref>. However, combining appearance and motion information is an open problem and the study on how and where different modalities should interchange representations and what temporal aspect/resolution each stream (or module) should focus on has been very limited.</p><p>In this paper, we investigate how to learn feature representations across spatial and motion visual clues. We propose a new multi-stream neural architecture search algorithm with connection learning guided evolution, which focuses on finding higher-level connectivity between network blocks taking multiple input streams at different temporal resolutions. Each block itself is composed of multiple residual modules with space-time convolutional layers, learning spatio-temporal representations. Our architecture learning not only considers the connectivity between such multi-stream, multi-resolution blocks, but also merges and splits network blocks to find better multi-stream video CNN architectures. Our objective is to address two main questions in video representation learning: (1) what feature representations are needed at each intermediate stage of the network and at which resolution and (2) how to combine or exchange such intermediate representations (i.e., connectivity learning). Unlike previous neural architecture search methods for images that focus on finding a good 'module' of convolutional layers to be repeated in a single-stream networks <ref type="bibr" target="#b17">Real et al., 2019)</ref>, our objective is to search for higher-level connections between multiple sequential or concurrent blocks to form multi-stream architectures.</p><p>We propose the concept of AssembleNet, a new method of fusing different sub-networks with different input modalities and temporal resolutions. AssembleNet is a general formulation that Our network has 4 block levels (+ the stem level connected to raw data). Each convolutional block has its own output channel size (i.e., the number of filters) C and the temporal resolution r controlling the 1D temporal convolutional layers in it.</p><p>allows representing various forms of multi-stream CNNs as directed graphs, coupled with an efficient evolutionary algorithm to explore the network connectivity. Specifically, this is done by utilizing the learned connection weights to guide evolution, in addition to randomly combining, splitting, or connecting sub-network blocks. AssembleNet is a 'family' of learnable architectures; they provide a generic approach to learn connectivity among feature representations across input modalities, while being optimized for the target task. We believe this is the first work to (i) conduct research on automated architecture search with multi-stream connections for video understanding, and (ii) introduce the new connection-learning-guided evolutionary algorithm for neural architecture search. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example learned AssembleNet. The proposed algorithm for learning video architectures is very effective: it outperforms all prior work and baselines on two very challenging benchmark datasets, and establishes a new state-of-the-art. AssembleNet models use equivalent number of parameters to standard two-stream (2+1)D ResNet models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PREVIOUS WORK</head><p>A video is a spatio-temporal data (i.e., image frames concatenated along time axis), and its representation must abstract both spatial and temporal information. Full 3D space-time (i.e., XYT) convolutional layers as well as (2+1)D convolutional layers have been popularly used to represent videos <ref type="bibr" target="#b22">(Tran et al., 2014;</ref><ref type="bibr" target="#b2">Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b23">Tran et al., 2018;</ref><ref type="bibr" target="#b29">Xie et al., 2018)</ref>. Researchers studied replacing 2D convolutional layers in standard image-based CNNs such as Inception <ref type="bibr" target="#b21">(Szegedy et al., 2016)</ref> and ResNet <ref type="bibr" target="#b10">(He et al., 2016)</ref>, so that it can be directly used for video classification.</p><p>Two-stream network designs, which combine motion and appearance inputs, are commonly used (e.g., <ref type="bibr" target="#b20">Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b5">Feichtenhofer et al., 2016a;</ref><ref type="bibr" target="#b6">2016b)</ref>. Combining appearance information at two different temporal resolutions (e.g., 24 vs. 3 frames per second) with intermediate connections has been proposed by <ref type="bibr" target="#b8">Feichtenhofer et al. (2018)</ref>. Late fusion of the two-stream representations or architectures with more intermediate connections <ref type="bibr" target="#b4">(Diba et al., 2019)</ref>, have also been explored. However, these video CNN architectures are the result of careful manual designs by human experts.</p><p>Neural Architecture Search (NAS), the concept of automatically finding better architectures based on data, is becoming increasingly popular <ref type="bibr" target="#b34">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b13">Liu et al., 2018)</ref>. Rather than relying on human expert knowledge to design a CNN model, neural architecture search allows the machines to generate better performing models optimized for the data. The use of reinforcement learning controllers <ref type="bibr" target="#b34">(Zoph &amp; Le, 2017;</ref> as well as evolutionary algorithms <ref type="bibr" target="#b17">(Real et al., 2019)</ref> have been studied, and they meaningfully outperform handcrafted architectures. Most of these works focus on learning architectures of modules (i.e., groupings of layers and their connections) to be repeated within a fixed single-stream meta-architecture (e.g., ResNet) for image-based object classification. One-shot architecture search to learn differentiable connections <ref type="bibr" target="#b1">(Bender et al., 2018;</ref><ref type="bibr" target="#b14">Liu et al., 2019)</ref> has also been successful for images. However, it is very challenging to directly extend such work to find multi-stream models for videos, as it requires preparing all possible layers and interactions the final architecture may consider using. In multi-stream video CNNs, there are many possible convolutional blocks with different resolutions, and fully connecting them requires a significant amount of memory and training data, which makes it infeasible.</p><p>Our work is also related to <ref type="bibr" target="#b0">Ahmed &amp; Torresani (2017)</ref> which used learnable gating to connect multiple residual module branches, and to the RandWire network <ref type="bibr" target="#b30">(Xie et al., 2019)</ref>, which showed that randomly connecting a sufficient number of convolutional layers creates performant architectures. However, similar to previous NAS work, the latter focuses only on generating connections between the layers within a block. The meta-architecture is fixed as a single stream model with a single input modality. In this work, our objective is to learn high-level connectivity between multi-stream blocks for video understanding driven by data. We confirm experimentally that in the multi-stream video CNNs, where multiple types of input modalities need to be considered at various resolutions, randomly connecting blocks is insufficient and the proposed architecture learning strategy is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ASSEMBLENET</head><p>We propose a new principled way to find better neural architectures for video representation learning. We first expand a video CNN to a multi-resolution, multi-stream model composed of multiple sequential and concurrent neural blocks, and introduce a novel algorithm to search for the optimal connectivity between the blocks for a given task.</p><p>We model a video CNN architecture as a collection of convolutional blocks (i.e., sub-networks) connected to each other. Each block is composed of a residual module of space-time convolutional layers repeated multiple times, while having its own temporal resolution. The objective of our video architecture search is to automatically (1) decide the number of parallel blocks (i.e., how many streams to have) at each level of the network, (2) choose their temporal resolutions, and (3) find the optimal connectivity between such multi-stream neural blocks across various levels. The highly interconnected convolutional blocks allow learning of the video representations combining multiple input modalities at various temporal resolutions. We introduce the concept of connection-learningguided architecture evolution to enable multi-stream architecture search.</p><p>We name our final architecture as an 'AssembleNet', since it is formulated by assembling (i.e., merging, splitting, and connecting) multiple building blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GRAPH FORMULATION</head><p>In order to make our neural architecture evolution consider multiple different streams with different modalities at different temporal resolutions, we formulate the multi-stream model as a directed acyclic graph. Each node in the graph corresponds to a sub-network composed of multiple convolutional layers (i.e., a block), and the edges specify the connections between such sub-networks. Each architecture is denoted as G i = (N i , E i ) where N i = {n 0i , n 1i , n 2i , · · · } is the set of nodes and E i is the set of edges defining their connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nodes.</head><p>A node in our graph representation is a ResNet block composed of a fixed number of interleaved 2D and (2+1)D residual modules. A '2D module' is composed of a 1x1 conv. layer, one 2D conv. layer with filter size 3x3, and one 1x1 convolutional layer. A '(2+1)D module' consists of a temporal 1D convolutional layer (with filter size 3), a 2D conv. layer, and a 1x1 conv. layer. In each block, we repeat a regular 2D residual module followed by the (2+1)D residual module m times.</p><p>Each node has its own block level, which naturally decides the directions of the edges connected to it. Similar to the standard ResNet models, we made the nodes have a total of four block levels (+ the stem level). Having multiple nodes of the same level means the architecture has multiple parallel 'streams'. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates an example. Each level has a different m value: 1.5, 2, 3, and 1.5. m = 1.5 means that there is one 2D module, one (2+1)D module, and one more 2D module. As a result, the depth of our network is 50 conv. layers. We also have a batch normalization layer followed by a ReLU after every conv. layer.</p><p>There are two special types of nodes with different layer configurations: source nodes and sink nodes. A source node in the graph directly takes the input and applies a small number of convolutional/pooling layers (it is often referred as the 'stem' of a CNN model). In video CNNs, the input is a 4D tensor (XYT + channel) obtained by concatenating either RGB frames or optical flow images along the time axis. Source nodes are treated as level-0 nodes. The source node is composed of one 2D conv. layer of filter size 7x7, one 1D temporal conv. layer of filter size 5, and one spatial max pooling layer. The 1D conv. is omitted in optical flow stems. A sink node generates the final output of the model, and it is composed of one pooling, one fully connected, and one softmax layer. The sink node is also responsible for combining the outputs of multiple nodes at the highest level, by concatenating them after the pooling. More details are provided in Appendix.</p><p>Each node in the graph also has two attributes controlling the convolutional block: its temporal resolution and the number of channels. We use temporally dilated 1D convolution to dynamically change the resolution of the temporal convolutional layers in different blocks, which are discussed more below. The channel size (i.e., the number of filters) of a node could take arbitrary values, but we constrain the sum of the channels of all nodes in the same block level to be a constant so that the capacity of an AssembleNet model is equivalent to a ResNet model with the same number of layers.</p><p>Temporally Dilated 1D Convolution. One of the objectives is to allow the video architectures to look at multiple possible temporal resolutions. This could be done by preparing actual videos with different temporal resolutions as in <ref type="bibr" target="#b8">Feichtenhofer et al. (2018)</ref> or by using temporally 'dilated convolutions as we introduce here. Having dilated filters allow temporal 1D conv. layers to focus on different temporal resolution without losing temporal granularity. This essentially is a 1D temporal version of standard 2D dilated convolutions used in  or <ref type="bibr" target="#b31">Yu &amp; Koltun (2016)</ref>:</p><p>Let k be a temporal filter (i.e., a vector) with size 2d + 1. The dilated convolution operator * r is similar to regular convolution but has different steps for the summation, described as:</p><formula xml:id="formula_0">(F * r k)(t) = t1+rt2=t F (t 1 )k(t 2 + d)<label>(1)</label></formula><p>where t, t 1 , and t 2 are time indexes. r indicates the temporal resolution (or the amount of dilation), and the standard 1D temporal convolution is a special case where r = 1. In the actual implementation, this is done by inserting r − 1 number of zeros between each element of k to generate k , and then convolving such zero-inflated filters with the input: F * r k = F * k . Importantly, the use of the dilated convolution allows different intermediate sub-network blocks (i.e., not just input stems) to focus on very different temporal resolutions at different levels of the convolutional architecture.</p><p>Note that our temporally dilated convolution is different from the one used in <ref type="bibr" target="#b12">Lea et al. (2017)</ref>, which designed a specific layer to combine representations from different frames with various step sizes. Our layers dilate the temporal filters themselves. Our dilated convolution can be viewed as a direct temporal version of the standard dilated convolutions used in ; <ref type="bibr" target="#b31">Yu &amp; Koltun (2016)</ref>.</p><p>Edges. Each directed edge specifies the connection between two sub-network blocks, and it describes how a representation is transferred from one block to another block. We constrain the direction of each edge so that it is connected from a lower level block to a higher level block to avoid forming a cycle and allow parallel streams. A node may receive inputs from any number of lower-level nodes (including skip connections) and provide its output to any number of higher-level nodes.</p><p>Our architectures use a (learnable) weighted summation to aggregate inputs given from multiple connected nodes. That is, an input to a node is computed as</p><formula xml:id="formula_1">F in = i sigmoid(w i ) · F out i , where F out i</formula><p>are output tensors (i.e., representations) of the nodes connected to the node and w i are their corresponding weights. Importantly, each w i is considered as a variable that has to be learned from training data through back propagation. This has two key advantages compared to conventional feature map concatenation: (i) The input tensor size is consistent regardless of the number of connections. (ii) We use learned connection weights to 'guide' our architecture evolution algorithm in a preferable way, which we discuss more in Section 3.2.</p><p>If the inputs from different nodes differ in their spatial size, we add spatial max pooling and striding to match their spatial size. If the inputs have different channel sizes, we add a 1x1 conv. layer to match the bigger channel size. Temporal sizes of the representations is always consistent in our graphs, as there is no temporal striding in our formulation and the layers in the nodes are fully convolutional.  The fitness of the third model was worse than the second model (due to random mutations), but it was high enough to survive in the population pool and eventually evolve into a better model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EVOLUTION</head><p>We design an evolutionary algorithm with discrete mutation operators that modify nodes and edges in architectures over iterations. The algorithm maintains a population of P different architectures, P = {G 1 , G 2 , · · · , G |P | }, where each architecture G is represented with a set of nodes and their edges as described above.</p><p>The initial population is formed by preparing a fixed number of randomly connected architectures (e.g., |P | = 20). Specifically, we (1) prepare a fixed number of stems and nodes at each level (e.g., two per level), (2) apply a number of node split/merge mutation operators which we discuss more below, and (3) randomly connect nodes with the probability p = 0.5 while discarding architectures with graph depth &lt; 4. As mentioned above, edges are constrained so that there is no directed edge reversing the level ordering. Essentially, a set of overly-connected architectures are used as a starting point. Temporal resolutions are randomly assigned to the nodes.</p><p>We use the tournament selection algorithm <ref type="bibr" target="#b9">(Goldberg &amp; Deb, 1991)</ref> as the main evolution framework: At each evolution round, the algorithm updates the population by selecting a 'parent' architecture and mutating (i.e., modifying) it to generate a new 'child' architecture. The parent is selected by randomly sampling a subset of the entire population P ⊂ P , and then computing the architecture with the highest 'fitness': G p = argmax Gi∈P f (G i ) where f (G) is the fitness function. Our fitness is defined as a video classification accuracy of the model, measured by training the model with a certain number of initial iterations and then evaluating it on the validation set as its proxy task. More specifically, we use top-1 accuracy + top-5 accuracy as the fitness function. The child is added into the population, and the model with the least fitness is discarded from the population.</p><p>A child is evolved from the parent by following two steps. First, it changes the block connectivity (i.e., edges) based on their learned weights: 'connection-learning-guided evolution'. Next, it applies a random number of mutation operators to further modify the node configuration. The mutation operators include (1) a random modification of the temporal resolution of a convolutional block (i.e., a node) as well as <ref type="formula" target="#formula_2">(2)</ref> a merge or split of a block. When splitting a node into two nodes, we make their input/output connections identical while making the number of channels in their convolutional layers half that of the node before the split (i.e., C = C p /2 where C p is the channel size of the parent). More details are found in Appendix. As a result, we maintain the total number of parameters, since splitting or merging does not change the number of parameters of the convolutional blocks.</p><p>Connection-Learning-Guided Mutation. Instead of randomly adding, removing or modifying block connections to generate the child architecture, we take advantage of the learned connection weights from its parent architecture. Let E p be the set of edges of the parent architecture. Then the edges of the child architecture E c are inherited from E p , by only maintaining high-weight connections while replacing the low-weight connections with new random ones. Specifically, E c = E 1 c ∪ E 2 c :</p><formula xml:id="formula_2">E 1 c = {e ∈ E p | W e &gt; B} , E 2 c = e ∈ (E * − E p ) | |E p − E 1 c | |E − E p | &gt; X e<label>(2)</label></formula><p>where X ∼ unif(0, 1) and E * is the set of all possible edges. E 1 c corresponds to the edges the child architecture inherits from the parent architecture, decided based on the learned weight of the edge W e . B, which controls whether or not to keep an edge from the parent architecture, could either be a constant threshold or a random variable following a uniform distribution: B = b or B = X B ∼ unif(0, 1). E 2 c corresponds to the new randomly added edges which were not in the parent architecture. We enumerate through each possible new edge, and randomly add it with the probably of |E p − E 1 c |/|E − E p |. This makes the expected total number of added edges to be |E p − E 1 c |, maintaining the size of E c . <ref type="figure" target="#fig_2">Figure 2</ref> shows an example of the evolution process and <ref type="figure" target="#fig_3">Figure 3</ref> shows final architectures.</p><p>Evolution Implementation Details. Initial architectures are formed by randomly preparing either {2 or 4} stems, two nodes per level at levels 1 to 3, and one node at level 4. We then apply 1∼5 random number of node split operators so that each initial architecture has a different number of nodes. Each node is initialized with a random temporal resolution of 1, 2, 4, or 8. As mentioned, each possible connection is then added with the probability of p = 0.5. At each evolution round, the best-performing parent architecture is selected from a random subset of 5 from the population. The child architecture is generated by modifying the connections from the parent architecture (Section 3.2). A random number of node split, merge, or temporal resolution change mutation operators (0∼4) are then applied. Evaluation of each architecture (i.e., measuring the fitness) is done by training the model for 10K iterations and then measuring its top-1 + top-5 accuracy on the validation subset. The Moments-in-Time dataset, described in the next section, is used as the proxy dataset to measure fitness. The evolution was run for ∼200 rounds, although a good performing architecture was found within only 40 rounds (e.g., <ref type="figure" target="#fig_3">Figure 3</ref>-right). <ref type="figure" target="#fig_0">Figure 1</ref> shows the model found at the 165th round. 10K training iterations of each model during evolution took 3∼5 hours; with our setting, evolving a model for 40 rounds took less than a day with 10 parallel workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>Charades Dataset. We first test on the popular Charades dataset <ref type="bibr" target="#b18">(Sigurdsson et al., 2016)</ref> which is unique in the activity recognition domain as it contains long sequences. It is one of the largest public datasets with continuous action videos, containing 9848 videos of 157 classes (7985 training and 1863 testing videos). Each video is ∼30 seconds. It is a challenging dataset due to the duration and variety of the activities. Activities may temporally overlap in a Charades video, requiring the model to predict multiple class labels per video. We used the standard 'Charades v1 classify' setting for the evaluation. To comply with prior work (e.g. <ref type="bibr" target="#b8">Feichtenhofer et al., 2018)</ref>, we also report results when pre-training on Kinetics <ref type="bibr" target="#b2">(Carreira &amp; Zisserman, 2017)</ref>, which is another large-scale dataset.   We note that Kinetics is shrinking in size (∼15% videos removed from the original Kinetics-400) and the previous versions are no longer available from the official site.</p><p>Moments in Time (MiT) Dataset. The Moments in Time (MiT) dataset <ref type="bibr" target="#b15">(Monfort et al., 2018)</ref> is a large-scale video classification dataset with more than 800K videos (∼3 seconds per video). It is a very challenging dataset with the state-of-the-art models obtaining less than 30% accuracy. We use this dataset for the architecture evolution, and train/test the evolved models. We chose the MiT dataset because it provides a sufficient amount of training data for video CNN models and allows stable comparison against previous models. We used its standard classification evaluation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS</head><p>Tables 1 and 2 compare the performance of AssembleNet against the state-of-the-art models. We denote AssembleNet more specifically as AssembleNet-50, since its depth is 50 layers and has an equivalent number of parameters to ResNet-50. AssembleNet-101 is its 101 layer version having equivalent number of parameters to ResNet-101. AssembleNet is outperforming prior works on both datasets, setting new state-of-the-art results for them. Its performance on MiT is the first above 34%. We also note that the performances on Charades is even more impressive at 58.6 whereas previous known best results are 42.5 and 45.2. For these experiments, the architecture search was done on the  MiT dataset, and then the found models are trained and tested on both datasets, which demonstrates that the found architectures are useful across datasets.</p><p>In addition, we compare the proposed connection-learning-guided evolution with random architecture search and the standard evolutionary algorithm with random connection mutations. We made the standard evolutionary algorithm randomly modify 1/3 of the total connections at each round, as that is roughly the number of edges the connection-learning-guided evolution modifies. <ref type="figure" target="#fig_4">Figure 4</ref> shows the results, visualizing the average fitness score of the three top-performing models in each pool. We observe that the connection-learning-guided evolution is able to find better architectures, and it is able to do so more quickly. The standard evolution performs similarly to random search and is not as effective. We believe this is due to the large search space the approach is required to handle, which is exponential to the number of possible connections. For instance, if there are N nodes, the search space complexity is 2 O(N 2 ) just for the connectivity search. Note that the initial ∼30 rounds are always used for random initialization of the model population, regardless of the search method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDIES</head><p>We conduct an ablation study comparing the evolved AssembleNet to multiple (2+1)D two-stream (or multi-stream) architectures which are designed to match the abilities of Assemblenet but without evolution. We note that these include very strong architectures that have not been explored before, such as the four-stream model with dense intermediate connectivity. We design competitive networks having various connections between streams, where the connection weights are also learned (see the supplementary material for detailed descriptions and visualizations). Note that all these models have equivalent capacity (i.e., number of parameters). The performance difference is due to network structure. <ref type="table" target="#tab_2">Table 3</ref> shows the results, demonstrating that these architectures with learnable interconnectivity are very powerful themselves and evolution is further beneficial. The Moments in Time models were trained from scratch, and the Charades models were pre-trained on MiT. In particular, we evaluated an architecture with intermediate connectivity from the flow stream to RGB, inspired by <ref type="bibr" target="#b6">Feichtenhofer et al. (2016b;</ref>) (+ connection weight learning). It gave 30.2% accuracy on MiT and 49.5% on Charades, which are not as accurate as AssembleNet. Randomly generated models (from 50 rounds of search) are also evaluated, confirming that such architectures do not perform well.</p><p>Further, we conduct another ablation to confirm the effectiveness of our search space. <ref type="table" target="#tab_3">Table 4</ref> compares the models found with our full search space vs. more constrained search spaces, such as only using two stems and not using temporal dilation (i.e., fixed temporal resolution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">GENERAL FINDINGS</head><p>As the result of connection-learning-guided architecture evolution, non-obvious and non-intuitive connections are found ( <ref type="figure" target="#fig_3">Figure 3</ref>). As expected, more than one possible "connectivity" solution can yield similarly good results. Simultaneously, models with random connectivity perform poorly compared to the found AssembleNet. Our observations also include: (1) The models prefer to have only one block at the highest level, although we allow the search to consider having more than one block at that level.</p><p>(2) The final block prefers simple connections gathering all outputs of the blocks in the 2nd to last level.</p><p>(3) Many models use multiple blocks with different temporal resolutions at the same level, justifying the necessity of the multi-stream architectures. (4) Often, there are 1 or 2 blocks heavily connected to many other blocks. (5) Architectures prefer using more than 2 streams, usually using 4 at many levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We present AssembleNet, a new approach for neural architecture search using connection-learningguided architecture evolution. AssembleNet finds multi-stream architectures with better connectivity and temporal resolutions for video representation learning. Our experiments confirm that the learned models significantly outperform previous models on two challenging benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 SUPER-GRAPH VISUALIZATION OF THE CONNECTIVITY SEARCH SPACE <ref type="figure" target="#fig_5">Figure 5</ref> visualizes all possible connections and channel/temporal resolution options our architecture evolution is able to consider. The objective of our evolutionary algorithm could be interpreted as finding the optimal sub-graph (of this super-graph) that maximizes the performance while maintaining the number of total parameters. Trying to directly fit such entire super-graph into the memory was infeasible in our experiments. A.2 CHANNEL SIZES OF THE LAYERS AND NODE SPLIT/MERGE MUTATIONS As we described in the paper, each node (i.e., a convolutional block) has a parameter C controlling the number of filters of the convolutional layers in the block. When splitting or merging blocks, the number of filters are split or combined respectively. <ref type="figure">Figure 6</ref> provides a visualization of a block with number of filter specified to the right and a split operation. While many designs are possible, we design the blocks and splitting as follows. The size of 1x1 convolutional layers and 1D temporal convolutional layers are strictly governed by C, having the channel size of C (some 4C). On the other hand, the number of 2D convolutional layer is fixed per level as a constant D v where v is the level of the block. D 1 = 64, D 2 = 128, D 3 = 256, and D 4 = 512. The layers in the stems have 64 channels if there are only two stems and 32 if there are four stems.</p><p>When a node is split into two nodes, we update the resulting two nodes' channel sizes to be half of their original node. This enables us to maintain the total number of model parameters before and after the node split to be identical. The first 1x1 convolutional layer will have half the parameters after the split, since its output channel size is now 1/2. The 2D convolutional layer will also have exactly half the parameters, since its input channel size is 1/2 while the output channel size is staying fixed. The next 1x1 convolutional layer will have the fixed input channel size while the output channel size becomes 1/2: thus the total number of parameters would be 1/2 of the original parameters.</p><p>Merging of the nodes is done in an inverse of the way we split. When merging two nodes into one, the merged node inherits all input/output connections from the two nodes: we take a union of all the connections. The channel size of the merged node is the sum of the channel sizes of the two nodes being merged. The temporal dilation rate of the merged node is randomly chosen between the two nodes before the merge. <ref type="figure" target="#fig_7">Figure 7</ref> illustrates the actual architectures of the hand-designed (2+1)D CNN models used in our ablation study. We also show the final learned weights of the connections, illustrating which connections the model ended up using or not using. We note that these architectures are also very enlightening as the connectivity within them are learned in the process. We observe that stronger connections tend to be formed later for 2-stream architectures. For 4-stream architectures, stronger connections do form early, and, not surprisingly, a connection to at least one node of a different ... ... <ref type="figure">Figure 6</ref>: An illustration of the node split mutation operator, used for both evolution and initial architecture population generation. modality is established, i.e. a node stemming from RGB will connect to at least one flow node at the next level and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 HAND-DESIGNED MODELS USED IN THE ABLATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node split operation</head><p>Below is a more detailed description of the networks used in the paper: "Two-stream (late fusion)" means that the model has two separate streams at every level including the level 4, and the outputs of such two level 4 nodes are combined for the final classification. "Fusion at lv. 4" is the model that only has one level 4 node to combine the outputs of the two level 3 nodes using a weighted summation. "Two-stream (fully)" means that the model has two nodes at each level 1-3 and one node at level 4, and each node is always connected to every node in the immediate next level. "Flow→RGB" means that only the RGB stream nodes combine outputs from both RGB and flow stream nodes of the immediate lower level.  <ref type="table" target="#tab_4">Table 5</ref>. In particular, the 2nd element of each block description shows the list of where the input to that block is coming from (i.e., the connections). As already mentioned, 2D and (2+1)D residual modules are repeated in each block. The number of repetitions m are 1.5, 2, 3, and 1.5 at each level. m = 1.5 means that we have one 2D residual module, one (2+1)D module, and one more 2D module. This makes the number of convolutional layers of each block at levels 1-4 to be 9, 12, 18, and 9. In addition, a stem has at most 2 convolutional layers. The total depth of our network is 50, similar to a conventional (2+1)D ResNet-50. For AssembleNet-101, we use m = 1.5, 2, 11.5, and 1.5 at each level.</p><p>If a block has a spatial stride of 2, the striding happens at the first 2D convolutional layer of the block.</p><p>In the stem which has the spatial stride of 4, the striding of size 2 happens twice, once at the 2D convolutional layer and at the max pooling layer. As mentioned, the model has a batch normalization layer followed by ReLU after every convolutional layer regardless of its type (i.e., 2D, 1D, and 1x1). 2D conv. filter sizes are 3x3, and 1D conv. filter sizes are 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 SINK NODE DETAILS</head><p>When each evolved or baseline (2+1)D model is applied to a video, it generates a 5D (BTYXC) tensor after the final convolutional layer, where B is the size of the batch and C is the number of channels. The sink node is responsible for mapping this into the output vector, whose dimensionality is identical to the number of video classes in the dataset. The sink node first applies a spatial average pooling to generate a 3D (BTC) tensor. If there are multiple level 4 nodes (which rarely is the case), the sink node combines them into a single tensor by averaging/concatenating them. Averaging or concatenating does not make much difference empirically. Next, temporal average/max pooling is applied to make the representation a 2D (BC) tensor (average pooling was used for the MiT dataset and max pooling was used for Charades), and the final fully connected layer and the soft max layer is applied to generate the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 TRAINING DETAILS</head><p>For the Moments in Time (MiT) dataset training, 8 videos are provided per TPU core (with 16GB memory): the total batch size (for each gradient update) is 512 with 32 frames per video. The batch size used for Charades is 128 with 128 frames per video. The base framerate we used is 12.5 fps for MiT and 6 fps for Charades. The spatial input resolution is 224x224 during training. We used the standard Momentum Optimizer in TensorFlow. We used a learning rate of 3.2 (for MiT) and 25.6 (for Charades), 12k warmup iterations, and cosine decay. No dropout is used, weight decay is set to 1e-4 and label smoothing set to 0.2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>AssembleNet with multiple intermediate streams. Example learned architecture. Darker colors of connections indicate stronger connections. At each convolutional block, multiple 2D and (2+1)D residual modules are repeated alternatingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An example showing a sequence of architecture evolution. These architectures have actual parent-child relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>More AssembleNet examples. Similarly good performing diverse architectures, all with higher-than-50% mean-average precision on Charades. For instance, even our simpler two-stem AssembleNet-50 (left) got 51.4% mAP on Charades. Darker edges mean higher weights. This is possible because our fitness measure involves initial proxy training of each model, providing the learned connection weight values W e of the parent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of different search methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the super-graph corresponding to our video architecture search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of hand-designed baseline (2+1)D CNN models used in our ablation study. Their connections weights are learned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Reported state-of-the-art action classification performances (vs. AssembleNet) on Charades. '2-stream (2+1)D ResNet-50' is the two-stream model with connection learning for level-4 fusion.</figDesc><table><row><cell>Method</cell><cell cols="3">pre-train modality mAP</cell></row><row><cell cols="4">2-stream (Simonyan &amp; Zisserman, 2014) UCF101 RGB+Flow 18.6</cell></row><row><cell>Asyn-TF (Sigurdsson et al., 2017)</cell><cell cols="3">UCF101 RGB+Flow 22.4</cell></row><row><cell>CoViAR (Wu et al., 2018b)</cell><cell cols="3">ImageNet Compressed 21.9</cell></row><row><cell>MultiScale TRN (Zhou et al., 2018)</cell><cell>ImageNet</cell><cell>RGB</cell><cell>25.2</cell></row><row><cell>I3D (Carreira &amp; Zisserman, 2017)</cell><cell>Kinetics</cell><cell>RGB</cell><cell>32.9</cell></row><row><cell>I3D (from Wang et al., 2018)</cell><cell>Kinetics</cell><cell>RGB</cell><cell>35.5</cell></row><row><cell>I3D-NL (Wang et al., 2018)</cell><cell>Kinetics</cell><cell>RGB</cell><cell>37.5</cell></row><row><cell>STRG (Wang &amp; Gupta, 2018)</cell><cell>Kinetics</cell><cell>RGB</cell><cell>39.7</cell></row><row><cell>LFB-101 (Wu et al., 2018a)</cell><cell>Kinetics</cell><cell>RGB</cell><cell>42.5</cell></row><row><cell cols="4">SlowFast-101 (Feichtenhofer et al., 2018) Kinetics RGB+RGB 45.2</cell></row><row><cell>2-stream (2+1)D ResNet-50 (ours)</cell><cell cols="3">None RGB+Flow 39.9</cell></row><row><cell>2-stream (2+1)D ResNet-50 (ours)</cell><cell>MiT</cell><cell cols="2">RGB+Flow 48.7</cell></row><row><cell>2-stream (2+1)D ResNet-50 (ours)</cell><cell cols="3">Kinetics RGB+Flow 50.4</cell></row><row><cell>2-stream (2+1)D ResNet-101 (ours)</cell><cell cols="3">Kinetics RGB+Flow 50.6</cell></row><row><cell>AssembleNet-50 (ours)</cell><cell cols="3">None RGB+Flow 47.0</cell></row><row><cell>AssembleNet-50 (ours)</cell><cell>MiT</cell><cell cols="2">RGB+Flow 53.0</cell></row><row><cell>AssembleNet-50 (ours)</cell><cell cols="3">Kinetics RGB+Flow 56.6</cell></row><row><cell>AssembleNet-101 (ours)</cell><cell cols="3">Kinetics RGB+Flow 58.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>State-of-the-art action classification accuracies on Moments in Time<ref type="bibr" target="#b15">(Monfort et al., 2018)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>0.63</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.62</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.61</cell></row><row><cell>Method</cell><cell>modality Top-1 Top-5</cell><cell></cell><cell>0.60</cell></row><row><cell>ResNet50-ImageNet TSN (Wang et al., 2016) Ioffe &amp; Szegedy (2015)</cell><cell>RGB 27.16 51.68 RGB 24.11 49.10 Flow 11.60 27.40</cell><cell>Fitness</cell><cell>0.58 0.59</cell></row><row><cell cols="2">TSN-Flow (Wang et al., 2016) TSN-2Stream (Wang et al., 2016) RGB+F 25.32 50.10 Flow 15.71 34.65 TRN-Multi (Zhou et al., 2018) RGB+F 28.27 53.87 Two-stream (2+1)D ResNet-50 RGB+F 28.97 55.55 I3D (Carreira &amp; Zisserman, 2017) RGB+F 29.51 56.06 AssembleNet-50 RGB+F 31.41 58.33</cell><cell></cell><cell>0.55 0.56 0.57</cell><cell>0</cell><cell>Evolution round 10 20 30 40 50 60 70 Connection learning guided Standard evolution Random</cell></row><row><cell>AssembleNet-50 (with Kinetics)</cell><cell>RGB+F 33.91 60.86</cell><cell></cell><cell></cell></row><row><cell cols="2">AssembleNet-101 (with Kinetics) RGB+F 34.27 62.71</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between AssembleNet and architectures without evolution, but with connection weight learning. Four-stream models are reported here for the first time, and are very effective. All these models have a similar number of parameters.</figDesc><table><row><cell>Architecture</cell><cell>MiT Charades</cell></row><row><cell>Two-stream (late fusion)</cell><cell>28.97 46.5</cell></row><row><cell>Two-stream (fusion at lv. 4)</cell><cell>30.00 48.7</cell></row><row><cell cols="2">Two-stream (flow→RGB inter.) 30.21 49.5</cell></row><row><cell>Two-stream (fully, fuse at 4)</cell><cell>29.87 50.5</cell></row><row><cell>Four-stream (fully, fuse at 4)</cell><cell>29.98 50.7</cell></row><row><cell cols="2">Random (+ connection learning) 29.91 50.1</cell></row><row><cell>AssembleNet-50</cell><cell>31.41 53.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation comparing different Assem-bleNet architectures found with full vs. constrained search spaces. The models are trained from scratch.</figDesc><table><row><cell>Architecture</cell><cell>MiT</cell></row><row><cell cols="2">Baseline (random + conn. learning) 29.91</cell></row><row><cell>No mutation</cell><cell>30.26</cell></row><row><cell>RGB-only</cell><cell>30.30</cell></row><row><cell>Without temporal dilation</cell><cell>30.49</cell></row><row><cell>Two-stem only</cell><cell>30.75</cell></row><row><cell>Full AssembleNet-50</cell><cell>31.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The table form of the AssembleNet model with detailed parameters. This model corresponds toFigure 1. The parameters correspond to {node level, input node list, C, r, and spatial stride}</figDesc><table><row><cell>Index</cell><cell>Block parameters</cell></row><row><cell>0</cell><cell>0, [RGB], 32, 4, 4</cell></row><row><cell>1</cell><cell>0, [RGB], 32, 4, 4</cell></row><row><cell>2</cell><cell>0, [Flow], 32, 1, 4</cell></row><row><cell>3</cell><cell>0, [Flow], 32, 1, 4</cell></row><row><cell>4</cell><cell>1, [1], 32, 1, 1</cell></row><row><cell>5</cell><cell>1, [0], 32, 4, 1</cell></row><row><cell>6</cell><cell>1, [0,1,2,3], 32, 1, 1</cell></row><row><cell>7</cell><cell>1, [2,3], 32, 2, 1</cell></row><row><cell>8</cell><cell>2, [0, 4, 5, 6, 7], 64, 2, 2</cell></row><row><cell>9</cell><cell>2, [0, 2, 4, 7], 64, 1, 2</cell></row><row><cell>10</cell><cell>2, [0, 5, 7], 64, 4, 2</cell></row><row><cell>11</cell><cell>2, [0, 5], 64, 1, 2</cell></row><row><cell>12</cell><cell>3, [4, 8, 10, 11], 256, 1, 2</cell></row><row><cell>13</cell><cell>3, [8, 9], 256, 4, 2</cell></row><row><cell>14</cell><cell>4, [12, 13], 512, 2, 2</cell></row><row><cell cols="2">A.4 ASSEMBLENET MODEL/LAYER DETAILS</cell></row></table><note>We also provide the final AssembleNet model in table form in</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Connectivity learning in multi-branch networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Meta-Learning (MetaLearn)</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Holistic large scale video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>CoRR:1904.11451</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparative analysis of selection schemes used in genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="69" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture seach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03150</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asynchronous temporal fields for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">0767</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Kaiming He, Philipp Krähenbühl, and Ross Girshick. Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05038</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Compressed video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6026" to="6035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>CoRR:1904.01569</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Training a model for 10k iterations (during evolution) took 3∼5 hours and fully training the model (for 50k iterations) took ∼24 hours per dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">We used the TV-L1 optical flow extraction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">with tensor operations by Piergiovanni &amp; Ryoo (2019) to obtain flow input</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">EVALUATION DETAILS When evaluating a model on the MiT dataset, we provide 36 frames per video. The duration of each MiT video is 3 seconds, making 36 frames roughly correspond to the entire video. For the Charades dataset where each video duration is roughly ∼30 seconds, the final class labels are obtained by applying the model to five random 128 frame crops (i.e., segments) of each video. The output multi-class labels are max-pooled to get the final label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and is compared to the ground truth to measure the average precision scores. The spatial resolution used for the testing is 256x256</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
