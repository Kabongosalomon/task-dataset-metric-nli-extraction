<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Attention Recurrent Q-Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sorokin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Seleznev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Fedorov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasiia</forename><surname>Ignateva</surname></persName>
						</author>
						<title level="a" type="main">Deep Attention Recurrent Q-Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN). We present an extension of DQN by "soft" and "hard" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions. * The authors are members of the 5vision team from the hackathon DeepHack.Game 2015.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>The recent success of Deep Q-Learning (DQL) in mastering human-level control policies on a variety of different Atari 2600 games <ref type="bibr" target="#b0">[1]</ref> inspires artificial intellegence researchers to seek possible improvements to Google DeepMind's algorithm in order to further enhance its learning abilities <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. The goal of this concise paper is to present the authors' approach to addressing this challenge by providing DQN, a deep neural network used in DQL as an analogue of a classic actionutility function, with such tools of modern machine learning as Long Short-Term Memory (LSTM) <ref type="bibr" target="#b4">[5]</ref> and visual attention mechanisms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Despite impressive results achieved by the Google DeepMind's intelligent agent, there are a number of elements to be improved in the existing algorithm. In particular, Hausknecht and Stone <ref type="bibr" target="#b1">[2]</ref> pointed out that in practice, DQN decides on the next optimal action based on the visual information corresponding to the last four game states encountered by the agent. Therefore, the algorithm cannot master those games that require a player to remember events more distant than four screens in the past. It is for this reason that Hausknecht and Stone proposed the Deep Recurrent Q-Network (DRQN), a combination of LSTM and DQN in which (i) the fully connected layer in the latter is replaced for a LSTM one, and (ii) only the last visual frame at each timestep is used as DQN's input. The authors report that despite seeing only one visual frame, DRQN is still capable integrating relevant information across the frames. Nonetheless, no systematic improvement in Atari game scores over the results of Mhih et al. <ref type="bibr" target="#b0">[1]</ref> was observed.</p><p>Another drawback of DQN is its long training time, which is a critical component to the researchers' ability to carry out experiments with different network architectures and algorithm's parameter settings. According to <ref type="bibr" target="#b0">[1]</ref>, it takes 12-14 days on a GPU to train the network. Nair et al. <ref type="bibr" target="#b2">[3]</ref> proposed a new massively parallel version of the algorithm geared to address this problem. They report that its performance surpassed non-distributed DQN in 41 of the 49 games. However, extensive parallelization is not the only and, probably, not the most efficient remedy to the problem.</p><p>Recent achievements of visual attention models in caption generation <ref type="bibr" target="#b5">[6]</ref>, object tracking <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref>, and machine translation <ref type="bibr" target="#b9">[10]</ref> have induced the authors of this paper to conduct a series of experiments so as to assess possible benefits from incorporating attention mechanisms into the structure of the</p><formula xml:id="formula_0">Q(s, a) t Q(s, a) t+1 LSTM LSTM z t z t+1 h t , c t g g h t CNN CNN v t v t+1</formula><p>s t s t+1 <ref type="figure">Figure 1</ref>: The Deep Attention Recurrent Q-Network DRQN algorithm. The main advantage of utilizing these mechanisms is that DRQN acquires the ability to select and then focus on relatively small informative regions of an input image, thus helping to reduce the total number of parameters in the deep neural network and computational operations needed for training and testing it. In contrast to DRQN, in this case, LSTM layer stores the data used not only for making decision on the next action, but also for choosing the next region of attention. In addition to computational speedups, attention-based models can also add some degree of interpretability to the Deep Q-Learning process by providing researchers with an opportunity to visualize "where" and "what" the agent's attention is focusing on.</p><p>The rest of the paper is organized as follows. In Section 2, two variants of the suggested DARQN algorithm are described. The results of applying the DARQN to two popular Atari 2600 games are presented in Section 3. Conclusions are formulated in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Attention Recurrent Q-Network</head><p>The DARQN architecture is schematically shown in <ref type="figure">Figure 1</ref> and consists of three types of networks: convolutional (CNN), attention, and recurrent. At each time step t, CNN receives a representation of the current game state s t in the form of a visual frame, based on which it produces a set of D feature maps, each having a dimension of m × m. The attention network transformes these maps into a set of vectors v t = {v 1 t , ..., v L t }, v i t ∈ R D , L = m * m and outputs their linear combination z t ∈ R D , called a context vector. The recurrent network, in our case LSTM, takes as input the context vector, along with the previous hidden state h t−1 and memory state c t−1 , and produces hidden state h t that is used by (i) a linear layer for evaluating Q-value of each action a t that the agent can take being in state s t , (ii) the attention network for generating a context vector at the next time step t + 1. In the following subsections, we consider two approaches to the context vector calculation. As will be shown, they have important differences in the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Soft attention</head><p>The "soft" attention mechanism assumes that the context vector z t can be represented as a weighted sum of all vectors v i t , each of which corresponds to the features extracted by CNN at different image regions. Weights in this sum are chosen in proportion to the vectors relative importance assessed by the attention network g. The g network contains two fully connected layers followed by a softmax activation. Its output may be written as:</p><formula xml:id="formula_1">g(v i t , h t−1 ) = exp(Linear(T anh(Linear(v i t ) + W h t−1 )))/Z,<label>(1)</label></formula><p>where Z is a normalizing constant, W is a weights matrix, Linear(x) = Ax + b is an affine transformation with some weights matrix A and bias b. Once we have defined the importance of each location vector v i t , we can calculate the context vector z t :</p><formula xml:id="formula_2">z t = L i=1 g(v i t , h t−1 )v i t .<label>(2)</label></formula><p>Other networks depicted in <ref type="figure">Figure 1</ref> have a standard form, the details of their realization are discussed in Section 3. The whole DARQN model is trained by minimizing a sequence of loss functions:</p><formula xml:id="formula_3">J t (θ t ) = E st,at∼ρ(·),rt [(E st+1∼E [Y t | s t , a t ] − Q(s t , a t ; θ t )) 2 ], (3) where Y t = r t + γ max at+1 Q(s t+1 , a t+1 ; θ t−1 )</formula><p>is an approximate target value, r t is an immediate reward after taking action a t in state s t , γ ∈ [0, 1] is a discount factor, E is an environment distribution, ρ(s t , a t ) is a behaviour distribution selected as -greedy strategy, θ t is a vector of all DARQN weights, including those belonging to the attention network. To optimize the loss function, we use the standard Q-learning update rule:</p><formula xml:id="formula_4">θ t+1 = θ t + α(Y t − Q(s t , a t ; θ t ))∇ θt Q(s t , a t ; θ t )<label>(4)</label></formula><p>All functions in DARQN are differentiable; therefore, the gradient exists for each parameter, and the whole model can be trained end-to-end. The suggested algorithm also utilizes two training techniques proposed by Mnih et al. <ref type="bibr" target="#b0">[1]</ref>, namely target network and experience replay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hard attention</head><p>The "hard" attention mechanism requires sampling only one attention location from L available at each time step t in accordance with some stochastic attention policy π g . In our case, this policy is represented by the neural network g whose output (1) consists of location selection probabilities and whose weights are the policy parameters. In order to train a network with stochastic units, the statistical gradient-following algorithm REINFORCE <ref type="bibr" target="#b10">[11]</ref> may be used. In literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>, there are several successful examples of integrating this algorithm with Deep Learning. Unlike models proposed in these papers and trained by maximizing likelihood, the suggested algorithm is trained by minimizing a sequence of loss functions (3). Therefore, its training process is different. Assume that s t (and therefore v t ) was sampled from the environment distribution affected by the attention policy π g (i t | v t , h t−1 ), a categorical distribution with parameters given by a softmax layer (1) of the attention network g. Then, in the policy gradient approach <ref type="bibr" target="#b11">[12]</ref>, updates of the policy parameters may be written as:</p><formula xml:id="formula_5">∆θ g t ∝ ∇ θ g t log π g (i t | v t , h t−1 )R t ,<label>(5)</label></formula><p>where R t is a future discounted return after the agent selects the attention location i t . In order to approximate this value, a separate neural network G t = Linear(h t ) has been introduced. This network is trained by regressing towards the expected value of Y t . The final update rule for the attention network's parameters has the following form:</p><formula xml:id="formula_6">θ g t+1 = θ g t + α∇ θ g t log π g (i t | v t , h t−1 )(G t − Y t )<label>(6)</label></formula><p>where the expression G t − Y t can be interpreted in terms of advantage function estimation <ref type="bibr" target="#b12">[13]</ref>. Training (6) can also be described <ref type="bibr" target="#b6">[7]</ref> as adjusting the parameters θ g t of the attention network so that the log-probability of attention location i t that has led to a higher expected future reward is increased, while that of locations having produced a lower reward is decreased. In order to reduce a high variance of the stochastic gradient, a practical trick proposed in <ref type="bibr" target="#b5">[6]</ref> is utilized. At each time step, the context vector z t is found based on (2) with a 50% chance. On the other hand, adding the entropy term on the categorical distribution has not resulted in any positive changes.</p><p>It is worth noting that for the hard attention DARQN model, CNN weights were preinitialized based on the corresponding weights of the trained soft attention model. In addition, the error backpropogation process does not affect weights at the previous time step, but does involve weights in convolutional layers. The latter receive the sum of two gradients: one from the attention network <ref type="bibr" target="#b5">(6)</ref> and the other from the recurrent network (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The proposed algorithm was tested on several popular Atari 2600 games: Breakout, Seaquest, Space Invaders, Tutankham, and Gopher. The results obtained were compared with the corresponding results of (i) DQN suggested by Mnih et al. <ref type="bibr" target="#b0">[1]</ref> and implemented in Torch, (ii) DRQN suggested by Hausknecht and Stone <ref type="bibr" target="#b1">[2]</ref> and implemented in Caffe. Our realization of DARQN is based on the source code <ref type="bibr" target="#b0">[1]</ref> and is available online 1 .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Architecture</head><p>The convolutional network architecture in DARQN is similar to that used in <ref type="bibr" target="#b0">[1]</ref>, except for two peculiarities: its input is a 84 × 84 × 1 tensor, and the output of its last (third) layer contains 256 feature maps 7 × 7. The attention network takes 49 vectors as input, each vector has a dimension of 256. The number of hidden units in the attention network is chosen to be equal to 256. The LSTM network also has 256 units, which is consistent with the number of attention network outputs.   Bottom row displays the process of submarine resurface in Seaquest. On the first screen, the agent mostly focuses on the oxygen indicator, but also notices enemies in its nearest vicinity. As the submarine rises to the surface, the attention of the agent switches to the submarine itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyper-parameters</head><p>In all experiments, the discount factor was set to γ = 0.99, the learning rate α starts at 0.01 and decays linearly to 0.00025 over 1M steps for the soft attention model and from 0.001 to 0.00025 for the one with the hard attention model. The number of steps between target network updates was 10, 000. Training was done over 5M steps. The agent was evaluated after every 50, 000 steps based on the average reward per episode obtained by running an -greedy policy with = 0.05 for 25, 000 steps. The size of the experience replay memory was 500, 000 tuples. The memory was sampled to update the network every 4 steps with minibatches of size 32. The model was trained using the backpropogation through time. For each new minibatch, the initial LSTM's hidden and memory states were zeroed. To update weights θ t , the RMSProp algorithm with momentum equal to 0.95 was utilized. The simple exploration policy used was an -greedy policy with the decreasing linearly from 1 to 0.1 over 1M steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The main results of models comparison on the five Atari games are presented in <ref type="table" target="#tab_0">Table 1</ref>. One can see that not on all of the games, the DARQN models achieve the results that are superior to corresponding results of DQN and DRQN. To provide some insight into advantages and disadvantages of the proposed models, the training process on the two games where DARQN obtains the best and the worst results is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>On Seaquest, both DARQN models demonstrate a high level of performance. However, the hard attention-based agent seems to be inferior with respect to the soft one. In particular, it is unable to learn that in order to survive, the submarine has to regularly resurface. This problem can be attributed to one of the shortcomings of the policy gradient approach used in the hard attention mechanism's training procedure, namely to its tendency to converge to a local optimum.</p><p>In the case of Breakout, models with LSTM have worse results than the original DQN. One possible reason for that is the low number of unroll steps used when training the LSTM network. To test this hypothesis, we repeated the whole experiment for the DARQN model with a greater number of unroll steps. The results presented in <ref type="figure" target="#fig_1">Figure 3</ref> show that despite some performance improvement, neither soft nor hard DARQN model can surpass the DQN results.</p><p>To visualize attention regions, we created 256 subsidiary features maps 7 × 7 filled by output values (1) and upsampled these maps through CNN layers, having the same structure as that used in DARQN model. The upsampled values were decreased to make an attention spot more transparent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we have presented one possible way of integrating attention mechanisms into the structure of Deep Q-Network. To test this model, a series of expirements was conducted on five Atari 2600 games. The results obtained allow us to arrive at conclusion that dispite having less optimized parameters, our model, at least on some Atari games, surpasses the results of the original DQN model, thereby demonstrating a greater generalization ability. Moreover, our attention-based algorithm allows gaining some insights into the logic of agent's behavior by displaying the regions of the game screen the agent is focusing on when making decisions.</p><p>Attention mechanisms can be considered as an additional filter gate in LSTM that processes structured visual data produced by CNN for the entire image. Therefore, one promising direction of future research would be to apply multi-scale <ref type="bibr" target="#b13">[14]</ref> or glimpse <ref type="bibr" target="#b7">[8]</ref> visual attention mechanisms to DQN. The simple policy gradient-based algorithm, introduced for training the hard attention DARQN model, has shown a relatively poor level of performance. That is why another auspicious direction of future research would be (i) to test different techniques for reducing stochastic gradient variability <ref type="bibr" target="#b12">[13]</ref>, (ii) to apply different approaches to training stochastic attention networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The average reward per episode for the four models on two Atari games as a function of the number of training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The average reward per episode for two DARQN models on Breakout with 4 and 10 unroll steps as a function of the number of training epochs. One epoch corresponds to 50, 000 steps. DARQN weights are updated one time per 4 steps. Weights of the CNN network in the hard attention model were preinitialized based on the weights of the trained soft attention model with 4 unroll steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of attention regions for the soft DARQN model. Top row demonstrates the ability of the agent to focus on the ball trajectory in Breakout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of attention regions for the hard DARQN model. Top row shows the agent's immediate response to the short-term ball disappearance in Breakout. Bottom row demonstrates the ability of the agent in Seaquest to focus attention on the enemy detected right up to the moment of its destruction. In Figures 4 and 5, some examples of highlighted attention regions are depicted. The corresponding game videos are available online 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The best average reward per episode of 100 epochs for the four models on five Atari games. One epoch corresponds to 50, 000 steps. The hard and soft attention models as well as DRQN are trained with 4 unroll steps. DRQN weights are updated at each step, whereas DQN and DARQN weights are updated one time per 4 steps.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="8">Breakout Seaquest S. Invaders Tutankham Gopher</cell><cell></cell></row><row><cell cols="2">DQN</cell><cell></cell><cell>241</cell><cell></cell><cell>1,284</cell><cell>916</cell><cell></cell><cell>197</cell><cell cols="2">1,976</cell><cell></cell></row><row><cell cols="2">DRQN</cell><cell></cell><cell>72</cell><cell></cell><cell>1,421</cell><cell>571</cell><cell></cell><cell>181</cell><cell cols="2">3,512</cell><cell></cell></row><row><cell cols="3">DARQN hard</cell><cell>20</cell><cell></cell><cell>3,005</cell><cell>558</cell><cell></cell><cell>128</cell><cell cols="2">2,510</cell><cell></cell></row><row><cell cols="2">DARQN soft</cell><cell></cell><cell>11</cell><cell></cell><cell>7,263</cell><cell>650</cell><cell></cell><cell>197</cell><cell cols="2">5,356</cell><cell></cell></row><row><cell>200</cell><cell>DQN DRQN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>100</cell><cell>hard soft</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2,000 4,000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell cols="3">(a) Breakout</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Seaquest</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>It is intresting to compare the DARQN capacity to the capacities of DQN and DRQN. Depending on the game type, they may slightly differ. For Seaquest, a game with 18 possible actions, both DQN and DRQN (with 1 unroll step) have 1, 693, 362 adjustable parameters, whereas the suggested hard and soft DARQN models have only 845, 428 and 845, 171 parameters, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/5vision/DARQN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.youtube.com/playlist?list=PLKK-nv55ZMg583wK4Ny5sZu9YoFo27NBi</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Deep Knowledge Venture for financial support. In developing the ideas presented here, we have received helpful input from organizers of DeepHack.Game 2015 hackathon, especially from Sergey Plis (Datalytic Solutions). We also thank Greg Scantlen, CEO CreativeC.com, for letting us work on his private GPU cloud.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep recurrent q-learning for partially observable mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stone</surname></persName>
		</author>
		<idno>abs/1507.06527</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagdas</forename><surname>Alcicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rory</forename><surname>Fearon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">De</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedavyas</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04296</idno>
		<title level="m">Massively parallel methods for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06461</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning where to attend with deep architectures for image tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2151" to="2184" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Highdimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning wake-sleep recurrent attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06812</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gradient estimation using stochastic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05254</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
