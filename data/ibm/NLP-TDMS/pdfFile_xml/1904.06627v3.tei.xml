<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
							<email>xunwang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
							<email>xinhan@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
							<email>whuang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
							<email>dongdk@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
							<email>mscott@malong.com</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A family of loss functions built on pair-based computation have been proposed in the literature which provide a myriad of solutions for deep metric learning. In this paper, we provide a general weighting framework for understanding recent pair-based loss functions. Our contributions are three-fold: (1) we establish a General Pair Weighting (GPW) framework, which casts the sampling problem of deep metric learning into a unified view of pair weighting through gradient analysis, providing a powerful tool for understanding recent pair-based loss functions; (2) we show that with GPW, various existing pair-based methods can be compared and discussed comprehensively, with clear differences and key limitations identified; (3) we propose a new loss called multi-similarity loss (MS loss) under the GPW, which is implemented in two iterative steps (i.e., mining and weighting). This allows it to fully consider three similarities for pair weighting, providing a more principled approach for collecting and weighting informative pairs. Finally, the proposed MS loss obtains new state-of-the-art performance on four image retrieval benchmarks, where it outperforms the most recent approaches, such as ABE <ref type="bibr" target="#b13">[14]</ref> and HTL [4], by a large margin, e.g., , and 80.9% â†’ 88.0% on In-Shop Clothes Retrieval dataset at Recall@1. Code is available at https://github. com/MalongTech/research-ms-loss arXiv:1904.06627v3 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Metric learning aims to learn an embedding space, where the embedded vectors of similar samples are encouraged to be closer, while dissimilar ones are pushed apart from each other <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">39</ref>]. With recent great success of deep neural networks in computer vision, deep metric learning has attracted increasing attention, and has been applied to various tasks, including image retrieval <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">5]</ref>, face recognition Corresponding author: whuang@malong.com <ref type="figure">Figure 1</ref>. The proposed multi-similarity loss is able to jointly measure the self-similarity and relative similarities of a pair, which allows it collect informative pairs by implementing iterative pair mining and weighting. <ref type="bibr" target="#b36">[36]</ref>, zero-shot learning <ref type="bibr" target="#b42">[42,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b14">15]</ref>, visual tracking <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">31]</ref> and person re-identification <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Many recent deep metric learning approaches are built on pairs of samples. Formally, their loss functions can be expressed in terms of pairwise cosine similarities in the embedding space <ref type="bibr">1</ref> . We refer to this group of methods as pair-based deep metric learning; and this family includes contrastive loss <ref type="bibr">[6]</ref>, triplet loss <ref type="bibr" target="#b9">[10]</ref>, triplet-center loss <ref type="bibr" target="#b7">[8]</ref>, quadruplet loss <ref type="bibr" target="#b17">[18]</ref>, lifted structure loss <ref type="bibr" target="#b25">[25]</ref>, N-pairs loss <ref type="bibr" target="#b29">[29]</ref>, binomial deviance loss <ref type="bibr" target="#b40">[40]</ref>, histogram loss <ref type="bibr" target="#b32">[32]</ref>, angular loss <ref type="bibr" target="#b34">[34]</ref>, distance weighted margin-based loss <ref type="bibr" target="#b38">[38]</ref>, hierarchical triplet loss (HTL) <ref type="bibr">[4]</ref>, etc. For these pair-based methods, training samples are constructed into pairs, triplets or quadruplets, resulting a polynomial growth of training pairs which are highly redundant and less informative. This gives rise to a key issue for pair-based methods, where training with random sampling can be overwhelmed by redundant pairs, leading to slow convergence and model degeneration with inferior performance.</p><p>Recent efforts have been devoted to improving sampling schemes for pair-based metric learning techniques. For example, Chopra et al. <ref type="bibr">[3]</ref> introduced a contrastive loss which discards negative pairs whose similarities are smaller than a given threshold. In triplet loss <ref type="bibr" target="#b9">[10]</ref>, a negative pair is sampled by using a margin computed from the similarity of a randomly selected positive pair. Alternatively, lifted structure loss <ref type="bibr" target="#b25">[25]</ref> and N-pairs loss <ref type="bibr" target="#b29">[29]</ref> introduced new weighting schemes by designing a smooth weighting function to assign a larger weight to a more informative pair. Though driven by different motivations, these methods share a common goal of learning from more informative pairs. Thus, sampling such informative pairs is the key to pair-based deep metric learning, while precisely identifying these pairs is particularly challenging, especially for the negative pairs whose number is quadratic to the size of dataset.</p><p>In this work, we cast the sampling problem of deep metric learning into a general pair weighting formulation. We investigated various weighting schemes of recent pair-based loss functions, attempted to understand their insights more deeply, and identify key limitations of these approaches. We observed that a key factor that impacts pair weighting is to compute multiple types of similarities for a pair, which can be defined as self-similarity and relative similarity, where the relative similarity is heavily dependent on the other pairs. Furthermore, we found that most existing methods only explore this factor partially, which limits their capability considerably. For example, contrastive loss <ref type="bibr">[6]</ref> and binomial deviance loss <ref type="bibr" target="#b40">[40]</ref> only consider the cosine similarity of a pair, while triplet loss <ref type="bibr" target="#b9">[10]</ref> and lifted structure loss <ref type="bibr" target="#b25">[25]</ref> mainly focus on the relative similarity. We propose a multi-similarity loss which fully considers multiple similarities during sample weighting. The major contributions of this paper are summarized as follows.</p><p>-We establish a General Pair Weighting (GPW) framework, which formulates deep metric learning into a unified view of pair weighting. It provides a general formulation for understanding and explaining various pair-based loss functions through gradient analysis.</p><p>-We analyze the key factor that impacts pair weighting with GPW, where various pair-based methods can be analyzed comprehensively, with main differences and key limitations clearly identified. This allows us to define three types of similarities for a pair: a selfsimilarity and two relative similarities. The relative similarities are computed by comparing to other pairs, which are of great importance to existing pair-based methods.</p><p>-We propose a new multi-similarity (MS) loss, which is implemented using two iterative steps with sampling and weighting, as shown in <ref type="figure">Figure 1</ref>. MS loss considers both self-similarity and relative similarities which enables the model to collect and weight informative pairs more efficiently and accurately, leading to boosts in performance.</p><p>-MS loss is evaluated extensively on a number of benchmarks for image retrieval, where it outperforms current state-of-the-art approaches by a large margin, e.g., improving recent ABE <ref type="bibr" target="#b13">[14]</ref> with +5.0% Recall@1 on CUB200, and HTL <ref type="bibr">[4]</ref> with +7.1% Recall@1 on In-Shop Clothes Retrieval dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Classical pair-based loss functions. Siamese network <ref type="bibr">[6]</ref> is a representative pair-based method that learns an embedding via contrastive loss. It encourages samples from a positive pair to be closer, and pushes samples from a negative pair apart from each other, in the embedding space. Triplet loss was introduced in [10] by using triplets as training samples. Each triplet consists of a positive pair and a negative pair by sharing the same anchor point. Triplet loss aims to learn an embedding space where the similarity of a negative pair is lower than that of a positive one, by giving a margin. Extended from triplet loss, quadruplets were also applied in recent work, such as histogram loss <ref type="bibr" target="#b32">[32]</ref>.</p><p>Recently, Song et al. <ref type="bibr" target="#b25">[25]</ref> argued that both contrastive loss and triplet loss are difficult to explore full pair-wise relations between samples in a mini-batch. They proposed a lifted structure loss attempted to fully utilize such pairwise relations. However, the lifted structure loss only samples approximately an equal number of negative pairs as the positive ones randomly, and thus discards a large number of informative negative pairs arbitrarily. In <ref type="bibr" target="#b40">[40]</ref>, Dong et al. proposed a binomial deviance loss by using a binomial deviance to evaluate the cost between labels and similarity, which emphasizes harder pairs. In this work, we propose a multi-similarity loss able to explore more meaningful pair-wise relations by jointly considering both selfsimilarity and the relative similarities.</p><p>Hard sample mining. Pair-based metric learning often generates a large amount of pair-wise samples, which are highly redundant and include many uninformative samples. Training with random sampling can be overwhelmed by these redundant samples, which significantly degrade the model capability and also slows the convergence. Therefore, sampling plays a key role in pair-based metric learning.</p><p>The importance of hard negative mining has been discussed extensively <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr">4]</ref>. Schroff et al. <ref type="bibr" target="#b28">[28]</ref> proposed a semi-hard mining scheme by exploring semi-hard triplets, which are defined to have a negative pair farther than the positive one. However, such semi-hard mining method only generates a small number of valid semi-hard triplets, so that it often requires a large batch-size to generate sufficient semi-hard triplets, e.g., 1800 as suggested in <ref type="bibr" target="#b28">[28]</ref>. Harwood et al. <ref type="bibr" target="#b6">[7]</ref> provided a framework named smart mining to collect hard samples from the whole dataset, which suffers from off-line computation burden. Recently, Ge et al. <ref type="bibr">[4]</ref> proposed a hierarchical triplet loss (HTL) which builds a hierarchical tree of all classes, where hard negative pairs are collected via a dynamic margin. Sampling matters in deep embedding learning was discussed in <ref type="bibr" target="#b38">[38]</ref>, and a distance weighted sampling was proposed to collect negative samples uniformly with respective to the pairwise distance. Unlike these methods which mainly focus on sampling or hard sample mining, we provide a more generalized formulation that casts sampling problem into general pair weighting.</p><p>Instance weighting. Instance weighting has been widely applied to various tasks. For example, Lin et al. <ref type="bibr" target="#b19">[20]</ref> proposed a focal loss that allows the model to focus on hard negative examples during training an object detector. In <ref type="bibr">[2]</ref>, an active bias learning was developed to emphasize high variance samples in training a neural network for classification. Self-paced learning <ref type="bibr" target="#b16">[17]</ref>, which pays more attention on samples with a higher confidence, was explored to design noise-robust algorithms <ref type="bibr" target="#b11">[12]</ref>. These approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b16">17]</ref> were developed for weighting individual instances that are only depended on itself (referred as selfsimilarity), while our method aims to compute both selfsimilarity and the relative similarities, which is a more complicated problem that requires to measure multiple sample correlations within a local data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">General Pair Weighting (GPW)</head><p>In this section, we formulate the sampling problem of metric learning into a unified weighting view, and provide a General Pair Weighting (GPW) framework for analyzing various pair-based loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">GPW Framework</head><p>Let x i âˆˆ R d be a real-value instance vector. Then we have an instance matrix X âˆˆ R mÃ—d , and a label vector y âˆˆ {1, 2, . . . , C} m for the m training samples respectively. Then an instance x i is projected onto a unit sphere in a l-dimension space by f (Â·; Î¸) : R d â†’ S l , where f is a neural network parameterized by Î¸. Formally, we define the similarity of two samples as S ij . . =&lt; f (x i ; Î¸), f (x j ; Î¸) &gt;, where &lt; Â·, Â· &gt; denotes dot product, resulting in an m Ã— m similarity matrix S whose element at (i, j) is S ij .</p><p>Given a pair-based loss L, it can be formulated as a function in terms of S and y: L(S, y). The derivative with respect to model parameters Î¸ at the t-th iteration can be calculated as:</p><formula xml:id="formula_0">âˆ‚L(S, y) âˆ‚Î¸ t = âˆ‚L(S, y) âˆ‚S t âˆ‚S âˆ‚Î¸ t = m i=1 m j=1 âˆ‚L(S, y) âˆ‚S ij t âˆ‚S ij âˆ‚Î¸ t .<label>(1)</label></formula><p>Eqn 1 is computed for optimizing model parameters Î¸ in deep metric learning. In fact, Eqn 1 can be reformulated into a new form for pair weighting through a new function F, whose gradient w.r.t. Î¸ at the t-th iteration is computed exactly the same as Eq. 1. F is formulated as below:</p><formula xml:id="formula_1">F(S, y) = m i=1 m j=1 âˆ‚L(S, y) âˆ‚S ij t S ij .<label>(2)</label></formula><p>Note that âˆ‚L(S,y) âˆ‚Sij t is regarded as a constant scalar that not involved in the gradient of F w.r.t. Î¸.</p><p>Since the central idea of deep metric learning is to encourage positive pairs to be closer, and push negatives apart from each other. For a pair-based loss L, we can assume âˆ‚L(S,y) âˆ‚Sij t 0 for a negative pair, and âˆ‚L(S,y) âˆ‚Sij t 0 for a positive pair. Thus, F in Eqn 2 can be transformed into the form of pair weighting as follows:</p><formula xml:id="formula_2">F = m i=1 ï£« ï£­ m yj =yi âˆ‚L(S, y) âˆ‚S ij t S ij + m yj =yi âˆ‚L(S, y) âˆ‚S ij t S ij ï£¶ ï£¸ = m i=1 ï£« ï£­ m yj =yi w ij S ij âˆ’ m yj =yi w ij S ij ï£¶ ï£¸ ,<label>(3)</label></formula><p>where w ij = âˆ‚L(S,y) âˆ‚Sij t . As indicted in Eqn 3, a pair-based method can be formulated as weighting of pair-wise similarities, where the weight for pair {x i , x j } is w ij . Learning with a pair-based loss function L is now transformed from Eq. 1 into designing weights for pairs. It is a general pair weighting (GPW) formulation, and sampling is only a special cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Revisit Pair-based Loss Functions</head><p>To demonstrate the generalization ability of GPW framework, we revisit four typical pair-based loss functions for deep metric learning: contrastive loss <ref type="bibr">[6]</ref>, triplet loss <ref type="bibr" target="#b9">[10]</ref>, binomial deviance loss <ref type="bibr" target="#b40">[40]</ref> and lifted structure loss <ref type="bibr" target="#b25">[25]</ref>.</p><p>Contrastive loss. Hadsell et al. <ref type="bibr">[6]</ref> proposed a Siamese network, where a contrastive loss was designed to encourage positive pairs to be as close as possible, and negative pairs to be apart from each other over a given threshold, Î»:</p><formula xml:id="formula_3">L contrast . . = (1 âˆ’ I ij )[S ij âˆ’ Î»] + âˆ’ I ij S ij ,<label>(4)</label></formula><p>where I ij = 1 indicates a positive pair, and 0 for a negative one. By computing partial derivative with respect to S ij in Eqn 4, we can find that all positive pairs and hard negative pairs with S ij &gt; Î» are assigned with an equal weight. This is a simple and special case of our pair weighting scheme, without considering any difference between the selected pairs. Triplet loss. In <ref type="bibr" target="#b9">[10]</ref>, a triplet loss was proposed to learn a deep embedding, which enforces the similarity of a negative pair to be smaller than that of a randomly selected positive one over a given margin Î»:</p><formula xml:id="formula_4">L triplet := [S an âˆ’ S ap + Î»] + ,<label>(5)</label></formula><p>where S an and S ap denote the similarity of a negative pair {x a , x n }, and a positive pair {x a , x p }, with an anchor sam, eplx a . According to the gradient computed for Eqn 5, a triplet loss weights all pairs equally on the valid triplets which are selected by S an + Î» &gt; S ap , while the triplets with S an + Î» S ap are considered as less informative, and are discarded. Triplet loss is different from contrastive loss on pair selection scheme, but both methods consider all the selected pairs equally, which limits their ability to identify more informative pairs among the selected ones. Lifted Structure Loss. Song et al. <ref type="bibr" target="#b25">[25]</ref> designed a lifted structure loss, which was further improved to a more generalized version in <ref type="bibr" target="#b8">[9]</ref>. It utilizes all the positive and negative pairs among the mini-batch as follows:</p><formula xml:id="formula_5">L lif ted . . = m i=1 log y k =yi e Î»âˆ’S ik + log y k =yi e S ik + ,<label>(6)</label></formula><p>where Î» is a fixed margin.</p><p>In Eqn 6, when the hinge function of an anchor x i returns a non-zero value, we can have a weight value, w ij , for the pair {x i , x j }, by differentiating L lif ted on S ij , according to Eqn 3. Then the weight for a positive pair is computed as:</p><formula xml:id="formula_6">w + ij = e Î»âˆ’Sij y k =yi e Î»âˆ’S ik = 1 y k =yi e Sij âˆ’S ik ,<label>(7)</label></formula><p>and the weight for a negative pair is:</p><formula xml:id="formula_7">w âˆ’ ij = e Sij y k =yi e S ik = 1 y k =yi e S ik âˆ’Sij .<label>(8)</label></formula><p>Eqn 7 shows that the weight for a positive pair is determined by its relative similarity, measured by comparing it to the remained positive pairs with the same anchor. The weight for a negative pair is computed similarly based on Eqn 8. Binomial Deviance Loss. Dong et al. introduced binomial deviance loss in <ref type="bibr" target="#b40">[40]</ref>, which utilizes softplus function instead of hinge function in contrastive loss:</p><formula xml:id="formula_8">L binomial = m i=1 1 P i yj =yi log 1 + e Î±(Î»âˆ’Sij ) + 1 N i yj =yi log 1 + e Î²(Sij âˆ’Î») ,<label>(9)</label></formula><p>where P i and N i denote the numbers of positive pairs and negative pairs with anchor x i , respectively. Î», Î±, Î² are fixed hyper-parameters. The weight for pair {x i , x j } is w ij in Eqn 1, which can be derived from differentiating L binomial on S ij as:</p><formula xml:id="formula_9">w + ij = 1 P i Î±e Î±(Î»âˆ’Sij ) 1 + e Î±(Î»âˆ’Sij ) , y j = y i w âˆ’ ij = 1 N i Î²e Î²(Sij âˆ’Î») 1 + e Î²(Sij âˆ’Î») , y j = y i<label>(10)</label></formula><p>As can be found, binomial deviance loss is a soft version of contrastive loss. In Eqn 3, a negative pair with a higher similarity is assigned with a larger weight, which means that it is more informative, by distinguishing two similar samples from different classes (which form a negative pair).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-Similarity Loss</head><p>In this section, we first illustrate three types of similarities that carries the information of pairs, and then design a multi-similarity loss that weighting pairs based on full information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multiple Similarities</head><p>Based on GPW, we can find that most pair-based approaches weight the pairs based on either self cosine similarities or relative similarities compared with other pairs. Specifically, we take one negative pair for example and summarize three types of similarities as below. (The analysis of one positive pair is in a similar way.)</p><p>Similarity-S: Self-similarity defined as the cosine similarity between the negative sample and the anchor is of the most importance. A negative pair with a larger self similarity means that it is harder to distinguish two paired samples from different classes. Such pairs are referred as hard negative pairs, which are more informative and meaningful to learn discriminative features. Contrastive loss and binomial deviance loss are based on this criterion. In the left of <ref type="figure" target="#fig_0">Figure 2</ref>, the weight of the negative pair in Eq n 10 increases, since the pair's self similarity grows larger as the pair becomes closer.</p><p>In fact, self-similarity is not able to fully evaluate the value of a negative pair solely, and correlations of other pairs also make significant impact to weight assignment.  <ref type="table">Table 1</ref>. The types of similarities considered by the pair-based methods on weighting negative pairs. In the table, we can find that most existing pair-based methods only consider the three types of similarities partly, while our MS loss weights the negative pairs by considering all the three perspectives comprehensively.</p><p>Thus, We further introduce two types of relative similarities by considering all other pairs to exploit the potentiality of each pair.</p><p>Similarity-P: Positive relative similarity is computed by considering the relationship with positive pairs. Specifically, we define it as the differences of its own cosine similarity and that of other positive pairs. In the middle of <ref type="figure" target="#fig_0">Figure 2</ref>, the Similarity-P stays the same, while Similarity-P increases as the positive pair becomes faraway. Obviously, such case brings more challenge of retrieve the right sample for the anchor, as the negative pair is more closer than the positive. Thus, the negative pair should assigned more weight to learn an embedding. Triplet loss and histogram loss are based on Similarity-P.</p><p>Similarity-N: Negative relative similarity is the differences between its cosine similarity and those of other negative pairs. In the right of <ref type="figure" target="#fig_0">Figure 2</ref>, the Similarity-N of the negative pair becomes larger, while its self-similarity is fixed. Indeed, when the Similarity-N become larger, the pair carries more valuable information compared with other negatives. Thus, to learn a good embedding, we should focus weights on such pairs. Lifted structure, N-pairs and NCA methods assign weights for each negative pair according to Similarity-N.</p><p>With the GPW, we analyze the weighting schemes of most existing pair-based approaches, and summarize the similarities they depends on in weighting (in <ref type="table">Table 1</ref>). We observe that these commonly used pair-based methods only rely on partial similarities, which cannot cover the information contained in current pair. For example, lifted structure method only considers N-similarity, compares current pair with other negative pairs for weighting. The weight of current pair will be unchanged when the S-similarity becomes larger (in the left of <ref type="figure" target="#fig_0">Fig. 2</ref>) or when negative samples depart from the anchor (in the right of <ref type="figure" target="#fig_0">Fig. 2</ref>). This conclusion can also be verified directly from Eqn 8, which only depends on the term: y k =yi e Sij âˆ’S ik . Thus, to properly weighting one pair, not only the S-similarity should be taken into account, but also the P-similarity and N-similarity.</p><p>While a weighting or sampling method based on each individual similarity has been explored previously, to the best of our knowledge, none of existing pair-based methods assign weights on pairs considering all the three similarities. In the following, we propose our MS loss, whose weighting strategy is designed by taking a general consideration of all the three types of similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Similarity Loss</head><p>Unlike sampling or weighting schemes developed for classification and detection tasks ( <ref type="bibr">[2,</ref><ref type="bibr" target="#b19">20]</ref>), where the weight of an instance is computed individually based on itself, it is difficult to precisely measure the informativeness of a pair based on itself individually in deep metric learning. Relationships with relevant samples or pairs should also be considered to make the measurement more accurate.</p><p>However, to the best our knowledge, none of the existing pair-based methods can consider all of the three similarities simultaneously (in table 1). To this end, we propose a Multi-Similarity (MS) loss, which consider all three perspectives by implementing a new pair weighting scheme using two steps: mining and weighting. (i) informative pairs are first sampled by measuring Similarity-P; and then (ii) the selected pairs are further weighted using Similarity-S and Similarity-N jointly. Details of the two steps are described as follows.</p><p>Pair mining. We first select informative pairs by computing Similarity-P, which measures the relative similarity between negativeâ†”positive pairs having a same anchor. Specifically, a negative pair is compared to the hardest positive pair (with the lowest similarity), while a positive pair is sampled by comparing to a negative one having the largest similarity. Formally, assume x i is an anchor, a negative pair {x i , x j } is selected if S ij satisfies the condition:</p><formula xml:id="formula_10">S âˆ’ ij &gt; min y k =yi S ik âˆ’ ,<label>(11)</label></formula><p>where is a given margin.</p><p>If {x i , x j } is a positive pair, the condition is:</p><formula xml:id="formula_11">S + ij &lt; max y k =yi S ik + .<label>(12)</label></formula><p>For an anchor x i , we denote the index set of its selected positive and negative pairs as P i and N i respectively. Our hard mining strategy is inspired by large margin nearest neighbor (LMNN) <ref type="bibr" target="#b35">[35]</ref>, a traditional distance learning approach which targets to seek an embedding space where neighboring positive points are encouraged to have the same class label with the anchor. The negative samples that satisfy Eqn 11 are approximately identical to impostors defined in LMNN <ref type="bibr" target="#b35">[35]</ref>.</p><p>Pair weighting. Pair mining with Similarity-P can roughly select informative pairs, and discard the less informative ones. We develop a soft weighting scheme that further weights the selected pairs more accurately, by considering both Similarity-S and Similarity-N. Our weighting mechanism is inspired by binomial deviance loss (considering similarity-S) and lifted structure loss (using Similarity-N). Specifically, given a selected negative pair {x i , x j } âˆˆ N i , its weight w âˆ’ ij can be computed as:</p><formula xml:id="formula_12">w âˆ’ ij = 1 e Î²(Î»âˆ’Sij ) + kâˆˆNi e Î²(S ik âˆ’Sij ) = e Î²(Sij âˆ’Î») 1 + kâˆˆNi e Î²(S ik âˆ’Î») .<label>(13)</label></formula><p>and the weight w + ij of a positive pair {x i , x j } âˆˆ P i is:</p><formula xml:id="formula_13">w + ij = 1 e âˆ’Î±(Î»âˆ’Sij ) + kâˆˆPi e âˆ’Î±(S ik âˆ’Sij ) ,<label>(14)</label></formula><p>where Î±, Î², Î» are hyper-parameters as in Binomial deviance loss (Eqn 9). In Eqn 13, the weight of a negative pair is computed jointly from its self-similarity by e Î²(Î»âˆ’Sij ) -Similarity-S, and its relative similarity -Similarity-N, by comparing to its negative pairs. Similar rules are applied for computing the weight for a positive pair, as in Eqn 14. With these two considerations, our weighting scheme updates the weights of pairs dramatically to the violation of its self-similarity and relative similarities.</p><p>The weight computed by Eqn 13 and Eqn 14 can be considered as a combination of the weight formulations of binomial deviance loss and lifted structure loss. However, it is non-trivial to combine both functions in an elegant and suitable manner. We will compare our MS weighting with an average weighting scheme of binomial deviance (Eqn 10) and lifted structure (Eqn 8), denoted as BinLifted. We demonstrate by experiments that direct combination of them can not lead to performance improvement (as shown in ablation study).</p><p>Finally, we integrate pair mining and weighting scheme into a single framework, and provide a new pair-based loss function -multi-similarity (MS) loss, whose partial derivative with respect to S ij is the weight defined in Eqn 13 and Eqn 14. Our MS loss is formulated as,</p><formula xml:id="formula_14">L M S = 1 m m i=1 1 Î± log 1 + kâˆˆPi e âˆ’Î±(S ik âˆ’Î») + 1 Î² log 1 + kâˆˆNi e Î²(S ik âˆ’Î») .<label>(15)</label></formula><p>where L M S can be minimized with gradient descent optimization, by simply implementing the proposed iterative pair mining and weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our MS method is implemented by PyTorch. For network architecture, we use the Inception network with batch normalization <ref type="bibr" target="#b10">[11]</ref> pre-trained on ILSVRC 2012-CLS <ref type="bibr" target="#b27">[27]</ref>, and fine-tuned it for our task. We add a FC layer on the top of the network following the global pooling layer. All the input images are cropped to 224 Ã— 224. Random crop with random horizontal flip is used for training, and single center crop for testing. Adam optimizer was used for all experiments.</p><p>We conduct experiments on four standard datasets: CUB200 <ref type="bibr" target="#b33">[33]</ref>, Cars-196 <ref type="bibr" target="#b15">[16]</ref>, Stanford Online Products (SOP) <ref type="bibr" target="#b25">[25]</ref> and In-Shop Clothes Retrieval (In-Shop) <ref type="bibr" target="#b20">[21]</ref>. We follow the data split protocol applied in <ref type="bibr" target="#b25">[25]</ref>. For the CUB200 dataset, we use the first 100 classes with 5,864 images for training, and the remaining 100 classes with 5,924 images for testing. The Cars-196 dataset is composed of <ref type="bibr" target="#b15">16</ref> For every mini-batch, we randomly choose a certain number of classes, and then randomly sample M instances from each class with M = 5 for all datasets in all experiments. in Eqn 11 and Eqn 12 is set to 0.1 and the hyperparameters in Eqn 15 are: Î± = 2, Î» = 1, Î² = 50, by following <ref type="bibr" target="#b32">[32]</ref>. Our method is evaluated on image retrieval task by using the standard performance metric: Recall@K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>To demonstrate the importance of weighting the pairs from multi-similarities, we conduct an ablation study on Cars-196 and results are shown in <ref type="table" target="#tab_0">Table 2</ref>. We describe LiftedStruct * , MS mining and MS weighting here, other methods have already been mentioned in section 3.</p><p>LiftedStruct * . Lifted structure loss is easy to get stuck in a local optima, resulting in poor performance. To evaluate the efficiency of three similarities more clearly and fairly, we make a minor modification to the lifted structure loss, allowing it to employ Similarity-N more effectively:</p><formula xml:id="formula_15">L lif t * = 1 m m i=1 1 Î± log y k =y i e âˆ’Î±S ik + 1 Î² log y k =y i e Î²S ik ,<label>(16)</label></formula><p>where Î± = 2, Î² = 50. This modification is motivated to make lift structure loss more focus on informative pairs, especially the hard negative pairs, and allows it to get rid of the side effect of enormous easy negative pairs. We found that this modification can boost the performance of liftedstructure loss empirically, e.g., with an over 20% improvement of Recall@1 on the CUB200 . MS mining. To investigate the impact of each component of MS Loss, we evaluate the performance of MS mining individually, where the pairs selected into N i and P i are assigned with an equal weight.</p><p>MS weighting. Similarly, MS weighting scheme is also evaluated individually without the mining step in the ablation study, allowing us to analyze the effect of each similar-ity more perspicaciously. In MS weighting, each pair in a mini-batch is assigned with a non-zero weight, according to the weighting strategy described in Eqn 13 and Eqn 14.</p><p>With the performance reported in <ref type="table" target="#tab_0">Table 2</ref>, we analyze the effect of each similarity as below:</p><p>Similarity-S: A cosine self-similarity is of the most importance.</p><p>Binomial deviance loss, based on the Similarity-S, achieves the best performance by using a single similarity. Moreover, our MS weighting outperforms LiftedStruct * m by 69.7% â†’ 73.2% at recall@1, and Binomial m also improves the recall@1 with 67.0% â†’ 74.6% over the MS mining, by adding the Similarity-S into their weighting schemes.</p><p>Similarity-N: Relative similarities are also helpful to measuring the importance of a pair more precisely. With Similarity-N, our MS weighting increases the Recall@1 by 1.3% over Binomial (71.9% â†’ 73.2%). Moreover, with Similarity-N, LiftedStruct * m obtains a significant performance improvement over MS sampling (67% â†’ 72.2%), by considering both Similarity-P and Similarity-N. Similarity-P: As shown in <ref type="table" target="#tab_0">Table 2</ref>, by adding a mining step based on Similarity-P, the performances of LiftedStruct * , Binomial and MS weighting are consistently improved by a large margin. For instance, Recall@1 of Binomial is increased by nearly 3%: 71.9% â†’ 74.6%.</p><p>Finally, the proposed MS loss achieves the best performance among these methods, by exploring multisimilarities for pair mining and weighting. However, it is critical to integrate the three similarities effectively into a single framework where the three similarities can be fully explored and optimized jointly by using a single loss function. For example, BinLifted, which uses a weighting scheme considering both similarities-S and similarities-N, has lower performance than that of single Binomial, since it implements a simple and straightforward combination of Binomial and LiftedStruct * m . More discussions on the difference between our MS weighting and the direct combination are presented in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">On Embedding Size</head><p>By following <ref type="bibr" target="#b28">[28]</ref>, we study the performance of MS loss with varying embedding sizes {64, 128, 256, 512, 1024}. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the performance is increased consistently with the embedding dimension except at 1024. This is different from lifted structure loss, which achieves its best performance at 64 on the Cars-196 dataset <ref type="bibr" target="#b25">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-Art</head><p>We further compare the performance of our method with the state-of-the-art techniques on image retrieval task. As shown in <ref type="table">Table 3</ref>    bedding size of 512 and attention modules, our MS loss achieves a higher Recall@1 by +5% improvement at the same dimension on the CUB200. On the Cars-196, our MS loss obtains the second best performance in terms of Re-call@1, while the best results are achieved by ABE, which applies an ensemble method with a much heavier model. Moreover, on the remaining three datasets, our MS loss is of much stronger performance than ABE.</p><p>In <ref type="table" target="#tab_3">Table 4</ref>   Shop respectively. Moreover, even with much compact embedding features of 128 dimension, our MS loss has already surpassed all existing methods, which utilize much higher dimensions of 384, 512 and 4096. To summarize, on both fine-grained datasets like Cars-196 and CUB200, and large-scale datasets with enormous categories like SOP and In-Shop, our method achieves new state-of-the-art or comparable performance, even taking those methods with ensemble techniques like ABE and BIER into consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have established a General Pair Weighting (GPW) framework and presented a new multi-similarity loss to fully exploit the information of each pair. Our GPW, for the first time, unifies existing pair-based metric learning approaches into general pair weighting through gradient analysis. It provides a powerful tool for understanding and explaining various pair-based loss functions, which allows us to clearly identify the main differences and key limitations of existing methods. Furthermore, we proposed a multi-similarity loss which considers all three similarities simultaneously, and developed an iterative pair mining and weighting scheme for optimizing the multi-similarity loss efficiently. Our method obtains new state-of-the-art performance on a number of image retrieval benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This supplementary material provides more details of General Pair Weighting (GPW) framework and Multi-Similarity (MS) loss. First, we further revisit three existing pair-based loss functions under our GPW framework. Then, we analyze the limitation of direct combination of binomial deviance loss <ref type="bibr">[6]</ref> and lifted structure loss <ref type="bibr">[1]</ref> (referred as BinLifted), compared to our weighting scheme. Finally, we analyze the impact of batch-size to the proposed MS loss with experiments conducted on CUB200 and SOP datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Revisit Pair-based Loss Functions</head><p>In this section, we further analyze three recent pair-based loss functions in addition to those presented in the main paper: N-pairs loss <ref type="bibr">[3]</ref>, NCA loss <ref type="bibr">[2]</ref> and histogram loss <ref type="bibr">[4]</ref>.</p><p>N-pairs Loss. Sohn et al. <ref type="bibr">[3]</ref> proposed a N-pairs loss, which is a special case of the lifted structure loss by only considering single positive pair. It can be analyzed by following the same process of analyzing the lifted structure loss in the main paper, as shown in Eq. (6)- <ref type="bibr" target="#b7">(8)</ref>.</p><p>NCA Loss. Salakhutdinov et al. <ref type="bibr">[2]</ref> introduced a NCA loss to learn a nonlinear embedding to optimize the classification performance of a soft-KNN classifier:</p><formula xml:id="formula_16">L nca := m i=1 log y k =yi e S ik m i=1 e S ik = m i=1 log y k =yi e S ik âˆ’ log m i=1 e S ik .<label>(1)</label></formula><p>By following GPW framework, the weight of a pair {x i , x j }, i.e., w ij , can be derived from differentiating L nca with respect to S ij :</p><formula xml:id="formula_17">w + ij = e Sij y k =yi e S ik âˆ’ e Sij m i=1 e S ik = 1 y k =yi e S ik âˆ’Sij âˆ’ 1 m i=1 e S ik âˆ’Sij , w âˆ’ ij = e Sij m i=1 e S ik = 1 m i=1 e S ik âˆ’Sij .<label>(2)</label></formula><p>As can be found in Eq. 2, the weight for a positive pair or a negative pair is calculated based on its relative similarities, which compute a similarity between a current pair and its neighboring pairs. The NCA loss focuses on hard negative pairs and the high-confident positive pairs, collected from the neighboring pairs of an anchor point, in the embedding space.</p><p>Histogram Loss. Ustinova et al. <ref type="bibr">[4]</ref> designed a histogram loss based on quadruplets, which can be formulated as:</p><formula xml:id="formula_18">L hist := R r=1 (h âˆ’ r r q=1 h + q ) = R r=1 ( r q=1 h + q )h âˆ’ r = R q=1 ( R r=q h âˆ’ r )h + q<label>(3)</label></formula><p>where R is the dimension of a histogram for positive or negative cosine similarities. h + q is the histogram estimation at node q of the cosine similarity for a positive pair, and h âˆ’ r is that of a negative pair at node r.</p><formula xml:id="formula_19">h + r = 1 |S + | yi=yj Î´ ijr ,<label>(4)</label></formula><p>where Î´ ijr is defined as:</p><formula xml:id="formula_20">Î´ ijr = ï£± ï£² ï£³ (S ij âˆ’ t râˆ’1 )/âˆ†, S ij âˆˆ [t râˆ’1 , t r ], (t r+1 âˆ’ S ij )/âˆ†, S ij âˆˆ [t r , t r+1 ], 0, otherwise,<label>(5)</label></formula><p>where âˆ† = 2/(R âˆ’ 1), t r = râˆ† âˆ’ 1. Estimation of h âˆ’ q is computed analogously.</p><p>To better understanding histogram loss within our GPW framework, we first provide the following formulations (more details can be found in <ref type="bibr">[4]</ref>), <ref type="figure">Figure 1</ref>. Limitations of the BinLifted weighting scheme: (i) in case-1, the negative pair on the bottom, which is of much lower cosine similarity than the top one, will be assigned with a larger weight; (ii) in case-2, a negative pair is fixed when the neighboring negative samples move closer to the anchor, which reduce the relative similarly (Similarity-N) of the negative pair.</p><formula xml:id="formula_21">âˆ‚L hist âˆ‚h + q = R r=q h âˆ’ âˆ‚L hist âˆ‚h âˆ’ r = r q=1 h + (6)</formula><p>For a positive pair S ij</p><formula xml:id="formula_22">âˆ‚h + r âˆ‚S ij = ï£± ï£² ï£³ +1 âˆ†|S + | , S ij âˆˆ [t râˆ’1 , t r ], âˆ’1 âˆ†|S + | , S ij âˆˆ [t r , t r+1 ], 0, otherwise,<label>(7)</label></formula><p>where |S + | is the number of positive pairs. Then we have</p><formula xml:id="formula_23">âˆ‚h âˆ’ r âˆ‚S ij = 0,<label>(8)</label></formula><p>since h âˆ’ r is calculated from negative pairs, and thus is unrelated to the similarities of positive pairs, S ij .</p><p>Finally, the partial derivative of L hist w.r.t. the similarity of a positive pair S ij âˆˆ [t p , t p+1 ] is:</p><formula xml:id="formula_24">âˆ‚L hist âˆ‚S ij = R q=1 âˆ‚L hist âˆ‚h + q âˆ‚h + q âˆ‚S ij + R r=1 âˆ‚L hist âˆ‚h âˆ’ r âˆ‚h âˆ’ r âˆ‚S ij = R q=1 R r=q h âˆ’ r âˆ‚h + q âˆ‚S ij = R r=p h âˆ’ r âˆ‚h + p âˆ‚S ij + R r=p+1 h âˆ’ r âˆ‚h + p+1 âˆ‚S ij = 1 âˆ†|S + | R r=p+1 h âˆ’ r âˆ’ R r=p h âˆ’ r = âˆ’1 âˆ†|S + | h âˆ’ p .<label>(9)</label></formula><p>Thus the weight value assigned to this positive pair is</p><formula xml:id="formula_25">1 âˆ†|S + | h âˆ’ p .</formula><p>Similarly, for a negative pair with a cosine similarity of S ij âˆˆ [t p , t p+1 ], the weight under a histogram loss is 1 âˆ†|S âˆ’ | h + p+1 . Though with complicated formulation and rough derivation, pair weight scheme of histogram loss can be presented in an extremely concise and clear form, as shown in Eq. 9. h âˆ’ p is approximately the ratio of negative pairs which have a lower cosine similarity compared with the current positive pair, and 1 âˆ†|S âˆ’ | can be regarded as a fixed normalizer. Similarly, the weight of a negative pair is the ratio of positive pairs having a lower cosine similarity than the current negative one. Therefore, the weighting scheme clearly indicates that a histogram loss estimates pair weights only based on Similarity-P, e.g., by comparing the negative pairs with the positive ones, and vise verse. This may reduce the performance of histogram loss, leading to lower performance than that of binomial deviance loss on the CUB200 and SOP, as reported in <ref type="bibr">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BinLifted v.s. MS Weighting</head><p>As discussed in the main paper, our MS loss is related to a direct combination of binomial deviance loss and lifted structure loss (referred as BinLifted), by jointly considering both Similarity-S and Similarity-N. In our ablation study presented in Section 5.1 (in main paper), we show by experiments that our weighting scheme is superior to that of BinLifted. Here we explain the benefits of our iterative mining and weighting scheme. Given an example of a negative pair {x i , x j }, the weight computed by Binlifted weighting scheme w ij is:</p><formula xml:id="formula_26">w ij = 1 2 e Î²(Sij âˆ’Î») 1 + e Î²(Sij âˆ’Î») + e Î²Sij y k =yi e Î²S ik .<label>(10)</label></formula><p>From Eq. 10, the weight of BinLifted satisfies:</p><formula xml:id="formula_27">w ij &gt; 1 2 max e Î²(Sij âˆ’Î») 1 + e Î²(Sij âˆ’Î») , e Î²Sij y k =yi e Î²S ik .<label>(11)</label></formula><p>which provides a lower bound for the weighting by Bin-Lifted method. We found that a negative pair with a large relative similarity, will have a weight of e Î²S ij y k =y i e Î²S ik which is close to 1, so that BinLifted method will assign a large weight to this pair, and ignore or reduce the impact from Similarity-S. As shown in case-1 of <ref type="figure">Fig. 1, a</ref> negative pair on the bottom has a much lower Similarity-S than the negative pair on the top, but BinLifted will compute a similar weight for the two pairs. Furthermore, when the Similarity-S of a pair is higher than a threshold Î», the weight will be computed as e Î²(S ij âˆ’Î») 1+e Î²(S ij âˆ’Î») , which can be a large value, regardless of Similarity-N. As illustrated in case-2 of <ref type="figure">Fig. 1</ref>, two negative pairs on the bottom will be assigned with a similar weight, due to large values of Similarity-S, while omitting the large difference on the relative similarities (Similarity-N) between the two pairs. However, a negative pair on the top of case-2 will also be assigned with a similar weight to the bottom one, even its Similarity-S is much larger.</p><p>In summary, BinLifted estimates the weight of a pair by mainly considering the larger value computed from Similarity-S or Similarity-N, while neglecting the smaller one. This drawback may reduce the performance considerably, which can be even lower than that of using a single binomial deviance loss, as shown in the ablation study.</p><p>In contrast, our MS weighting is able to compute the weight for a negative pair dynamically: w ij = e Î²(Sij âˆ’Î») 1 + y k =yi e Î²(S ik âˆ’Î») .</p><p>which addresses the issue raised in case-1 and case-2 of <ref type="figure">Fig.  1</ref>. This allows it to explore more meaningful information from both Similarity-S and Similarity-S (Eq. 12), rather than only focusing on the single one with the larger value. Positive pairs can be analyzed analogously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Effect of Batch Size</head><p>To analyze the performance of our MS loss with different batch-size, we conduct experiments on the SOP [1] and CUB200 <ref type="bibr">[5]</ref> datasets. We set the embedding size to 512 and K = 5, and use Adam optimizer with a learning rate of 10 âˆ’5 for all experiments. The recall@1 performance of MS loss on the CUB200 and the SOP are reported in Tables 1 and 2, with batch-size set at {20, 40, 80, 160, 240} and {20, 40, 80, 160, 320, 640, 1000} respectively.</p><p>We observed that batch-size effects the performance of MS loss differently on the CUB200 and SOP: (i) the CUB200 is less sensitive to the change of batch-size than the SOP; (ii) the performance on the CUB200 decreases when the batch-size is increased, while the performance on the SOP benefits from large batch-sizes significantly.</p><p>We found that the batch-size impacts a dataset with large inter-class variations more significantly than the small ones.</p><p>The CUB200 is a fine-grained dataset with smaller interclass variations than the SOP, making it easier to collect hard negative pairs with subtle difference. However, by using a small batch-size of 20 on the SOP dataset, our MS mining is difficult to collect any informative pair at some iterations, which happened at over 20% iterations in our experiments. Therefore, a large batch-size, e.g., &gt; 320, is required for a dataset with large variations like the SOP. This ensures it to collect enough hard negative pairs, which are meaningful to train a discriminative model, and thus improve the performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Three types of similarities of a negative pair. From left to right, S: its cosine similarity to the anchor; P: its relative similarity compared with the positive pair; N: its relative similarity compared with other negative pairs. From top to bottom, the similarities becomes higher. Their weights should be larger since they contains more information to improve the current model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Results on the Cars-196 dataset using different embedding sizes of our MS loss. Larger embedding size can considerably improve recall@1, while embedding sizes larger than 512 are not necessary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>,185 images of cars from 196 model categories. The first 98 model categories are used for training, with the rest for testing. For the SOP dataset, we use 11,318 classes for training, and 11,316 classes for testing. For the In-Shop Results on Cars-196 using different pair-based methods. The first column lists the methods and the types of similarities considered in their weighting. Embedding size is set to 64. Subscript m denotes applying our MS mining step before weighting.</figDesc><table><row><cell>Recall@K (%)</cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>Binomial</cell><cell>S</cell><cell>71.9</cell><cell>80.0</cell><cell>86.4</cell><cell>91.0</cell></row><row><cell>LiftedStruct  *</cell><cell>N</cell><cell>69.7</cell><cell>79.3</cell><cell>86.2</cell><cell>91.0</cell></row><row><cell>MS mining</cell><cell>P</cell><cell>67.0</cell><cell>77.4</cell><cell>84.7</cell><cell>90.0</cell></row><row><cell>BinLifted</cell><cell>SN</cell><cell>70.4</cell><cell>79.5</cell><cell>86.2</cell><cell>91.1</cell></row><row><cell>MS weighting</cell><cell>SN</cell><cell>73.2</cell><cell>81.5</cell><cell>87.6</cell><cell>92.6</cell></row><row><cell>Binomialm</cell><cell>SP</cell><cell>74.6</cell><cell>83.8</cell><cell>89.7</cell><cell>94.1</cell></row><row><cell>LiftedStruct  *  m</cell><cell>NP</cell><cell>72.2</cell><cell>81.7</cell><cell>88.0</cell><cell>92.4</cell></row><row><cell>MS</cell><cell>SNP</cell><cell>77.3</cell><cell>85.3</cell><cell>90.5</cell><cell>94.2</cell></row><row><cell cols="6">dataset, 3,997 classes with 25,882 images are used for train-ing. The test set is partitioned to a query set with 14,218 im-ages of 3,985 classes, and a gallery set having 3,985 classes with 12,612 images.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, our MS loss improves Recall@1 by 7% on the CUB200, and 4% on the Cars-196 over Proxy-NCA at dimension 64. Compared with ABE employing an em-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CUB-200-2011</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Cars-196</cell></row><row><cell cols="3">Recall@K (%)</cell><cell></cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell cols="3">Clustering 64 [30]</cell><cell></cell><cell>48.2</cell><cell>61.4</cell><cell>71.8</cell><cell>81.9</cell><cell>-</cell><cell>-</cell><cell>58.1</cell><cell>70.6</cell><cell>80.3</cell><cell>87.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">ProxyNCA 64 [24]</cell><cell></cell><cell>49.2</cell><cell>61.9</cell><cell>67.9</cell><cell>72.4</cell><cell>-</cell><cell>-</cell><cell>73.2</cell><cell>82.4</cell><cell>86.4</cell><cell>87.8</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Smart Mining 64 [7]</cell><cell></cell><cell>49.8</cell><cell>62.3</cell><cell>74.1</cell><cell>83.3</cell><cell>-</cell><cell>-</cell><cell>64.7</cell><cell>76.2</cell><cell>84.2</cell><cell>90.2</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Margin 128 [38]</cell><cell></cell><cell>63.6</cell><cell>74.4</cell><cell>83.1</cell><cell>90.0</cell><cell>94.2</cell><cell>-</cell><cell>79.6</cell><cell>86.5</cell><cell>91.9</cell><cell>95.1</cell><cell>97.3</cell><cell>-</cell></row><row><cell cols="3">HDC 384 [30]</cell><cell></cell><cell>53.6</cell><cell>65.7</cell><cell>77.0</cell><cell>85.6</cell><cell>91.5</cell><cell>95.5</cell><cell>73.7</cell><cell>83.2</cell><cell>89.5</cell><cell>93.8</cell><cell>96.7</cell><cell>98.4</cell></row><row><cell cols="3">HTL 512 [4]</cell><cell></cell><cell>57.1</cell><cell>68.8</cell><cell>78.7</cell><cell>86.5</cell><cell>92.5</cell><cell>95.5</cell><cell>81.4</cell><cell>88.0</cell><cell>92.7</cell><cell>95.7</cell><cell>97.4</cell><cell>99.0</cell></row><row><cell cols="3">ABIER 512 [26]</cell><cell></cell><cell>57.5</cell><cell>68.7</cell><cell>78.3</cell><cell>86.2</cell><cell>91.9</cell><cell>95.5</cell><cell>82.0</cell><cell>89.0</cell><cell>93.2</cell><cell>96.1</cell><cell>97.8</cell><cell>98.7</cell></row><row><cell cols="3">ABE 512 [14]</cell><cell></cell><cell>60.6</cell><cell>71.5</cell><cell>79.8</cell><cell>87.4</cell><cell>-</cell><cell>-</cell><cell>85.2</cell><cell>90.5</cell><cell>94.0</cell><cell>96.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">MS 64</cell><cell></cell><cell></cell><cell>57.4</cell><cell>69.8</cell><cell>80.0</cell><cell>87.8</cell><cell>93.2</cell><cell>96.4</cell><cell>77.3</cell><cell>85.3</cell><cell>90.5</cell><cell>94.2</cell><cell>96.9</cell><cell>98.2</cell></row><row><cell cols="2">MS 512</cell><cell></cell><cell></cell><cell>65.7</cell><cell>77.0</cell><cell>86.3</cell><cell>91.2</cell><cell>95.0</cell><cell>97.3</cell><cell>84.1</cell><cell>90.4</cell><cell>94.0</cell><cell>96.5</cell><cell>98.0</cell><cell>98.9</cell></row><row><cell cols="14">Table 3. Recall@K(%) performance on CUB200 and Cars-196. Superscript denotes embedding size. ABIER [26] and ABE [14] are ensemble methods.</cell></row><row><cell></cell><cell>85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall@1 (%)</cell><cell>79 81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>77</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>75</cell><cell>64</cell><cell>128</cell><cell cols="2">256 Embedding Dims</cell><cell>512</cell><cell>1024</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Recall@K(%) performance on In-Shop.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>and 5, our MS loss outperforms Proxy-NCA by 0.4% and Clustering by 7% at the same embedding size of 64. Furthermore, when compared with ABE, MS loss increases Recall@1 by 1.9% and 2.7% on the SOP and In-</figDesc><table><row><cell>Recall@K (%)</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>1000</cell></row><row><cell>Clustering 64 [30]</cell><cell>67.0</cell><cell>83.7</cell><cell>93.2</cell><cell>-</cell></row><row><cell>ProxyNCA 64 [24]</cell><cell>73.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Margin 128 [38]</cell><cell>72.7</cell><cell>86.2</cell><cell>93.8</cell><cell>98.0</cell></row><row><cell>HDC 384 [30]</cell><cell>69.5</cell><cell>84.4</cell><cell>92.8</cell><cell>97.7</cell></row><row><cell>HTL 512 [4]</cell><cell>74.8</cell><cell>88.3</cell><cell>94.8</cell><cell>98.4</cell></row><row><cell>ABIER 512 [26]</cell><cell>74.2</cell><cell>86.9</cell><cell>94.0</cell><cell>97.8</cell></row><row><cell>ABE 512 [14]</cell><cell>76.3</cell><cell>88.4</cell><cell>94.8</cell><cell>98.2</cell></row><row><cell>MS 64</cell><cell>74.1</cell><cell>87.8</cell><cell>94.7</cell><cell>98.2</cell></row><row><cell>MS 128</cell><cell>76.6</cell><cell>89.2</cell><cell>95.2</cell><cell>98.4</cell></row><row><cell>MS 512</cell><cell>78.2</cell><cell>90.5</cell><cell>96.0</cell><cell>98.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Recall@K(%) performance on SOP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Recall@1 performance of MS loss at the batch-size of {20, 40, 80, 120, 160, 240} on the CUB200 database. Recall@1 performance of MS loss at the batch-size of {20, 40, 80, 160, 320, 640} on the SOP database.</figDesc><table><row><cell cols="2">Batch-size Recall@1 (%)</cell></row><row><cell>20 40 80 160 240</cell><cell>64.75 65.07 65.65 65.43 64.60</cell></row><row><cell cols="2">Batch-size Recall@1 (%)</cell></row><row><cell>20 40 80 160 320 640 1000</cell><cell>71.40 73.82 75.61 76.63 77.59 78.19 78.35</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For simplicity, we use a cosine similarity instead of Euclidean distance, by assuming an embedding vector is L 2 normalized.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving semantic embedding consistency by metric learning for zero-shot classiffication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d pose estimation and 3d model retrieval for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>B G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Triplet-center loss for multi-view 3d object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737v4</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIMBAD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention-based ensemble for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A zero-shot framework for sketch based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yelamarthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Krishna</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quadruplet-wise image similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning by tracking: Siamese cnn for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-TaixÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Similarity metric learning for a variable-kernel classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mullers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks for signal processing</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<title level="m">Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly. PAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Dataset. Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep metric learning for practical person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.4979</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hardaware point-to-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zero-shot learning via joint latent similarity embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class npair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset. Master&apos;s thesis</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep metric learning for practical person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.4979</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
