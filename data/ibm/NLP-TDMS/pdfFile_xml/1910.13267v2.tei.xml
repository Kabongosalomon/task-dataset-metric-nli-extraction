<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BPE-Dropout: Simple and Effective Subword Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Provilkov</surname></persName>
							<email>iv-provilkov@yandex-team.ru</email>
							<affiliation key="aff0">
								<orgName type="institution">Moscow Institute of Physics and Technology</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Emelianenko</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Research University Higher School of Economics</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
							<email>lena-voita@yandex-team.ru</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">Scotland</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yandex</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russia</surname></persName>
						</author>
						<title level="a" type="main">BPE-Dropout: Simple and Effective Subword Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm <ref type="bibr" target="#b16">(Kudo, 2018)</ref>. In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout -simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Using subword segmentation has become de-facto standard in Neural Machine Translation <ref type="bibr" target="#b4">(Bojar et al., 2018;</ref><ref type="bibr" target="#b3">Barrault et al., 2019)</ref>. Byte Pair Encoding (BPE) <ref type="bibr" target="#b24">(Sennrich et al., 2016)</ref> is the dominant approach to subword segmentation. It keeps the common words intact while splitting the rare and unknown ones into a sequence of subword units. This potentially allows a model to make * Equal contribution. use of morphology, word composition and transliteration. BPE effectively deals with an openvocabulary problem and is widely used due to its simplicity.</p><p>There is, however, a drawback of BPE in its deterministic nature: it splits words into unique subword sequences, which means that for each word a model observes only one segmentation. Thus, a model is likely not to reach its full potential in exploiting morphology, learning the compositionality of words and being robust to segmentation errors. Moreover, as we will show further, subwords into which rare words are segmented end up poorly understood.</p><p>A natural way to handle this problem is to enable multiple segmentation candidates. This was initially proposed by <ref type="bibr" target="#b16">Kudo (2018)</ref> as a subword regularization -a regularization method, which is implemented as an on-the-fly data sampling and is not specific to NMT architecture. Since standard BPE produces single segmentation, to realize this regularization the author had to propose a new subword segmentation, different from BPE. However, the introduced approach is rather complicated: it requires training a separate segmentation unigram language model, using EM and Viterbi algorithms, and forbids using conventional BPE.</p><p>In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. BPE builds a vocabulary of subwords and a merge table, which specifies which subwords have to be merged into a bigger subword, as well as the priority of the merges. During segmentation, words are first split into sequences of characters, then the learned merge operations are applied to merge the characters into larger, known symbols, till no merge can be done <ref type="figure">(Figure 1(a)</ref>). We introduce BPE-dropout -a subword regularization method based on and compatible with conventional BPE. It uses a vocabulary and a arXiv:1910.13267v2 [cs.CL] 1 May 2020 (a) (b) <ref type="figure">Figure 1</ref>: Segmentation process of the word 'unrelated' using (a) BPE, (b) BPE-dropout. Hyphens indicate possible merges (merges which are present in the merge table); merges performed at each iteration are shown in green, dropped -in red. merge table built by BPE, but at each merge step, some merges are randomly dropped. This results in different segmentations for the same word <ref type="bibr">(Figure 1(b)</ref>). Our method requires no segmentation training in addition to BPE and uses standard BPE at test time, therefore is simple. BPE-dropout is superior compared to both BPE and <ref type="bibr" target="#b16">Kudo (2018)</ref> on a wide range of translation tasks, therefore is effective.</p><p>Our key contributions are as follows:</p><p>• We introduce BPE-dropout -a simple and effective subword regularization method;</p><p>• We show that our method outperforms both BPE and previous subword regularization on a wide range of translation tasks;</p><p>• We analyze how training with BPE-dropout affects a model and show that it leads to a better quality of learned token embeddings and to a model being more robust to noisy input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we briefly describe BPE and the concept of subword regularization. We assume that our task is machine translation, where a model needs to predict the target sentence Y given the source sentence X, but the methods we describe are not task-specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Byte Pair Encoding (BPE)</head><p>To define a segmentation procedure, BPE (Sennrich et al., 2016) builds a token vocabulary and a merge table. The token vocabulary is initialized with the character vocabulary, and the merge table is initialized with an empty table. First, each word is represented as a sequence of tokens plus a special end of word symbol. Then, the method iteratively counts all pairs of tokens and merges the most frequent pair into a new token. This token is added to the vocabulary, and the merge operation is added to the merge table. This is done until the desired vocabulary size is reached. The resulting merge table specifies which subwords have to be merged into a bigger subword, as well as the priority of the merges. In this way, it defines the segmentation procedure. First, a word is split into distinct characters plus the end of word symbol. Then, the pair of adjacent tokens which has the highest priority is merged. This is done iteratively until no merge from the table is available <ref type="figure">(Figure 1(a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Subword regularization</head><p>Subword regularization <ref type="bibr" target="#b16">(Kudo, 2018)</ref> is a training algorithm which integrates multiple segmentation candidates. Instead of maximizing log-likelihood, this algorithm maximizes log-likelihood marginalized over different segmentation candidates. Formally,</p><formula xml:id="formula_0">L = (X,Y )∈D E x∼P (x|X) y∼P (y|Y ) log P (y|x, θ),<label>(1)</label></formula><p>where x and y are sampled segmentation candidates for sentences X and Y respectively, P (x|X) and P (y|Y ) are the probability distributions the candidates are sampled from, and θ is the set of model parameters. In practice, at each training step only one segmentation candidate is sampled.</p><p>Since standard BPE segmentation is deterministic, to realize this regularization <ref type="bibr" target="#b16">Kudo (2018)</ref> proposed a new subword segmentation. The introduced approach requires training a separate segmentation unigram language model to predict the probability of each subword, EM algorithm to optimize the vocabulary, and Viterbi algorithm to make samples of segmentations.</p><p>Subword regularization was shown to achieve significant improvements over the method using a single subword sequence. However, the proposed method is rather complicated and forbids using conventional BPE. This may prevent practitioners from using subword regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach: BPE-Dropout</head><p>We show that to realize subword regularization it is not necessary to reject BPE since multiple segmentation candidates can be generated within the BPE framework. We introduce BPE-dropout -a method which exploits the innate ability of BPE to be stochastic. It alters the segmentation procedure while keeping the original BPE merge table. During segmentation, at each merge step some merges are randomly dropped with the probability p. This procedure is described in Algorithm 1.</p><p>Algorithm 1: BPE-dropout current split ← characters from input word; do merges ← all possible merges 1 of tokens from current split; for merge from merges do / * The only difference from BPE * / remove merge from merges with the probability p; end if merges is not empty then merge ← select the merge with the highest priority from merges; apply merge to current split; end while merges is not empty; return current split;</p><p>If p is set to 0, the segmentation is equivalent to the standard BPE; if p is set to 1, the segmentation splits words into distinct characters. The values between 0 and 1 can be used to control the segmentation granularity.</p><p>We use p &gt; 0 (usually p = 0.1) in training time to expose a model to different segmentations and p = 0 during inference, which means that at inference time we use the original BPE. We discuss the choice of the value of p in Section 5.</p><p>When some merges are randomly forbidden during segmentation, words end up segmented in different subwords; see for example <ref type="figure">Figure 1</ref> We hypothesize that exposing a model to different 1 In case of multiple occurrences of the same merge in a word (for example, m-e-r-g-e-r has two occurrences of the merge (e, r)), we decide independently for each occurrence whether to drop it or not. segmentations may result in better understanding of the whole words as well as their subword units; we will verify this in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup 4.1 Baselines</head><p>Our baselines are the standard BPE and the subword regularization by <ref type="bibr" target="#b16">Kudo (2018)</ref>.</p><p>Subword regularization by <ref type="bibr" target="#b16">Kudo (2018)</ref> has segmentation sampling hyperparameters l and α. l specifies how many best segmentations for each word are produced before sampling one of them, α controls the smoothness of the sampling distribution. In the original paper (l = ∞, α = 0.2/0.5) and (l = 64, α = 0.1) were shown to perform best on different datasets. Since overall they show comparable results, in all experiments we use (l = 64, α = 0.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Vocabularies</head><p>There are two ways of building vocabulary for models trained with BPE-dropout: (1) take the vocabulary built by BPE; then the segmented with BPE-dropout text will contain a small number of unknown tokens (UNKs) 2 ; (2) add to the BPE vocabulary all tokens which can appear when segmenting with BPE-dropout.</p><p>In the preliminary experiments, we did not observe any difference in quality; therefore, either of the methods can be used. We choose the first option to stay in the same setting as the standard BPE. Besides, a model exposed to some UNKs in training can be more reliable for practical applications where unknown tokens can be present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data sets and preprocessing</head><p>We conduct our experiments on a wide range of datasets with different corpora sizes and languages; information about the datasets is summarized in <ref type="table" target="#tab_0">Table 1</ref>. These datasets are used in the main experiments (Section 5.1) and were chosen to match the ones used in the prior work <ref type="bibr" target="#b16">(Kudo, 2018)</ref>. In the additional experiments (Sections 5.2-5.5), we also use random subsets of the WMT14 English-French data; in this case, we specify dataset size for each experiment.</p><p>Prior to segmentation, we preprocess all Relying on a recent study of how the choice of vocabulary size influences translation quality <ref type="bibr" target="#b9">(Ding et al., 2019)</ref>, we choose vocabulary size depending on the dataset size <ref type="table" target="#tab_0">(Table 1)</ref>.</p><p>In training, translation pairs were batched together by approximate sequence length. For the main experiments, the values of batch size we used are given in <ref type="table" target="#tab_0">Table 1</ref> (batch size is the number of source tokens). In the experiments in Sections 5.2, 5.3 and 5.4, for datasets not larger than 500k sentence pairs we use vocabulary size and batch size of 4k, and 32k for the rest. <ref type="bibr">4</ref> In the main text, we train all models on lowercased data. In the appendix, we provide additional experiments with the original case and casesensitive BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model and optimizer</head><p>The NMT system used in our experiments is Transformer base <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>. More precisely, the number of layers is N = 6 with h = 8 parallel attention layers, or heads. The dimensionality of input and output is d model = 512, and the inner-layer of feed-forward networks has dimensionality d f f = 2048. We use regularization and optimization procedure as described in <ref type="bibr" target="#b27">Vaswani et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training time</head><p>We train models till convergence. For all experiments, we provide number of training batches in the appendix <ref type="table" target="#tab_8">(Tables 6 and 7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Inference</head><p>To produce translations, for all models, we use beam search with the beam of 4 and length normalization of 0.6.</p><p>In addition to the main results, Kudo (2018) also report scores using n-best decoding. To translate a sentence, this strategy produces multiple segmentations of a source sentence, generates a translation for each of them, and rescores the obtained translations. While this could be an interesting future work to investigate different sampling and rescoring strategies, in the current study we use 1-best decoding to fit in the standard decoding paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Evaluation</head><p>For evaluation, we average 5 latest checkpoints and use BLEU <ref type="bibr" target="#b20">(Papineni et al., 2002)</ref> computed via SacreBleu 5 <ref type="bibr" target="#b22">(Post, 2018)</ref>. For Chinese, we add option --tok zh to SacreBLEU. For Japanese, we use character-based BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main results</head><p>The results are provided in <ref type="table" target="#tab_2">Table 2</ref>. For all datasets, BPE-dropout improves significantly over the standard BPE: more than 1.5 BLEU for En-Vi, Vi-En, En-Zh, Zh-En, Ar-En, De-En, and 0.5-1.4 BPE <ref type="bibr" target="#b16">Kudo (2018)</ref>   BLEU for the rest. The improvements are especially prominent for smaller datasets; we will discuss this further in Section 5.4.</p><p>Compared to <ref type="bibr" target="#b16">Kudo (2018)</ref>, among the 12 datasets we use BPE-dropout is beneficial for 8 datasets with improvements up to 0.92 BLEU, is not significantly different for 3 datasets and underperforms only on En-Ja. While <ref type="bibr" target="#b16">Kudo (2018)</ref> uses another segmentation, our method operates within the BPE framework and changes only the way a model is trained. Thus, lower performance of BPE-dropout on En-Ja and only small or insignificant differences for Ja-En, En-Zh and Zh-En suggest that Japanese and Chinese may benefit from a language-specific segmentation.</p><p>Note also that <ref type="bibr" target="#b16">Kudo (2018)</ref> report larger improvements over BPE from using their method than we show in <ref type="table" target="#tab_2">Table 2</ref>. This might be explained by the fact that <ref type="bibr" target="#b16">Kudo (2018)</ref> used large vocabulary size (16k, 32k), which has been shown counterproductive for small datasets <ref type="bibr" target="#b25">(Sennrich and Zhang, 2019;</ref><ref type="bibr" target="#b9">Ding et al., 2019)</ref>. While this may not be the issue for models trained with subword regularization (see Section 5.4), this causes drastic drop in performance of the baselines.  <ref type="table">Table 3</ref>: BLEU scores for models trained with BPEdropout on a single side of a translation pair or on both sides. Models trained on random subsets of WMT14 En-Fr dataset. Bold indicates the best score and all scores whose difference from the best is not statistically significant (with p-value of 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Single side vs full regularization</head><p>In this section, we investigate whether BPEdropout should be used only on one side of a translation pair or for both source and target languages. We select random subsets of different sizes from WMT14 En-Fr data to understand how the results are affected by the amount of data. We show that:</p><p>• for small and medium datasets, full regularization performs best;</p><p>• for large datasets, BPE-dropout should be used only on the source side.</p><p>Since full regularization performs the best for most of the considered dataset sizes, in the subsequent sections we use BPE-dropout on both source and target sides. <ref type="table">Table 3</ref> indicates that using BPE-dropout on the source side is more beneficial than on the target side; for the datasets not smaller than 0.5m sentence pairs, BPE-dropout can be used only the source side. We can speculate that it is more important for the model to understand a source sentence than being exposed to different ways to generate the same target sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Small and medium datasets: use full regularization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Large datasets: use only for source</head><p>For larger corpora (e.g., starting from 4m instances), it is better to use BPE-dropout only on the source side <ref type="table">(Table 3)</ref>. Interestingly, using BPE-dropout for both source and target languages hurts performance for large datasets.  <ref type="figure" target="#fig_1">Figure 2</ref> shows BLEU scores for the models trained on BPE-dropout with different values of p (the probability of a merge being dropped). Models trained with high values of p are unable to translate due to a large mismatch between training segmentation (which is close to char-level) and inference segmentation (BPE). The best quality is achieved with p = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Choice of the value of p</head><p>In our experiments, we use p = 0.1 for all languages except for Chinese and Japanese. For Chinese and Japanese, we take the value of p = 0.6 to match the increase in length of segmented sentences for other languages. 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Varying corpora and vocabulary size</head><p>Now we will look more closely at how the improvement from using BPE-dropout depends on corpora and vocabulary size.</p><p>First, we see that BPE-dropout performs best for all dataset sizes <ref type="figure" target="#fig_2">(Figure 3</ref>). Next, models trained with subword regularization are less sensitive to the choice of vocabulary size: differences in performance of models with 4k and 32k vocabulary are much less than for models trained with the standard BPE. This makes BPE-dropout attractive since it allows (i) not to tune vocabulary size for each dataset, (ii) choose vocabulary size depending on the desired model properties: models with smaller vocabularies are beneficial in terms of number of parameters, models with larger vocabularies are beneficial in terms of inference time. 7 Finally, we see that the effect from using 6 Formally, for English/French/etc. with BPE-dropout, p = 0.1 sentences become on average about 1.25 times longer compared to segmented with BPE; for Chinese and Japanese, we need to set the value of p to 0.6 to achieve the same increase.</p><p>7 <ref type="table" target="#tab_4">Table 4</ref> shows that inference for models with 4k vocab- BPE-dropout vanishes when a corpora size gets bigger. This is not surprising: the effect of any regularization is less in high-resource settings; however, as we will show later in Section 6.3, when applied to noisy source, models trained with BPEdropout show substantial improvements up to 2 BLEU even in high-resource settings.</p><p>Note that for larger corpora, we recommend using BPE-dropout only for source language (Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Inference time and length of generated sequences</head><p>Since BPE-dropout produces more fine-grained segmentation, sentences segmented with BPEdropout are longer; distribution of sentence lengths are shown in <ref type="figure" target="#fig_3">Figure 4</ref> (a) (with p = 0.1, on average about 1.25 times longer). Thus there is a potential danger that models trained with BPEdropout may tend to use more fine-grained segmentation in inference and hence to slow inference down. However, in practice this is not the case: distributions of lengths of generated translations for models trained with BPE and with BPEdropout are close <ref type="figure" target="#fig_3">(Figure 4 (b)</ref>). 8 <ref type="table" target="#tab_4">Table 4</ref> confirms these observations and shows that inference time of models trained with BPEdropout is not substantially different from the ones trained with BPE. ulary is more than 1.4 times longer than models with 32k vocabulary.</p><p>8 This is the result of using beam search: while samples from a model reproduce training data distribution quite well, beam search favors more frequent tokens <ref type="bibr" target="#b19">(Ott et al., 2018)</ref>. Therefore, beam search translations tend not to use less frequent fine-grained segmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>In this section, we analyze qualitative differences between models trained with BPE and BPEdropout. We find, that</p><p>• when using BPE, frequent sequences of characters rarely appear in a segmented text as individual tokens rather than being a part bigger ones; BPE-dropout alleviates this issue;</p><p>• by analyzing the learned embedding spaces, we show that using BPE-dropout leads to a better understanding of rare tokens;</p><p>• as a consequence of the above, models trained with BPE-dropout are more robust to misspelled input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Substring frequency</head><p>Here we highlight one of the drawbacks of BPE's deterministic nature: since it splits words into unique subword sequences, only rare words are split into subwords. This forces frequent sequences of characters to mostly appear in a segmented text as part of bigger tokens, and not as individual tokens. To show this, for each token in the BPE vocabulary we calculate how often it appears in a segmented text as an individual token and as a sequence of characters (which may <ref type="figure">Figure 5</ref>: Distribution of token to substring ratio for texts segmented using BPE or BPE-dropout for the same vocabulary of 32k tokens; only 10% most frequent substrings are shown. (Token to substring ratio of a token is the ratio between its frequency as an individual token and as a sequence of characters.) be part of a bigger token or an individual token). <ref type="figure">Figure 5</ref> shows distribution of the ratio between substring frequency as an individual token and as a sequence of characters (for top-10% most frequent substrings). For frequent substrings, the distribution of token to substring ratio is clearly shifted to zero, which confirms our hypothesis: frequent sequences of characters rarely appear in a segmented text as individual tokens. When a text is segmented using BPE-dropout with the same vocabulary, this distribution significantly shifts away from zero, meaning that frequent substrings appear in a segmented text as individual tokens more often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Properties of the learned embeddings</head><p>Now we will analyze embedding spaces learned by different models. We take embeddings learned by models trained with BPE and BPE-dropout and for each token look at the closest neighbors in the corresponding embedding space. <ref type="figure" target="#fig_4">Figure 6</ref> shows several examples. In contrast to BPE, nearest neighbours of a token in the embedding space of BPE-dropout are often tokens that share sequences of characters with the original token. To verify this observation quantitatively, we computed character 4-gram precision of top-10 neighbors: the proportion of those 4-grams of the top-10 closest neighbors which are present among 4grams of the original token. As expected, embeddings of BPE-dropout have higher character 4gram precision (0.29) compared to the precision of BPE (0.18).</p><p>This also relates to the study by <ref type="bibr" target="#b11">Gong et al. (2018)</ref>. For several tasks, they analyze the em-  bedding space learned by a model. The authors find that while a popular token usually has semantically related neighbors, a rare word usually does not: a vast majority of closest neighbors of rare words are rare words. To confirm this, we reduce dimensionality of embeddings by SVD and visualize <ref type="figure" target="#fig_5">(Figure 7</ref>). For the model trained with BPE, rare tokens are in general separated from the rest; for the model trained with BPE-dropout, this is not the case. While to alleviate this issue <ref type="bibr" target="#b11">Gong et al. (2018)</ref> propose to use adversarial training for embedding layers, we showed that a trained with BPE-dropout model does not have this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Robustness to misspelled input</head><p>Models trained with BPE-dropout better learn compositionality of words and the meaning of subwords, which suggests that these models have to be more robust to noise. We verify this by measuring the translation quality of models on a test set augmented with synthetic misspellings. We augment the source side of a test set by modifying each word with the probability of 10% by applying one of the predefined operations. The operations we consider are (1) removal of one character from a word, (2) insertion of a random character into a word, (3) substitution of a character in a word with a random one. This augmentation produces words  with the edit distance of 1 from the unmodified words. Edit distance is commonly used to model misspellings <ref type="bibr" target="#b5">(Brill and Moore, 2000;</ref><ref type="bibr" target="#b0">Ahmad and Kondrak, 2005;</ref><ref type="bibr" target="#b21">Pinter et al., 2017)</ref>. <ref type="table" target="#tab_6">Table 5</ref> shows the translation quality of the models trained on WMT 14 dataset when given the original source and augmented with misspellings. We deliberately chose large datasets, where improvements from using BPE-dropout are smaller. We can see that while for the original test sets the improvements from using BPE-dropout are usually modest, for misspelled test set the improvements are a lot larger: 1.6-2.3 BLEU. This is especially interesting since models have not been exposed to misspellings during training. Therefore, even for large datasets using BPE-dropout can result in substantially better quality for practical applications where input is likely to be noisy.</p><p>Closest to our work in motivation is the work by <ref type="bibr" target="#b16">Kudo (2018)</ref>, who introduced the subword regularization framework multiple segmentation candidates and a new segmentation algorithm. Other segmentation algorithms include <ref type="bibr" target="#b8">Creutz and Lagus (2006)</ref>, <ref type="bibr" target="#b23">Schuster and Nakajima (2012)</ref>, Chitnis and DeNero <ref type="formula" target="#formula_0">(2015)</ref>, <ref type="bibr" target="#b17">Kunchukuttan and Bhattacharyya (2016)</ref>, <ref type="bibr" target="#b28">Wu and Zhao (2018)</ref>, <ref type="bibr" target="#b2">Banerjee and Bhattacharyya (2018)</ref>.</p><p>Regularization techniques are widely used for training deep neural networks. Among regularizations applied to a network weights the most popular are Dropout <ref type="bibr" target="#b26">(Srivastava et al., 2014)</ref> and L 2 regularization. Data augmentation techniques in natural language processing include dropping tokens at random positions or swapping tokens at close positions <ref type="bibr" target="#b12">(Iyyer et al., 2015;</ref><ref type="bibr" target="#b1">Artetxe et al., 2018;</ref><ref type="bibr" target="#b18">Lample et al., 2018)</ref>, replacing tokens at random positions with a placeholder token <ref type="bibr" target="#b29">(Xie et al., 2017)</ref>, replacing tokens at random positions with a token sampled from some distribution (e.g., based on token frequency or a language model) <ref type="bibr" target="#b10">(Fadaee et al., 2017;</ref><ref type="bibr" target="#b29">Xie et al., 2017;</ref><ref type="bibr" target="#b13">Kobayashi, 2018)</ref>. While BPE-dropout can be thought of as a regularization, our motivation is not to make a model robust by injecting noise. By exposing a model to different segmentations, we want to teach it to better understand the composition of words as well as subwords, and make it more flexible in the choice of segmentation during inference.</p><p>Several works study how translation quality depends on a level of granularity of a segmentation <ref type="bibr" target="#b6">(Cherry et al., 2018;</ref><ref type="bibr" target="#b15">Kreutzer and Sokolov, 2018;</ref><ref type="bibr" target="#b9">Ding et al., 2019)</ref>. <ref type="bibr" target="#b6">Cherry et al. (2018)</ref> show that trained long enough character-level models tend to have better quality, but it comes with the increase of computational cost for both training and inference. <ref type="bibr" target="#b15">Kreutzer and Sokolov (2018)</ref> find that, given flexibility in choosing segmentation level, the model prefers to operate on (almost) character level. <ref type="bibr" target="#b9">Ding et al. (2019)</ref> explore the effect of BPE vocabulary size and find that it is better to use small vocabulary for low-resource setting and large vocabulary for a high-resource setting. Following these observations, in our experiments we use different vocabulary size depending on a dataset size to ensure the strongest baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We introduce BPE-dropout -simple and effective subword regularization, which operates within the standard BPE framework. The only difference from BPE is how a word is segmented during model training: BPE-dropout randomly drops some merges from the BPE merge table, which results in different segmentations for the same word. Models trained with BPE-dropout (1) outperform BPE and the previous subword regularization on a wide range of translation tasks, (2) have better quality of learned embeddings, (3) are more robust to noisy input. Future research directions include adaptive dropout rates for different merges and an in-depth analysis of other pathologies in learned token embeddings for different segmentations.</p><p>A Training time <ref type="table" target="#tab_8">Table 6</ref> shows number of training batches for the experiments in Section 5.1 <ref type="table" target="#tab_2">(Table 2)</ref>, <ref type="table" target="#tab_9">Table 7</ref> for the experiments in Section 5.2 <ref type="table">(Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional experiments</head><p>In the main text, all models were trained (and evaluated) on lowercased data. Here we provide results of the models trained and evaluated without lower case <ref type="table" target="#tab_10">(Table 8)</ref>. <ref type="bibr">BPE Kudo (2018)</ref>      <ref type="bibr" target="#b14">(Koehn, 2004)</ref>.)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>BLEU scores for the models trained with BPE-dropout with different values of p. WMT14 En-Fr, 500k sentence pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>BLEU scores. Models trained on random subsets of WMT14 En-Fr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Distributions of length (in tokens) of (a) the French part of WMT14 En-Fr test set segmented using BPE or BPE-dropout; and (b) the generated translations for the same test set by models trained with BPE or BPE-dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Examples of nearest neighbours in the source embedding space of models trained with BPE and BPEdropout. Models trained on WMT14 En-Fr (4m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of source embeddings. Models trained on WMT14 En-Fr (4m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of the datasets and dataset-dependent hyperparametes; values of p are shown in pairs: source language / target language. (We explain the choice of the value of p for BPE-dropout in Section 5.3.)</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Number of sentences Voc size Batch size The value of p</cell></row><row><cell></cell><cell></cell><cell>(train/dev/test)</cell><cell></cell><cell></cell><cell>in BPE-dropout</cell></row><row><cell cols="2">IWSLT15 En ↔ Vi</cell><cell>133k / 1553 / 1268</cell><cell>4k</cell><cell>4k</cell><cell>0.1 / 0.1</cell></row><row><cell></cell><cell>En ↔ Zh</cell><cell>209k / 887 / 1261</cell><cell>4k / 16k</cell><cell>4k</cell><cell>0.1 / 0.6</cell></row><row><cell cols="2">IWSLT17 En ↔ Fr</cell><cell>232k / 890 / 1210</cell><cell>4k</cell><cell>4k</cell><cell>0.1 / 0.1</cell></row><row><cell></cell><cell>En ↔ Ar</cell><cell>231k / 888 / 1205</cell><cell>4k</cell><cell>4k</cell><cell>0.1 / 0.1</cell></row><row><cell>WMT14</cell><cell cols="2">En ↔ De 4.5M / 3000 / 3003</cell><cell>32k</cell><cell>32k</cell><cell>0.1 / 0.1</cell></row><row><cell>ASPEC</cell><cell>En ↔ Ja</cell><cell>2M / 1700 / 1812</cell><cell>16k</cell><cell>32k</cell><cell>0.1 / 0.6</cell></row><row><cell cols="3">datasets with the standard Moses toolkit. 3 How-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ever, Chinese and Japanese have no explicit word</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">boundaries, and Moses tokenizer does not segment</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">sentences into words; for these languages, sub-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">word segmentations are trained almost from un-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">segmented raw sentences.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>BLEU scores. Bold indicates the best score and all scores whose difference from the best is not sta- tistically significant (with p-value of 0.05). (Statisti- cal significance is computed via bootstrapping (Koehn, 2004).)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Relative inference time of models trained with</cell></row><row><cell>different subword segmentation methods. Results ob-</cell></row><row><cell>tained by (1) computing averaged over 1000 runs time</cell></row><row><cell>needed to translate WMT14 En-Fr test set, (2) dividing</cell></row><row><cell>all results by the smallest of the obtained times.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: BLEU scores for models trained on WMT14</cell></row><row><cell>dataset evaluated given the original and misspelled</cell></row><row><cell>source. For En-Fr trained on 16m sentence pairs, BPE-</cell></row><row><cell>dropout was used only on the source side (Section 5.2).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Number of thousands of training batches for the experiments from Table 2.</figDesc><table><row><cell></cell><cell>BPE</cell><cell></cell><cell>BPE-dropout</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">src-only dst-only both</cell></row><row><cell>250k</cell><cell>47</cell><cell>53</cell><cell>53</cell><cell>85</cell></row><row><cell cols="2">500k 160</cell><cell>210</cell><cell>250</cell><cell>320</cell></row><row><cell>1m</cell><cell>30</cell><cell>114</cell><cell>67</cell><cell>180</cell></row><row><cell>4m</cell><cell>100</cell><cell>321</cell><cell>180</cell><cell>600</cell></row><row><cell cols="2">16m 345</cell><cell>345</cell><cell>-</cell><cell>400</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Number of thousands of training batches for the experiments fromTable 3. Note that we use batch size 4k tokens for small corpora (250k and 500k) and 32k tokens for large corpora (1m, 4m and 16m).</figDesc><table><row><cell cols="2">BPE BPE-dropout</cell></row><row><cell>IWSLT15</cell><cell></cell></row><row><cell>En-Vi 31.44</cell><cell>32.70</cell></row><row><cell>Vi-En 32.19</cell><cell>33.22</cell></row><row><cell>IWSLT17</cell><cell></cell></row><row><cell>En-Fr 38.79</cell><cell>39.83</cell></row><row><cell>Fr-En 38.06</cell><cell>38.60</cell></row><row><cell>En-Ar 14.30</cell><cell>15.20</cell></row><row><cell>Ar-En 31.56</cell><cell>33.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>BLEU scores. Bold indicates the best score; differences with the baselines are statistically significant (with p-value of 0.05). (Statistical significance is computed via bootstrapping</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, for the English part of the IWSLT15 En-Vi corpora, these UNKs make up 0.00585 and 0.00085 of all tokens for 32k and 4k vocabularies, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/moses-smt/ mosesdecoder 4 Large batch size can be reached by using several of GPUs or by accumulating the gradients for several batches and then making an update.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Our SacreBLEU signature is: BLEU+case.lc+ lang.[src-lang]-[dst-lang]+numrefs.1+ smooth.exp+tok.13a+version.1.3.6</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers for the helpful feedback, Rico Sennrich for valuable comments on the first version of this paper, and Yandex Machine Translation team for discussions and inspiration.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a spelling error model from search query logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farooq</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="955" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1399</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3632" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meaningless yet meaningful: Morphology grounded subword-level NMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamali</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-1207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Subword/Character LEvel Models</title>
		<meeting>the Second Workshop on Subword/Character LEvel Models<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2019 conference on machine translation (WMT19)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="61" />
		</imprint>
	</monogr>
	<note>Task Papers, Day 1)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Findings of the 2018 conference on machine translation (WMT18)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6401</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="272" to="303" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An improved error model for noisy channel spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.3115/1075218.1075255</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Revisiting character-based neural machine translation with capacity and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1461</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4295" to="4305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variablelength word encodings for neural translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chitnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1249</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2088" to="2093" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Morfessor in the morpho challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PAS-CAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes</title>
		<meeting>the PAS-CAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A call for prudent choice of subword merge operations in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuoyang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit XVII</title>
		<meeting>Machine Translation Summit XVII<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
	<note>Research Track. European Association for Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2090</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="573" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Frage: Frequency-agnostic word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1334" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2072</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to segment inputs for nmt favors character-level processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Spoken Language Translation</title>
		<meeting>the 15th International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Orthographic syllable as basic unit for SMT between related languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1196</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1912" to="1917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analyzing uncertainty in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3956" to="3965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mimicking word embeddings using subword RNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="102" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting lowresource neural machine translation: A case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="211" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS, Los Angeles</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finding better subword segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data noising as smoothing in neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lvy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
