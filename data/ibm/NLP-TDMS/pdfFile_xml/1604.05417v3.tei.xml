<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Triplet Probabilistic Embedding for Face Verification and Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
							<email>swamiviv@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Alavi</surname></persName>
							<email>azadeh@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
							<email>carlos@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Triplet Probabilistic Embedding for Face Verification and Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite significant progress made over the past twenty five years, unconstrained face verification remains a challenging problem. This paper proposes an approach that couples a deep CNN-based approach with a low-dimensional discriminative embedding step, learned using triplet probability constraints to address the unconstrained face verification problem. Aside from yielding performance improvements, this embedding provides significant advantages in terms of memory and for post-processing operations like subject specific clustering. Experiments on the challenging IJB-A dataset show that the proposed algorithm performs close to the state of the art methods in verification and identification metrics, while requiring much less training data and training/test time. The superior performance of the proposed method on the CFP dataset shows that the representation learned by our deep CNN is robust to large pose variation. Furthermore, we demonstrate the robustness of deep features to challenges including age, pose, blur and clutter by performing simple clustering experiments on both IJB-A and LFW datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, with the advent of curated face datasets like Labeled faces in the Wild (LFW) <ref type="bibr" target="#b0">[1]</ref> and advances in learning algorithms like Deep neural nets, there is more hope that the unconstrained face verification problem can be solved. A face verification algorithm compares two given templates that are typically not seen during training. Research in face verification has progressed well over the past few years, resulting in the saturation of performance on the LFW dataset, yet the problem of unconstrained face verification remains a challenge. This is evident by the performance of traditional algorithms on the publicly available IJB-A dataset ( <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>) that was released recently. Moreover, despite the superb performance of CNN-based approaches compared to traditional methods, a drawback of such methods is the long training time needed. In this work, we present a Deep CNN (DCNN) architecture that ensures faster training, and inves-tigate how much the performance can be improved if we are provided domain specific data. Specifically, our contributions are as follows:</p><p>• We propose a deep network architecture and a training scheme that ensures faster training time. • We formulate a triplet probability embedding learning method to improve the performance of deep features for face verification and subject clustering. During training, we use a publicly available face dataset to train our deep architecture. Each image is pre-processed and aligned to a canonical view before passing it to the deep network whose features are used to represent the image. In the case of IJB-A dataset, the data is divided into 10 splits, each split containing a training set and a test set. Hence, to further improve performance, we learn the proposed triplet probability embedding using the training set provided with each split over the features extracted from our DCNN model. During the deployment phase, given a face template, we extract the deep features using the raw CNN model after implementing automatic pre-processing steps such as face detection and fiducial extraction. The deep features are projected onto a low-dimensional space using the embedding matrix learned during training (note that the projection involves only matrix multiplication). We use the 128-dimensional feature as the final representation of the given face template. This paper is organized as follows: Section 2 places our work among the recently proposed approaches for face verification. Section 3 details the network architecture and the training scheme. The triplet probabilistic embedding learning method is described in Section 4 followed by results on IJB-A and CFP datasets and a brief discussion in Section 5. In Section 6, we demonstrate the ability of the proposed method to cluster a media collection from LFW and IJB-A datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the past few years, there have been numerous works in using deep features for tasks related to face verification. The DeepFace <ref type="bibr" target="#b3">[4]</ref> approach uses a carefully crafted 3D alignment procedure to preprocess face images and feeds arXiv:1604.05417v3 [cs.CV] 18 Jan 2017 them to a deep network that is trained using a large training set. More recently, Facenet <ref type="bibr" target="#b4">[5]</ref> uses a large private dataset to train several deep network models using a triplet distance loss function. The training time for this network is of the order of few weeks. Since the release of the IJB-A dataset <ref type="bibr" target="#b1">[2]</ref>, there have been several works that have published verification results for this dataset. Previous approaches presented in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref> train deep networks using the CASIA-WebFace dataset <ref type="bibr" target="#b7">[8]</ref> and the VGG-Face dataset respectively, requiring substantial training time. This paper proposes a network architecture and a training scheme that needs shorter training time and a small query time.</p><p>The idea of learning a compact and discriminative representation has been around for decades. Weinberger et al. <ref type="bibr" target="#b8">[9]</ref> used a Semi Definite Programming (SDP)-based formulation to learn a metric satisfying pairwise and triplet distance constraints in a large margin framework. More recently, this idea has been successfully applied to face verification by integrating the loss function within the deep network architecture ( <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>). Joint Bayesian metric learning is also another popular metric used for face verification ( <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>). These methods either require a large dataset for convergence or learn a metric directly and therefore are not amenable to subsequent operations like discriminative clustering or hashing. Classic methods like t-SNE <ref type="bibr" target="#b11">[12]</ref>, t-STE <ref type="bibr" target="#b12">[13]</ref> and Crowd Kernel Learning (CKL) <ref type="bibr" target="#b13">[14]</ref> perform extremely well when used to visualize or cluster a given data collection. They either operate on the data matrix directly or the distance matrix generated from data by generating a large set of pairwise or triplet constraints. While these methods perform very well on a given set of data points, they do not generalize to out-of-sample data. In the current work, we aim to generalize such formulations, to a more traditional classification setting, where domain specific training and testing data is provided. We formulate an optimization problem based on triplet probabilities that performs dimensionality reduction aside from improving the discriminative ability of the test data. The embedding scheme described in this work is a more general framework that can be applied to any setting where labeled training data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><p>This section details the architecture and training algorithm for the deep network used in our work. Our architecture consists of 7 convolutional layers with varying kernel sizes. The initial layers have a larger size rapidly subsampling the image and reducing the parameters while subsequent layers consist of small filter sizes, which has proved to be very useful in face recognition tasks ( <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>). Furthermore, we use the Parametric Rectifier Linear units (PReLUs) instead of ReLUs, since they allow a negative value for the output based on a learned threshold and have been shown to improve the convergence rate <ref type="bibr" target="#b14">[15]</ref>.  <ref type="bibr" target="#b17">[18]</ref>) have empirically shown that this transfer of knowledge across different networks, albeit for a different objective, improves performance and more significantly reduces the need to train over a large number of iterations. The compared methods either learn their deep models from scratch ( <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>) or finetune only the last layer of fully pretrained models. The former results in large training time and the latter does not generalize well to the task at hand (face verification) and hence resulting in sub optimal performance. In the current work, even though we use a pretrained model (AlexNet) to initialize the proposed deep network, we do so only for the first three convolutional layers, since they retain more generic information ( <ref type="bibr" target="#b16">[17]</ref>). Subsequent layers learn representations which are more specific to the task at hand. Thus, to learn more task specific information, we add 4 convolutional layers each consisting of 512 kernels of size 3 × 3. The layers conv4-conv7 do not downsample the input thereby learning more complex higher dimensional representations. This hybrid architecture proves to be extremely effective as our raw CNN representation outperforms some very deep CNN models on the IJB-A dataset ( <ref type="table" target="#tab_2">Table 2</ref> in Results). In addition, we achieve that performance by training the proposed deep network using the relatively smaller CASIA-WebFace dataset. The architecture of our network is shown in <ref type="table">Table 1</ref>. Layers conv4-conv7 and the fully connected layers fc6-fc8 are initialized from scratch using random Gaussian distributions. PReLU activation functions are added between each layer.</p><p>Since the network is used as a feature extractor, the last layer fc8 is removed during deployment, thus reducing the number of parameters to 29M. The inputs to the network are 227x227x3 RGB images. When the network is deployed, the features are extracted from the fc7 layer resulting in a dimensionality of 512. The network is trained using the Softmax loss function for multiclass classification using the Caffe deep learning platform <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning a Discriminative Embedding</head><p>In this section, we describe our algorithm for learning a low-dimensional embedding such that the resulting projections are more discriminative. Aside from an improved performance, this embedding provides significant advantages in terms of memory and enables post-processing operations like visualization and clustering.</p><p>Consider a triplet t :</p><formula xml:id="formula_0">= (v i , v j , v k ), where v i (anchor)</formula><p>and v j (positive) are from the same class, but v k (negative) belongs to a different class. Consider a function</p><formula xml:id="formula_1">S W : R N × R N → R that is parameterized by the ma- trix W ∈ R n×N , that measures the similarity between two vectors v i , v j ∈ R N .</formula><p>Ideally, for all triplets t that exist in the training set, we would like the following constraint to be satisfied:</p><formula xml:id="formula_2">S W (v i , v j ) &gt; S W (v i , v k )<label>(1)</label></formula><p>Thus, the probability of a given triplet t satisfying (1) can be written as:</p><formula xml:id="formula_3">p ijk = e S W (vi,vj ) e S W (vi,vj ) + e S W (vi,v k )<label>(2)</label></formula><p>The specific form of the similarity function is given as:</p><formula xml:id="formula_4">S W (v i , v j ) = (Wv i ) T · (Wv j )</formula><p>. In our case, v i and v j are deep features normalized to unit length. To learn the embedding W from a given set of triplets T, we solve the following optimization:</p><formula xml:id="formula_5">argmin W (vi,vj ,v k )∈T − log(p ijk )<label>(3)</label></formula><p>(3) can be interpreted as maximizing the likelihood <ref type="bibr" target="#b0">(1)</ref> or minimizing the negative log-likelihood (NLL) over the triplet set T. In practice, the above problem is solved in a Large-Margin framework using Stochastic Gradient Descent (SGD) and the triplets are sampled online. The gradient update for W is given as:</p><formula xml:id="formula_6">W τ +1 = W τ − η * W τ * (1 − p ijk ) * (v i (v j − v k ) T +(v j − v k )v T i ) (4) where W τ is the estimate at iteration τ , W τ +1 is the updated estimate, (v i , v j , v k )</formula><p>is the triplet sampled at the current iteration and η is the learning rate.</p><p>By choosing the dimension of W as n×N with n &lt; N , we achieve dimensionality reduction in addition to improved performance. For our work, we fix n = 128 based on cross validation and N = 512 is the dimensionality of our deep <ref type="figure">Figure 1</ref>: Gradient update scenarios for the TDE method (5). The notation is explained in the text features. W is initialized with the first n principal components of the training data. At each iteration, a random anchor and a random positive data point are chosen. To choose the negative, we perform hard negative mining, ie. we choose the data point that has the least likelihood <ref type="formula" target="#formula_3">(2)</ref> among the randomly chosen 2000 negative instances at each iteration.</p><p>Since we compute the embedding matrix W by optimizing over triplet probabilities, we call this method Triplet Probability Embedding (TPE). The technique closest to the one presented in this section, which is used in recent works ( <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>) computes the embedding W based on satisfying a hinge loss constraint:</p><formula xml:id="formula_7">argmin W (vi,vj ,v k )∈T max{0, α + (v i − v j ) T W T W(v i − v j )− (v i − v k ) T W T W(v i − v k )} (5)</formula><p>α acts a margin parameter for the loss function. To be consistent with the terminology used in this paper, we call it Triplet Distance Embedding (TDE). To appreciate the difference between the two approaches, <ref type="figure">Figure 1</ref> shows the case where the gradient update for the TDE method (5) occurs. If the value of α is not appropriately chosen, a triplet is considered good even if the positive and negative are very close to one another. But under the proposed formulation, both cases referred to in <ref type="figure">Figure 1</ref> will update the gradient but their contribution to the gradient will be modulated by the probability with which they violate the constraint in (1). This modulation factor is specified by the (1 − p ijk ) term in the gradient update for TPE in <ref type="bibr" target="#b3">(4)</ref> implying that if the likelihood of a sampled triplet satisfying (1) is high, then the gradient update is given a lower weight and vice-versa. Thus, in our method, the margin parameter (α) is automatically set based on the likelihood. To compare the relative performances of the raw features before projection, with TDE and with TPE (proposed method), we plot the traditional ROC curve (TAR (vs) FAR) for split 1 of the IJB-A verify protocol for the three methods in <ref type="figure" target="#fig_0">Figure 2</ref>. The Equal Error Rate (EER) metric is specified for each method. The performance improvement due to TPE is significant, especially at regions of FAR= {10 −4 , 10 −3 }. We observed a similar behaviour for all the ten splits of the IJB-A dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental setup and Results</head><p>In this section we evaluate the proposed method on two challenging datasets: 1. IARPA Janus Benchmark-A (IJB-A) <ref type="bibr" target="#b1">[2]</ref>: This dataset contains 500 subjects with a total of 25,813 images (5,399 still images and 20,414 video frames sampled at a rate of 1 in 60). The faces in the IJB-A dataset contain extreme poses and illuminations, more challenging than LFW <ref type="bibr" target="#b0">[1]</ref>. Some sample images from the IJB-A dataset are shown in <ref type="figure">Figure 3</ref>. An additional challenge of the IJB-A verification protocol is that the template comparisons include image to image, image to set and set to set comparisons. In this work, for a given test template of the IJB-A data we perform two kinds of pooling to produce its final representation:</p><p>• Average pooling (CNN ave ): The deep features of the images and/or frames present in the template are combined by taking a componentwise average to produce one feature vector. Thus each feature equally contributes to the final representation. • Media pooling (CNN media ): The deep features are combined keeping in mind the media source they come from. The metadata provided with IJB-A gives us the media id for each item of the template. Thus to get the final feature vector, we first take an intra-media average and then combine these by taking the intermedia average. Thus each feature's contribution to the final representation is weighted based on its source. 2. Celebrities in Frontal-Profile (CFP) <ref type="bibr" target="#b20">[21]</ref>: This dataset contains 7000 images of 500 subjects. The dataset is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pre-processing</head><p>In the training phase, given an input image, we use the Hy-perFace method <ref type="bibr" target="#b21">[22]</ref> for face detection and fiducial point extraction. The HyperFace detector automatically extracts many faces from a given image. For the IJB-A dataset, since most images contain more than one face, we use the bounding boxes provided along with the dataset to select the person of interest from the list of automatic detections. We select the detection that has the maximum area overlap with the manually provided bounding box. In the IJB-A dataset, there are few images for which the HyperFace detector cannot find the relevant face. For the missed cases, we crop the face using the bounding box information provided with the dataset and pass it to HyperFace to extract the fiducials. We use six fiducial points (eyes and mouth corners) to align the detected image to a canonical view using the similarity transform. For the CFP dataset, since the six keypoints cannot be computed for profile faces we only use three keypoints on one side of the face for aligning them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Parameters and training times</head><p>The training of the proposed deep architecture is done using SGD with momentum, which is set to 0.9 and the learning rate is set to 1e-3 and decreased uniformly by a factor of 10 every 50K iterations. The weight decay is set to 5e-4 for all layers. The training batch size is set to 256. The training time for our deep network is 24 hours on a single NVIDIA TitanX GPU. For the IJB-A dataset, we use the training data provided with each split to obtain the triplet embedding which takes 3 mins per split. This is the only additional splitwise processing that is done by the proposed approach. During deployment, the average enrollment time per image after pre-processing, including alignment and feature extraction is 8ms.   <ref type="table">Table 3</ref>: Results on the CFP dataset <ref type="bibr" target="#b20">[21]</ref>. The numbers are averaged over ten test splits and the numbers in brackets indicate standard deviations of those runs. The best results are given in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation Pipeline</head><p>Given an image, we pre-process it as described in Section 5.1. The deep features are computed as an average of the image and its flip. Given two deep features to compare, we compute their cosine similarity score. More specifically, for the IJB-A dataset, given a template containing multiple faces, we flatten the template features by average pooling or media pooling to obtain a vector representation. For each split, we learn the TPE projection using the provided training data. Given two templates for comparison, we compute the cosine similarity score using the projected 128-dimensional representations. matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation Metrics</head><p>We report two types of results for the IJB- More details on the evaluation metrics for the IJB-A protocol can be found in <ref type="bibr" target="#b1">[2]</ref>. For the CFP dataset, following the protocol set in <ref type="bibr" target="#b20">[21]</ref>, we report the Area under the curve (AUC) and Equal Er-ror Rate (EER) values as averages across splits, in addition to the classification accuracy. To obtain the accuracy for each split, we threshold our CNN similarity scores where the threshold is set to the value that provides the highest classification accuracy over the training data for each split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Discussion</head><p>Performance on IJB-A <ref type="table" target="#tab_2">Table 2</ref> presents the results for the proposed methods compared to existing results for the IJB-A Verification and Identification protocol. The compared methods are described below:</p><p>• Government-of-the-Shelf (GOTS) <ref type="bibr" target="#b1">[2]</ref> is the baseline performance provided along with the IJB-A dataset.  <ref type="bibr" target="#b26">[27]</ref> to tune the performance of their raw features specifically to the IJB-A dataset.</p><p>Compared to these methods, the proposed method trains a single CNN model on the CASIA-WebFace dataset which consists of about 500K images and requires much shorter training time and has a very fast query time (0.08s after face detection per image pair). As shown in <ref type="table" target="#tab_2">Table 2</ref>, our raw CNN features after media pooling perform better than most compared methods across both the verification and identification protocols of the IJB-A dataset, with the exception of the template adaptation method by Crosswhite et al. <ref type="bibr" target="#b25">[26]</ref> which is discussed below. The TPE method provides significant improvement for both identification and verification tasks as shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>The method by Crosswhite et al. <ref type="bibr" target="#b25">[26]</ref> uses the VGG-Face network <ref type="bibr" target="#b6">[7]</ref> descriptors (4096-d) as the raw features. They use the concept of template adaptation <ref type="bibr" target="#b26">[27]</ref> to improve their performance as follows: when pooling multiple faces of a given template, they train a linear SVM with the features of this template as positive and a fixed set of negatives extracted from the training data of the IJB-A splits. Let's denote the pooled template feature and classifier pair as (t, w). Then, at query time when comparing two templates (t 1 , w 1 ) and (t 2 , w 2 ), the similarity score is computed as:</p><formula xml:id="formula_8">1 2 (t 1 · w 2 + t 2 · w 1 )</formula><p>. Even when using a carefully engineered fast linear classifier training algorithm, this procedure increases the run time of the pooling procedure. The query time per template comparison is also higher due to the high dimensionality of the input features. In contrast, the proposed approach requires a matrix multiplication and a vector dot product per comparison. By using a simple neural network architecture, a relatively smaller training dataset and a fast embedding method we have realized a faster and more efficient end-to-end system. To improve our performance further, we are currently incorporating the use of video data into our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance on CFP</head><p>On the CFP dataset, we achieve a new state-of-art on both Frontal-Frontal and Frontal-Profile comparisons, the latter by a large margin. More specifically, for the Frontal-Profile case, we manage to reduce the error rate by 40.8%. It should be noted that for a fair comparison we have used our raw CNN features without performing TPE. This shows that the raw CNN features we learn are effective even at extreme pose variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Clustering Faces</head><p>This section illustrates how the proposed TPE method can be used to cluster a given data collection. We perform two clustering experiments:</p><p>1. We perform clustering on the entire LFW <ref type="bibr" target="#b0">[1]</ref> dataset that consists of 13233 images of 5749 subjects. It should be noted that about 4169 subjects have only one image. 2. We use the IJB-A dataset and cluster the templates corresponding to the query set for each split in the IJB-A verify protocol. For evaluating the clustering results, we use the metrics defined in <ref type="bibr" target="#b27">[28]</ref>. These are summarized below: in the same cluster, over the total number of same-class pairs. Using these metrics, the F 1 -score is computed as:</p><formula xml:id="formula_9">F 1 = 2 * P pair * R pair R pair + P pair<label>(6)</label></formula><p>The simplest way we found to demonstrate the effectiveness of our deep features and the proposed TPE method, is to use the standard MATLAB implementation of the agglomerative clustering algorithm with the average linkage metric. We use the cosine similarity as our basic clustering metric. The simple clustering algorithm that we have used here has computational complexity of O(N 2 ). In its current form, this does not scale to large datasets with millions of images. We are currently working on a more efficient and scalable (yet approximate) version of this algorithm.</p><p>Clustering LFW:-The images in the LFW dataset are pre-processed as described in Section 5.1. For each image and its flip, the deep features are extracted using the proposed architecture, averaged and normalized to unit L 2 norm. We run the clustering algorithm over the entire data in a single shot. The clustering algorithm takes as input a cut-off parameter which acts as a distance threshold (below which any two clusters will not be merged). In our experiments, we vary this cut-off parameter over a small range and evaluate the resulting clustering using the F 1 -score. We pick the result that yields the best F 1 -score. <ref type="table" target="#tab_5">Table 4</ref> shows the result of our approach and compares it to a recently released clustering approach based on approximate Rankorder clustering <ref type="bibr" target="#b27">[28]</ref>. It should be noted that, in the case of <ref type="bibr" target="#b27">[28]</ref>, the clustering result is chosen by varying the number of clusters and picking the one with the best F 1 -score. In our approach, we vary the cut-off threshold which is the property of deep features and hence is a more intuitive parameter to tune. We see from <ref type="table" target="#tab_5">Table 4</ref> that aside from better performance, our total cluster estimate is closer to the ground truth value of 5749 than <ref type="bibr" target="#b27">[28]</ref>.  <ref type="figure">Figure 5</ref>: Sample clusters output from the Clustering approach discussed in Section 6 for the data from the split 1 of the IJB-A dataset. Top row (a,b) shows robustness to pose and blur; Bottom row (c,d) contains clusters that are robust to age Method F 1 -score Clusters <ref type="bibr" target="#b27">[28]</ref> 0.87 6508 CNN (Ours) 0.955 5351  <ref type="table">Table 5</ref>: Clustering metrics over the IJB-A 1:1 protocol. The standard deviation is indicated in brackets. The ground truth subjects per each split is 167.</p><p>Clustering IJB-A:-The IJB-A dataset is processed as described in Section 5. In this section, we aim to cluster the query templates provided with each split for the verify protocol. We report the results of two experiments: with the raw CNN features (CNN media in <ref type="table" target="#tab_2">Table 2</ref>) and with the projected CNN features, where the projection matrix is learned through the proposed TPE method (CNN media +TPE in <ref type="table" target="#tab_2">Table 2</ref>). The cut-off threshold required for our clustering algorithm is learned automatically based on the training data, i.e. we choose the threshold that gives the maximum F 1score over the training data. The scores reported in <ref type="table">Table 5</ref> are average values over ten splits. As expected, the TPE method improves the clustering performance of raw features. The subject estimate is the number of clusters produced as a direct result of our clustering algorithm. The pruned estimate is obtained by ignoring clusters that have fewer than 3 images. For a more complete evaluation of our performance over varying threshold values, we plot the Precision-Recall (PR) curve for the IJB-A clustering experiment in <ref type="figure" target="#fig_4">Figure 6</ref>. As can be observed, the PR curve for clustering the IJB-A data using embedded features exhibits a better performance at all operating points. This is a more transparent evaluation than reporting only the F 1 -score since the latter effectively fixes the operating point but the PR curve reveals the performance at all operating points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>In this paper, we proposed a deep CNN-based approach coupled with a low-dimensional discriminative embedding learned using triplet probability constraints in a large margin fashion. The proposed pipeline enables a faster training time and improves face verification performance especially at low FMRs. We demonstrated the effectiveness of the proposed method on two challenging datasets: IJB-A and CFP and achieved performance close to the state of the art while using a deep model which is more compact and trained using a moderately sized dataset. We demonstrated the robustness of our features using a simple clustering algorithm on the LFW and IJB-A datasets. For future work, we plan to use videos directly during training and also embed our TPE approach into training the deep network. We intend to scale our clustering algorithm to handle large scale scenarios such as large impostor sets of the order of millions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Performance improvement on IJB-A split 1: FAR (vs) TAR plot. EER values are specified in brackets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Images from the IJB-A dataset (a) Frontal-Frontal (b) Frontal-Profile Sample comparison pairs from the CFP dataset used for evaluating how face verification approaches handle pose variation. Hence, it consists of 5000 images in frontal view and 2000 images in extreme profile. The data is organized into 10 splits, each containing equal number of frontal-frontal and frontal-profile comparisons. Sample comparison pairs of the CFP dataset are shown in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Pairwise Precision (P pair ): The fraction of pairs of samples within a cluster among all possible pairs which are of the same class, over the total number of same cluster pairs. • Pairwise Recall (R pair ): The fraction of pairs of samples within a class among all possible pairs which are placed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Precision-Recall curve plotted over cut-off threshold varied from 0 to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Identification and Verification results on the IJB-A dataset. For identification, the scores reported are TPIR values at the indicated points. The results are averages over 10 splits and the standard deviation is given in the brackets for methods which have reported them. − implies that the result is not reported for that method. The best results are given in bold.</figDesc><table><row><cell></cell><cell></cell><cell>Frontal-Frontal</cell><cell></cell><cell></cell><cell>Frontal-Profile</cell><cell></cell></row><row><cell>Algorithm</cell><cell>Accuracy</cell><cell>EER</cell><cell>AUC</cell><cell>Accuracy</cell><cell>EER</cell><cell>AUC</cell></row><row><cell cols="7">Sengupta et al. [21] 96.40 (0.69) 3.48 (0.67) 99.43 (0.31) 84.91 (1.82) 14.97 (1.98) 93.00 (1.55)</cell></row><row><cell>Human Accuracy</cell><cell cols="5">96.24 (0.67) 5.34 (1.79) 98.19 (1.13) 94.57 (1.10) 5.02 (1.07)</cell><cell>98.92 (0.46)</cell></row><row><cell>CNN (Ours)</cell><cell cols="5">96.93 (0.61) 2.51 (0.81) 99.68 (0.16) 89.17 (2.35) 8.85 (0.99)</cell><cell>97.00 (0.53)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>A dataset: Verification and Identification. For the verification protocol, we report the False Non-Match Rate (FNMR) values at several False Match Rates (FMR). For the identification results, we report open set and closed set metrics. For the open set metrics, the True Positive Identification Rate quantifies the fraction of subjects that are classified correctly among the ones that exist in probe but not in gallery. For the closed set metrics, we report the CMC numbers at different values of False Positive Identification Rates (FPIRs) and Ranks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>F 1 -score for comparison of the two clustering schemes on the LFW dataset. The ground truth cluster number is 5749.</figDesc><table><row><cell>Method</cell><cell>F 1 -score</cell><cell>Clusters After Pruning</cell></row><row><cell>CNN media</cell><cell>0.79 (0.02)</cell><cell>293 (22) 173</cell></row><row><cell cols="3">CNN media +TPE 0.843 (0.03) 258 (17) 167</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgement</head><p>This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA),via IARPA R&amp;D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">algorithms</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unconstrained face verification using fisher vectors computed from frontalized faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BTAS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Face search at scale: 80 million gallery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07242</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fisher Vector Faces in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unconstrained face verification using deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01722</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic triplet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>2012 IEEE International Workshop on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adaptively learning the crowd kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tamuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1105.1033</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05474</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Frontal to profile face verification in the wild,&quot; in WACV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01249</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv ppreprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Do we really need to collect millions of faces for effective face recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07057</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward large-population face identification in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1874" to="1884" />
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Going deeper with convolutions,&quot; in CVPR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Template adaptation for face verification and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03958</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The one-shot similarity kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Clustering millions of faces by identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00989</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
