<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Strong-Weak Distribution Alignment for Adaptive Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country>3 RIKEN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country>3 RIKEN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Strong-Weak Distribution Alignment for Adaptive Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an approach for unsupervised adaptation of object detectors from label-rich to label-poor domains which can significantly reduce annotation costs associated with detection. Recently, approaches that align distributions of source and target images using an adversarial loss have been proven effective for adapting object classifiers. However, for object detection, fully matching the entire distributions of source and target images to each other at the global image level may fail, as domains could have distinct scene layouts and different combinations of objects. On the other hand, strong matching of local features such as texture and color makes sense, as it does not change category level semantics. This motivates us to propose a novel method for detector adaptation based on strong local alignment and weak global alignment. Our key contribution is the weak alignment model, which focuses the adversarial alignment loss on images that are globally similar and puts less emphasis on aligning images that are globally dissimilar. Additionally, we design the strong domain alignment model to only look at local receptive fields of the feature map. We empirically verify the effectiveness of our method on four datasets comprising both large and small domain shifts. Our code is available at https://github.com/ VisionLearningGroup/DA_Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks have greatly improved object recognition accuracy <ref type="bibr" target="#b17">[18]</ref>, but remain reliant on large quantities of labeled training data. For object detection, annotation is particularly burdensome: each instance of an object category in every image must be annotated with a precise bounding box. Transferring pre-trained models from label-rich domains is an attractive solution, but dataset bias often reduces their generalization to novel data <ref type="bibr" target="#b30">[31]</ref>.</p><p>Various methods for unsupervised domain adaptation (UDA) have been proposed to tackle the dataset bias prob- <ref type="bibr">Figure 1</ref>. Upper: Our Strong-Weak model learns domain-invariant features that are strongly aligned at the local patch level and weakly (partially) aligned at the global scene level. Lower: Global features obtained by our proposed weak alignment method on Pascal to Clipart. The target features are partially aligned with source, which improves detection performance, as shown in our experiments. lem <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b24">25]</ref>, most of which are based on domaininvariant alignment of the feature <ref type="bibr" target="#b31">[32]</ref> or image <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14]</ref> distributions. Recent methods align the source and target distributions of examples using adversarial learning and are motivated by theoretical results that bound the generalization error partially by the size of the discrepancy between domains <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. The conventional wisdom is therefore that discrepancy must be reduced at all costs, which can only be done if one fully aligns the distributions. In this paper, we argue that such strong domain alignment is only reasonable in closed problems, such as object classification settings where the source and target examples share the same categories and prior label distributions. In settings such as open-set classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref> or partial domain adaptation <ref type="bibr" target="#b41">[42]</ref>, strong alignment can be infeasible and could actually hurt performance.</p><p>In object detection this is particularly evident, as aligning global (image-level) features means that not only the object categories, but also backgrounds and scene layouts must be similar across domains. Yet this is precisely what the current state-of-the-art UDA method for detection, Adaptive Faster RCNN <ref type="bibr" target="#b4">[5]</ref>, attempts to do. It trains Faster RCNN with a domain classifier trained to distinguish source and target examples, while the feature extractor learns to deceive the domain classifier. Feature alignment is done both at the global image scale and at the instance (object) scale.</p><p>While the global matching might work well for small domain shifts that only affect the appearance/texture of objects (e.g. weather related shifts), it is likely to hurt performance for larger shifts that affect the layout of the scene, the number of objects and/or their co-occurrence. For example, source images may contain single objects, while target images may contain multiple smaller objects. Forcing invariance to such global features can hurt performance.On the other hand, strong alignment of local features would match the texture or color of the domains and should improve performance in most cases, because it will not change the category information but is likely to reduce the domain gap. In this paper, by "local" scale we do not mean the instance (object) scale but rather texture or color features with small receptive fields.</p><p>Motivated by these observations, we propose an unsupervised adaptation method for object detection that combines weak global alignment with strong local alignment, called the Strong-Weak Domain Alignment model (top of <ref type="figure">Fig. 1</ref>). We propose to apply weak alignment to the global features, partially aligning them to reduce the domain gap without hurting the performance of the model. We show an example of weak global alignment in the bottom of <ref type="figure">Fig. 1</ref>, where only the target images which contain one object are aligned with the source. Our key contribution is the weak global alignment model, which focuses the adversarial alignment loss toward images that are globally similar, and away from images that are globally dissimilar. Additionally, we achieve strong local alignment by constructing a domain classifier designed to look only at local features and to strictly align them with the other domain. We verify the effectiveness of our method in adaptation between both similar and dissimilar domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection. The development of deep convolutional neural networks has boosted the performance of object detection. Having a strong backbone feature extractor is key for accurate detection models. Current detection networks can be categorized into two types: two-stage and one-stage. Faster-RCNN (FRCNN) <ref type="bibr" target="#b29">[30]</ref> is a representative two-stage detector that generates coarse object proposals using region proposal networks (RPN) as the first stage, and feeds the proposals and cropped features into a classification module as the second stage. In this paper, we use the FRCNN as a base detector, however, our method should be applicable to other two-stage detectors and one-stage detectors such as YOLO <ref type="bibr" target="#b28">[29]</ref> or SSD <ref type="bibr" target="#b21">[22]</ref>. Detector back-bone networks are usually pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref> and need to be fine-tuned again with a large number of annotated object bounding boxes. Various datasets have been publicized for this purpose <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>. To deal with the deficit in such large annotated datasets, weakly supervised and semi-supervised object detection has been proposed in the literature <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b2">3]</ref>. Although cross-domain object detection and especially unsupervised cross-domain object detection can also help with this problem, as far as we know, there is only one work that has tackled the task of unsupervised domain transfer of deep object detectors <ref type="bibr" target="#b4">[5]</ref>. In this work, the feature alignment at the instance (object) scale was done for features cropped by region proposals. To effectively conduct feature alignment, region proposals have to precisely localize objects of interest. However, this is difficult to do for the target domain as we are not given ground truth proposals. The feature alignment may therefore hurt the performance of the model as we show in our experiments, which is why we do not conduct instance scale alignment in our work. Domain Adaptation. The problem of bridging a gap between domains has been investigated for various visual applications such as image classification and semantic segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b35">36]</ref>. To solve the problem, a large number of methods utilize feature distribution matching between training and testing domains. The basic idea is to measure some type of distance between different domains' feature distributions and train a feature extractor to minimize that distance. Various ways of measuring the distance have been proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref>. Motivated by a theoretical result <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>, various approaches utilize the domain classifier <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40]</ref> to measure domain discrepancy. They train a domain classifier and feature extractor in an adversarial way, as done for training GANs <ref type="bibr" target="#b10">[11]</ref>. Such methods are designed to strictly align the feature distribution of the target with that of the source. In addition, Long et al. designed a loss function of the domain classifier to fully match features between domains <ref type="bibr" target="#b23">[24]</ref> for image classification.</p><p>In this paper, we instead propose a weak feature alignment model for global features, and use strong alignment only at the local level to strictly align the style of images across domains. Some research on GANs and domain adaptive semantic segmentation has shown that regularizing the domain classifier with task-specific classification loss can stabilize the adversarial training <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>. Motivated by this approach, we further propose a method to regularize the domain classifier by the detection loss on source examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The architecture of our proposed Strong-Weak DA model is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. We extract global features just before the RPN and local features from lower layers, and perform weak global alignment in the high-level feature space and strong local alignment in the low-level feature space. We further propose to stabilize the training of domain classifiers with the detection loss (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Weak Global Feature Alignment</head><p>We utilize a domain classifier to align the target features with the source for the global-level feature alignment. Easyto-classify target examples are far from source examples in the feature space while hard-to-classify target examples are near the source as shown in the left of <ref type="figure" target="#fig_0">Fig. 2</ref>. Therefore, focusing on hard-to-classify examples should achieve a weak alignment between domains. We propose to train a domain classifier to ignore easy-to-classify examples while focusing on hard-to-classify examples with respect to the classification of the domain.</p><p>We have access to a labeled source image x s and bounding boxes for each image y s drawn from a set of annotated source images {X s , Y s }, as well as an unlabeled target image x t drawn from unlabeled target images X t . The global feature vector is extracted by F . The domain classifier, D g , is trained to predict the domain of input global features. Our learning formulation optimizes F so that the features are discriminative for the primary task of object detection, but are uninformative for the task of domain classification. The domain-label d is 1 for the source and 0 for the target. The network R takes features from F and outputs bounding boxes with a class label. R includes the Region Proposal Network (RPN) and other modules in Faster RCNN. The objective of the detection loss is summarized as:</p><formula xml:id="formula_0">L cls (F, R) = − 1 n s ns i=1 L det (R(F (x i s )), y i s )<label>(1)</label></formula><p>where we assume that L det contains all losses for detection such as a classification loss and a bounding-box regression loss. n s denotes the number of source examples. In existing methods <ref type="bibr" target="#b4">[5]</ref>, the objective for domain classification is the cross-entropy loss. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the loss of the easy-to-classify examples, which have high probability, is not negligible in this cross-entropy loss. This indicates that D g and F account for all examples in the training procedure. Therefore, F tries to match the entire feature distribution, which is not desirable in domain adaptive object detection. Instead, we want the domain classifier to ignore easy-toclassify examples while focusing on hard-to-classify examples. The problem with cross-entropy (CE) loss (− log p) is that it puts non-negligible values of easy-to-classify examples where p ∈ [0, 1] is the model's estimated probability for the class with label d = 1. We propose to add a modulating factor f (p t ) to the cross-entropy loss, resulting in</p><formula xml:id="formula_1">− f (p t ) log(p t )<label>(2)</label></formula><p>where we define p t :</p><formula xml:id="formula_2">p t = p if d = 1 1 − p otherwise.<label>(3)</label></formula><p>We choose a function that decreases as p t increases. One example of such a loss function is Focal Loss (FL) <ref type="bibr" target="#b18">[19]</ref> FL</p><formula xml:id="formula_3">(p t ) = −f (p t ) log(p t ), f (p t ) = (1 − p t ) γ<label>(4)</label></formula><p>where γ controls the weight on hard-to-classify examples. FL is designed to put more weight on hard-to-classify examples than on easy ones during training, as shown in the right of <ref type="figure" target="#fig_0">Fig. 2</ref>. The feature extractor tries to deceive the domain classifier, that is, tries to increase the loss. However, the feature extractor cannot align the well-classified target examples with the source because the scale of gradients of such examples is very small. The same can be said about aligning source examples to the target. f (p t ) can take other formulations if it satisfies the requirement described above. In experiments, we will show the result of another loss function that satisfies the condition. We denote the loss of the weak global-level domain classifier as L global as follows, </p><formula xml:id="formula_4">L globals = − 1 n s ns i=1 (1 − D g (F (x i s )) γ log(D g (F (x i s ))) (5) L globalt = − 1 n t nt i=1 D g (F (x i t )) γ log(1 − D g (F (x i t )))<label>(6)</label></formula><formula xml:id="formula_5">L global (F, D g ) = 1 2 (L globals + L globalt )<label>(7)</label></formula><p>where n t denotes the number of target examples.</p><p>The gradients of this loss should change the parameters of low-level layers, which should also align low-level features, but the effect may not be strong enough. We thus propose to directly perform the alignment in local-level features in the next sub-section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Strong Local Feature Alignment</head><p>The architecture of the local domain classifier, D l , is designed to focus on the local features rather than global features. D l is a fully-convolutional network with kernel-size equal to one. The feature extractor F is decomposed as F 2 • F 1 and the output of F 1 is the input to D l as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. F 1 outputs a feature whose width and height is W and H respectively. D l outputs a domain prediction map which has the same width and height as the input feature. We employed a least-squares loss to train the domain classifier following <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref>. This loss function stabilizes the training of the domain classifier and is empirically shown to be useful for aligning low-level features. The loss function of the strong local alignment L loc is summarized as</p><formula xml:id="formula_6">L locs = 1 n s HW ns i=1 W w=1 H h=1 D l (F 1 (x i s )) 2 wh (8) L loct = 1 n t HW nt i=1 W w=1 H h=1 (1 − D l (F 1 (x i t )) wh ) 2 (9) L loc (F, D l ) = 1 2 (L locs + L loct )<label>(10)</label></formula><p>where D l (F 1 (x i s )) wh denotes the output of the domain classifier in each location. The loss is designed to align each receptive field of features with the other domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Context Vector based Regularization</head><p>We further propose a regularization technique to improve the performance of our model. As discussed above, regularizing the domain classifier with the segmentation loss was effective for stabilizing the adversarial training in domain adaptive segmentation <ref type="bibr" target="#b35">[36]</ref>. The authors designed a domain classifier that outputs both the domain label and a semantic segmentation map. Motivated by this approach, we propose to stabilize the training of the domain classifier by the detection loss computed on source examples. We extract vectors v 1 and v 2 from the middle layers of the two domain classifiers respectively. These vectors should contain information about whole input image, which we call "context". Then, we concatenate the vectors with all region-wise features as shown in <ref type="figure" target="#fig_1">Fig. 3</ref> and train the domain classifiers to minimize the detection loss on source examples as well as minimize domain classification loss. During the test phase, the vectors are forwarded to obtain outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Overall Objective</head><p>We denote the objective of detection modules as L det , which contains the loss for region proposal networks and final classification and localization error. The adversarial loss L adv (F, D) is summarized as,</p><formula xml:id="formula_7">L adv (F, D) = L loc (F 1 , D l ) + L global (F, D g )<label>(11)</label></formula><p>Combined with the loss of detection on source examples, the overall objective is, where λ controls the trade-off between detection loss and adversarial training loss. The sign of gradients is flipped by a gradient reversal layer proposed by <ref type="bibr" target="#b8">[9]</ref>. Each mini-batch has one labeled source and one unlabeled target example.</p><formula xml:id="formula_8">max D min F,R L cls (F, R) − λL adv (F, D)<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our approach on four domain shifts-PASCAL <ref type="bibr" target="#b7">[8]</ref> to Clipart <ref type="bibr" target="#b14">[15]</ref>, PASCAL to Watercolor <ref type="bibr" target="#b14">[15]</ref>, Cityscapes <ref type="bibr" target="#b5">[6]</ref> to FoggyCityscapes <ref type="bibr" target="#b34">[35]</ref>, and GTA <ref type="bibr" target="#b16">[17]</ref> to Cityscapes-to demonstrate that it is effective for adaptation between both dissimilar and similar domains. Additionally, we provide experiments to verify our claim that complete feature matching can degrade the performance of the model in the target domain. Implementation Details. In all experiments, we set the shorter side of the image to 600 following the implementation of Faster RCNN <ref type="bibr" target="#b29">[30]</ref> with ROI-alignment <ref type="bibr" target="#b11">[12]</ref>. We first trained the networks with learning rate 0.001 for 50K iterations, then with learning rate 0.0001 for 20K more iterations and reported the final performance. All models are trained with this scheduling and we reported the performance trained after 70K iterations. Without specific notation, we set λ as 1.0 and γ as 5.0. We implemented all methods with Pytorch <ref type="bibr" target="#b27">[28]</ref>. Please see our supplemental material for the detail of the network architecture.</p><p>We compared our method with three baselines: FRCNN model, FRCNN with a baseline domain classifier, and domain adaptive FRCNN (DA-Faster) <ref type="bibr" target="#b4">[5]</ref>. FRCNN model was trained only on source examples without any adaptation. The FRCNN with a baseline domain classifier has exactly the same architecture as our proposed weak-global alignment model, but its domain classifier is trained with cross-entropy loss in Eq. 5 and 6. The model does not have a local-level domain classifier. By comparing with this model, we can directly observe the effectiveness of our proposed weak alignment approach. Hereafter, we call the baseline BDC-Faster. DA-Faster <ref type="bibr" target="#b4">[5]</ref> employs two domain classifiers, an image-level one for high-level features and an instance-level one for features cropped by the region proposal network. Both domain classifiers are trained by cross-entropy loss. In addition, it utilizes a technique called consensus regularization, which makes the outputs of two domain classifiers similar. Since we did not observe any benefit of the technique, we report the results without it. Since we implemented the method ourselve, the results reported in the original paper and in our paper are different. We denote their reported performance as DA-Faster*.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Adaptation between dissimilar domains</head><p>We first show experiments on dissimilar domains, specifically, adaptation from real images to artistic images. We utilized the Pascal VOC Dataset as the real source domain <ref type="bibr" target="#b7">[8]</ref>. This dataset contains 20 classes of images and their bounding box annotations. Following a prevalent evaluation protocol, we employed PASCAL VOC 2007 and 2012 training and validation splits for training, resulting in about 15k images. The target domain consists of either the Clipart or the Watercolor datasets <ref type="bibr" target="#b14">[15]</ref>. Clipart contains comical images whereas Watercolor has artistic images. Clipart contains 1K images in total, which have the same 20 categories as PASCAL VOC. All images were used for both training (without labels) and testing. Watercolor contains 6 categories in common with PASCAL and 2K images in total. 1K training images were utilized during training and our model is evaluated on 1K test images. In this experiment, we used the ResNet101 <ref type="bibr" target="#b12">[13]</ref> pre-trained on <ref type="bibr" target="#b6">[7]</ref> as a backbone network. For other details see our supplemental material. Results on Clipart. As shown in <ref type="table">Table 1</ref>, our proposed method outperformed all baselines. Just by replacing the domain classifier's objective with the focal loss, MAP improved by 10.8% (25.6 to 36.4). In addition, the context vector based regularization and local alignment (C, L in  <ref type="figure">Fig. (a)</ref> and (b) are the results of adaptation between dissimilar domains (from pascal to clipart). For <ref type="figure">Fig. (a)</ref>, images with green lines are from PASCAL VOC (source). Images with orange lines are from Clipart (target). Our method does not match feature distributions strictly whereas the baseline method matches. However, our method outperformed the baseline with a large margin, which demonstrates the effectiveness of global-weak alignment. <ref type="figure">Fig. (d)</ref> and (c) are adaptation between similar domains (from Cityscape to FoggyCityscape). When the domains are very similar, the baseline method works well though our method performs better. <ref type="table">Table)</ref>, further improved MAP. The performance on the source domain, PASCAL VOC, is shown in <ref type="table">Table 2</ref>. Compared with the performance of the source only model, BDC-Faster and DA-Fastster significantly decrease its performance. This fact indicates that strictly aligning feature distributions between different domains can disturb the training for object detection while our method does not degrade the performance on the source domain. We further visualized the features obtained by two models, our proposed global-level adaptation model and BDC-Faster in <ref type="figure" target="#fig_2">Fig. 4</ref>(a) and 4(b). The target features obtained by a baseline domain classifier are matched compactly with the source domain ( <ref type="figure" target="#fig_2">Fig. 4(b)</ref>). On the other hand, with our proposed method ( <ref type="figure" target="#fig_2">Fig. 4(a)</ref>), some features are aligned with the source features, but most of them are separated from source features. Source images usually focus on one or two objects whereas target images usually contain multiple images. Some target images focusing on single object are likely to be aligned with source as shown in the figure. Many existing methods for image classification aimed to match the feature distributions closely. However, this visualization implies that such distribution matching does not always help domain adaptive object detection. Results on Watercolor. According to <ref type="table" target="#tab_1">Table 3</ref>, our method outperformed the baseline methods. There was a large improvement on this domain. The improvement by the local alignment is especially large, about 3%, because the target images have a characteristic "painting" style. Therefore, the reducing the domain-gap based on local-level features improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adaptation between similar domains</head><p>In this experiment, we aim to analyze our method by evaluating the adaptation between very similar domains. We used Cityscape <ref type="bibr" target="#b5">[6]</ref> as the source domain. The images in the dataset are captured by a car-mounted video camera. As the target domain, we used FoggyCityscape datasets <ref type="bibr" target="#b34">[35]</ref>. The images are rendered from Cityscape using depth information and it simulates the change of weather condition. The important difference from other adaptation scenario is that source and target images are originally the same one. Target images are generated from source images by adding fog noise. In such adaptation scenario, strictly aligning feature distributions should be effective because there exists a correct matching between source and target images. Both dataset have 2, 975 images in the training set, and 500 images in the validation set. We utilized the training set during training and evaluated on the validation set. Since Cityscapes dataset does not have bounding-box annotation, we take the tightest rectangles of its instance masks as groundtruth bounding boxes. We used the VGG16 model <ref type="bibr" target="#b37">[38]</ref> as a backbone network following <ref type="bibr" target="#b4">[5]</ref>. As shown in <ref type="table">Table 4</ref>, our proposed method performed much better than the baseline methods. MAP of a model with only strong local alignment was 27.9. Combining strong local and weak global alignment boosted MAP to 34.3. The domain-shift is caused by fog noise, a local-level shift. Hence, strong local alignment largely contributed to the improvement. In this adaptation scenario, the method with a baseline domain classifier performs better than the source only model. This is because the target images have exactly the same layout and number/combination of objects. Thus, strong alignment between different domains was effective. The visualized features in <ref type="figure" target="#fig_2">Fig. 4</ref> show completely different characteristics from the experiments on PASCAL to Clipart dataset. The features are matched in both methods. The results indicate that our proposed method performs both when two domains are dissimilar and similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adaptation from synthetic to real images</head><p>We evaluate the performance of our model in an adaptation from synthetic images to real images. As the synthetic domain, we used Sim10k <ref type="bibr" target="#b16">[17]</ref>. The dataset contains images of the synthetic driving scene, 10,000 training images which are collected from the computer game Grand Theft Auto (GTA). We employed the same architecture as used in the previous section. Following the protocol of <ref type="bibr" target="#b4">[5]</ref>, we eval-uated detection performance on car. As a real domain, we used Cityscape. All training images are used during training for both domains. Average precision was evaluated on the validation split of the Cityscape. We set the value of λ = 0.1 following <ref type="bibr" target="#b4">[5]</ref> in Eq. 12. We show the performance when varying the value of λ in our supplemental material. The two domains have similar layout in that both domains are driving scene images. However, the color and lighting are clearly different. In this respect, the two domains are more different than Cityscape and Foggycityscape are. We extensively evaluated our method by ablating some components. Moreover, we show the results using instance-level adaptation as proposed in <ref type="bibr" target="#b4">[5]</ref>. We also show the comparison and results of combination with a model trained with images translated by CycleGAN <ref type="bibr" target="#b42">[43]</ref>. We trained Cycle-GAN to translate different domains' images, then utilized the translated source images for training. Whether we employed the translated images is denoted by the colum of P in <ref type="table" target="#tab_2">Table 5</ref>. The details are shown in supplemental material. In addition, we demonstrate that our idea of weak alignment can be achieved with a loss function other than focal loss. In Eq. 2, we set f (p t ) = e −ηpt , which is a decreasing function with the value of p t . We call the loss function exponential focal loss (EFL). We set η = 5.0.</p><p>The results are summarized in <ref type="table" target="#tab_2">Table 5</ref>. Our method constantly performed better than the baseline models. Comparing the results of BDC-Faster (31.8) and our method with only global-level alignment <ref type="bibr">(36.4)</ref>, the weak feature distribution alignment outperformed the strict alignment. Setting the value of γ = 3.0 in Focal Loss significantly improved the performance. In addition, with regard to a model trained with EFL, we could observe the improvement over the baseline models. The results demonstrate that our idea of weak global alignment is effective and can be achieved by functions other than Focal Loss.</p><p>Context vector based regularization and local-level alignment further improved the performance. The performance did not degrade when we did not use the context vector in test phase as seen in the table. This implies that the network does not use the vector for the prediction whereas the performance improved compared to the model without the regularization. Therefore, the context vector seems to contribute to the regularization of the domain classifier.</p><p>We could not see a positive effect of instance-level adaptation (Weak Align in <ref type="table" target="#tab_2">Table 5</ref>). Instance-level alignment utilizes the cropped features by region proposal networks, but the proposals may not localize objects in the target domain well, so it can hurt the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis</head><p>Examples of detection results. We show the examples of detection results in <ref type="figure" target="#fig_3">Fig. 5</ref>. Even when the style of the images is different between the source and target, our model localizes objects correctly in these cases. As seen in Clipart's example, when the appearance of the objects is largely different, the detection results are not successful. Also, as seen in case of Watercolor, the detector tends to output multiple predictions to one object. In case of FoggyCityscape's examples, our model tends to assign one bounding box to multiple neighboring bicycles.</p><p>Visualization of domain evidence. To analyze the behavior of the feature extractor and domain classifier, we visualize the evidence for the global-level domain classifier's prediction using Grad-cam <ref type="bibr" target="#b36">[37]</ref> in <ref type="figure" target="#fig_3">Fig. 5</ref>. We use Grad-cam to show the evidence (heatmap) for why the domain classifier thinks the image comes from the source or the target, for the adaptation from Sim10k to Cityscapes. Please see our supplemental material for other examples. For the target images, the domain classifier does not look at cars as the evidence for the target. Similarly, for source images, it also does not look at cars as the evidence for the source. This indicates that the feature extractor seems to focus on cars to deceive the domain classifier, which means that the feature extractor learns to partially align global-image features, specifically around cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a novel approach for detector adaptation based on strong local alignment and weak global alignment for unsupervised adaptation of object detectors. Our key contribution is the weak alignment model, which focuses the adversarial alignment loss on images that are globally similar and puts less emphasis on aligning images that are globally dissimilar. Additionally, we design the strong domain alignment model to only look at local receptive fields of the feature map. Our method outperformed other existing methods with a large-margin in several datasets. Through extensive experiments, we verified the effectiveness of weak global and strong local alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We used the same architecture of the domain classifier for Faster RCNN with ResNet101 and VGG16. As the number of the channels in input features is different, we changed the channel size of network according to the backbone network.</p><p>In case of VGG16 model, the feature in conv3 3 layer is fed into the local domain classifier. The feature in the last res2c layer is fed into the local domain classifier with regard to ResNet101 model. The name of the layer is cited by the Caffe <ref type="bibr" target="#b15">[16]</ref> prototxt. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Domain Classifier</head><p>The global domain classifier has three layered convolution layers, global average pooling and one Linear layer. The kernel size of the convolution layers is set as three. Batch Normalization, ReLU, and dropout layers are attached after each convolution layer. The output of the global domain classifier is activated by softmax function. Context vector is extracted after the average pooling layer. Therefore, the vector has 128 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Domain Classifier</head><p>The local domain classifier has three layered convolution layers. The kernel size of the convolution layers is set as one. The output of the local domain classifier is activated by sigmoid function. Context vector is extracted before the last convolution layer. Therefore, the vector has 128 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Pixel-level Adaptation</head><p>The results on a model trained with images generated by CycleGAN were provided in our paper. We describe the details of how we trained the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training of CycleGAN</head><p>To train the CycleGAN, we used the Pytorch implementation https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix. We used all training images in both domains. We trained CycleGAN for 10 epochs and employed the source images translated into the target domain for training our Faster RCNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training of Faster RCNN</head><p>We found that some of the translated images are not translated correctly. Cars are completely hidden by large noise. In order to suppress the effect of such corrupted images, we trained our model using source, translated source and target images. Translated images are utilized just for training detection modules and not utilized for domain classification. Namely, we have three images in each mini-batch, source, translated source and target image. Source and target images are used as we mentioned in our main paper. The translated source one is used to calculate and back-propagate the detection loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Additional Results</head><p>Results on source domain <ref type="table">Table 7</ref>, 10, and 11 show the results on source domain in three adaptation scenarios. In all scenarios, our method does not significantly degrade the detection performance on the source domain. Results on target domain <ref type="table">Table 8</ref>, 10 and 12 provide results including local-level only adaptation and pixel-level adaptation. We did not show the pixel-level adaptation results on Clipart and Watercolor dataset in our main paper. With regard to Cityscape, we show more ablations in this table. In adaptation for clipart dataset, training with the images generated by CycleGAN does not improve the performance. The possible reasons are that the style of the target images is largely different and that target images have diverse styles of examples. Then, CycleGAN may not generate images suitable for adaptive detection. In the experiments on Watercolor, the performance greatly improved with the use of pixel-level adaptation. The model demonstrates almost oracle-level performance. <ref type="table">Table 9</ref> denotes the results when using VGG network for the adaptation from PASCAL VOC to clipart. The performance largely improved with our method. Parameter Sensitivity <ref type="figure" target="#fig_4">Fig. 6(a)</ref> presents the sensitivity to the parameter λ in Eq. 12 in our main paper. The parameter is the trade-off between training of detection and adversarial training. Our method performed better than the baseline domain classifier in all values ranging from 0 to 1. <ref type="figure" target="#fig_4">Fig. 6(b)</ref> presents the sensitivity to the parameter γ of Focal Loss. The result is obtained in adaptation from Sim10k to Cityscape. The parameter controls how strictly we align features between domains. As the parameter gets small, the domain classifier will look at all examples. As shown in the figure, the peak of the performance was around 3.0, in which AP was 42.3. <ref type="figure">Fig. 7</ref> is the additional domain evidence visualization. Although the behavior differs from dataset to dataset, the feature extractor tries to partially fool the domain classifier.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Evidence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Images</head><p>Target Images <ref type="figure">Figure 7</ref>. Visualization of domain evidence using Grad-Cam. The evidence is obtained by the global-domain classifier. The pictures show results on target and source images respectively. From left to right, input images, images of evidence for the target, evidence of the source domain. The behavior of the domain classifier seems to be different in the adaptation scenarios. However, the feature extractor tries to partially fool the domain classifier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Left: Weak-distribution alignment using a domain classifier. Right: Standard cross-entropy loss and focal loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Proposed Network Architecture. Our method performs strong-local alignment by a local domain classifier network and weakglobal alignment by a global domain classifier. The context vector is extracted by the domain classifiers and is concatenated in the layer before the final fully connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>(a) Proposed (MAP: 36.4) (b) Baseline DC (MAP: 25.6) (c) Proposed (MAP: 29.1) (d) baseline DC (MAP: 27.6) Visualization of features obtained by two different models. Blue: source examples, Red: target examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Upper: Examples of detection results on the target domain. From left to right column, Clipart, Watercolor, FoggyCityscape and Cityscape dataset. Bottom: Visualization of domain evidence using Grad-Cam. The evidence is obtained by the global-domain classifier. The pictures show results on target (Top) and source images (Bottom). From left to right, input images, images of evidence for the target, evidence of the source domain. The feature extractor seems to focus on deceiving the domain classifier in regions with cars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Parameter sensitivity to the value of λ (Left) and γ (Right) in adaptation from Sim10k to Cityscape and from Pascal to Watercolor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Results on adpatation from PASCAL VOC to Clipart Dataset. Average precision (%) is evaluated on target images. G, I, CTX, L indicate global alignment, instance-level alignment, context-vector based regularization, and local-alignment respectively. Method G I CTX L aero bcycle bird boat bottle bus car cat chair cow table dog hrs bike prsn plnt sheep sofa train tv MAP Source Only 35.6 52.5 24.3 23.0 20.0 43.9 32.8 10.7 30.6 11.7 13.8 6.0 36.8 45.9 48.7 41.9 16.5 7.3 22.9 32.0 27.8 BDC-Faster 20.2 46.4 20.4 19.3 18.7 41.3 26.5 6.4 33.2 11.7 26.0 1.7 36.6 41.5 37.7 44.5 10.6 20.4 33.3 15.5 25.6 DA-Faster 15.0 34.6 12.4 11.9 19.8 21.1 23.2 3.1 22.1 26.3 10.6 10.0 19.6 39.4 34.6 29.3 1.0 17.1 19.7 24.8 19.8 Proposed 30.5 48.5 33.6 24.8 41.2 48.9 32.4 17.2 34.5 55.0 19.0 13.6 35.1 66.2 63.0 45.3 12.5 22.6 45.0 38.9 36.4 31.7 55.2 30.9 26.8 43.4 47.5 40.0 7.9 36.7 50.0 14.3 18.0 29.2 68.1 62.3 50.4 13.4 24.5 54.2 45.8 37.5 26.2 48.5 32.6 33.7 38.5 54.3 37.1 18.6 34.8 58.3 17.0 12.5 33.8 65.5 61.6 52.0 9.3 24.9 54.1 49.1 38.1 Results on PASCAL VOC in adaptation from PASCAL VOC to Clipart Dataset. Average precision (%) is evaluated on PASCAL. Our method does not degrade the performance on the source whereas BDC-Faster and DC-Faster degrade it.</figDesc><table><row><cell>Method</cell><cell>G I CTX L MAP</cell></row><row><cell>Source Only</cell><cell>77.5</cell></row><row><cell>BDC-Faster</cell><cell>73.6</cell></row><row><cell>DA-Faster</cell><cell>66.4</cell></row><row><cell></cell><cell>78.0</cell></row><row><cell>Proposed</cell><cell>77.6</cell></row><row><cell></cell><cell>77.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>AP on adpatation from PASCAL VOC to WaterColor (%). 31.0 40.5 22.1 35.3 20.2 20.0 27.1 27.6 Proposed 33.5 33.3 42.7 22.2 27.1 40.3 11.6 22.3 29.1 34.3 32.2 36.2 23.7 27.5 39.3 5.4 24.4 27.9 38.0 31.2 41.8 20.7 26.6 37.6 19.7 20.5 29.5 36.2 35.3 43.5 30.0 29.9 42.3 32.6 24.5 34.3 Oracle 50.0 36.2 49.7 34.7 33.2 45.9 37.4 35.6 40.3</figDesc><table><row><cell cols="2">The definition of G, I, CTX, L is following Table 1.</cell></row><row><cell></cell><cell>AP on a target domain</cell></row><row><cell>Method</cell><cell>G I CTX L bike bird car cat dog prsn MAP</cell></row><row><cell>Source Only</cell><cell>68.8 46.8 37.2 32.7 21.3 60.7 44.6</cell></row><row><cell>BDC-Faster</cell><cell>68.6 48.3 47.2 26.5 21.7 60.5 45.5</cell></row><row><cell>DA-Faster</cell><cell>75.2 40.6 48.0 31.5 20.6 60.0 46.0</cell></row><row><cell></cell><cell>66.4 53.7 43.8 37.9 31.9 65.3 49.8</cell></row><row><cell>Proposed</cell><cell>71.3 52.0 46.6 36.2 29.2 67.3 50.4</cell></row><row><cell></cell><cell>82.3 55.9 46.5 32.7 35.5 66.7 53.3</cell></row><row><cell cols="2">Table 4. AP on adaptation from Cityscape to FoggyCityscape (%).</cell></row><row><cell cols="2">The performance of our method is very near to oracle, which is</cell></row><row><cell cols="2">trained on labeled target images.</cell></row><row><cell></cell><cell>AP on a target domain</cell></row><row><cell cols="2">Method G I CTX L bus bcycle car bike prsn rider train truck MAP</cell></row><row><cell>Faster RCNN</cell><cell>22.3 26.5 34.3 15.3 24.1 33.1 3.0 4.1 20.3</cell></row><row><cell>BDC-Faster</cell><cell>29.2 28.9 42.4 22.6 26.4 37.2 12.3 21.2 27.5</cell></row><row><cell>DA-Faster</cell><cell>33.1 23.3 25.5 15.6 23.4 29.0 10.9 19.6 22.5</cell></row><row><cell>DA-Faster*</cell><cell>25.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Results on adpatation from Sim10k to Cityscape Dataset (%). Average precision is evaluated on target images. FL (γ = 3)* indicates the experiments in which shorter side of image is scaled to 1000 during training and testing. P indicates pixel-level alignment, whether we used images generated by cyclegan during training. † indicates the performance when the context vector is zero-padded and not used for the output.</figDesc><table><row><cell>Method</cell><cell>G</cell><cell>I CTX L P</cell><cell>AP on Car</cell></row><row><cell>Faster RCNN</cell><cell></cell><cell></cell><cell>34.6</cell></row><row><cell>BDC-Faster</cell><cell></cell><cell></cell><cell>31.8</cell></row><row><cell>DA-Faster</cell><cell></cell><cell></cell><cell>34.2</cell></row><row><cell>DA-Faster*</cell><cell></cell><cell></cell><cell>38.9</cell></row><row><cell>Weak Align</cell><cell></cell><cell></cell><cell>35.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>36.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>38.2 (38.3 †)</cell></row><row><cell>Proposed (FL)</cell><cell></cell><cell></cell><cell>40.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>41.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40.7</cell></row><row><cell cols="4">Proposed Method with different parameters</cell></row><row><cell>EFL</cell><cell></cell><cell></cell><cell>38.7</cell></row><row><cell>FL (γ = 3)</cell><cell></cell><cell></cell><cell>42.3</cell></row><row><cell>FL (γ = 3)*</cell><cell></cell><cell></cell><cell>47.7</cell></row><row><cell>Oracle</cell><cell></cell><cell></cell><cell>53.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>The architecture of the domain classifiers.</figDesc><table><row><cell>Global Domain Classifier</cell><cell></cell></row><row><cell>Conv 3 × 3 × 512, stride 2, pad 1 Batch Normalization, ReLU, Dropout Conv 3 × 3 × 128, stride 2, pad 1 Batch Normalization, ReLU, Dropout Conv 3 × 3 × 128, stride 2, pad 1 Batch Normalization, ReLU, Dropout Average Pooling Fully connected 128 × 2</cell><cell>Local Domain Classifier Conv 1 × 1 × 256, stride 1, pad 0 ReLU Conv 1 × 1 × 128, stride 1, pad 0 ReLU Conv 1 × 1 × 1, stride 1, pad 0 Sigmoid</cell></row><row><cell>Softmax</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .Table 8 .Table 9 .Table 11 .Table 12 .</head><label>7891112</label><figDesc>Results on PASCAL VOC in adaptation from PASCAL VOC to Clipart Dataset. Average precision (%) is evaluated on PASCAL. Our method does not degrade the performance on the source whereas BDC-Faster and DC-Faster degrade it. 80.7 82.5 78.6 62.2 86.1 85.4 87.5 60.2 79.0 68.8 88.6 86.2 88.7 78.9 52.5 78.5 71.6 87.9 76.9 78.0 78.6 81.7 83.4 74.7 62.9 86.9 85.4 90.6 56.8 85.7 57.9 89.0 87.4 87.7 78.1 52.3 79.3 67.6 87.6 79.5 77.6 77.5 84.7 81.1 71.1 63.8 88.5 84.7 87.7 54.5 81.6 60.8 89.4 87.8 88.2 78.2 49.9 78.9 70.2 82.0 79.5 77.0 Results on adaptation from PASCAL VOC to Clipart Dataset. Average precision (%) is evaluated on target images. G, I, CTX, L, P indicate global alignment, instance-level alignment, context-vector based regularization, local-alignment and pixel-alignment respectively. Faster RCNN* indicates Faster RCNN trained on source images and source images translated by CycleGAN. Method G I CTX L P aero bcycle bird boat bottle bus car cat chair cow table dog hrs bike prsn plnt sheep sofa train tv MAP Faster RCNN 35.6 52.5 24.3 23.0 20.0 43.9 32.8 10.7 30.6 11.7 13.8 6.0 36.8 45.9 48.7 41.9 16.5 7.3 22.9 32.0 27.8 Faster RCNN* 26.4 52.7 28.3 24.1 28.5 49.7 30.2 13 35.3 26.5 15.8 7.6 26.1 68.1 47.2 42.5 5.9 23.2 41 42.6 31.7 BDC-Faster 20.2 46.4 20.4 19.3 18.7 41.3 26.5 6.4 33.2 11.7 26.0 1.7 36.6 41.5 37.7 44.5 10.6 20.4 33.3 15.5 25.6 DA-Faster 15.0 34.6 12.4 11.9 19.8 21.1 23.2 3.1 22.1 26.3 10.6 10.0 19.6 39.4 34.6 29.3 1.0 17.1 19.7 24.8 19.8 Proposed 30.5 48.5 33.6 24.8 41.2 48.9 32.4 17.2 34.5 55.0 19.0 13.6 35.1 66.2 63.0 45.3 12.5 22.6 45.0 38.9 36.4 19.8 50.7 25.4 21.7 30.2 47.2 27.1 8.5 33.5 26.8 14.0 11.7 31.5 62.0 49.9 39.6 9.1 23.8 39.5 38.4 30.5 31.7 55.2 30.9 26.8 43.4 47.5 40.0 7.9 36.7 50.0 14.3 18.0 29.2 68.1 62.3 50.4 13.4 24.5 54.2 45.8 37.5 26.2 48.5 32.6 33.7 38.5 54.3 37.1 18.6 34.8 58.3 17.0 12.5 33.8 65.5 61.6 52.0 9.3 24.9 54.1 49.1 38.1 31.1 53.7 28.9 24.9 40.3 49.0 38.1 14.6 41.9 43.8 15.3 7.2 27.9 75.5 57.3 41.8 6.7 23.3 48.5 44.1 35.7 Results on adaptation from PASCAL VOC to Clipart Dataset with VGG. Average precision (%) is evaluated on target images. Method G CTX L aero bcycle bird boat bottle bus car cat chair cow table dog hrs bike prsn plnt sheep sofa train tv MAP Faster RCNN 15.7 31.9 22.4 8.2 38.8 59.4 17.8 6.6 37.0 5.7 12.7 7.2 17.4 49.0 36.0 32.1 11.2 2.9 29.8 28.4 23.5 BDC-Faster 11.0 40.9 12.2 10.1 28.9 29.2 28.0 6.0 23.5 8.8 13.1 6.5 22.4 45.6 46.9 35.9 9.7 9.3 18.9 20.9 21.4 Proposed 18.6 45.0 22.2 23.2 23.9 21.1 28.6 5.2 31.8 39.1 19.7 0.9 25.2 56.1 54.3 36.1 27.8 6.8 32.4 40.4 27.9 16.0 53.2 27.5 21.6 32.0 48.4 32.4 12.2 32.5 27.3 12.3 13.1 24.3 62.4 55.5 41.2 21.0 13.2 37.8 46.1 31.5 Table 10. Results on adaptation from PASCAL VOC to WaterColor Dataset (%). Left: AP evaluated on PASCAL VOC. Right: AP evaluated on WaterColor. AP on adaptation from PASCAL VOC to WaterColor (%). The definition of G, I, CTX, L is the same as defined in the main paper. Faster RCNN* indicates Faster RCNN trained by source images and translated source images by CycleGAN. AP on a source domain AP on a target domain Method G I CTX L P bike bird car cat dog prsn MAP bike bird car cat dog prsn MAP Faster RCNN 82.1 82.3 86.5 89.3 85.6 84.3 85.0 68.8 46.8 37.2 32.7 21.3 60.7 44.6 Faster RCNN* 79.7 82.7 86 88.9 85 82.1 84.1 83.3 52.7 45.3 33.1 28.8 64 51.2 BDC-Faster 80.9 82.6 86.4 87.9 82.7 83.3 84.0 68.6 48.3 47.2 26.5 21.7 60.5 45.5 DA-Faster 75.7 84.4 84.5 88.5 83.6 81.6 83.1 75.2 40.6 48.0 31.5 20.6 60.0 46.0 Proposed 79.4 84.6 85.8 89.6 85.7 84.1 84.9 66.4 53.7 43.8 37.9 31.9 65.3 49.8 80.0 82.0 84.7 86.8 83.6 81.3 83.1 79.4 54.8 47.2 37.1 31.5 62.4 52.1 79.5 84.6 86.1 89.2 84.5 82.8 84.4 71.3 52.0 46.6 36.2 29.2 67.3 50.4 79.8 87.4 85.5 88.1 84.5 84.0 84.9 82.3 55.9 46.5 32.7 35.5 66.7 53.3 78.9 83.1 84.2 87.4 85.6 82.8 83.7 90.5 54.8 49.4 38.6 38.8 67.9 56.7 Oracle 82.1 82.3 86.5 89.3 85.6 84.3 85.0 83.6 59.4 50.7 43.7 39.5 74.5 58.6 Results on adaptation from Cityscape to FoggyCityscape Dataset (%). The performance is evaluated on Cityscape. 38.8 52.6 42.5 35.1 47.6 44.1 34.2 44.2 Proposed 62.6 37.9 52.2 35.1 35.0 48.5 47.7 34.9 44.2 57.0 39.3 52.3 39.9 33.6 48.3 41.6 36.0 43.5 57.9 39.4 52.6 39.4 35.2 48.3 47.7 37.4 44.7 Results on adaptation from Sim10k to Cityscape Dataset (%). Average precision is evaluated on target images. Faster RCNN* indicates Faster RCNN trained by source images and translated source images by CycleGAN.</figDesc><table><row><cell>Method</cell><cell>G I C L aero bcycle bird boat bottle bus car cat chair cow table dog hrs bike prsn plnt sheep sofa train tv MAP</cell></row><row><cell>Faster RCNN</cell><cell>77.7 80.3 82.5 79.0 68.0 88.1 85.7 87.0 53.1 87.3 58.2 88.3 85.0 87.9 80.5 52.8 75.9 69.4 86.3 77.8 77.5</cell></row><row><cell>BDC-Faster</cell><cell>77.6 80.0 77.6 61.8 61.3 83.0 86.1 86.5 50.9 79.8 59.8 84.7 82.2 79.5 78.1 45.3 73.4 70.1 80.8 73.8 73.6</cell></row><row><cell>DA-Faster</cell><cell>64.5 70.6 66.4 62.9 49.9 77.2 78.0 70.7 44.7 78.3 56.1 70.8 75.6 85.2 74.5 37.2 65.4 60.2 72.3 66.9 66.4</cell></row><row><cell></cell><cell>79.5</cell></row><row><cell>Proposed</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bird: 1.00</head><p>This work was supported by Honda, DARPA and NSF Award No. 1535797 and partially supported by JST CREST Grant Number JPMJCR1403, Japan.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Network Architecture</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Crossdomain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rosaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Smolley. Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Semantic foggy scene understanding with synthetic data. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large scale semi-supervised object detection using visual and semantic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Importance weighted adversarial nets for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via classbalanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
