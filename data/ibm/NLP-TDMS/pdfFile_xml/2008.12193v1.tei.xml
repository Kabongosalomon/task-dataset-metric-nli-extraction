<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Heyman</surname></persName>
							<email>geert.heyman@nokia-bell-labs.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Nokia Bell Labs Antwerp</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Van</forename><surname>Cutsem</surname></persName>
							<email>tom.vancutsem@nokia-bell-labs.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Nokia Bell Labs Antwerp</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose and study annotated code search: the retrieval of code snippets paired with brief descriptions of their intent using natural language queries. On three benchmark datasets, we investigate how code retrieval systems can be improved by leveraging descriptions to better capture the intents of code snippets. Building on recent progress in transfer learning and natural language processing, we create a domain-specific retrieval model for code annotated with a natural language description. We find that our model yields significantly more relevant search results (with absolute gains up to 20.6% in mean reciprocal rank) compared to state-of-the-art code retrieval methods that do not use descriptions but attempt to compute the intent of snippets solely from unannotated code.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Many modern-day applications and software systems are built from a variety of open source libraries and components. As a result, developers find themselves working with a growing number of libraries and tools in their day-to-day coding activities. Quickly and efficiently retrieving relevant documentation for these libraries is becoming increasingly important. Concrete code examples that illustrate the usage of a library or specific library feature are particularly helpful to software developers, as witnessed by the rapid growth of online Q&amp;A platforms such as Stack Overflow.</p><p>To address this need, the research community has been investigating systems that retrieve code snippets from natural language queries <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. All of these works use neural models to compute the similarity between code snippets and natural language queries. While this setup enables code search on large code corpora, understanding the intent of an unannotated code fragment remains challenging even for experienced developers, let alone for automated systems. For instance, programmers that were asked to assess the relevance of code snippets to a query reported that it would have been helpful to get additional context about the code snippet <ref type="bibr" target="#b3">[4]</ref>. Moreover, the low percentage of keyword matches between the search terms and the code tokens <ref type="bibr" target="#b1">[2]</ref> makes the retrieval of unannotated code snippets particularly challenging.</p><p>In this paper, we take a step back and investigate code search in a more restricted (but, we argue, equally useful) setting where code snippets are accompanied with brief natural language descriptions that capture their intent. Code snippets that are meant to be reused by others are frequently accompanied by descriptions. Such annotated snippets are found for instance in coding cheat sheets, interactive notebook environments, official library documentation, technical blog posts, programming textbooks, or on Q&amp;A websites such as Stack Overflow. While none of these resources use a common, structured format to label code snippets with their intent, simple heuristics can often be used to pair code snippets with meaningful descriptions. We demonstrate that the quality of the search results improves significantly when these descriptions are taken into account by the snippet ranking algorithm.</p><p>The key contributions of this paper are as follows:</p><p>• We propose the annotated code search task: the problem of retrieving code snippets annotated with short descriptions, from natural language queries. To evaluate this task, we created three benchmark datasets, each of which is comprised of an annotated code snippet collection and a set of queries linked to one or more relevant snippets in the collection.</p><p>• We investigate different code retrieval models that make use of code descriptions to rank search results. We propose a new method for fine-tuning a recent NLP transfer learning model to predict the semantic similarity between queries and code descriptions. Using this approach we significantly outperform both classic and neural retrieval baselines. <ref type="bibr">•</ref> We contrast our results with state-of-the-art retrieval models that rank search results based on the source code only. We find that the descriptions are paramount for obtaining relevant search results on all three benchmarks, even though incorporating the snippet source code itself is still beneficial. • All retrieval models and datasets used in this work are released. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND &amp; RELATED WORK A. Code search</head><p>There has been a growing interest in retrieving code using natural language queries <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. These works use neural networks to project code and queries in the same vector space. During training, the model parameters are optimized to minimize the distance between pairs of code snippets and corresponding natural language descriptions. These descriptions are typically extracted from the method documentation. The rationale is that, at prediction time, a query will be projected to a vector that is similar to the vector representations of related code snippets. As such, code search can be reduced to a k-nearest neighbors problem in the joint query-snippet vector space. Gu et al. <ref type="bibr" target="#b0">[1]</ref>, Sachdev et al. <ref type="bibr" target="#b1">[2]</ref>, Cambronero et al. <ref type="bibr" target="#b2">[3]</ref>, Husain et al. <ref type="bibr" target="#b3">[4]</ref>, Yao et al. <ref type="bibr" target="#b4">[5]</ref>, Feng et al. <ref type="bibr" target="#b5">[6]</ref> have all built retrieval models for snippet collections that consist of pure source code. By contrast, in this work we study a different code search setting where every snippet is annotated with a brief natural language description describing the code's intent.</p><p>Yao et al. <ref type="bibr" target="#b4">[5]</ref> proposed a model to automatically generate descriptions from the code and utilize reinforcement learning to directly optimize the descriptions for improving code retrieval. They showed that an ensemble of 1) a model that used the generated descriptions and 2) an existing code search model yielded better performance for retrieving SQL code snippets. Ye et al. <ref type="bibr" target="#b6">[7]</ref> improved on this work with a dual learning model that simultaneously optimizes for code generation and code summarization. Both of these works describe an alternative framework to obtain code descriptions but are otherwise orthogonal to our work.</p><p>Recent work explored the application of NLP transfer learning techniques to code <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>. CodeBERT <ref type="bibr" target="#b5">[6]</ref>, a bimodal transformer model for code and natural language obtained state-of-the-art results on highly specific tasks (for example, identifying methods/functions given their documentation among a list of "distractor" snippets), but performance on such proxy tasks correlates poorly with performance on actual code search <ref type="bibr" target="#b3">[4]</ref>. It therefore remains to be seen how well CodeBERT performs on downstream tasks such as code search. In this work, we study retrieval of code snippets that leverage both the code tokens and a short description of the code. For computing the similarity between the query and the code, we compare with existing models such as NCS <ref type="bibr" target="#b1">[2]</ref> and UNIF <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transfer learning in natural language processing</head><p>In the past two years, natural language processing has seen substantial improvements across a variety of tasks including question answering, summarization, and information retrieval <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Researchers have shown that fine-tuning large models, pre-trained on vast amounts of unlabeled text data, is often far superior to learning models from scratch on labeled, task-specific data only <ref type="bibr">[11, 12,</ref> inter alia]. During pre-training, a neural net is optimized to predict words and/or sentences from their context (i.e., the previous or surrounding words/sentences), and as a result, learns to compute word representations that capture semantic and syntactic properties of the words and the context in which they appear. During fine-tuning, the last layer of the pre-trained model is replaced with a task-specific layer and all model parameters are fine-tuned on labeled data of the downstream task.</p><p>All recent state-of-the-art systems use the Transformer architecture <ref type="bibr" target="#b12">[13]</ref>, a neural network that processes sequences using multi-head self-attention. This facilitates capturing long-range dependencies and makes the computations more parallelizable than those of recurrent neural networks (RNNs). Due to the increased scale of the models and text corpora, pre-training has become impractical for the average researcher. Fortunately, several institutions have made pre-trained models publicly available: the universal sentence encoder (USE) <ref type="bibr" target="#b13">[14]</ref>, GPT and GPT-2 <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, BERT <ref type="bibr" target="#b10">[11]</ref>, RoBERTa <ref type="bibr" target="#b16">[17]</ref>, T5 <ref type="bibr" target="#b8">[9]</ref> are all publicly available. The models differ in a) the pre-training objectives (e.g., BERT predicts words given the surrounding words, while GPT models only use the previous words); b) the text corpora on which they were trained; c) the model size; and d) variations on how the transformer architecture is used (e.g., BERT computes representations of (sub)words, sentences, and pairs of sentences, whereas the universal sentence encoder is aimed at computing sentence representations only).</p><p>Our interest in these transfer learning models stems from the need to compute the semantic similarity between search queries and short code descriptions. Because software development is terminology-heavy, the domains of the training corpora play a crucial role in performance on downstream tasks such as code search. Although the T5 model yields state-of-the-art results on many benchmarks 2 , it is less suited for tasks in the software domain because its pre-training data was filtered with a heuristic to exclude documents that contain code. By contrast, the text corpora on which the universal sentence encoder was trained included data from Q&amp;A websites such as Stack Overflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ANNOTATED CODE SEARCH</head><p>We cast annotated code search as a retrieval task: given a collection of annotated code snippets C = s 1 , s 2 , ..., s N where each annotated snippet s i consists of a code fragment c i paired with a brief natural language description d i and a natural language query q, the goal is to generate a ranked list of code snippets s q1 , ..., s qk . In the context of this paper, a code snippet will always be paired with a description, so for convenience, we will use the terms annotated code snippet and code snippet interchangeably. Prior work has studied the retrieval of code snippets without description, we will refer to this setting as code-only code search.</p><p>To benchmark the retrieval performance of a model on a snippet collection C, we require a ground truth dataset that links queries to code snippets from C that match the query. Performance can then be measured objectively with standard information retrieval metrics: mean reciprocal rank (MRR) and recall@k (r@k). The reciprocal rank is the inverse of the rank of the first matching snippet for a given query, MRR computes the average of the reciprocal ranks for the queries in the evaluation set. For MRR the top-ranked snippets are most influential, e.g., finding a relevant snippet at rank 1 instead of rank 2 will have a bigger impact on the score than finding a snippet at rank 4 instead of rank 5. Recall@k is defined as the percentage of queries for which at least one matching snippet was found in the top-k results.</p><p>To get insight into the difficulty of a benchmark, we will also measure the word overlap between the queries and descriptions of matching snippets. We define word overlap between a query q and a description d as the number of unique words that occur in both q and d. Relative overlap is computed as the ratio between the overlap and the number of unique words in q. Before computing the overlap, q and d are tokenized and lowercased, and stop words are filtered out. <ref type="figure">Fig. 1</ref>: Illustration of the CoNaLa corpus <ref type="bibr" target="#b17">[18]</ref> is converted in a benchmark for annotated code search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PACS: BENCHMARKS FOR ANNOTATED CODE SEARCH</head><p>Existing code search benchmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> link queries to relevant code snippets but these snippets are not consistently paired with descriptions and hence not directly applicable to this study. <ref type="bibr" target="#b2">3</ref> To evaluate and train annotated code retrieval models, we therefore created three benchmarking datasets, which we collectively refer to as the Python Annotated Code Search benchmark (PACS). <ref type="bibr" target="#b3">4</ref>  <ref type="table" target="#tab_1">Table I</ref> gives an overview of the PACS datasets. In the remainder of this section, we provide details on the construction of the benchmarks and explain how we obtain relevant training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Snippet collections and ground truth</head><p>We compile snippet collections from two existing datasets (CoNaLa and StaQC) and mine a third snippet collection from Stack Overflow posts in the data science domain. a) CoNaLa: The CoNaLa corpus <ref type="bibr" target="#b17">[18]</ref> 5 is a curated collection of short Python code snippets annotated with their natural language intent. Yin et al. <ref type="bibr" target="#b17">[18]</ref> used crowd-sourcing to label a sample of Stack Overflow posts. An example record of the corpus is shown in <ref type="figure">Figure 1</ref>. It consists of a code snippet, intent, re-written intent, and the id of the Stack Overflow post. The code snippets are selected from post answers. The intent is a high-level, natural language description of the code snippet and typically corresponds to the Stack Overflow post title. Because more than one code snippet can be extracted from the same post, different code snippets are sometimes annotated with the same intent. The re-written intent is a reformulation of the intent that should better reflect the full meaning of the code. <ref type="figure">Figure 1</ref> illustrates how we create a benchmark for annotated code search from the corpus. The snippet collection is constructed by selecting the re-written intents and their corresponding code snippets. This results in 2,777 snippets. To create the ground truth, we group the CoNaLa records by their Stack Overflow post id. For each group, we use the intent of one of the snippets as a query. The matching snippets are the code snippets that were constructed from the records in this group. Many of the re-written intents are similar to the original intent. Queries that have a relative word overlap of more than 0.5 with the description of a matching snippet are removed. This results in a ground truth dataset with 762 queries.  <ref type="bibr" target="#b18">[19]</ref>: we strip prompts, filter out code snippets that do not parse, and automatically rewrite questions as descriptions with simple regular expressions.  <ref type="bibr" target="#b5">6</ref> is a large collection of Python and SQL question-code snippet pairs that are mined from Stack Overflow. These pairs were extracted with machine learning models that identify how to questions and their corresponding code snippet(s) in the answers. The dataset has been used in prior work on code summarization <ref type="bibr" target="#b19">[20]</ref> and (code-only) code search <ref type="bibr" target="#b4">[5]</ref>. To create a snippet collection we started from the 272K Python question-code pairs in StaQC and performed additional cleaning as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>: we strip prompt prefixes ( &gt;&gt;&gt;, · · ·, In [1]: , etc.), filter out code snippets that do not parse in Python 2 and 3, and use simple heuristics to reformulate questions to descriptions. This results in a snippet collection of 204K (description, code snippet) pairs.</p><p>To create the ground truth, we leverage the fact that some Stack Overflow posts are tagged as a duplicate of another post. The titles of duplicate posts are descriptions that are created independently by different users but reflect the same underlying intent. Therefore, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, we collect the duplicates of the posts used for the StaQC snippet collection and take the duplicates' titles as queries and the corresponding StaQC snippets as the matching results.</p><p>The duplicate posts are collected from the Stack Overflow dump of February 2020. <ref type="bibr" target="#b6">7</ref> Some posts are tagged as a duplicate of multiple questions, and it also happens that a post A is tagged as a duplicate of post B, while post C is tagged as a duplicate of A. We transitively group all such duplicate relations (i.e., A, B, and C will be part of the same group) and end up with 195K duplicate groups. Most groups consist of two posts, but popular posts can have more than 100 duplicates. To create the StaQC ground truth, we select all groups containing 1) a post that was the source for a StaQC snippet (to have snippets that match the query), as well as 2) a post that was not used for the StaQC data (to select the query). This resulted in 5,497 duplicate groups. From each such group we extracted a query with matching results. The ground truth set is split evenly in a validation and a test set. For the validation set we additionally filter out any queries that overlap with the CoNaLa and SO-DS test sets to avoid overfitting when tuning model hyper-parameters. c) SO-DS: The CoNaLa and StaQC benchmarks have different merits: the CoNaLa snippets and descriptions are curated, but the collection is small and the queries in the ground truth were not created independently from the descriptions. The StaQC benchmark, on the other hand, has a large snippet collection with realistic ground truth queries that were written independently from the snippet descriptions, but despite StaQC's advanced mining methodology the snippet collection contains more noise. We therefore collect a third snippet collection, SO-DS, where we control the quality by only mining from Stack Overflow posts with a large number of upvotes. More specifically, we mine snippets from the most upvoted Stack Overflow posts that are labeled with "python" and one or more tags related to data science libraries such as "tensorflow", "matplotlib" and "beautifulsoup". <ref type="bibr">8 9</ref> We iterate over each tag's posts in decreasing order of their upvote scores. For each post, we extract a maximum of two snippets from the post answers, prioritizing answers with more votes. A snippet is extracted by concatenating all the parseable code blocks from a post answer. We opted to not automatically select the most relevant lines of code with a machine learning model, as was done for StaQC, because incorrect predictions would introduce noise. Note that, unlike CoNaLa and StaQC this dataset is specifically intended for code search, not for code generation or code summarization, where a one-to-one alignment between description and source code is important. Similar to the StaQC benchmark, we use the post title as the snippet description and apply the same cleaning pipeline: we remove prompts, filter out snippets that can not be parsed, and use simple heuristics to reformulate questions to descriptions. When we extracted 250 snippets or ran out of posts with the correct tag, we move on to the next tag. As a final step, we filter out duplicate snippets, which occur because some Stack Overflow posts with multiple tags were mined more than once. The resulting snippet collection consists of 12,137 snippets and 7,674 unique descriptions. <ref type="table" target="#tab_1">Table II displays a few examples from the collection.</ref> The ground truth is collected analogously to StaQC-py by creating queries from Stack Overflow duplicate posts. This results in 2,225 annotated queries, which are evenly split in a validation and test set. The validation set is again filtered to ensure that no queries overlap with the CoNaLa and StaQC-py test sets.</p><p>d) Benchmark statistics: <ref type="table" target="#tab_1">Table I</ref> reports summary statistics on the three benchmarks. The CoNaLa snippets are mostly one-liners and its descriptions are on average slightly longer than those in the other collections. The SO-DS snippets are the longest, which is to be expected since the SO-DS mining procedure does not attempt to automatically extract the most relevant lines of code in Stack Overflow posts, as was done for StaQC. Concerning the different ground truth sets, we observe that most of the CoNaLa queries only have one matching snippet, whereas SO-DS and StaQC on average have 1.7 and 3.4 matching snippets. To assess the difficulty of each benchmark, <ref type="figure" target="#fig_2">Figure 4</ref> shows histograms of the relative word overlap between the queries and the descriptions of matching snippets. The three benchmarks have an   average relative word overlap between 0.28 and 0.29 and there are queries with a relative overlap of more than 0.6. 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training data</head><p>Because it is infeasible to construct a code search ground truth dataset that is large enough for model training, we resort to different proxy datasets. We train query-code similarity models (i.e., the code-only models that will be presented in Section V-B ) on the description-code snippet pairs in the respective collections. To train the query-description similarity models (i.e., the description-only models that will be   presented in Section V-A), we create two additional datasets. Firstly, we collect all Python post titles on Stack Overflow before February 2020, resulting in a total of 1.30M questions. Secondly, we create pairs of related code descriptions from the Stack Overflow duplicate post records that were not used to compile ground truth datasets and snippet collections. To not bias questions with many duplicates (e.g., How to clone or copy a list has more than 289 duplicates), we only sample one pair per duplicate group. This resulted in 187K pairs of related code descriptions.  <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, we formulate snippet retrieval as a k-nearest neighbor problem: queries and snippets are projected in a shared vector space such that any given query and its matching snippets are represented by similar vectors. The snippet collection s 1 , s 2 , ..., s N can then be ranked w.r.t. a query q by sorting snippets according to the cosine similarity between their respective vectors s 1 , s 2 , ..., s N and the query vector q. An annotated code snippet s consists of a description d and the actual code snippet c. Ideally, we would train a model that jointly embeds d and c. However, as mentioned in Section IV-B, we lack sufficient ground truth data to train such a model. Instead, we independently train two embedding models that separately capture the similarity between 1) descriptions and queries (Section V-A); and 2) code fragments and queries (Section V-B). In Section V-C, we explain how these models can be efficiently combined in a single ensemble model.</p><formula xml:id="formula_0">cos(s i , q) = s i · q ||s i || ||q||</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Query-description similarity</head><p>In this section, we present two query-description similarity models: Neural bag-of-words, which will be used as a baseline, and the universal sentence encoder.</p><p>Neural bag-of-words (NBOW): Bag-of-words models treat input texts as unordered word sets. While throwing away the sequence order may seem a crude approximation, for information retrieval it results in hard-to-beat baselines. <ref type="figure" target="#fig_3">Figure 5 A)</ref> illustrates how bag-of-words is applied in a neural setting. We obtain embeddings q bow and d bow for the query q and description d by summing their respective word embeddings:</p><formula xml:id="formula_1">q bow = m j=1 e 1 (q j ), d bow = n j=1 e 1 (d j )</formula><p>Where q j and d j denote the j th words in the query/description after pre-processing, and e 1 (·) is the word embedding model that maps a description/query word to its embedding.</p><p>We pre-process queries and descriptions by applying tokenization, lemmatization, and stopword removal. To learn the word embeddings, we used fastText's <ref type="bibr" target="#b20">[21]</ref> continuous skip-gram model with negative sampling. This model is similar to the skip-gram model from the word2vec toolkit, but in contrast to the latter, fastText also incorporates subword information. The fastText embeddings are trained on the Python Stack Overflow questions dataset (see Section IV-B).</p><p>Universal sentence encoder (USE): The universal sentence encoder (USE) is a transformer-based transfer learning model that computes a vector for a sequence of words (e.g., a sentence or a paragraph). <ref type="bibr" target="#b10">11</ref> We leverage USE to compute representations for the queries and snippet descriptions as shown in <ref type="figure" target="#fig_3">Figure 5</ref> B): USE utilizes transformer to obtain contextualized representations h q 1 , h q 2 , ..., h q m for each query token. An embedding for the query is then calculated by summing the transformer's outputs h q 1 , h q 2 , ..., h q m . It is worth noting that each representation h q i that is produced by transformer is contextualized on all the query tokens, not solely from the current and preceding tokens q 1 , ..., q i as would be the case with RNNs. The embeddings of the descriptions are computed analogously, with the same architecture and model parameters.</p><p>For our experiments, we used a pre-trained model available on TensorFlow Hub 12 that outputs 512dimensional embeddings and has a total of 147M parameters. The model was pre-trained in a multi-task learning setting with three tasks: 1) skip-thought's next sentence prediction <ref type="bibr" target="#b21">[22]</ref>; 2) a conversational inputresponse task <ref type="bibr" target="#b22">[23]</ref>, where to goal is to assess the relevance of a response on a given input (e.g., a reply to an email or a Stack Overflow post); and 3) natural language inference using the SNLI corpus <ref type="bibr" target="#b23">[24]</ref>. The training data for tasks 1 and 2 comes from a variety of sources crawled from the web, including Stack Overflow. This makes USE a good choice for downstream tasks in the software development domain, such as code search.</p><p>Although USE's pre-training ingests textual data from the software domain, without fine-tuning the model does not significantly outperform our baselines (see Section VII). Training USE directly on code search is infeasible due to the lack of annotated data. Therefore, we propose the detection of duplicate Stack Overflow posts as a proxy task. That is, we tune the USE model to distinguish similar Stack Overflow post titles from unrelated title pairs. This task stimulates that titles of duplicate posts are mapped to similar vectors, while vectors of random title pairs are adjusted to be more orthogonal. As such, the USE embeddings are tailored to semantic similarity computation in the software domain.</p><p>A dataset of duplicate title pairs was created as described in Section IV. For every "positive" instance, we construct five negative examples by sampling random title pairs (for which we verify that they were not tagged as each other's duplicate). Given a title pair (t 1 , t 2 ), we model the probability that t 1 and t 2 are duplicates by first computing the cosine similarity of their USE embeddings and clipping negative similarities to zero with the rectified linear unit (ReLU) activation function, see Equation 1. Next, the result is fed to a layer with weight parameter w, bias b and the logistic activation function σ, see Equation 2. We fine-tune w, b, and the parameters of USE θ to minimize binary cross-entropy. The inclusion of the ReLU activation avoids that the training objective pushes the cosine similarity between embeddings of unrelated titles to −1, as this corresponds to having a strong negative correlation between the embeddings.</p><formula xml:id="formula_2">sim(t 1 , t 2 ) = ReLU ( cos(U SE(t 1 ; θ), U SE(t 2 ; θ)) ) (1) P (duplicate = 1|t 1 , t 2 ) = σ(sim(t 1 , t 2 ) w + b))<label>(2)</label></formula><p>When w, b are initialized randomly, we found that the model sometimes degenerates to labeling every pair as unrelated. This issue is avoided by initializing w, b such that, initially, all pairs with a high cosine similarity are considered a duplicate with high probability. For our experiments, we initialized w and b with 15 and −5 respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Query-code similarity</head><p>To compute the similarity between query and code, we experimented with models whose designs are based on two recently published (unannotated) code search models: NCS <ref type="bibr" target="#b1">[2]</ref> and UNIF <ref type="bibr" target="#b2">[3]</ref>.</p><p>NCS: The architecture for the neural code search (NCS) model <ref type="bibr" target="#b1">[2]</ref> is shown in <ref type="figure" target="#fig_3">Figure 5 C)</ref>. Like the neural bag-of-words model introduced in the previous section, the query representation is computed as a sum of the query word embeddings: q ncs = m j=1 e 2 (q j ). Similarly, an embedding for the code is computed by summing the code token embeddings weighted with their respective IDF scores: c ncs = n j=1 idf (c j ) e 2 (c j ). Where e 2 (·) is the embedding model that maps code tokens and query words to their respective embeddings.</p><p>The embedding model e 2 (·) is trained with fastText on a multi-modal text corpus containing both source code and natural language code descriptions. For each of the PACS benchmarks, we train a fastText skip-gram model using the respective snippet collections to create the text corpora. In the original NCS model, each sentence in the training corpus is constructed from a different description-code snippet pair by concatenating the description words with the code tokens. We found it to be beneficial to further augment the corpora by also: 1) inserting the description words in the middle of the code tokens; and 2) appending the description words to the code tokens (see <ref type="figure" target="#fig_4">Figure 6</ref>). We also increase fastText's window size parameter to 20 whereas the original model kept fastText's default value (i.e., 5). These modifications are aimed at better capturing the relations between code tokens and description words and are inspired by work on multilingual embedding models <ref type="bibr" target="#b24">[25]</ref>. In particular, by enforcing that code tokens and description words that belong to the same snippet appear close to each other in the fastText training corpus, their representations will become more similar.</p><p>We pre-process queries by applying tokenization and stopword removal. Code snippets are tokenized with TreeSitter 13 and we retain only code identifiers (method names, imports, variable names, keyword argument names) other code identifiers <ref type="bibr" target="#b13">14</ref> , and inline comments. The code identifiers are split on camel casing and underscores.</p><p>UNIF: The UNIF <ref type="bibr" target="#b2">[3]</ref> and NCS architectures are similar, but instead of using TF-IDF weighting, UNIF computes the code token weights with a neural network. More specifically, the representation for the code c unif is calculated as a weighted sum of code token embeddings e 3 (c 1 ), e 3 (c 2 ), ... , e 3 (c m ):</p><formula xml:id="formula_3">c unif = m i w i e 3 (c i ) w i = exp(e 3 (c i ) · a) m j exp(e 3 (c j ) · a)</formula><p>Where a is a vector with the same dimensionality as the embeddings. UNIF will train a and fine-tune the description and code token embeddings by maximizing the margin between 1) the cosine distance of descriptions and corresponding code snippets; and 2) the cosine distance of descriptions and random code snippets.</p><p>Although making the bag-of-words assumption yields a rudimentary view on source code, state-of-theart results on code search are obtained under this assumption <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. We hypothesize that one reason may be that for many snippets in the collections, re-ordering function calls does not lead to a meaningful snippet with a different intent (i.e., solving a different task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ensemble</head><p>We expect that the different similarity models we presented will make different types of errors. Therefore, an ensemble model that computes query-snippet similarity as a linear combination of the fine-tuned USE and NCS model outputs should obtain better results. We note that an ensemble of cosine similarity models can be easily reformulated as a single cosine similarity model: sim(s, q) = λ 1 cos(d use , q use ) + λ 2 cos(c ncs , q ncs ) is equivalent to sim(s, q) = cos(s ens , q ens )</p><formula xml:id="formula_4">s ens =&lt; λ 1 ||d use || d use , λ 2 ||c ncs || c ncs &gt; q ens =&lt; q use , q ncs &gt;</formula><p>This implementation trick is relevant when scaling up to very large snippets collections, as it allows using approximate nearest neighbor libraries such as faiss <ref type="bibr" target="#b25">[26]</ref>. <ref type="bibr" target="#b14">15</ref> These libraries can do nearest neighbor search with billions of instances but expect that each respective instance/query is represented by a single vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL SETUP A. Baselines</head><p>To compare with classic information retrieval approaches, we set up two Okapi BM25 retrieval models using the rank-bm25 library. <ref type="bibr" target="#b15">16</ref> Okapi BM25 is a popular bag-of-words ranking function based on exact keyword matching. BM25 code indexes the code, and serves as a baseline for the neural code models. It uses the same pre-processing as NCS and UNIF, except that we replace each query and code token by its lemma. BM25 descr indexes the code descriptions, and serves as a baseline for the neural bag-of-words and  , recall@3 (r@3) and recall@10 (r@10) are expressed as percentages. NCS original* and UNIF * are the results obtained with our implementations of the NCS/UNIF models as described in <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b2">[3]</ref>, respectively (the original models were not released).</p><p>universal sentence encoder models. We also report the results of the original NCS model (i.e., without our proposed adaptations) which we reproduced from <ref type="bibr" target="#b1">[2]</ref> (to our knowledge, the model is not publicly available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyper-parameters</head><p>For training the embeddings of the neural bag-of-words model, we kept fastText's default parameters except for the number of epochs, which we increased to 300. The USE fine-tuning was implemented in TensorFlow <ref type="bibr" target="#b26">[27]</ref> using stochastic gradient descent (SGD) with mini-batches of size 512, a learning rate of 1e-4, and Adam optimization <ref type="bibr" target="#b27">[28]</ref>. We fine-tuned USE for 15 epochs, while tracking the retrieval performance on the SO-DS validation set every 51, 200 examples, after which we retained the best performing snapshot to evaluate on the test set. The NCS embeddings (our version) were trained for 30 epochs, with window size 20, and without filtering out infrequent words. Other parameters were kept at fastText's default setting. The original NCS model did not change fastText's defaults. The UNIF model was trained for 20 epochs, using SGD with mini-batches of size 32, a learning rate of 1e-4, and Adam optimization <ref type="bibr" target="#b27">[28]</ref>. For StaQC-py and SO-DS, we track retrieval performance on the respective validation sets every epoch and keep the best performing checkpoint for evaluation on the test set. For CoNaLa, we do not have a validation set so we keep the model after 20 epochs. The λ weights for the ensemble model were tuned on the SO-DS validation set and were set to 0.5 for NCS and 1 for USE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation metrics</head><p>As discussed in Section III, we use mean reciprocal rank (MRR) and recall@k (r@k) to measure snippet retrieval performance. We cut-off the search result list at 10 snippets when computing MRR as users will rarely look beyond the first 10 results. For recall@k, we set k = 3 and k = 10, measuring the percentage of queries for which at least one matching snippet was found in the top-3 and top-10 results respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RESULTS &amp; DISCUSSION</head><p>The performance of the various models on our annotated code search benchmarks is listed in <ref type="table" target="#tab_1">Table IV</ref>. The results are grouped by the information that the models index: the source code, the description, or both. As expected, we find that leveraging code descriptions significantly boosts retrieval performance. The big   <ref type="figure" target="#fig_4">Figure 6</ref>).</p><p>gap between the best code-only and best description-only models confirms that retrieving snippets on the source code alone remains a very challenging task. This is likely due to the fact that the vocabulary of code tokens and description tokens is quite disjoint <ref type="bibr" target="#b1">[2]</ref>. We also observe that our NCS variant and UNIF outperform classic keyword matching. The modifications to NCS that we proposed in Section V-B improve performance over the original model across all three benchmarks, and achieve results on par with or better than UNIF. <ref type="table" target="#tab_9">Table V</ref> reports an ablation study of the proposed adjustments. It demonstrates that fastText hyper-parameter tuning, the more inclusive code pre-processing (i.e., retaining all code identifiers), and the fastText training corpus augmentation illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>, all contribute to this performance increase.</p><p>In an additional experiment, we verified whether using more training data for NCS/UNIF could help close the gap with the description-only models. To this end, we constructed a superset of the SO-DS snippet collection by mining annotated snippets with the procedure described in Section IV, except that we retain 1,000 instead of 250 snippets per tag. Somewhat surprisingly, we found that having four times more training data did not improve the performance for NCS and UNIF.</p><p>When comparing the description-only baselines, we find that on CoNaLA classic keyword matching (i.e., BM25) outperforms the neural baselines (i.e., neural bag-of-words model and USE without fine-tuning), whereas on StaQC-py and SO-DS the inverse is true. This indicates that for StaQC-py and SO-DS there tends to be more (accidental) keyword overlap between queries and descriptions of irrelevant snippets. This is to be expected given that the StaQC-py and SO-DS snippet collections are considerably larger than the CoNaLa collection.</p><p>Another important finding is that, across all three benchmarks, the proposed fine-tuning recipe boosts USE's performance significantly. The fine-tuned USE models outperform all other models with large margins. The experiments also validate the benefit of introducing the ReLU activation before the output layer as it consistently leads to the best results. Given these large improvements, we also considered finetuning other pre-trained transformer models, We conducted preliminary experiments with sentence-BERT models <ref type="bibr" target="#b28">[29]</ref>, which outperformed USE on semantic similarity benchmarks, and GPT-2 <ref type="bibr" target="#b15">[16]</ref>. Even after fine-tuning these models were unable to beat the baselines. For the sentence-BERT models, we attribute this to the mismatch between the domain of the pre-training corpora and the software development domain. For the GPT-2 model, on the other hand, we found that the cosine similarities between hidden states of unrelated sequences are particularly high, a phenomenon that is studied in Ethayarajh <ref type="bibr" target="#b29">[30]</ref>. This makes Q: export plot to png Q: regex to find urls D: Save plot to image file instead of displaying it using matplotlib D: Regular expression to extract url from an html link C:   <ref type="table" target="#tab_1">Table IV</ref> is that the ensemble model, which combines USE tuned+ReLU with NCS ours , yields the best results. This indicates that the incorporation of the source code is still beneficial. To demonstrate that the ensemble model can answer non-trivial queries, <ref type="table" target="#tab_1">Table VI</ref> reports a top-3 result from the SO-DS collection for four non-trivial example queries. Although all queries have at most one word in common with the description or code, the results capture the query intent very well. Interestingly, the result for the query check that tf uses gpu also uncovers a limitation of current code search systems. The code snippet is relevant but outdated as it will not work in the latest versions of the TensorFlow library. Dealing with the evolution of software libraries and tools in a code search engine is an interesting challenge for future research.</p><p>To further improve code search, we see the most potential in learning better code representations. An open question is whether the transfer learning techniques that have pushed the state of the art in natural language processing, can yield similar improvements when leveraged for code representation learning. One could question to what extent the distributional hypothesis (i.e., the idea that words that occur in the same contexts tend to have similar meanings <ref type="bibr" target="#b30">[31]</ref>) is as powerful for source code. Because software developers are trained to avoid code duplication (by refactoring repeated code fragments into functions, methods, modules, etc.), what can be learned from token co-occurrence is inherently more limited, due to the sparsity of patterns. This raises the question of whether we should not incorporate other modalities, such as runtime information, when learning code representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Threats to validity</head><p>Stack Overflow titles as queries: Similar to previous code search studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>, ground truth queries are compiled from Stack Overflow post titles. While the resulting evaluation and test sets are a good reflection of the types of questions that programmers have, Stack Overflow titles can be more verbose than actual code search queries.</p><p>Training data: Due to the inherent differences between their architectures, NCS/UNIF and USE can not be trained on the same data. Therefore the results are sensitive to the training data we chose for the respective models. As in this work we were mainly concerned with investigating the benefit of code descriptions, we aimed to optimize the training data for the code-only models. We verified if the performance of these models improved when training them with more in-domain code snippet-description pairs but this was not the case.</p><p>Interpreting performance: Our approach to construct the respective ground truth datasets does not yield an exhaustive set of all the relevant snippets for a given query. As such, the performance metrics in this paper are only meaningful to compare the performance of models relative to each other, not to measure absolute performance. For instance, a recall@10 score of 50% does not reflect that the model retrieves relevant snippets in the top 10 for (only) half of the queries.</p><p>Snippet collection: The choice of the snippet collection unavoidably impacts the results. For instance, one could argue that the lack of static types makes learning code representations for Python more challenging compared to languages like Java. Given the considerable performance gaps between the codeonly and description-only models, across three different snippet collections, we expect our conclusions to remain valid on other collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>As the number of software libraries and tools increases, so does the need for effective code search tools to identify relevant example code. Most work on code search considers the problem of retrieving unannotated code snippets based on natural language queries. In this work, we considered the problem of retrieving code snippets annotated with a brief description of their intent, which is usually available for code that is part of tutorials, cheat sheets, textbooks, notebooks, and other forms of code documentation.</p><p>We showed that by leveraging code descriptions and by applying recent advances in natural language processing, we can build retrieval models that produce far more relevant search results compared to models that solely use source code. In particular, we described how we can fine-tune the universal sentence encoder for code search, showed how it can be effectively combined with a new variant of the neural code search model, and evaluated the models on three new code search benchmarks. On these benchmarks, we significantly outperformed state-of-the-art code search models with absolute gains up to 20.6%, 23.9%, and 26.4% in mean reciprocal rank (MRR), recall@3, and recall@10.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of the additional cleaning for the StaQC corpus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>To create the StaQC-py and SO-DS ground truth we make use of Stack Overflow posts that were tagged as duplicates by users. b) StaQC-py: StaQC<ref type="bibr" target="#b18">[19]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Histograms of the relative word overlap between the queries and the matching snippet descriptions in the PACS test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Architectures of query/snippet similarity models presented in Section V: A) depicts the neural bag-of-words model of queries and snippet descriptions; B) depicts the query and snippet description embedding using USE; C) depicts the NCS architecture for embedding queries and actual code; and D) depicts an ensemble of embedding retrieval models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Illustration of how sentences are created from description-code snippet pairs to serve as input for training fastText embeddings. If we only append the code tokens to the description words (top right) as is done in the original NCS model, hist and histogram appear far apart. By creating the other contexts (middle and bottom right) they appear in closer proximity, causing fastText to push their embeddings closer in the vector space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>&lt;a href="http://www.ptop.se" target=" _blank"&gt;http://www.ptop.se&lt;/a&gt;' r = re.compile('(?&lt;=href="). * ?(?=")') r.findall(url) S: https://stackoverflow.com/questions/9622163 S: https://stackoverflow.com/questions/499345 Q: create df from json lines Q: check that tf uses gpu D: Loading a file with more than one line of json into pandas D: Tell if tensorflow is using gpu acceleration from inside python shell C: import pandas as pd data = pd.read_json( '/path/to/file.json', lines=True) C: sess = tf.Session(config=tf.ConfigProto( log_device_placement=True)) S: https://stackoverflow.com/questions/30088006 S: https://stackoverflow.com/questions/38009682</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Overview of the Python annotated code search (PACS) benchmarks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Examples from the SO-DS snippet collection.Python functions call by referenceIs it possible to make a function that modifies the input Scrape / eavesdrop AJAX data using Javascript?Intercept AJAX responses in Chrome Extension How to write conditional import statements in QML? QML import later module version only if available Boost.Asio SSL thread safety Boost Asio and usage of OpenSSL in multithreaded app Is it possible to stop Javascript execution?System.exit in javascript</figDesc><table><row><cell>Original</cell><cell>Duplicate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc></figDesc><table /><note>Examples of Stack Overflow post titles that were tagged as duplicates.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table III shows a few examples from the dataset. V. MODELS FOR ANNOTATED CODE SEARCH In line with recent work in code search</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Results on the Python annotated code search (PACS) benchmark test sets. Mean reciprocal rank (MRR)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Ablation study of the changes to the original NCS model: NCS original*+x corresponds to the original model with one of our modifications, and NCS ours-x corresponds to our NCS variant without one of our modifications. contextAugment refers to the method we propose for generating the fastText input data (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Example queries (Q) with one of the top-3 results (a description D, a code snippet C, and a url to the source S) from the SO-DS collection) returned by the ensemble model. the model a poor fit for our fine-tuning procedure.</figDesc><table /><note>A final observation from</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/nokia/codesearch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://gluebenchmark.com/leaderboard</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset 7 Monthly dumps of Stack Overflow can be found here: https://archive.org/details/stackexchange</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We check against a manually curated set of tags that can be consulted here: https://github.com/nokia/codesearch/tree/master/nbs/datasets/ so-ds-tags.txt<ref type="bibr" target="#b8">9</ref> We focus on the data science domain because it provides a rich, rapidly evolving open source library ecosystem. Data scientists often need a combination of libraries for data crawling and cleaning, model training and validation, and data visualization needs. As such it is an area of software development where code search tools can add significant value.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Recall that in order to create the CoNaLa ground truth, we discarded all queries for which the relative overlap with the descriptions of their matching snippets was more than 0.5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Cer et al.<ref type="bibr" target="#b13">[14]</ref> also experimented with a simpler architecture that is also referred to as USE. In this work, we always mean the transformer variant when referring to USE, which obtained the highest scores.<ref type="bibr" target="#b11">12</ref> https://tfhub.dev/google/universal-sentence-encoder/4</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">https://github.com/tree-sitter/tree-sitter<ref type="bibr" target="#b13">14</ref> The original NCS model did not keep variable and keyword argument names but we found that including these improves results on our benchmarks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">https://github.com/facebookresearch/faiss 16 https://pypi.org/project/rank-bm25/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="933" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retrieval on source code: a neural code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sachdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">When deep learning met code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cambronero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="964" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">CodeSearchNet Challenge: Evaluating the State of Semantic Code Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CoaCor: code annotation for code retrieval with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Peddamail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2203" to="2214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<title level="m">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380295</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380295" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020, ser. WWW &apos;20</title>
		<meeting>The Web Conference 2020, ser. WWW &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2309" to="2319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pre-trained Contextual Embedding of Source Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<ptr target="https://arxiv.org/pdf/1910.10683.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applying BERT to Document Retrieval with Birch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-3004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
	<note>System Demonstrations. Hong Kong</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N18-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Universal sentence encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<ptr target="https://arxiv.org/pdf/1803.11175.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<ptr target="https://arxiv.org/pdf/1907.11692.pdf" />
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Mining Software Repositories, ser. MSR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="476" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1693" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Comprehensive Study of StaQC for Deep Code Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peddamail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Lond, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR abs/1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skipthought Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient natural language response suggestion for smart reply</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lukács</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miklos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00652</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Potts</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bilingual Word Embeddings from Comparable Data with Application to Bilingual Lexicon Extraction and Word Translation Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artifical Intelligence Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ethayarajh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Distributional structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
