<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TextBoxes++: A Single-Shot Oriented Scene Text Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Xiang</forename><forename type="middle">Bai</forename></persName>
						</author>
						<title level="a" type="main">TextBoxes++: A Single-Shot Oriented Scene Text Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Scene text detection</term>
					<term>multi-oriented text</term>
					<term>word spotting</term>
					<term>scene text recognition</term>
					<term>convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scene text detection is an important step of scene text recognition system and also a challenging problem. Different from general object detection, the main challenges of scene text detection lie on arbitrary orientations, small sizes, and significantly variant aspect ratios of text in natural images. In this paper, we present an end-to-end trainable fast scene text detector, named TextBoxes++, which detects arbitrary-oriented scene text with both high accuracy and efficiency in a single network forward pass. No post-processing other than an efficient non-maximum suppression is involved. We have evaluated the proposed TextBoxes++ on four public datasets. In all experiments, TextBoxes++ outperforms competing methods in terms of text localization accuracy and runtime. More specifically, TextBoxes++ achieves an f-measure of 0.817 at 11.6fps for 1024×1024 ICDAR 2015 Incidental text images, and an f-measure of 0.5591 at 19.8fps for 768×768 COCO-Text images. Furthermore, combined with a text recognizer, TextBoxes++ significantly outperforms the stateof-the-art approaches for word spotting and end-to-end text recognition tasks on popular benchmarks. Code is available at: https://github.com/MhLiao/TextBoxes plusplus.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Scene text is one of the most general visual objects in natural scenes, which frequently appears on road signs, license plates, product packages, etc. Reading scene text facilitates a lot of useful applications, such as image-based geolocation. Some applications of scene text detection and recognition are <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. Despite the similarity to traditional OCR, scene text reading is much more challenging due to large variations in both foreground text and background objects, arbitrary orientations, aspect ratios, as well as uncontrollable lighting conditions, etc., as summarized in <ref type="bibr" target="#b4">[5]</ref>. Owing to these inevitable challenges and complexities, traditional text detection methods tend to involve multiple processing steps, e.g. character/word candidate generation <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>, candidate filtering <ref type="bibr" target="#b7">[8]</ref>, and grouping <ref type="bibr" target="#b8">[9]</ref>. They often end up struggling to get each module work properly, requiring much effort in tuning parameters and designing heuristic rules, also slowing down detection speed.</p><p>Inspired by the recent developments in object detection <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, we propose to detect text by directly predicting word bounding boxes with quadrilaterals via a single neural network that is end-to-end trainable. We call it TextBoxes++. Concretely, we replace the rectangular box representation in conventional object detector by a quadrilateral or oriented rectangle representation. Furthermore, to achieve better receptive field that covers text regions which are usually long, we design some "long" convolutional kernels to predict the bounding boxes. TextBoxes++ directly outputs word bounding boxes at multiple layers by jointly predicting text presence and coordinate offsets to anchor boxes <ref type="bibr" target="#b10">[11]</ref>, also known as default boxes <ref type="bibr" target="#b9">[10]</ref>. The final outputs are the non-maximum suppression outputs on all boxes. A single forward pass in the network densely detects multi-scale text boxes all over the image. As a result, the detector has a great advantage in speed. We will show by experiments that TextBoxes++ achieves both high accuracy and high speed with a single forward pass on single-scale inputs, and even higher accuracy when performing multiple passes on multi-scale inputs. Some text detection examples on several challenging images are depicted in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>We further combine TextBoxes++ with CRNN <ref type="bibr" target="#b11">[12]</ref>, an open-source text recognition module. The recognizer not only produces extra recognition outputs but also regularizes text detection with its semantic-level awareness, thus further boosts the accuracy of word spotting considerably. The combination of TextBoxes++ and CRNN yields state-of-the-art performance on both word spotting and end-to-end text recognition tasks, which appears to be a simple yet effective solution to robust text reading in the wild.</p><p>A preliminary version of this study called TextBoxes was exposed in <ref type="bibr" target="#b12">[13]</ref>. The current paper is an extension of <ref type="bibr" target="#b12">[13]</ref>, which extends TextBoxes with four main improvements: 1) We extend TextBoxes, a horizontal text detector, to a detector that can handle arbitrary-oriented text; 2) We revisit and improve the network structure and the training process, which leads to a further performance boost; 3) More comparative experiments have been conducted to further demonstrate the efficiency of TextBoxes++ in detecting arbitrary-oriented text in natural images; 4) We refine the combination of detection and recognition by proposing a novel score which elegantly utilizes both the information of detection and recognition.</p><p>The main contributions of this paper are three folded: 1) The proposed arbitrary-oriented text detector TextBoxes++ is accurate, fast, and end-to-end trainable. As compared to closely related methods, TextBoxes++ has a much simpler yet effective pipeline. 2) This paper also offers some comparative studies on some important design choices and other issues, including bounding box representations, model configurations, and the effect of different evaluation methods. The conclusions of these studies may generalize to other text reading algorithms and give insights on some important issues. 3) We also introduce the idea of using recognition results to further refine the detection results thanks to the semantic-level awareness of recognized text. To the best of our knowledge, this intuitive yet effective combination has not been exploited before.</p><p>The rest of the paper is organized as follows. Section II briefly reviews some related works on object detection and scene text detection. The proposed method is detailed in Section III. We present in Section IV some experimental results. A detailed comparison with some closely related methods is given in Section V. Finally, conclusions are drawn in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object detection</head><p>Object detection is a fundamental problem in computer vision, which aims to detect general objects in images. Recently, there are two mainstream CNN-based methods on this topic: R-CNN based methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> and YOLO-based methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>. a) R-CNN based object detection: R-CNN <ref type="bibr" target="#b13">[14]</ref> views a detection problem as a classification problem leveraging the development of classification using convolutional neural networks(CNN). It first generates proposals by selective search <ref type="bibr" target="#b16">[17]</ref> and then feeds the proposals into a CNN to extract deep features based on which an SVM <ref type="bibr" target="#b17">[18]</ref> is applied for classification. Fast R-CNN <ref type="bibr" target="#b14">[15]</ref> improves R-CNN by extracting deep features of the proposals from the feature maps via RoI pooling <ref type="bibr" target="#b14">[15]</ref> instead of cropping from the origin image. This significantly simplifies the pipeline of R-CNN. Furthermore, a regression branch is also used in fast R-CNN to get more accurate bounding boxes. Faster R-CNN <ref type="bibr" target="#b10">[11]</ref> further improves the speed of fast R-CNN <ref type="bibr" target="#b14">[15]</ref> by introducing an end-to-end trainable region proposal network based on anchor boxes to generate proposals instead of using selective search. The generated proposals are then fed into a Fast R-CNN network for detection. b) YOLO-based object detection:</p><p>The original YOLO <ref type="bibr" target="#b15">[16]</ref> directly regresses the bounding boxes on the feature maps of the whole image. A convolutional layer is used to predict the bounding boxes on different areas instead of the RPN and RoI pooling used in <ref type="bibr" target="#b10">[11]</ref>. This results in a significantly reduced runtime. Another YOLO-based method is SSD <ref type="bibr" target="#b9">[10]</ref> which predicts object bounding boxes using default boxes of different aspect ratios on different stages. The concept of default boxes <ref type="bibr" target="#b9">[10]</ref> is similar to anchor boxes <ref type="bibr" target="#b10">[11]</ref>, which are fixed as the reference systems of the corresponding ground truths. The translation and scale invariances for regression are achieved by using default boxes of different aspect ratios and scales at each location, which eases the regression problem. SSD further improves the performance of original YOLO while maintaining its runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Text detection</head><p>A scene text reading system is usually composed of two main components: text detection and text recognition. The former component localizes text in images mostly in the form of word bounding boxes. The latter one transcripts cropped word images into machine-interpretable character sequences. In this paper, we cover both aspects, but more attention is paid to detection. In general, most text detectors can be roughly classified into several categories following two classification strategies based on primitive detection targets and shape of target bounding boxes, respectively. a) Classification strategy based on primitive detection targets: Most text detection methods can be roughly categorized into three categories: 1) Character-based: Individual characters or parts of the text are first detected and then grouped into words <ref type="bibr" target="#b6">[7]</ref>. A representative example is the method proposed in <ref type="bibr" target="#b6">[7]</ref> which locates characters by classifying Extremal Regions and then groups the detected characters by an exhaustive search method.</p><p>Other popular examples of this type are the works in <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b22">[23]</ref>; 2) Word-based: Words are directly extracted in the similar manner as general object detection <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. In the representative work <ref type="bibr" target="#b7">[8]</ref>, the authors propose an R-CNN-based <ref type="bibr" target="#b13">[14]</ref> framework, where word candidates are first generated with class-agnostic proposal generators followed by a random forest classifier. Then a convolutional neural network for bounding box regression is adopted to refine the bounding boxes. YOLO network <ref type="bibr" target="#b15">[16]</ref> is used in <ref type="bibr" target="#b26">[27]</ref> which also relies on a classifier and a regression step to remove some false positives; 3) Text-line-based: Text lines are detected and then broken into words. The works in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref> are such examples. In <ref type="bibr" target="#b27">[28]</ref>, the authors propose to detect text lines making use of the symmetric characteristics of text. This idea is further exploited in <ref type="bibr" target="#b28">[29]</ref> by using a fully convolutional network to localize text lines.</p><p>b) Classification strategy based on the shape of target bounding boxes: Following this classification strategy, the text detection methods can be categorized into two categories: 1) Horizontal or nearly horizontal: These methods focus on detecting horizontal or nearly horizontal text in images. An algorithm based on AdaBoost is proposed in <ref type="bibr" target="#b30">[31]</ref>. Then, Yi et al. <ref type="bibr" target="#b31">[32]</ref> propose a 3-stage framework which consists of boundary clustering, stroke segmentation, and string fragment classification. Some examples which are inspired by object detection methods are <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b26">[27]</ref>. They all use horizontal rectangle bounding boxes as predict targets, which is very similar to general object detection methods. Another popular method of this type is the work in <ref type="bibr" target="#b32">[33]</ref> which detects nearly horizontal text parts and then links them together to form word candidates. Besides, Cao et al. <ref type="bibr" target="#b33">[34]</ref> try to use deblurring techniques for more robust detection results.</p><p>2) Multi-oriented: As compared to horizontal or nearly horizontal detection, multi-oriented text detection is more robust because scene text can be in arbitrary orientations in images. There exist several works which attempt to detect multioriented text in images. In <ref type="bibr" target="#b18">[19]</ref>, the authors propose two sets of rotation-invariant features for detecting multi-oriented text. The first set is component level features such as estimated center, scale, direction before feature computation. The second one is chain level features such as size variation, color self-similarity, and structure self-similarity. Kang et al. <ref type="bibr" target="#b34">[35]</ref> build a graph of MSERs <ref type="bibr" target="#b35">[36]</ref> followed by a higher-order correlation clustering to generate multi-oriented candidates. A unified framework for multi-oriented text detection and recognition is proposed in <ref type="bibr" target="#b36">[37]</ref>. They use the same features for text detection and recognition. Finally, a texton-based texture classifier is used to discriminate text and no-text candidates. In <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>, multi-oriented text bounding boxes are generated from text saliency maps given by a dedicated segmentation network. Recently, Shi et al. <ref type="bibr" target="#b38">[39]</ref> propose to detect text with segments and links. More precisely, they first detect a number of text parts named segments and meanwhile predict the linking relationships among neighboring segments. Then related segments are linked together to form text bounding boxes. A U-shape fully convolutional network is used in <ref type="bibr" target="#b39">[40]</ref> for detecting multi-oriented text. In this work, the authors also introduce a PVANet <ref type="bibr" target="#b40">[41]</ref> for efficiency. Quadrilateral sliding windows, a Monte-Carlo method, and a smooth Ln loss are proposed in <ref type="bibr" target="#b41">[42]</ref> to detect oriented text, which is effective while complicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. TextBoxes++ Versus some related works</head><p>TextBoxes++ is a word-based and multi-oriented text detector. In contrast to <ref type="bibr" target="#b7">[8]</ref>, which consists of three detection steps and each step includes more than one algorithm, TextBoxes++ has a much simpler pipeline. Only an end-to-end training of one network is required. As described in Section II-B, Tian et al. <ref type="bibr" target="#b32">[33]</ref> and Shi et al. <ref type="bibr" target="#b38">[39]</ref> propose to detect text parts and then link them together. Both of them achieve impressive results for long words. However, the proposed method in <ref type="bibr" target="#b32">[33]</ref> has limited adaptability for oriented text due to the single orientation of the Bidirectional Long Short-Term Memory (BLSTM) <ref type="bibr" target="#b42">[43]</ref>, and the work in <ref type="bibr" target="#b38">[39]</ref> has two super parameters for linking the segments, which are determined by grid search and difficult to adjust. The method in <ref type="bibr" target="#b39">[40]</ref> is considered as the current state-of-the-art approach. It relies on a U-shape network to simultaneously generate a score map for text segmentation and the bounding boxes. Yet, an accurate text region segmentation is challenging in itself. Besides, The extra pyramid-like deconvolutional layers involved in text region segmentation require additional computation. Whereas, TextBoxes++ directly classifies and regresses the default boxes on the convolutional feature maps, where richer information is reserved as compared to the segmentation score map. TextBoxes++ is thus much simpler, avoiding the time consuming on pyramid-like deconvolution.</p><p>One of the most related work to TextBoxes++ is SSD <ref type="bibr" target="#b9">[10]</ref>, a recent development in object detection. Indeed, TextBoxes++ is inspired by SSD <ref type="bibr" target="#b9">[10]</ref>. The original SSD aims to detect general objects in images but fails on words having extreme aspect ratios. TextBoxes++ relies on specifically designed textbox layers to efficiently solve this problem. This results in a significant performance improvement. Furthermore, SSD can only generate bounding boxes in terms of horizontal rectangles, while TextBoxes++ can generate arbitrarily oriented bounding boxes in terms of oriented rectangles or general quadrilaterals to deal with oriented text.</p><p>Another closely related work to TextBoxes++ is the method in <ref type="bibr" target="#b41">[42]</ref>, which proposes quadrilateral sliding windows and a Monte-Carlo method for detecting oriented text. In TextBoxes++, we use horizontal rectangles as default boxes, and hence have much fewer default boxes in every region. Benefiting from the horizontal rectangle default boxes, TextBoxes++ also enjoys a much simpler strategy for matching default boxes. Moreover, TextBoxes++ simultaneously regresses the maximum horizontal rectangles of the bounding boxes and the quadrilateral bounding boxes, which makes the training more stable. Furthermore, we propose a novel score by combining the detection and recognition scores to further refine the detection results.</p><p>The ultimate goal of text detection is to spot words or recognize text in images. On the other hand, the semantic-level awareness of spotted words or recognized words can also help to further regularize text detection results. Following this idea, we propose to combine a text recognizer with TextBoxes++ for word spotting and end-to-end recognition, and use the confidence scores of recognized words to regularize the detection outputs of TextBoxes++. For that, we adopt a stateof-the-art text recognizer called CRNN <ref type="bibr" target="#b11">[12]</ref>, which directly outputs character sequences given input images and is also end-to-end trainable. Other text recognizers such as <ref type="bibr" target="#b7">[8]</ref> are also applicable. Such simple pipeline for word spotting and end-toend recognition is very different from the classical pipelines. A non-maximum suppression is applied during test phase to merge the results of all 6 text-box layers. Note that "#c" stands for the number of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DETECTING ORIENTED TEXT WITH TEXTBOXES++</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>TextBoxes++ relies on an end-to-end trainable fully convolutional neural network to detect arbitrary-oriented text. The basic idea is inspired by an object detection algorithm SSD proposed in <ref type="bibr" target="#b9">[10]</ref>. We propose some special designs for adapting SSD network to efficiently detect oriented text in natural images. More specifically, we propose to represent arbitrary-oriented text by quadrilaterals <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b43">[44]</ref> or oriented rectangles <ref type="bibr" target="#b44">[45]</ref>. Then we adapt the network to predict the regressions from default boxes to oriented text represented by quadrilaterals or oriented rectangles. To better cover the text which could be dense in some area, we propose to densify default boxes with vertical offsets. Furthermore, we adapt the convolution kernels to better handle text lines which are usually long objects as compared to general object detection. These network adaptions are detailed in Section III-B. Some specific training adaptions for arbitrary-oriented text detection are presented in Section III-C, including on-line hard negative mining and data augmentation by a novel random cropping strategy specifically designed for text which is usually small. TextBoxes++ detects oriented text at 6 different scales in 6 stages. During the test phase, the multi-stage detection results are merged together by an efficient cascaded nonmaximum suppression based on IOU threshold of quadrilaterals or oriented rectangles (see Section III-D). Finally, we also propose an intuitive yet effective idea of using text recognition to further refine detection results thanks to the semantic-level awareness of recognized text. This is discussed in Section III-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed network</head><p>1) Network architecture: The architecture of TextBoxes++ is depicted in <ref type="figure" target="#fig_2">Fig. 2</ref>. It inherits the popular VGG-16 architecture <ref type="bibr" target="#b45">[46]</ref>, keeping the layers from conv1_1 through conv5_3, and converting the last two fully-connected layers of VGG-16 into convolutional layers (conv6 and conv7) by parameters down-sampling <ref type="bibr" target="#b9">[10]</ref>. Another eight convolutional layers divided into four stages (conv8 to conv11) with different resolutions by max-pooling are appended after conv7. Multiple output layers, which we call text-box layers, are inserted after the last and some intermediate convolutional layers. They are also convolutional layers to predict outputs for aggregation and then undergo an efficient non-maximum suppression (NMS) process. Putting all above together, TextBoxes++ is a fully-convolutional structure consisting of only convolutional and pooling layers. As a result, TextBoxes++ can adapt to images of arbitrary size in both training and testing phases. Compared with a preliminary study in <ref type="bibr" target="#b12">[13]</ref> of this paper, TextBoxes++ replaces the last global average pooling layer with a convolutional layer, which is furthermore beneficial for multi-scale training and testing.</p><p>2) Default boxes with vertical offsets: Text-box layers are the key component of TextBoxes++. A text-box layer simultaneously predicts text presence and bounding boxes, conditioned on its input feature maps. The output bounding boxes of TextBoxes++ include oriented bounding boxes {q} or {r}, and minimum horizontal bounding rectangles {b} containing the corresponding oriented bounding boxes. This is achieved by predicting the regression of offsets from a number of pre-designed horizontal default boxes at each location (see <ref type="figure" target="#fig_3">Fig. 3</ref> for an example). More precisely, let b 0 = (x 0 , y 0 , w 0 , h 0 ) denote a horizontal default box, which can also be written as q 0 = (x q 01 , y q 01 , x q 02 , y q 02 , x q 03 , y q 03 , x q 04 , y q 04 ) or r 0 = (x r 01 , y r 01 , x r 02 , y r 02 , h r 0 ), where (x 0 , y 0 ) means the center point of a default box and w 0 and h 0 are the width and height of a default box respectively. The relationships among q 0 , r 0 and b 0 are as following:</p><formula xml:id="formula_0">x q 01 = x 0 − w 0 /2, y q 01 = y 0 − h 0 /2, x q 02 = x 0 + w 0 /2, y q 02 = y 0 − h 0 /2, x q 03 = x 0 + w 0 /2, y q 03 = y 0 + h 0 /2, x q 04 = x 0 − w 0 /2, y q 04 = y 0 + h 0 /2, x r 01 = x 0 − w 0 /2, y r 01 = y 0 − h 0 /2, x r 02 = x 0 + w 0 /2, y r 02 = y 0 − h 0 /2, h r 0 = h 0 .<label>(1)</label></formula><p>At each map location, it outputs the classification score and offsets to each associated default box denoted as q 0 or r 0 in a convolutional manner. For the quadrilateral representation of oriented text, the text-box layers predict the values of (∆x, ∆y, ∆w, ∆h, ∆x 1 ,</p><formula xml:id="formula_1">∆y 1 , ∆x 2 , ∆y 2 , ∆x 3 , ∆y 3 , ∆x 4 , c), indicating that a horizontal rectangle b = (x, y, w, h) and a quadrilateral q = (x q 1 , y q 1 , x q 2 , y q 2 , x q 3 , y q 3 , x q 4 , y q 4 )</formula><p>given in the following are detected with confidence c:</p><formula xml:id="formula_2">x = x 0 + w 0 ∆x, y = y 0 + h 0 ∆y, w = w 0 exp(∆w), h = h 0 exp(∆h), x q n = x q 0n + w 0 ∆x q n , n = 1, 2, 3, 4 y q n = y q 0n + h 0 ∆y q n , n = 1, 2, 3, 4.<label>(2)</label></formula><p>When the representation by rotated rectangles is used, the text-box layers predict the value of (∆x, ∆y, ∆w, ∆h, ∆x 1 , ∆y 1 , ∆x 2 , ∆y 2 , ∆h r , c), and the rotated rectangle r = (x r 1 , y r 1 , x r 2 , y r 2 , h r ) is calculated as following:</p><p>x r n = x r 0n + w 0 ∆x r n , n = 1, 2 y r n = y r 0n + h 0 ∆y r n , n = 1, 2 h r = h r 0 exp(∆h r ).</p><p>(</p><p>In the training phase, ground-truth word boxes are matched to default boxes according to box overlap following the matching scheme in <ref type="bibr" target="#b9">[10]</ref>. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the minimum bounding horizontal rectangle of a rotated rectangle or a quadrilateral is used to match the default boxes for efficiency. Note that there are a number of default boxes with different aspect ratios at each location. In this way, we effectively divide words by their aspect ratios, allowing TextBoxes++ to learn specific regression and classification weights that handle words of similar aspect ratio. Therefore, the design of default boxes is highly task-specific.</p><p>Different from general objects, words tend to have large aspect ratios. Therefore, the preliminary study TextBoxes in <ref type="bibr" target="#b12">[13]</ref> include "long" default boxes that have large aspect ratios. Specifically, for the horizontal text dataset, we defined 6 aspect ratios for default boxes, including 1,2,3,5,7, and 10. However, TextBoxes++ aims to detect arbitrary-oriented text. Consequently, we set the aspect ratios of default boxes to 1, 2, 3, 5, 1/2, 1/3, 1/5. Furthermore, text is usually dense on a certain area, so each default box is set with a vertical offset to better cover all text, which makes the default boxes dense in the vertical orientation. An example is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. In   3) convolutional layers: In the preliminary study of this paper for horizontal text detection in <ref type="bibr" target="#b12">[13]</ref>, we have adopted irregular 1 × 5 convolutional filters instead of the standard 3×3 ones in the text-box layers. This is because that words or text lines in natural images are usually long objects. However, these long convolutional filters are not appropriate for oriented text. Instead, we use 3 × 5 convolutional filters for oriented text. These inception-style <ref type="bibr" target="#b46">[47]</ref> filters use rectangular receptive fields, which better fit words with larger aspect ratios. The noise signals that a square-shaped receptive field would bring in are also avoided thanks to these inceptions.</p><p>C. Adapted training for arbitrary-oriented text detection 1) Ground Truth representation: For the two target box representations described in Section III-B, we adapt the ground truth representation of oriented text as following: a) Quadrilateral: : For each oriented text ground truth</p><formula xml:id="formula_4">T , let G b = (x b 0 ,ỹ b 0 ,w b 0 ,h b 0 ) be its horizontal rectangle ground truth (i.e., the minimum horizontal rectangle enclosing T ), where (x b 0 ,ỹ b 0 ) is the center of G b ,w b 0 andh b 0</formula><p>are the width and the height of G b respectively. This rectangle ground truth can also be denoted as</p><formula xml:id="formula_5">G b = (b 1 , b 2 , b 3 , b 4 ) following Eq. (1), where (b 1 , b 2 , b 3 , b 4 )</formula><p>are the four vertices in clockwise order of G b with b 1 the top-left one. We use the four vertices of the oriented text ground truth T to represent T in terms of a general quadrilateral denoted by</p><formula xml:id="formula_6">G q = (q 1 , q 2 , q 3 , q 4 ) = (x q 1 ,ỹ q 1 ,x q 2 ,ỹ q 2 ,x q 3 ,ỹ q 3 ,x q 4 ,ỹ q 4 )</formula><p>. The four vertices (q 1 , q 2 , q 3 , q 4 ) are also organized in clockwise order such that the sum of Euclidean distances between four point pairs (b i , q i ), i = 1, 2, 3, 4 is minimum. More precisely, let (q 1 , q 2 , q 3 , q 4 ) in clockwise order represent the same quadrilateral G q with q 1 being the top point (top-left point in case of G q being a rectangle). Then the relationship between q and q is given by:</p><formula xml:id="formula_7">d i,∆ = d E (b i , q (i+∆−1)%4+1 ), ∆ = 0, 1, 2, 3 ∆ m = arg min ∆ (d 1,∆ + d 2,∆ + d 3,∆ + d 4,∆ ), q i = q (i+∆m−1)%4+1 ,<label>(4)</label></formula><p>where d E (b, q ) is the Euclidean distance between two points, and ∆ m is the shift of points that gives the minimal sum of distance of four pair of corresponding points between G b and G q . b) Rotated rectangle: : There are several different representations for rotated rectangles. A popular one is given by a horizontal rectangle and a rotated angle, which could be written as (x, y, w, h, θ). However, due to the bias of the dataset, there is usually an uneven distribution on θ, which may make the model dataset-dependent. To ease this problem, we propose to use another representation proposed in <ref type="bibr" target="#b44">[45]</ref> for a ground truth rotated rectangle G r : G r = (x r 1 ,ỹ r 1 ,x r 2 ,ỹ r 2 ,h r ), where (x r 1 ,ỹ r 1 ) and (x r 2 ,ỹ r 2 ) are the first and second vertex of the ground truth (i.e., the first and second vertex of G q ),h r is the height of the rotated rectangle.</p><p>2) Loss function: We adopt a loss function similar to the one used in <ref type="bibr" target="#b9">[10]</ref>. More specifically, let x be the match indication matrix. For the i-th default box and the j-th ground truth, x ij = 1 means a match following the box overlap between them, otherwise x ij = 0. Let c be the confidence, l be the predicted location, and g be the ground-truth location. The loss function is defined as:</p><formula xml:id="formula_8">L(x, c, l, g) = 1 N (L conf (x, c) + αL loc (x, l, g)),<label>(5)</label></formula><p>where N is the number of default boxes that match groundtruth boxes, and α is set to 0.2 for quick convergence. We adopt the smooth L1 loss <ref type="bibr" target="#b14">[15]</ref> for L loc and a 2-class soft-max loss for L conf .</p><p>3) On-line hard negative mining: Some textures and signs are very similar to text, which are hard for the network to distinguish. We follow the hard negative mining strategy used in <ref type="bibr" target="#b9">[10]</ref> to suppress them. More precisely, the training on the corresponding dataset is divided into two stages. The ratio between the negatives and positives for the first stage is set to 3:1, and then changed to 6:1 for the second stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Data Augmentation:</head><p>Similar to many CNN-based vision problems, data augmentation is a classical and necessary way to increase the limited size of a training set. For example, a random crop augmentation based on the minimum Jaccard overlap between crops and ground truths is applied in <ref type="bibr" target="#b9">[10]</ref>. However, this strategy is not appropriate for text which is usually small. This is because that the Jaccard overlap constraint is difficult to satisfy for small objects. As depicted by an example in <ref type="figure" target="#fig_6">Fig. 5(a-b)</ref>, for a small object, even if the Jaccard overlap is satisfied, the object in the resized image after data augmentation is extremely large that almost covers the whole image. This is not the usual case for text in natural images. To ease this problem, we propose to add a new overlap constraint called object coverage in addition to the Jaccard overlap. For a cropped bounding box B and a ground truth bounding box G, The Jaccard overlap J and object coverage C are defined as follows:</p><formula xml:id="formula_9">J = |B ∩ G|/|B ∪ G|, C = |B ∩ G|/|G|,<label>(6)</label></formula><p>where | · | denotes the cardinality (i.e. area). The random crop strategy based on object coverage C is more appropriate for small objects such as most text in natural images. An example is given in <ref type="figure" target="#fig_6">Fig. 5(c-d)</ref>. In this paper, we use both random crop strategies with minimum overlap or coverage thresholds randomly set to 0, 0.1, 0.3, 0.5, 0.7 and 0.9. Note that a threshold set to 0 implies that neither minimum Jaccard overlap nor object coverage constraint is used. Each cropped region is then resized to a fixed size image that feeds into the network. 5) Multi-scale training: For the sake of training speed, the randomly cropped regions are resized to images of a relatively small size. However, the input images of the proposed TextBoxes++ can have arbitrary size thanks to its fully convolutional architecture. To better handle multi-scale text, we also use larger scale input size for the last several thousand iterations in training phase. The training details are discussed in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Testing with efficient cascaded non-maximum suppression</head><p>Similar to classical methods for object detection, we apply a Non-Maximum Suppression (NMS) during the test period to extract predicted boxes of arbitrary-oriented text. Firstly, we resize the six multi-scale prediction results to the original size of an input image, and fusion these resized results into one dense confidence map. Then the NMS operation is applied on this merged confidence map. Since the NMS operation on quadrilaterals or rotated rectangles is more timeconsuming than that on horizontal rectangles, we divide this NMS operation into two steps to accelerate the speed. First, we apply the NMS with a relatively high IOU threshold (e.g. 0.5) on the minimum horizontal rectangles containing the predicted quadrilaterals or rotated rectangles. This operation on horizontal rectangles is much less time-consuming and removes many candidate boxes. Then the time-consuming NMS on quadrilaterals or rotated rectangles is applied to a few remaining candidate boxes with a lower IOU threshold (e.g. 0.2). The remaining boxes after this second NMS operation are considered as the final detected text boxes. This cascaded nonmaximum suppression is much faster than directly applying NMS on the quadrilaterals or rotated rectangles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Word spotting, end-to-end recognition, and detection refining</head><p>Intuitively, a text recognizer would help to eliminate some false-positive detection results that are unlikely to be meaningful words, e.g. repetitive patterns. Particularly, when a lexicon is present, a text recognizer could effectively remove the detected bounding boxes that do not match any of the given words. Following this intuitive idea, we propose to improve the detection results of TextBoxes++ with word spotting and end-to-end recognition.</p><p>1) Word spotting and end-to-end recognition: Word spotting is to localize specific words that are given in a lexicon. End-to-end recognition concerns both detection and recognition. Both tasks can be achieved by simply connecting TextBoxes++ with a text recognizer. We adopt the CRNN model <ref type="bibr" target="#b11">[12]</ref> as our text recognizer. CRNN uses CTC <ref type="bibr" target="#b47">[48]</ref> as its output layer, which estimates a sequence probability conditioned on the input image I denoted as p(w|I), where w represents a character sequence output. If no lexicon is given, w is considered as the recognized word, and the probability p(w|I) measures the compatibility of an image to that particular word w. CRNN also supports the use of the lexicon. For a given lexicon W, CRNN outputs the probability that measures how the input image I matches each word w ∈ W. We define the recognition score s r in the following:</p><formula xml:id="formula_10">s r = p(w|I), Without lexicon max w∈W p(w|I), With lexicon W<label>(7)</label></formula><p>Note that the use of lexicon is not a necessary in the proposed method. We only use lexicons for fair comparisons with other methods.</p><p>2) Refining detection with recognition: We propose to refine detection with recognition by integrating the recognition score s r to the original detection s d score. In practice, the value of recognition score is generally not comparable to the value of detection score. For example, the threshold of the detection score s d is usually set to 0.6, and the threshold of recognition score s r is usually set to 0.005. A trivial combination of these two scores would lead to a severe bias of detection score. In this paper, we propose to define the novel score S as following:</p><formula xml:id="formula_11">S = 2 × e (s d +sr) e s d + e sr .<label>(8)</label></formula><p>There are two motivations in Eq. <ref type="bibr" target="#b7">(8)</ref>. First, we use the exponential function to make the two score values comparable. Then, a harmonic mean is adopted to get the final combined score. This combined score S is more convenient than applying a grid search on two scores, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Inherited from object detection, all existing scene text detection benchmarks rely on an IOU threshold to evaluate the performance of text detectors. However, text detection is quite different from object detection because the ultimate purpose of detecting text is text recognition. A text recognizer may yield totally different results with the same IOU. For example, the three detection results in <ref type="figure">Fig. 6(a-c)</ref> have the same IOU. However, the detection results in <ref type="figure">Fig. 6(a)</ref> and <ref type="figure">Fig. 6(b)</ref> fail to correctly recognize the underlying text due to the lack of text parts. The detection results in <ref type="figure">Fig. 6</ref>(c) tends to give an accurate recognition result thanks to the full text coverage. Thus, in addition to classical text detection benchmarks, we also conduct word spotting and end-to-end recognition experiments using the text detection results to further demonstrate the performance of TextBoxes++. <ref type="figure">Fig. 6</ref>: An example of recognition results (yellow text) with different text detection results having the same IOU. The red (resp. green) bounding box is the ground truth (resp. detection result).</p><formula xml:id="formula_12">(a) (b) (c) LAN AI TAN (a) (a) (b) (c) LAN AI TAN (b) (a) (b) (c) LAN AI TAN (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets &amp; evaluation protocol</head><p>The proposed TextBoxes++ detects arbitrary-oriented text. We have tested its performance on two datasets including oriented text: ICDAR 2015 Incidental Text (IC15) dataset <ref type="bibr" target="#b48">[49]</ref> and COCO-Text dataset <ref type="bibr" target="#b49">[50]</ref>. To further demonstrate the versatility of TextBoxes++, we have conducted experiments on two popular horizontal text datasets: ICDAR 2013 (IC13) dataset <ref type="bibr" target="#b50">[51]</ref> and Street View Text (SVT) dataset <ref type="bibr" target="#b51">[52]</ref>. Besides these benchmark datasets, the SynthText dataset <ref type="bibr" target="#b26">[27]</ref> is also used to pre-train our model. A short description of all these concerned datasets is given in the following (see the corresponding references for more details). SynthText: The SynthText dataset <ref type="bibr" target="#b26">[27]</ref> contains 800k synthesized text images, created via blending rendered words with natural images. As the location and transform of text are carefully chosen with a learning algorithm, the synthesized images look realistic. This dataset is used for pre-training our model. ICDAR 2015 Incidental Text (IC15): The ICDAR 2015 Incidental Text dataset <ref type="bibr" target="#b48">[49]</ref> issues from the Challenge 4 of the ICDAR 2015 Robust Reading Competition. The dataset is composed of 1000 training images and 500 testing images, which are captured by Google glasses with relatively low resolutions. Each image may contain multi-oriented text. Annotations are provided in terms of word bounding boxes. This dataset also provides 3 lexicons of different sizes for word spotting and end-to-end recognition challenge: 1) strong lexicon which gives 100 words as an individual lexicon for each test image; 2) weakly lexicon containing hundreds of words for the whole test set; 3) generic lexicon with 90k words. COCO-Text: The COCO-Text dataset <ref type="bibr" target="#b49">[50]</ref> is currently the largest dataset for scene text detection and recognition. It contains 43686 training images and 20000 images for validation/testing. The COCO-Text dataset is very challenging since text in this dataset are in arbitrary orientations. This difficulty also holds for annotations which are not as accurate as the other tested datasets in this paper. Therefore, even though this dataset provides oriented annotations, its standard evaluation protocol still relies on horizontal bounding rectangles. For TextBoxes++, we make use of both the annotations in terms of horizontal bounding rectangles and the quadrilateral annotations to train our model. For evaluation, we follow the standard protocol based on horizontal bounding rectangles.</p><p>ICDAR 2013 (IC13): The ICDAR 2013 dataset <ref type="bibr" target="#b50">[51]</ref> consists of 229 training images and 233 testing images in different resolutions. This dataset contains only horizontal or nearly horizontal text. The lexicon setting for this dataset is the same as the IC15 dataset described before. Street View Text (SVT): The SVT dataset <ref type="bibr" target="#b51">[52]</ref> is more challenging than previous ICDAR 2013 dataset due to lower resolutions of images. There are 100 training images and 250 testing images in the SVT dataset. The images have only horizontal or nearly horizontal text. A lexicon containing 50 words is also provided for each image. Note that not all the text in the dataset are labeled. As a result, this dataset is only used for word spotting evaluation. Evaluation Protocols: The classical evaluation protocols for text detection, word spotting, and end-to-end recognition all rely on precision (P), recall (R), and f -measure (F). They are given by:</p><formula xml:id="formula_13">P = T P T P + F P R = T P T P + F N F = 2 × P × R P + R<label>(9)</label></formula><p>where TP, FP, and FN is the number of hit boxes, incorrectly identified boxes, and missed boxes, respectively. For text detection, a detected box b is considered as a hit box if the IOU between b and a ground truth box is larger than a given threshold (generally set to 0.5). The hit boxes in word spotting and end-to-end recognition require not only the same IOU restriction but also correct recognition results. Since there is a trade-off between precision and recall, f -measure is the most used measurement for performance assessment. TextBoxes++ is trained with Adam <ref type="bibr" target="#b52">[53]</ref>. The whole training process is composed of three stages as shown in <ref type="table">Table.</ref> I. Firstly, we pre-train TextBoxes++ on SynthText dataset for all tested datasets. Then the training process is continued on the corresponding training images of each dataset. Finally, we continue this training with a smaller learning rate and a larger negative ratio. Note also that at the last training stage, a larger input image size is used to achieve better detections of multi-scale text. The number of iterations for the pre-training step is fixed at 60k for all tested datasets. However, this number differs in the second and third training stage which are conducted on each dataset's own training images. This difference is decided by the different dataset size. Text recognition is performed with a pre-trained CRNN model <ref type="bibr" target="#b11">[12]</ref>, which is implemented and released by the authors <ref type="bibr" target="#b0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>All the experiments presented in this paper are carried out on a PC equipped with a single Titan Xp GPU. The whole training process (including the pre-training time on SynthText dataset) takes about 2 days on ICDAR 2015 Incidental Text dataset, which is currently the most tested dataset. </p><formula xml:id="formula_14">(a) (b) (d) (c) (a) (a) (b) (d) (c) (b) (a) (b) (d) (c) (c) (a) (b) (d) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quadrilateral VS Rotated Rectangle</head><p>The rotated rectangle is an approximate simplification of the quadrilateral, which is more flexible in representing arbitraryoriented text bounding box. Although both representations may equally suit for normal text fonts and styles, the quadrilateral representation may adapt better to resized images. An example is given in <ref type="figure" target="#fig_7">Fig. 7</ref>, where the bounding boxes represented by quadrilaterals and rotated rectangles in the original image (see <ref type="figure" target="#fig_7">Fig. 7(a-b)</ref>) are almost the same. However, as shown in <ref type="figure" target="#fig_7">Fig. 7(c-d)</ref>, the rotated rectangles match less well the text than quadrilaterals in the resized image. This is because that 1 https://github.com/bgshih/crnn  <ref type="bibr" target="#b48">[49]</ref> 0.37 0.77 0.50 Zhang et al. <ref type="bibr" target="#b29">[30]</ref> 0.43 0.71 0.54 Tian et al. <ref type="bibr" target="#b32">[33]</ref> 0.52 0.74 0.61 Yao et al. <ref type="bibr" target="#b37">[38]</ref> 0.59 0.72 0.65 Liu et al. <ref type="bibr" target="#b41">[42]</ref> 0.682 0.732 0.706 Shi et al. <ref type="bibr" target="#b38">[39]</ref> 0.768 0.731 0.750 EAST PVANET2x. RBOX <ref type="bibr" target="#b39">[40]</ref> 0.735 0.836 0.782 EAST PVANET2x RBOX MS <ref type="bibr" target="#b39">[40]</ref> 0  a rotated rectangle generally become a parallelogram when resized directly, which leads to a small deviation when trying to keep it as a rotated rectangle. In this sense, TextBoxes++ with quadrilateral representation would be more accurate than its variant using rotated rectangles. We have conducted experiments on the widely tested IC-DAR 2015 Incidental Text dataset to compare these two variants of TextBoxes++; The quantitative comparison is given in <ref type="table" target="#tab_0">Table II</ref>. Following the standard text detection evaluation, the TextBoxes++ using quadrilateral representation significantly outperforms the version using rotated rectangles with at least 2.5 percents improvements. Note also that using multi-scale inputs would improve both versions of TextBoxes++. The quadrilateral version still performs better especially when the IOU threshold for matching evaluation is set to 0.7.</p><p>Besides, under such a high IOU threshold setting, the difference between quadrilateral version and rotated rectangle version with multi-scale inputs is more significant. This is because a much more accurate text detector is expected for a high IOU threshold setting. This confirms that quadrilateral representation is more accurate than the rotated rectangle for TextBoxes++. Consequently, we choose the TextBoxes++ using the quadrilateral representation for the rest experiments in this paper and denote it simply as TextBoxes++ when no ambiguity is present. TextBoxes++ MS stands for this version of TextBoxes++ with multi-scale inputs.</p><p>D. Text localization 1) Performance: We have first evaluated the proposed TextBoxes++ on two popular oriented text datasets to assess its ability of handling arbitrary-oriented text in natural images. To further validate the versatility of TextBoxes++, we have also tested on two widely used horizontal text datasets.</p><p>(a) Results given by Zhang et al. <ref type="bibr" target="#b29">[30]</ref> (b) Results using Shi et al. <ref type="bibr" target="#b38">[39]</ref> (c) TextBoxes++ results  Oriented Text Dataset: The first tested oriented text dataset is the widely used ICDAR 2015 Incidental text dataset. Some qualitative comparisons are illustrated in <ref type="figure" target="#fig_8">Fig. 8</ref>. As depicted in this figure, TextBoxes++ is more robust than the competing methods in detecting oriented text and text of a variety of scales. Quantitative results following the standard evaluation protocol is given in <ref type="table">Table.</ref> III. TextBoxes++ with single input scale outperforms all the state-of-the-art results. More specifically, TextBoxes++ improves the state-of-the-method [40] by 3.5 percents when single scale inputs are used in both methods.</p><p>Furthermore, the single scale version of TextBoxes++ is still 1.0 percent than the multi-scale version of <ref type="bibr" target="#b39">[40]</ref>. Note that a better performance is achieved for TextBoxes++ using multiscale inputs.</p><p>TextBoxes++ also significantly exceeds the state-of-the-art methods on COCO-Text dataset with its latest annotations v1.4. As depicted in <ref type="table">Table.</ref> IV, TextBoxes++ outperforms the competing methods by at least 16 percents with single scale input. Furthermore, the performance of TextBoxes++ is boosted by 2.81 percents when multi-scale inputs are used.</p><p>Horizontal Text Dataset: We have also evaluated TextBoxes++ on ICDAR 2013 dataset, one of the most popular horizontal text dataset. The comparison with some state-ofthe-art methods is depicted in <ref type="table">Table.</ref> V. Note that there are many methods evaluated on this dataset, but only some of the best results are shown. TextBoxes++ achieves at least 1.0 percent improvement over other methods except for <ref type="bibr" target="#b62">[63]</ref> on this dataset. However, Tang et al. <ref type="bibr" target="#b62">[63]</ref> use a cascaded architecture which contains two networks, taking 1.36 seconds per image. Moreover, it can only detect horizontal text. We have also compared TextBoxes++ with one state-of-the-art general object detector SSD <ref type="bibr" target="#b9">[10]</ref>, which is the most related method. The same training procedures of TextBoxes++ is used to train SSD for this comparison. As reported in <ref type="table">Table.</ref> V, such a straightforward adaption of SSD for text detection does not perform as well as the state-of-the-art methods. In particular, we have observed that SSD fails to correctly detect the words with large aspect ratios. TextBoxes++ performs much better thanks to the proposed text-box layers which are specifically designed to overcome the length variation of words. Compared with TextBoxes <ref type="bibr" target="#b12">[13]</ref>, TextBoxes++ achieves almost the same performance with a single scale, and better performance is achieved with multi-scales, owing to the multi-scale training strategy adopted in TextBoxes++. However, note that the experiment on this dataset is to verify that TextBoxes++, although dedicated to arbitrary-oriented text detection, has no performance loss compared with the preliminary study TextBoxes, which is specifically designed for horizontal text detection. 2) Runtime: TextBoxes++ is not only accurate but also efficient. We have compared its runtime with the state-ofthe-art methods on ICDAR 2015 Incidental Text dataset. As shown in <ref type="table" target="#tab_0">Table VI</ref>, TextBoxes++ achieves an F-measure of 0.817 with 11.6 fps, which has a better balance on runtime and performance than the other competing methods. Note that ss-600 for the method proposed by Tian et al. <ref type="bibr" target="#b32">[33]</ref> means the short side of images is resized to 600. The best result on ICDAR 2015 Incidental dataset for this method is given by using a short edge of 2000, which would lead to a much slower runtime for this method. For Zhang et al. <ref type="bibr" target="#b29">[30]</ref>, MS means that they used three scales (i.e., 200, 500, 1000) on MSRA-TD500 dataset <ref type="bibr" target="#b18">[19]</ref>. The method proposed in <ref type="bibr" target="#b39">[40]</ref> performs at 16.8 fps with PVANet <ref type="bibr" target="#b40">[41]</ref>, a faster backbone compared to VGG-16. However, the performance is 6 percents lower than TextBoxes++. To improve the performance, the authors double the number of channels of PVANet, which results in a runtime at 13.2 fps. TextBoxes++ has a similar runtime but with a 3.5 percents performance improvement. Furthermore, when the same backbone (VGG-16) is applied, the method in <ref type="bibr" target="#b39">[40]</ref> is much lower and still performs less well than TextBoxes++. For Shi et al. <ref type="bibr" target="#b38">[39]</ref>, the reported runtime is tested on 768 × 768 MSRA-TD 500 images, but the reported performance is achieved with 720 × 1280 ICDAR 2015 Incidental text images. E. Word spotting and end-to-end recognition to refine text detection 1) Word spotting and end-to-end recognition: At the beginning of Section IV, we have discussed the limitations of the standard text detection evaluation protocols which rely on the classical IOU threshold setting. It is meaningless to only detect text without correct recognition. In this sense, an evaluation based on the ultimate purpose of text detection would further assess the quality of text detection. For that, we have also evaluated the proposed text detector TextBoxes++ combined with a recent text recognizer CRNN model <ref type="bibr" target="#b11">[12]</ref> in the framework of word spotting and end-to-end recognition. Note that although word spotting is similar to end-to-end recognition, the evaluation of word spotting and end-to-end recognition is slightly different. For word spotting, only some specified words are required to be detected and recognized, which implies that word spotting is generally easier than end-toend recognition. We have tested the pipeline of TextBoxes++ followed by CRNN model <ref type="bibr" target="#b11">[12]</ref> on three popular word spotting or end-to-end recognition benchmark datasets: ICDAR 2015 Incidental Text dataset, SVT dataset, and ICDAR 2013 dataset.</p><p>Oriented text datasets: As TextBoxes++ can detect arbitrary-oriented text in natural images, we first evaluate it for word spotting and end-to-end recognition on ICDAR 2015 Incidental Text dataset. Some qualitative results are given in <ref type="figure" target="#fig_9">Fig. 9(a)</ref>. In general, the proposed pipeline correctly recognize most oriented text. A quantitative comparison with other competing methods is depicted in <ref type="table">Table.</ref> VII. Note that there are not yet published papers for the competing methods in this table. These results are public on the ICDAR 2017 competition website 2 , and only some of the best results are shown. Our method significantly outperforms the other methods under all strong, weak, and generic lexicon for both word spotting and end-to-end recognition. More specifically, for strong lexicon, the proposed method outperforms the best competing method by 6 percent for both tasks. For weak lexicon, the proposed method improves the state-of-the-art results by 6 percents for word spotting and 4 percents for end-to-end recognition. The improvement is less significant (2.7 percents and 1.2 percents for the two tasks, respectively) when a generic lexicon is used.</p><p>Horizontal text datasets: We have also evaluated the proposed method for word spotting and end-to-end recognition on two horizontal text datasets: ICDAR 2013 dataset and SVT dataset. Some qualitative results on ICDAR 2013 dataset are depicted in <ref type="figure" target="#fig_9">Fig. 9</ref>. In general, TextBoxes++ achieves good results in various occasions, regardless of the sizes, aspect ratios, fonts, and complex backgrounds. Some quantitative results are given in <ref type="table">Table.</ref> VIII. Our method outperforms the state-of-the-art methods. More specifically, on ICDAR 2013 dataset, our method outperforms the best competing method by at least 2 percents for all the evaluation protocols listed in <ref type="table">Table.</ref> VIII. The performance improvement is even more significant on SVT dataset. TextBoxes++ outperforms the state-of-the-art method <ref type="bibr" target="#b26">[27]</ref> by at least 8 percents on both SVT and SVT-50. This is mainly because that TextBoxes++ is more robust when dealing with low-resolution images in SVT thanks to its training on relatively low-resolution images. Note that Jaderberg <ref type="bibr" target="#b7">[8]</ref> and FCRNall+filts <ref type="bibr" target="#b26">[27]</ref> adopt a much smaller lexicon (50k words) than our method (90k words), yet the proposed method still performs better. Compared with TextBoxes <ref type="bibr" target="#b12">[13]</ref>, TextBoxes++ achieves better performance on ICDAR 2013 dataset.    2) Refining detection with recognition: We propose to use recognition to further refine detection results by integrating recognition score into detection score with Eq. (8). We have evaluated this idea on two datasets. As depicted in Tab. IX, the recognition without lexicon improves detection results of TextBoxes++ by 0.5 percent and 1.3 percents on ICDAR 2013 and ICDAR 2015 Incidental Text dataset, respectively. This improvement is further boosted using a specified lexicon, achieving 0.8 percent and 1.9 percents on ICDAR 2013 and ICDAR 2015 Incidental Text dataset, respectively. Note that the current text recognizer still has difficulties in dealing with vertical text and recognizing low-resolution text. A further performance improvement is expected with a better text recognizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Weaknesses</head><p>As demonstrated by previous experimental results, TextBoxes++ performs well in most situations. However, it still fails to handle some difficult cases, such as object occlusion and large character spacing. TextBoxes++ also fails to detect some vertical text due to the lack of enough vertical training data. Even with hard negative mining, some text-like areas are still falsely detected. Another failure case is curved text detection. Different from some part-based methods, e.g. <ref type="bibr" target="#b22">[23]</ref>, TextBoxes++ is hard to fit the accurate boundary for curved texts due to the limitation of quadrilateral representation. Note that all these difficulties also hold for the other state-of-the-art methods <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Some failure cases are shown in <ref type="figure" target="#fig_1">Fig. 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. COMPARISONS WITH RECENT WORKS</head><p>We compare in detail the proposed TextBoxes++ with EAST <ref type="bibr" target="#b39">[40]</ref>, one of the previous state-of-the-art method, and one of the most related method DMPNet <ref type="bibr" target="#b41">[42]</ref> from two aspects: simplicity and performance.</p><p>A. TextBoxes++ vs. EAST 1) Simplicity: EAST generates a text region map, or named as score map, using a U-shape network <ref type="bibr" target="#b64">[65]</ref>. It also regresses the oriented rectangles or quadrilaterals based on the same feature which generates the score map. It is a combination of segmentation and detection. In this way, it relies on pyramidlike deconvolutional layers for accurate segmentation. These extra pyramid-like deconvolutional layers require additional computation. However, TextBoxes++ directly classifies and regresses the default boxes on the convolutional feature maps, which is much simpler, avoiding the time consuming on pyramid-like deconvolution. This is evidenced by the speed comparison shown in Tab VI, where TextBoxes++ (with a VGG-16 backbone) has a speed of 11.6fps while the VGG16 RBOX version of EAST runs at 6.52fps.</p><p>2) Performance: EAST relies on an accurate segmentation score map as the score of the bounding boxes. Yet, the text region segmentation is challenging in itself. If the score map is not accurate enough, it is difficult to achieve correct results. For example, it is possible that the partition between two close words is predicted as text region in the segmentation score map, in this case, it is rather difficult to separate these two words in the detection. To alleviate this problem, EAST shrinks the text region in the ground truth of segmentation score map. TextBoxes++ does not suffer from such limitations. It relies on default boxes, and regresses the bounding boxes directly from the convolutional feature maps, where richer information is reserved as compared to the segmentation score map. Thus, TextBoxes++ achieves higher performance (see <ref type="table">Table.</ref> III ). Specifically, TextBoxes++ outperforms <ref type="bibr" target="#b41">[42]</ref> by 3.5 percents (PVANET2x RBOX version) and 6 percents (VGG16 RBOX version) with a single scale.</p><p>B. TextBoxes++ vs. DMPNet 1) Simplicity: 1) TextBoxes++ uses horizontal rectangles as default boxes instead of quadrilaterals with different orientations used in <ref type="bibr" target="#b41">[42]</ref>. In this way, we use much fewer default boxes in every region. Furthermore, we argue that the receptive fields of the convolutional feature map are all in terms of horizontal rectangles, so the target quadrilaterals can be well regressed if it matches the receptive field. The use of oriented rectangle default boxes adopted in <ref type="bibr" target="#b41">[42]</ref> is not necessary for general scene text detection. 2) Benefiting from the horizontal rectangle default boxes, TextBoxes++ enjoys a much simpler strategy for matching default boxes and ground truth by using the maximum horizontal rectangles instead of quadrilaterals. In fact, computing the intersection area between two horizontal rectangles is much easier (just using subtract operation and multiply operation once) than computing the intersection area between two arbitrary quadrilaterals, even though a Monte-Carlo method is used in <ref type="bibr" target="#b41">[42]</ref>.</p><p>2) Performance: 1) TextBoxes++ simultaneously regresses the maximum horizontal rectangles of the bounding boxes and the quadrilateral bounding boxes, which makes the training more stable than the method in <ref type="bibr" target="#b41">[42]</ref>. 2) As compared to the method in <ref type="bibr" target="#b41">[42]</ref>, TextBoxes++ goes further study on small texts in the images. We adopt a new scheme for data augmentation which is beneficial to small texts. 3) In this paper, we not only focus on scene text detection, but also concern the combination between detection and recognition. We proposed a novel score which effectively and efficiently combines the detection scores and the recognition scores.</p><p>DMPNet <ref type="bibr" target="#b41">[42]</ref> did not report the runtime. However, we argue that it is slower than TextBoxes++ based on above analysis. Moreover, TextBoxes++ outperforms DMPNet <ref type="bibr" target="#b41">[42]</ref> by 11 percents (with a single scale setting) in terms of Fmeasure on ICDAR 2015 dataset (see Tab. III).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have presented TextBoxes++, an end-to-end fully convolutional network for arbitrary-oriented text detection, which is highly stable and efficient to generate word proposals against cluttered backgrounds. The proposed method directly predicts arbitrary-oriented word bounding boxes via a novel regression model by quadrilateral representation. The comprehensive evaluations and comparisons on some popular benchmark datasets for text detection, word spotting, and end-to-end scene text recognition, clearly validate the advantages of TextBoxes++. In all experiments, TextBoxes++ has achieved state-of-the-art performance with high efficiency for both horizontal text datasets and oriented text datasets. In the future, we plan to investigate the common failure cases (e.g., large character spacing and vertical text) faced by almost all stateof-the-art text detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by National Natural Science Foundation of China (NSFC) (No. 61733007 and 61573160), the National Program for Support of Top-notch Young Professionals and the Program for HUST Academic Frontier Youth Team. (Corresponding author: Xiang Bai.) Minghui Liao, Baoguang Shi, Xiang Bai are with the School of Electronic Information and Communications, Huazhong University of Science and Technology (HUST), Wuhan, 430074, China. Email: {mhliao, xbai}@hust.edu.cn shibaoguang@gmail.com.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Detection results on some challenging images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>The architecture of TextBoxes++, a fully convolutional network including 13 layers from VGG-16 followed by 10 extra convolutional layers, and 6 Text-box layers connected to 6 intermediate convolutional layers. Each location of a text-box layer predicts an n-dimensional vector for each default box consisting of the text presence scores (2 dimensions), horizontal bounding rectangles offsets (4 dimensions), and rotated rectangle bounding box offsets (5 dimensions) or quadrilateral bounding box offsets (8 dimensions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of the regression (red arrows) from a matched default box (green dashed) to a ground truth target quadrilateral (yellow) on a 3 × 3 grid. Note that the black dashed default box is not matched to the ground truth. The regression from the matched default box to the minimum horizontal rectangle (green solid) containing the ground truth quadrilateral is not shown for a better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 (</head><label>4</label><figDesc>a), the normal default box (black dashed) in the middle can not handle the two words close to it at the same time. In this case, one word would be missed for detection if no vertical offset is applied. InFig. 4(b), the normal default boxes does not cover the bottom word at all, which also demonstrates the necessity of using default boxes with vertical offsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Vertical offsets of default boxes on a 3 × 3 grid. Black (resp. yellow) dashed bounding boxes are normal default boxes (resp. default boxes with vertical offsets). Note that only the default boxes of appropriate aspect ratio are shown for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Data augmentation by random cropping based on Jaccard overlap (a-b) and object coverage constraints (c-d). Images in (b) and (d) are the corresponding resized crops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Ground truth representations in terms of quadrilaterals in red and rotated rectangles in blue. The underlying image in (c) and (d) is resized from the original image shown both in (a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative comparisons of text detection results on some ICDAR 2015 Incidental text images. Green bounding boxes: correct detections; Red solid boxes: false detections; Red dashed boxes: missed ground truths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>CONVERSEFig. 9 :</head><label>9</label><figDesc>(a) Some results on ICDAR 2013 images CONVERSE (b) Some results on ICDAR 2015 Incidental text images Some examples of end-to-end recognition results represented by yellow words. Note that following the evaluation protocol, Words less than 3 letters are ignored. The box colors have the same meaning asFig. 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Some failure examples. Green bounding boxes: correct detections; Red solid boxes: false detections; Red dashed boxes: missed ground truths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Implementation details. "lr" is short for learning rate. The size of input image is denoted as "size"."nr" refers to the negative ratio in hard negative mining. "#iter" stands for the number of training iterations.</figDesc><table><row><cell>Dataset</cell><cell cols="2">All datasets</cell><cell></cell><cell cols="2">IC15 COCO-Text</cell><cell cols="2">SVT IC13</cell></row><row><cell>Settings</cell><cell>lr</cell><cell>size</cell><cell>nr</cell><cell>#iter</cell><cell>#iter</cell><cell>#iter</cell><cell>#iter</cell></row><row><cell cols="2">Pre-train 10 −4</cell><cell>384</cell><cell>3</cell><cell>60k</cell><cell>60k</cell><cell>60k</cell><cell>60k</cell></row><row><cell>Stage 1</cell><cell>10 −4</cell><cell>384</cell><cell>3</cell><cell>8k</cell><cell>20k</cell><cell>2k</cell><cell>2k</cell></row><row><cell>Stage 2</cell><cell>10 −5</cell><cell>768</cell><cell>6</cell><cell>4k</cell><cell>30k</cell><cell>8k</cell><cell>8k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Performance comparisons on ICDAR 2015 Incidental Text dataset with different IOU threshold settings between four variants of TextBoxes++, which use different bounding box representation and different input scales. "RR" stands for the rotated rectangle, and "Quad" represents the quadrilateral. "MS" is short for using multi-scale inputs.</figDesc><table><row><cell>Method</cell><cell cols="3">IOU threshold=0.5 R P F</cell><cell cols="3">IOU threshold=0.7 R P F</cell></row><row><cell>RR</cell><cell>0.764</cell><cell>0.822</cell><cell>0.792</cell><cell>0.613</cell><cell>0.574</cell><cell>0.593</cell></row><row><cell>Quad</cell><cell>0.767</cell><cell>0.872</cell><cell>0.817</cell><cell>0.577</cell><cell>0.676</cell><cell>0.623</cell></row><row><cell>RR MS</cell><cell>0.766</cell><cell>0.875</cell><cell>0.817</cell><cell>0.569</cell><cell>0.623</cell><cell>0.594</cell></row><row><cell>Quad MS</cell><cell>0.785</cell><cell>0.878</cell><cell>0.829</cell><cell>0.617</cell><cell>0.690</cell><cell>0.651</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Text localization results on ICDAR 2015 Incidental Text dataset.</figDesc><table><row><cell>Methods</cell><cell>recall</cell><cell cols="2">precision f-measure</cell></row><row><cell>CNN MSER [49]</cell><cell>0.34</cell><cell>0.35</cell><cell>0.35</cell></row><row><cell>AJOU [54]</cell><cell>0.47</cell><cell>0.47</cell><cell>0.47</cell></row><row><cell>NJU [49]</cell><cell>0.36</cell><cell>0.70</cell><cell>0.48</cell></row><row><cell>StradVision1 [49]</cell><cell>0.46</cell><cell>0.53</cell><cell>0.50</cell></row><row><cell>StradVision2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Text localization results on COCO-Text dataset.</figDesc><table><row><cell>Methods</cell><cell>recall</cell><cell cols="2">precision f-measure</cell></row><row><cell>Baseline A [50]</cell><cell>0.233</cell><cell>0.8378</cell><cell>0.3648</cell></row><row><cell>Baseline B [50]</cell><cell>0.107</cell><cell>0.8973</cell><cell>0.1914</cell></row><row><cell>Baseline C [50]</cell><cell>0.047</cell><cell>0.1856</cell><cell>0.0747</cell></row><row><cell>Yao et al. [38]</cell><cell>0.271</cell><cell>0.4323</cell><cell>0.3331</cell></row><row><cell>Zhou et al. [40]</cell><cell>0.324</cell><cell>0.5039</cell><cell>0.3945</cell></row><row><cell>TextBoxes++</cell><cell>0.5600</cell><cell>0.5582</cell><cell>0.5591</cell></row><row><cell>TextBoxes++ MS</cell><cell>0.5670</cell><cell>0.6087</cell><cell>0.5872</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Text localization on ICDAR 2013 dataset. P, R, and F refer to precision, recall and f-measure, respectively.</figDesc><table><row><cell>Evaluation protocol</cell><cell></cell><cell>IC13 Eval</cell><cell></cell><cell></cell><cell>DetEval</cell><cell></cell></row><row><cell>Methods</cell><cell>R</cell><cell>P</cell><cell>F</cell><cell>R</cell><cell>P</cell><cell>F</cell></row><row><cell>fasttext [55]</cell><cell>0.69</cell><cell>0.84</cell><cell>0.77</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMser [56]</cell><cell>0.70</cell><cell>0.86</cell><cell>0.77</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Lu et al. [57]</cell><cell>0.70</cell><cell>0.89</cell><cell>0.78</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TextFlow [58]</cell><cell>0.76</cell><cell>0.85</cell><cell>0.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>He et al. [59]</cell><cell>0.76</cell><cell>0.85</cell><cell>0.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>He et al. [60]</cell><cell>0.73</cell><cell>0.93</cell><cell>0.82</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FCRNall+filts [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.76</cell><cell>0.92</cell><cell>0.83</cell></row><row><cell>FCN [30]</cell><cell>0.78</cell><cell>0.88</cell><cell>0.83</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Tian et al [61]</cell><cell>0.84</cell><cell>0.84</cell><cell>0.84</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Qin et al. [62]</cell><cell>0.79</cell><cell>0.89</cell><cell>0.83</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Shi et al. [39]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.83</cell><cell>0.88</cell><cell>0.85</cell></row><row><cell>Tian et al. [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.83</cell><cell>0.93</cell><cell>0.88</cell></row><row><cell>Tang et al. [63]</cell><cell>0.87</cell><cell>0.92</cell><cell>0.90</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSD [10]</cell><cell>0.60</cell><cell>0.80</cell><cell>0.68</cell><cell>0.60</cell><cell>0.80</cell><cell>0.69</cell></row><row><cell>TextBoxes [13]</cell><cell>0.74</cell><cell>0.86</cell><cell>0.80</cell><cell>0.74</cell><cell>0.88</cell><cell>0.81</cell></row><row><cell cols="2">TextBoxes MS [13] 0.83</cell><cell>0.88</cell><cell>0.85</cell><cell>0.83</cell><cell>0.89</cell><cell>0.86</cell></row><row><cell>TextBoxes++</cell><cell>0.74</cell><cell>0.86</cell><cell>0.80</cell><cell>0.74</cell><cell>0.88</cell><cell>0.81</cell></row><row><cell>TextBoxes++ MS</cell><cell>0.84</cell><cell>0.91</cell><cell>0.88</cell><cell>0.86</cell><cell>0.92</cell><cell>0.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Runtime and performance comparison on ICDAR 2015 Incidental Text dataset. "F" is short for F-measure. See corresponding text for detailed discussions.</figDesc><table><row><cell>Method</cell><cell>Res</cell><cell>FPS</cell><cell>F</cell></row><row><cell>Zhang et al. [30]</cell><cell>MS*</cell><cell>0.476</cell><cell>0.54</cell></row><row><cell>Tian et al. [33]</cell><cell>ss-600*</cell><cell>7.14</cell><cell>0.61</cell></row><row><cell>Yao et al. [38]</cell><cell>480p</cell><cell>1.61</cell><cell>0.65</cell></row><row><cell>EAST PVANET [40]</cell><cell>720p</cell><cell>16.8</cell><cell>0.757</cell></row><row><cell>EAST PVANET2x [40]</cell><cell>720p</cell><cell>13.2</cell><cell>0.782</cell></row><row><cell>EAST VGG16 [40]</cell><cell>720p</cell><cell>6.52</cell><cell>0.764</cell></row><row><cell>Shi et al. el. [39]</cell><cell>768 × 768</cell><cell>8.9</cell><cell>0.750</cell></row><row><cell>TextBoxes++</cell><cell>1024 × 1024</cell><cell>11.6</cell><cell>0.817</cell></row><row><cell>TextBoxes++ MS</cell><cell>MS*</cell><cell>2.3</cell><cell>0.829</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Note that the runtime for TextBoxes++ on 768 × 768 COCO-Text images is 19.8 fps. TextBoxes++ MS achieves about 2.3 fps with four input scales (384 × 384, 768 × 768, 1024 × 1024, 1536 × 1536).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>F-measures for word spotting and end-to-end results on ICDAR 2015 Incidental Text dataset. See the corresponding dataset description in Section IV-A for strong, weak, and generic lexicon settings. Note that the methods marked by "*" are published on the ICDAR 2017 Robust Reading Competition website: http://rrc.cvc.uab.es.</figDesc><table><row><cell>Methods</cell><cell cols="3">IC15 word spotting strong weak generic</cell><cell cols="3">IC15 end-to-end strong weak generic</cell></row><row><cell>Megvii-Image++ *</cell><cell>0.4995</cell><cell>0.4271</cell><cell>0.3457</cell><cell>0.4674</cell><cell>0.4</cell><cell>0.3286</cell></row><row><cell>Yunos Robot1.0*</cell><cell>0.4947</cell><cell>0.4947</cell><cell>0.4947</cell><cell>0.4729</cell><cell>0.4729</cell><cell>0.4729</cell></row><row><cell cols="2">SRC-B-TextProcessingLab* 0.5408</cell><cell>0.5186</cell><cell>0.3712</cell><cell>0.526</cell><cell>0.5019</cell><cell>0.3579</cell></row><row><cell>TextProposals + DictNet*</cell><cell>0.56</cell><cell>0.5226</cell><cell>0.4973</cell><cell>0.533</cell><cell>0.4961</cell><cell>0.4718</cell></row><row><cell>Baidu IDL*</cell><cell>0.6578</cell><cell>0.6273</cell><cell>0.5165</cell><cell>0.64</cell><cell>0.6138</cell><cell>0.5071</cell></row><row><cell>HUST MCLAB*</cell><cell>0.7057</cell><cell>-</cell><cell>-</cell><cell>0.6786</cell><cell>-</cell><cell>-</cell></row><row><cell>TextBoxes++</cell><cell>0.7645</cell><cell>0.6904</cell><cell>0.5437</cell><cell>0.7334</cell><cell>0.6587</cell><cell>0.5190</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII :</head><label>VIII</label><figDesc>F-measures for word spotting and end-to-end results on ICDAR 2013 dataset. The lexicon settings are the same as for ICDAR 2015 Incidental Text dataset.</figDesc><table><row><cell>Methods</cell><cell>SVT spotting</cell><cell>SVT-50 spotting</cell><cell cols="4">IC13 spotting strong weak generic strong</cell><cell cols="2">IC13 end-to-end weak generic</cell></row><row><cell>Alsharif [64]</cell><cell>-</cell><cell>0.48</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Jaderberg [8]</cell><cell>0.56</cell><cell>0.68</cell><cell>-</cell><cell>-</cell><cell>0.76</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FCRNall+filts [27]</cell><cell>0.53</cell><cell>0.76</cell><cell>-</cell><cell>-</cell><cell>0.85</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TextBoxes</cell><cell>0.64</cell><cell>0.84</cell><cell>0.94</cell><cell>0.92</cell><cell>0.87</cell><cell>0.91</cell><cell>0.89</cell><cell>0.84</cell></row><row><cell>TextBoxes++</cell><cell>0.64</cell><cell>0.84</cell><cell>0.96</cell><cell>0.95</cell><cell>0.87</cell><cell>0.93</cell><cell>0.92</cell><cell>0.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX :</head><label>IX</label><figDesc>Refined detection results with recognition. The evaluation method for ICDAR 2013 dataset is the IC13 Eval.</figDesc><table><row><cell cols="7">"Det": TextBoxes++ MS; "Rec": recognition without lexicon;</cell></row><row><cell cols="7">"Rec-lex" : recognition with the given strong lexicon in each</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets</cell><cell>R</cell><cell>IC13 P</cell><cell>F</cell><cell>R</cell><cell>IC15 P</cell><cell>F</cell></row><row><cell>Det</cell><cell>0.844</cell><cell>0.912</cell><cell>0.876</cell><cell>0.785</cell><cell>0.878</cell><cell>0.829</cell></row><row><cell>Det+Rec</cell><cell>0.847</cell><cell>0.918</cell><cell>0.881</cell><cell>0.804</cell><cell>0.881</cell><cell>0.842</cell></row><row><cell cols="2">Det+Rec-lex 0.838</cell><cell>0.957</cell><cell>0.894</cell><cell>0.792</cell><cell>0.912</cell><cell>0.848</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://rrc.cvc.uab.es</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene text recognition in mobile applications by character descriptor and structure configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2972" to="2982" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Text detection in stores using a repetition prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detection and recognition of text embedded in online images via neural context models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4103" to="4110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing text-based traffic guide panels with cascaded localization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="109" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A hybrid approach to detect and localize texts in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Image Proc</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="800" to="813" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scene text localization using gradient local correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1380" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Systems and their applications</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="18" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Characterness: An indicator of text in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1666" to="1677" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced mser trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-script text extraction from natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="467" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text line detection based on cost optimized local text line direction estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE 9395, Color Imaging XX: Displaying, Processing, Hardcopy, and Applications</title>
		<meeting>SPIE 9395, Color Imaging XX: Displaying, essing, Hardcopy, and Applications</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">939507</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Text detection in images using sparse representation with discriminative dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1590" to="1599" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeptext: A unified framework for text proposal generation and text detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<idno>abs/1605.07314</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Textproposals: a text-specific selective search algorithm for word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="60" to="74" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Symmetry-based text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2558" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="366" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Localizing text in scene images by boundary clustering, stroke segmentation, and string fragment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4256" to="4268" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene text deblurring using text-specific multiscale dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1302" to="1314" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Orientation robust text line detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4034" to="4041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust wide-baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scene text detection via holistic, multi-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<idno>abs/1606.09002</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3482" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">EAST: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PVANET: deep but lightweight neural networks for real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<idno>abs/1608.08021</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multioriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Sren: Shape regression network for comic storyboard extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4937" to="4938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">R2CNN: rotational region CNN for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno>abs/1706.09579</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on robust reading</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>Proc. ICDAR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cocotext: Dataset and benchmark for text detection and recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1601.07140</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
	<note>Icdar 2013 robust reading competition,&quot; in ICDAR</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scene text detection via connected component clustering and nontext filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2296" to="2305" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fastext: Efficient unconstrained scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1206" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Text localization based on fast feature pyramids and multi-resolution maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamberletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="91" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scene text extraction based on edges and support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDAR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="135" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Aggregating local context for accurate scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Text-attentional convolutional neural network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2529" to="2541" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Natural scene text detection with mc-mr candidate extraction and coarse-to-fine filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Neurocomputing</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A fast and robust text spotter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Scene text detection and segmentation based on cascaded convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1509" to="1520" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with hybrid HMM maxout models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alsharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno>abs/1310.1811</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">He is currently a Ph.D. student with the School of Electronic Information and Communications, HUST. His research interests include scene text detection and recognition</title>
	</analytic>
	<monogr>
		<title level="m">Minghui Liao received his B.S. degree from the School of Electronic Information and Communications, Huazhong University of Science and Technology (HUST)</title>
		<meeting><address><addrLine>Wuhan, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">China in 2012, where he is currently a Ph.D. candidate. He was an intern at Microsoft Research Asia in 2014, and a visiting student at Cornell University from 2016 to 2017. His research interests include scene text detection and recognition</title>
	</analytic>
	<monogr>
		<title level="m">Baoguang Shi received his B.S. degree from the School of Electronic Information and Communications</title>
		<meeting><address><addrLine>Wuhan</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>Huazhong University of Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note>3D shape recognition, and facial recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
