<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Back-projection Network for Point Cloud Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
						</author>
						<title level="a" type="main">Geometric Back-projection Network for Point Cloud Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Point Cloud Classification</term>
					<term>3D Deep Learn- ing</term>
					<term>Attention Mechanism</term>
					<term>Geometric Features</term>
					<term>Error-correcting Feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the basic task of point cloud analysis, classification is fundamental but always challenging. To address some unsolved problems of existing methods, we propose a network that captures geometric features of point clouds for better representations. To achieve this, on the one hand, we enrich the geometric information of points in low-level 3D space explicitly. On the other hand, we apply CNN-based structures in highlevel feature spaces to learn local geometric context implicitly. Specifically, we leverage an idea of error-correcting feedback structure to capture the local features of point clouds comprehensively. Furthermore, an attention module based on channel affinity assists the feature map to avoid possible redundancy by emphasizing its distinct channels. The performance on both synthetic and real-world point clouds datasets demonstrate the superiority and applicability of our network. Comparing with other state-of-the-art methods, our approach balances accuracy and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P OINT clouds are one of the fundamental representations of 3D data, and widely used for both research and applications in multimedia <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref> because of the development of 3D sensing technology. Generally, 3D point clouds can be collected by scanners <ref type="bibr" target="#b5">[6]</ref> utilizing physical touch or non-contact measurements: e.g., light, sound, LiDAR etc. Particularly, LiDAR scanners <ref type="bibr" target="#b6">[7]</ref> are in service in many fields including agriculture, biology, and robotics, etc. Due to its tremendous contributions, point cloud analysis attracts much interest for research. More importantly, point cloud classification is always a fundamental challenge due to the following properties:</p><p>• Necessity. In the same way that ImageNet <ref type="bibr" target="#b7">[8]</ref> is a litmus test, most 3D work verifies basic performance on classification. Particularly, standalone research on point cloud classification <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b13">[14]</ref> is essential in 3D vision. • Practicability. Recently 3D applications are popular and strongly rely on classification. For example, unlocking via face ID requires accurate and efficient 3D classification solutions to identify the users. • Difficulty. Existing methods face challenges in robustness and efficiency, especially for the rapidly growing needs in the industry. Point cloud classification still requires more efforts to resolve theoretical and practical issues. Traditional algorithms <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref> for 3D data usually incorporate geometry estimation and model reconstruction. Assisted by deep learning, recent works on 3D focus on data-driven approaches via Convolutional Neural Networks. Guo et al. <ref type="bibr" target="#b18">[19]</ref> categorized CNN approaches to point clouds classification as: multi-view methods (e.g., MVCNN <ref type="bibr" target="#b19">[20]</ref>), volumetric/mesh methods (e.g., VoxNet <ref type="bibr" target="#b20">[21]</ref>), and 3D point methods (e.g., PointNet <ref type="bibr" target="#b21">[22]</ref>).</p><p>Currently, many works explore different methods to improve 3D point cloud processing, but some issues remain unsolved: 1) Besides the 3D coordinates, can we provide more geometric clues for CNN-based feature learning; 2) How can we make the network automatically learn a better representation from the abstract high-level feature space; 3) Moreover, how do we refine the output features to focus on critical information?</p><p>State-of-the-art methods <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b26">[27]</ref> show that CNN-based feature learning in high-level space perform better when the additional low-level 3D relations are involved. Particularly, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b26">[27]</ref> encode both 3D and high-level features in each layer of their networks. However, repeating similar or incorporating ineffective 3D information in all embedding scales seems redundant and computationally expensive, especially for regular cascaded architecture. Unlike these, we only provide the 3D geometric cues at the beginning, serving as the prior knowledge for our network. To maximize the advantage, such low-level geometric clues should be carefully formed with physically explicit relations to enrich the geometric information for later processing in high-level spaces.</p><p>To regulate CNN-based feature learning in embedding space, we present a novel attentional back-projection module capturing an idea of error-correcting feedback structure for point clouds via the incorporation of local geometric context. As supported by substantial biological evidence <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, the feedback mechanism allows modification of the original output via a response to it, which can guide visual tasks for relevant results. Some early work manages to involve the idea of error-feedback in CNNs: e.g., 2D human pose estimation <ref type="bibr" target="#b29">[30]</ref>, image super-resolution (SR) <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, point cloud generation (3D-SR) <ref type="bibr" target="#b32">[33]</ref>, etc. In order to coordinate the error-feedback idea with complex CNN frameworks, the solutions are generally in two tracks: as in <ref type="bibr" target="#b29">[30]</ref>, the feedback loop is realized by minimizing the additional error loss during the back-propagation procedure; while <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref> choose to correct the errors by introducing skip-connections in forwardpass. To avoid possible stability problems in training, we prefer the latter solution to realize the error-feedback in a concise way. To the best of our knowledge, such attentional error-correcting feedback structures have not been used for the fundamental problem of point cloud feature learning. Our <ref type="figure">Fig. 1</ref>. Enriching geometric features. In low-level space, we explicitly estimate geometric items as prior knowledge for the network e.g., edges (green vectors), normals (red vectors). In high-level space, we aggregate neighbors (green points) to implicitly capture both prominent (red points) and fine-grained geometric features (purple points). motivation is to automatically complement the output feature map by comparing the difference between the input and the corresponding restored input. Leveraging this structure, we enable the network to learn a better representation of point cloud features.</p><p>PointNet <ref type="bibr" target="#b21">[22]</ref> suggested that symmetric functions can deal with unorderedness of regular point cloud data, since the features can be aggregated consistently regardless of the internal order. Our attentional back-projection module may apply an efficient max-pooling function to extract prominent features from each local neighborhood. However, the extracted information is still insufficient for the precise classification task, as max-pooling can only describe an outline while some local details could be omitted. To address this problem, we propose a simple but effective way to learn complementary fine-grained features via a shared fully connected operation over each local neighborhood. By taking advantage of both prominent and fine-grained local features, our attentional back-projection module is supposed to learn comprehensive representations for point clouds.</p><p>Regarding the high-level representations for 3D point clouds, another widely applied mechanism: Attention can assist the network to put more emphasis on useful information <ref type="bibr" target="#b33">[34]</ref>. Attention modules are used in many 2D visual problems e.g., image segmentation <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b37">[38]</ref>, image denoising <ref type="bibr" target="#b38">[39]</ref>, person re-identification <ref type="bibr" target="#b37">[38]</ref>, etc. For 3D point clouds, <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref> leverage the idea of self-attention <ref type="bibr" target="#b33">[34]</ref> to find significant pointwise features, while recent work <ref type="bibr" target="#b26">[27]</ref> distributes weights on neighbors for local aggregation. In terms of the classification task, given a limited number of points, every point-wise feature would be informative. In contrast, possible channelwise redundancy, accumulated from implicit learning in highlevel spaces, would cause side effects for the network. In this case, we propose a Channel-wise Affinity Attention module to refine the feature map based on affinity between channels.</p><p>The contributions of our work can be summarized as:</p><p>• We propose a CNN network for learning geometric features of point clouds in both low-level and high-level spaces. • We design a back-projection CNN module leveraging an idea of error-correcting feedback structure to learn local features of point clouds.</p><p>• We introduce a channel-wise affinity attention module to refine the feature representations of point clouds. • We present experimental results showing that our proposed network outperforms state-of-the-art methods on synthetic and real-world 3D point cloud classification benchmarks. The contents of this paper are in the following order: Section I introduces the paper in general. Section II reviews previous works on related topics. The details of our approach are explained in Section III. In Section IV, we present necessary experimental results and ablation studies on different benchmarks. Finally, the conclusions are given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Estimating Geometric Relations</head><p>Although data collection for 3D scattered point cloud is convenient, the main drawback is the lack of geometric information compared with well-constructed mesh or volumetric data. In order to acquire more underlying knowledge of point clouds, conventional methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> attempted to estimate the geometry of point clouds i.e., face, normals, curvature, etc. In addition, some works proposed different types of hand-crafted features for point cloud recognition or matching: e.g., shape context <ref type="bibr" target="#b44">[45]</ref>, point histograms <ref type="bibr" target="#b16">[17]</ref>, etc.</p><p>Recently, some CNN-based state-of-the-art methods benefit from permutation invariance of low-level geometry. Xie et al. <ref type="bibr" target="#b39">[40]</ref> leveraged the idea of 3D geometry estimation, and reproduced shape context calculation using CNN learned features. Yan et al. <ref type="bibr" target="#b45">[46]</ref> continually fed the original 3D coordinates into each layer in their network. Further, the encoders in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b26">[27]</ref> tried to incorporate some 3D relations: e.g., relative positions or distances between the points. Unlike existing solutions, we intend to offer some clues about lowlevel geometry for the network, instead of repeating similar information for each layer. More explicitly and effectively, we expect that following geometric feature learning in high-level space can benefit from geometric relationships in low-level space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Local Features</head><p>PointNet <ref type="bibr" target="#b21">[22]</ref> passed the data through multi-layer perceptrons (MLPs) for a high-level feature representation of every single point, and successfully solved the unordered problem of point cloud with a symmetric function. Due to its effectiveness, later works <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b46">[47]</ref>- <ref type="bibr" target="#b48">[49]</ref> also adopted MLP based operations for point cloud processing. Meanwhile, researchers realized that local features are promising because they involve a more regional context than global features. Although the points are unordered in point cloud data, we may group points based on various metrics. In general, one approach is to select seed points as centroids using a downsampling method (e.g., Farthest Point Sampling in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> or Random Sampling in <ref type="bibr" target="#b26">[27]</ref>), and then apply a query algorithm (e.g., Ball Query in <ref type="bibr" target="#b46">[47]</ref> or k-nearest neighbors in <ref type="bibr" target="#b26">[27]</ref>) based on the 3D Euclidean distance to group points for local clusters. By processing the clusters, the feature of each centroid may represent more local context. Another track finds each point's neighbors in embedding space based on N-dimensional Euclidean distance, and then groups them in the form of a high dimensional graph, e.g., as in <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. In contrast to previous methods, this approach can update the local clusters dynamically on different scales. In terms of feature aggregation, max-pooling <ref type="bibr" target="#b21">[22]</ref> is widely employed since it can efficiently gather the representative information for an unordered point set. In spite of the benefits, there are some weaknesses to this approach, such as it may involve bias and lose local details. To overcome these issues, we apply an error-correcting feedback structure to regulate the feature learning to reduce possible bias. Moreover, additional fine-grained features aggregated from local neighbors can contribute to more comprehensive feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention Mechanism for CNN</head><p>The idea of attention has been successfully used in many areas of Artificial Intelligence. As with human beings, the computational resources of machines are also limited; thus, we need to focus on essential aspects. Previously, Vaswani et al. <ref type="bibr" target="#b33">[34]</ref> proposed different types of attention mechanisms for neural machine translation. Subsequently, attention was incorporated in visual tasks: Wang et al. <ref type="bibr" target="#b34">[35]</ref> extended the idea of self-attention in the spatial domain. Also, SENet <ref type="bibr" target="#b35">[36]</ref> credits winning the ImageNet <ref type="bibr" target="#b50">[51]</ref> challenge to its channelwise attention module. Other works <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> derive benefits from both spatial and channel domains of 2D images.</p><p>In terms of 3D point clouds, attention modules contribute to point cloud approaches for detection <ref type="bibr" target="#b53">[54]</ref>, generation <ref type="bibr" target="#b54">[55]</ref>, segmentation <ref type="bibr" target="#b55">[56]</ref>- <ref type="bibr" target="#b57">[58]</ref>, etc. However, there has been limited work in well-designed attention mechanisms targeting 3D point clouds classification. On this front, Xie et al. <ref type="bibr" target="#b39">[40]</ref> utilized a spatial self-attention module for the learned features based on shape context. Similarly, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> intended to enhance the point-wise features by comparing the similarities between points. Subsequent works <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref> applied a Graph Attention <ref type="bibr" target="#b60">[61]</ref> module for the constructed graph features on point clouds, while <ref type="bibr" target="#b26">[27]</ref> re-weighted the neighbors for local feature aggregation. Point cloud classification is challenging since a limited number of points are discretely distributed in an unlimited 3D space. Therefore, each point would be informative for the representation of the whole set. Unlike other methods assigning varying weights point-wisely, we attempt to enhance the high-level representation of point cloud by capturing the long-range dependencies along its channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>In general, our approach is a point-based method. As stated in Section I, there exists some unsolved problems with the current methods. To tackle the challenges, we start from classical MLP mapping <ref type="bibr" target="#b21">[22]</ref> and edge features <ref type="bibr" target="#b48">[49]</ref>. Specifically, MLP performs as a channel-wise fully-connected layer, which is shared among the points in a point cloud feature map. In practice, this operation can be implemented as a 1×1 convolution operating on the point cloud feature map, followed by a batch normalization layer and an activation function:</p><formula xml:id="formula_0">M(·) := τ BN c 1×1 (·) ,<label>(1)</label></formula><p>where M is the MLP, τ is the activation function, BN is batch normalization, c is convolution and its subscript presents the filter size.</p><p>Besides, edge features f ψ are considered as a type of implicit geometric feature in embedding space since they are crafted following geometric relations between points based on the constraint of high-dimensional Euclidean distance. Specifically, we form the edge features by concatenating the point feature x i with its edge vectors x j − x i :</p><formula xml:id="formula_1">f ψ (x i ) = [x i , x j − x i ], x i ∈ R d ; ∀x j ∈ N i(x i ). (2)</formula><p>The x i means a point feature in d-dimensional embedding space, where the entire point cloud feature map is X N ×d . The x j 's are the local neighbors of x i found by the k-nearest neighbors (knn) algorithm, and "[ ]" denotes a concatenation operation.</p><p>Particularly, MLP can encode the local context given by the edge features, denoted as EdgeConv (E):</p><formula xml:id="formula_2">E(·) := M f ψ (·) ,<label>(3)</label></formula><p>Physically, EdgeConv crafts radial local-graphs consisting of edges pointing from the neighbors to the centroids, and then mapped by a shared MLP in feature space. With the symmetric function being applied afterward, corresponding regional information will be encoded in each point feature.</p><p>As reviewed in Section II, symmetric functions like maxpooling can summarize an outline of a region by simply extracting the prominent characters. This process is efficient and effective, especially for a large set of points. However, for a relatively small local area with fewer points, we still need a more reasonable way that takes all neighbors into account. In this case, we may intuitively aggregate the local details by calculating a weighted sum of neighboring features. In order to perform the local aggregation adaptively, we can learn the weights through a shared local fully-connected (LFC) layer. Different from an MLP that mainly encodes a single point feature using a 1 × 1 convolution, an LFC acts as a 1 × k convolution over a local area, learns the weights for k neighbors in this area, and finally aggregates the weighted sum of k neighbors' features as the detailed local context.</p><formula xml:id="formula_3">L(·) := τ BN c 1×k (·) ,<label>(4)</label></formula><p>where L stands for an LFC layer. Therefore, the edge features processed by LFC is denoted as EdgeLFC (J ):</p><formula xml:id="formula_4">J (·) := L f ψ (·) .<label>(5)</label></formula><p>In general, EdgeLFC can aggregate the local context and map geometric features simultaneously via learnable weights, by which the details of a local area will be retained.</p><p>Based on the motivation of enriching geometric features for the point cloud, we intend to provide more low-level geometric clues for following high-level geometric feature learning. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the network consists of a series of modules learning local context-based feature maps from different scales of embedding space. In this section, we introduce the structures of critical modules and mathematically formulate the relevant operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Geometric Point Descriptor</head><p>Generally, our geometric network works under a cooperative mechanism between low-level and high-level geometric features. Specifically, the Geometric Point Descriptor module aims to provide information about explicit geometric relations in low-level space for the following implicit geometric feature learning in high-level space. As regular scattered point clouds give 3D coordinates only, we may reinvent additional features with respect to 3D geometry.</p><p>As introduced in Section I, triangular faces are used in mesh or volumetric data, since they can illustrate the outline of 3D objects. Although reconstructing the faces is not the topic of this work, some relevant features can be estimated to expand the low-level geometric representation of the 3D point clouds. As long as the estimating procedure performs equally on all points, the generated features can offer reasonable geometric clues for later processing.</p><p>Specifically, we first search the two nearest neighbors of point p i ∈ R 3 in 3D space, denoting these as p j1 , p j2 ∈ R 3 . By forming a triangle using them, we can explicitly estimate a geometric descriptor, p i , corresponding to p i :</p><formula xml:id="formula_5">p i = (p i , normal, edge 1 , edge 2 , length 1 , length 2 ),<label>(6)</label></formula><p>concretely:</p><formula xml:id="formula_6">pi ∈ R 14                  pi = (x, y, z); pi ∈ R 3 , normal = edge 1 × edge 2 ; normal ∈ R 3 , edge 1 = pj1 − pi; edge 1 ∈ R 3 , edge 2 = pj2 − pi; edge 2 ∈ R 3 , length1 = |edge 1 |; length1 ∈ R 1 , length2 = |edge 2 |; length2 ∈ R 1 .</formula><p>Through this simple process of concise formulation and reduced computation, we expand the low-level geometric features of p i with estimated edges and a normal vector besides its 3D coordinates. In general, the Geometric Point Descriptor module helps to enhance the scattered point clouds using additional internal geometric relations, and further benefits the following feature learning in high-level space. For instance, the CNN will explicitly recognize an outlier based on the geometric characters of its larger edge lengths, i.e., 5 th and 6 th features in Equation <ref type="bibr" target="#b5">6</ref>. More discussions on this module are in Section IV-C2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attentional Back-projection Edge Features Module</head><p>Besides the low-level geometric information, our network also requires the modules for geometric feature learning in high-level space. Mainly, an error feedback mechanism aims to accurately control a process by monitoring its actual output, feeding the errors, and attaining the desired output. Inspired by this, we can design a structure to regulate the learning process by projecting a restored signal back. Assisted by neural network training mechanism, a specifically defined error signal is expected to constrain feature learning and mitigate possible bias against desired output as training continues.</p><p>Here we propose the Attentional Back-projection Edge Features Module (ABEM) to represent the local geometric context for point clouds in embedding space. In general, ABEM consists of two branches encoding local prominent and fine-grained geometric features, respectively.</p><p>1) Prominent Feature Encoding: Similar to previous annotations, x i ∈ R d is a point feature of the feature map X N ×d . Specifically, we add different subscripts to the operations defined in Equation 1,3,4,5 if they are applied more than once.</p><p>Firstly, we employ an EdgeConv denoted as E Φ for the input feature map since this operation can represent the geometric relations of a local area in high-level space. With Equation 3 applied, the output of E Φ is:</p><formula xml:id="formula_7">f Φi = E Φ (x i ) = M Φ f ψ (x i ) ; f Φi ∈ R d ×k ,<label>(7)</label></formula><p>where k is the number of neighbours of a point. Following our EdgeConv E Φ , an ideal output feature map should fully encode local geometric details. On the other hand, if the output f Φi is indeed informative, we may restore the original input x i from f Φi through a back-projection. Therefore, such back-projection should simulate the reverse process of EdgeConv. In our proposal, we use a shared LFC (L Γ ) layer to realize the back-projection for f Φi . In contrast to EdgeConv expanding a center point to local neighbors in the new embedding space, the physical meaning of LFC operation is to pull the neighbors back to the previous space (d → d) and aggregate at a center point (k → 1) via learnable weights. In general, we define the restored feature as Back-projection Signal x i . Based on Equations 4 and 7:</p><formula xml:id="formula_8">x i = L Γ (f Φi ) = L Γ M Φ f ψ (x i ) ; x i ∈ R d . (8)</formula><p>Further, we take the difference between the original and restored (i.e., Back-projection Signal) inputs, ∆x i , to formulate the corresponding Error Signal:</p><formula xml:id="formula_9">∆x i = x i − x i ; ∆x i ∈ R d ,<label>(9)</label></formula><p>and the impact of the Error Signal on f Φi can be estimated by another EdgeConv (E Υ ) projection:</p><formula xml:id="formula_10">f Υi = E Υ (∆x i ) = M Υ f ψ (∆x i ) ; f Υi ∈ R d ×k . (10)</formula><p>Although this approach implies a similar idea to the autoencoder of restoring the input <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b61">[62]</ref>, there are clear differences between them. Auto-encoders usually changes the size of the point cloud and concatenates the features from different resolutions for a comprehensive representation; in our method, the size of the point cloud is constant while the dimension of features is manipulated to estimate lost information (i.e. Error Signal) of the input. Moreover, different from the auto-encoder concatenating features for complementary information, we can correct the output f Φi by adding the Error Signal impact, f Υi . As <ref type="figure" target="#fig_3">Figure 4</ref> shows, max-pooling is applied over the local area to extract prominent features, and the Channel-wise Affinity Attention (CAA, see Section III-C and Equation <ref type="bibr" target="#b14">15</ref> for details) module is placed afterwards in order to refine the final feature representation. In general, the output of the Prominent Feature Encoding branch is:</p><formula xml:id="formula_11">f Mi = CAA max {k} (f Φi + f Υi ) ; f Mi ∈ R d . (11)</formula><p>2) Fine-grained Feature Encoding: Although prominent features extracted by max-pooling can efficiently encode much geometric information for simple point clouds, more finegrained features are required for comprehensive feature representation, especially for some challenging cases e.g., real objects, complex scenes, or similar shapes. Especially, we apply an EdgeLFC (explained in <ref type="figure" target="#fig_4">Equation 5</ref>), J Θ , to aggregate the local geometric details from all neighbors, in order to complement the prominent feature encoding outputs. With a CAA module applied, the output of the Fine-grained Feature Encoding branch is:</p><formula xml:id="formula_12">f Ai = CAA J Θ (x i ) = CAA L Θ f ψ (x i ) ; f Ai ∈ R d .<label>(12)</label></formula><p>In the end, we concatenate the outputs of the two encoding branches as the learned geometric features by the ABEM:</p><formula xml:id="formula_13">f i = ABEM x i = concat(f Mi , f Ai ); f i ∈ R 2d . (13)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Channel-wise Affinity Attention Module</head><p>As explained in Section II, most attention structures regarding point clouds operate in point-space, but the effects are not apparent as work <ref type="bibr" target="#b39">[40]</ref> and <ref type="table" target="#tab_3">Table V</ref> indicate. Instead, distributing attention weights along channels is worth investigating. Inspired by the spacetime non-local block <ref type="bibr" target="#b34">[35]</ref>, we calculate the long-range dependencies of channels without being concerned by point cloud's unorderedness. Practically, the corresponding computation usually costs a lot of time and memory. Therefore, we need to search for an effective and efficient way to avoid channel-wise redundancy in an abstract embedding space.</p><p>We propose our Channel-wise Affinity Attention (CAA) module targeting the channels of high-level point cloud feature maps. As <ref type="figure" target="#fig_4">Figure 5</ref> shows, the main structure includes a Compact Channel-wise Comparator (CCC) block, a Channel Affinity Estimator (CAE) block, and a residual connection.</p><p>1) Compact Channel-wise Comparator block: Since the CAA module mainly focuses on channel-space, it is necessary to reduce the computing cost caused by the complexity in point-space. In the case of a given d-dimensional feature map F N ×d , the Compact Channel-wise Comparator (CCC) block can simplify context in each channel by an shared MLP operating on channel vector c i (where c i ∈ R N ; and F N ×d = [c 1 , c 2 , ... , c d ]) to implicitly replace N original points with a smaller number N = N/ratio; ratio &gt; 1. In contrast to taking all points or selecting certain points based on additional metrics, CCC can efficiently reduce the size but sufficiently retain the information of each channel:</p><formula xml:id="formula_14">q i = M q (c i ); q i ∈ R N , k i = M k (c i ); k i ∈ R N .</formula><p>Specifically, M q (·) and M k (·) are two MLPs operating for Query Matrix and Key Matrix <ref type="bibr" target="#b33">[34]</ref>:  and we apply the product of the transposed Query Matrix and Key Matrix to estimate the corresponding channel-wise Similarity Matrix:</p><formula xml:id="formula_15">Q N ×d = [q 1 , q 2 , ... , q d ], K N ×d = [k 1 , k 2 , ... , k d ],</formula><formula xml:id="formula_16">S d×d = Q T K,</formula><p>where S i,j approximates the similarity between the i th channel and the j th channel of the given feature map F N ×d .</p><p>2) Channel Affinity Estimator block: Typical self-attention structures used to calculate the long-range dependencies in spatial data based on inner-products since the values can somehow represent the similarities between the items. In contrast, we define the non-similarities between channels and term it Channel Affinity. In our approach, the Channel Affinity Matrix of the feature map F N ×d :</p><formula xml:id="formula_17">A d×d = sof tmax expand 1→d max d→1 (S) − S .<label>(14)</label></formula><p>Particularly, we select the maximum similarities along the columns of S, and then expand them into the same size of S. By subtracting the original S from the expanded matrix, the channels with higher similarities have lower affinities (illustrated in <ref type="figure" target="#fig_4">Figure 5(b)</ref>). Besides, softmax is added to normalize the values, since A d×d is used as the weight matrix for refinement. In this way, each channel can put higher weights on other distinct ones, thereby avoid aggregating similar/redundant information.</p><p>According to the weight matrix, we can refine each channel of a point feature by taking a weighted sum of all channels. Here, we apply another MLP, M v (·), to get the Value Matrix:</p><formula xml:id="formula_18">V N ×d = [v 1 , v 2 , ... , v d ], v i = M v (c i ); v i ∈ R N .</formula><p>Therefore, such a refining process can be easily realized by multiplying the Value Matrix V N ×d and the Channel Affinity Matrix A d×d . Additionally, we use a residual connection with a learnable weight α to ease training. The refined feature map by CAA is given:</p><formula xml:id="formula_19">F N ×d = CAA(F) = F + α · VA.<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first provide the experimental settings utilized for evaluation. We then compare the performances of our approach against state-of-the-art methods on synthetic and real-world point clouds. Furthermore, we analyze the effects of different components of our network by conducting relevant ablation studies. By measuring the complexity, visualizing the intermediate outputs, and testing on semantic segmentation task, we comprehensively demonstrate our network's properties.  A. Experimental Settings 1) Implementation: Our proposed network starts with a Geometric Point Descriptor module, which expands the 3D coordinates of each input point into a 14-degree low-level geometric vector as in Equation 6. Next, the expanded features are passed through four cascaded Attentional Back-projection Edge Features Modules (ABEMs), aggregating (k = 20) local neighbors to extract high-level geometric features in different scales <ref type="bibr" target="#b63">(64,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128</ref>, and 256) of embedding space. These parameters are similarly adopted from <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b48">[49]</ref>.</p><p>To incorporate the information from different scales, we concatenate the output feature maps of the ABEMs together, and a shared MLP with CAA module can further integrate them into a 1024-dimensional representation. Then we pointwisely apply max-pooling and average-pooling in parallel to form a global embedding vector, by which the other three fully connected layers (having 512, 256, c output) can regress the confidence scores for all possible categories. In the end, we employ cross-entropy between predictions and ground-truth labels as our loss function. This project is implemented with PyTorch, relevant training and testing works are on Linux and GeForce RTX 2080Ti GPUs. <ref type="bibr" target="#b0">1</ref> 2) Training: We apply Stochastic Gradient Descent (SGD) with the momentum of 0.9 as the optimizer for training, and its initial learning rate of 0.1 decreases to 0.001 by cosine annealing <ref type="bibr" target="#b62">[63]</ref>. The batch size is set to 32 for 300 epochs of training. Besides, training data is augmented with random scaling and translation as in <ref type="bibr" target="#b48">[49]</ref>, while there is no pre or post-processing performed during testing.</p><p>3) Datasets: We show the performance of the proposed network on two datasets: a classical ModelNet40 <ref type="bibr" target="#b70">[71]</ref>, which contains synthetic object point clouds, and the recent ScanOb-jectNN <ref type="bibr" target="#b11">[12]</ref> composed of real-world object point clouds.</p><p>• ModelNet40. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular and au-  counterpart due to the complex background, missing parts, and various deformations, as in <ref type="figure" target="#fig_6">Figure 6(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification Performance</head><p>1) Results on synthetic point clouds: <ref type="table" target="#tab_0">Table I</ref> shows the quantitative results on the synthetic ModelNet40 classification benchmark. The result of our network (overall acc: 93.8% and average class acc: 91.0%) exceeds state-of-the-art methods comparing under the same given input i.e. 1k coordinates only. It is worth mentioning that our approach is even better than some methods using extra input points e.g., DGCNN <ref type="bibr" target="#b48">[49]</ref> with 2k inputs obtained overall acc: 93.5% and average class acc: 90.7%. Similarly, our algorithm outperforms SO-Net <ref type="bibr" target="#b25">[26]</ref>, which uses rich information such as 5k inputs with normals, achieving an overall accuracy of 93.4%, and average class accuracy of 90.8%. We are also higher than RS-CNN <ref type="bibr" target="#b22">[23]</ref>, which uses post-processing of a ten votes evaluation arrangement during testing. Rarely do recent methods result in an improvement of more than 0.5%, while we achieve nearly 1% improvement compared to the latest work <ref type="bibr" target="#b45">[46]</ref>. In terms of the network architecture itself, our approach is indicated to be promising and effective for classification.</p><p>2) Results on real-world point clouds: For real-world point cloud classification, we use the same network architecture, training strategy, and 1k of 3D coordinates as input. To have </p><formula xml:id="formula_20">p i = (p i ) 3 93.4 2 p i = (p i , n i , l 1 , l 2 ) 8 93.5 3 p i = (p i , e 1 , e 2 , l 1 , l 2 ) 11 93.5 4 p i = (p i , p j1 , p j2 , l 1 , l 2 ) 11 93.3 5 p i = (p i , n i , e 1 , e 2 )</formula><p>12 93.7 6 p i = (p i , n i , e 1 , e 2 , l1, l2) 14 93.8 7 p i = (p i , n i , n j1 , n j2 , e 1 , e 2 ) 18 93.3 8 p i = (p i , n i , n j1 , n j2 , e 1 , e 2 , e 3 , l 1 , l 2 , l 3 ) 24 93.1 fair comparisons with state-of-the-art methods, we conduct the classification experiment with its most challenging variant 2 as in <ref type="bibr" target="#b11">[12]</ref>. We present <ref type="table" target="#tab_0">Table II</ref> with the accuracies of competing methods on the real-world ScanObjectNN dataset. The results of our network with an overall accuracy of 80.5% and an average class accuracy of 77.8% have significantly improved the classification accuracy on the benchmark. We perform better than other methods in 6 out of 15 categories, and for challenging cases like box or display, we improve the accuracy by more than 10%. Furthermore, our approach performs even better than DGCNN <ref type="bibr" target="#b48">[49]</ref> and PointNet++ <ref type="bibr" target="#b46">[47]</ref> with background-aware network (BGA) <ref type="bibr" target="#b11">[12]</ref> , which is specially designed for this dataset. Despite the fact that the ScanObjectNN dataset contains hard cases for point cloud classification, the main difficulties are effectively managed by our approach. For example, the remaining background points in real data may confuse the network because they are irrelevant to the shape structure. However, the CAA module is intended to re-weight the points according to their channel affinities, by which the importance of background points can be reduced. Further, in order to form a global representation of a point cloud, we learn both local prominent and fine-grained features using our ABEM module. Such comprehensive local context for a global representation can alleviate the side effects of the missing parts in real point clouds. Although some deformations or distortions may exist  <ref type="table" target="#tab_0">Non-local ARE ADAPTED FROM THE 2D CASE BY  LETTING THE 3D POINT SPACE BE EQUAL TO THE 2D SPATIAL SPACE. N IS  THE NUMBER OF POINTS, AND C IS THE NUMBER OF CHANNELS.  COMMONLY IN DEEP LEARNING-BASED POINT CLOUD NETWORKS, N IS  MUCH LARGER THAN C.)</ref> module operating space attention map size overall acc. SE <ref type="bibr" target="#b35">[36]</ref> Channel-wise 1 × C 93.4 Non-local <ref type="bibr" target="#b34">[35]</ref> Point-wise N × N 93.3 L2G-SA <ref type="bibr" target="#b41">[42]</ref> Point-wise N × N 93.2 Pointwise-SA <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> Point-wise N × N 93.2 PSA <ref type="bibr" target="#b72">[73]</ref> Point-wise 2 × (N × N ) 93.3 Criss-Cross <ref type="bibr" target="#b73">[74]</ref> Point-wise N × N 93.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAA (ours)</head><p>Channel-wise C × C 93.8</p><p>in real point cloud data, our network can be relatively robust since the geometric point descriptor is designed to enhance the relations between points and enrich the low-level geometric information.</p><p>To further demonstrate the effectiveness and robustness, we also apply F 1 score, the harmonic mean of the precision and recall, as another quantitative measurement comparing with the best two state-of-the-art methods on ScanObjectNN official leaderboard <ref type="bibr" target="#b71">[72]</ref>: DGCNN <ref type="bibr" target="#b48">[49]</ref> and PointCNN <ref type="bibr" target="#b47">[48]</ref>, as well as the BGA-based methods 3 in <ref type="bibr" target="#b11">[12]</ref>. As the bottom rows of <ref type="table" target="#tab_0">Table II</ref> show, our approach achieves better results in terms of both overall and average class F 1 scores. Specifically, we have a higher F 1 score on 5 out of 15 testing categories. In general, our network has a better balance between the precision and recall targeting real-world point cloud classification. As stated before, the point cloud analysis aims to solve practical problems. The excellent performance on real-world point clouds is a strong affirmation of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>To verify the functions and effectiveness of different components in our network, we conduct ablation studies about the proposed modules in this work. Besides, we check the network complexity, visualize the learned features, and test preliminary segmentation performance.</p><p>1) Effects of geometric features: <ref type="table" target="#tab_0">Table III</ref> shows the results of the ablation study concerning the effects of different geometric features learned by our network. It can be observed from model 1,2,3 that the high-level geometric features learned by our Attentional Back-projection Edge Features Module (ABEM) can benefit point cloud classification. Comparing with the fined-grained features in <ref type="bibr">Equation 12</ref>, the local prominent features in Equation 11 contribute more to the high-level geometric features from ABEM. However, it is worth noting that the effect of increasing low-level geometric information alone (i.e., model 4) is minimal because the network lacks modules in high-level space to utilize the provided information fully. In contrast, once we enrich the geometric features from both levels, as in model 5, the performance can significantly boost. In general, our geometric network is tested to work as a whole with inherent relations between low-level and high-level geometric features.</p><p>2) Forms of Geometric Point Descriptor: Although estimating low-level geometric features is intuitive, the expanded information can provide more geometric cues for the following processing in high-level space. To avoid possible overfitting caused by involving the redundant information, we investigate various combinations of low-level geometric items to find a better form for the Geometric Point Descriptor. Besides the items introduced in Equation 6, here we investigate more features:</p><formula xml:id="formula_21">                     pj1 = (xj1, yj1, zj1); pj1 ∈ R 3 , pj2 = (xj2, yj2, zj2); pj2 ∈ R 3 , edge 3 = pj1 − pj2; edge 3 ∈ R 3 , length3 = |edge 2 |; length3 ∈ R 1 , normali = edge 1 × edge 2 ; normali ∈ R 3 , normalj1 = (−edge 1 ) × (−edge 3 ); normalj1 ∈ R 3 , normalj2 = (−edge 2 ) × edge 3 ; normalj2 ∈ R 3 .</formula><p>In general, the geometric point descriptor is explicitly formed in 3D space, containing four types of low-level geometric features: the coordinates, the edges, the normals, and the length of edges. As a fundamental feature, the point's coordinates (i.e., position) can intuitively indicate the global information. On the other hand, the point's edges can imply more local information by involving its neighbors' relative positions, and the lengths of edges can partly estimate the density distribution of the point's neighborhood. In addition, we calculate the point's normal using the cross-product of its edge vectors to enhance the descriptor's robustness against possible deformations such as scaling, translation, rotation, etc. However, it is not always optimal to combine all of these features since it may incorporate redundant information: for example, model 7 in <ref type="table" target="#tab_0">Table IV</ref> contains the normal vectors of neighbors, i.e., n j1 and n j2 , which do not help describe the low-level geometry of the point p i ; and for model 8, the edge clues between two neighbors (i.e., e 3 and l 3 ) share similar information to e 1 , l 1 and e 2 , l 2 .</p><p>According to the experiments shown in <ref type="table" target="#tab_0">Table IV</ref>, we conclude the best form is model 6, as in Equation 6, since it can adequately represent the low-level geometric features of the estimated triangle face while in a relatively compact form.</p><p>However, the effects of the current geometric point descriptor may be marginal in some downstream tasks (e.g., semantic segmentation, detection) for large-scale data since the point distributions can be quite uneven as the data is  scanned from real scenes. Even for similar underlying surfaces, possible distortions can cause inconsistent geometric relations. As a consequence, the representation ability of our low-level geometry-based descriptor will be affected. Hence, we intend to further investigate a generalized form of the geometric point descriptor in the future.</p><p>3) Attention modules for point cloud classification: To have fair comparisons, we replace all CAA modules in our network with other advanced attention modules. Besides the accuracies, we also extract the size of the intermediate attention map to estimate the corresponding memory requirement of each approach. As <ref type="table" target="#tab_3">Table V</ref> presents, comparing with classical 2D adapted and state-of-the-art 3D designed attention modules, our Channel Affinity Attention module shows both effectiveness and efficiency. 4) Network Complexity: Since our network consists of several key modules, it is necessary to compare our model's complexity with other state-of-the-art methods. Specifically, we apply similar measurements as in <ref type="bibr" target="#b48">[49]</ref>: model size and inference time. To be fair with the competing methods, we test under the same conditions (GeForce GTX 1080, batch size = 1) as in <ref type="bibr" target="#b48">[49]</ref>. Although we apply complex operations like FC layers or knn searching as others, we manage to simplify the complexity by sharing weights, removing alignment blocks <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b48">[49]</ref>, compacting features, etc. <ref type="table" target="#tab_0">Table VI</ref> indicates that we have a relatively good compromise between  accuracy and efficiency. Furthermore, our model's inference time running on GeForce RTX 2080Ti is about 17.5ms, and we expect to optimize it for real-time applications further. 5) Visualization: From <ref type="figure" target="#fig_8">Figure 7</ref>(a) we can visualize corresponding learned features by the prominent feature encoding branch (i.e., <ref type="bibr">Equation 11</ref>) and the fine-grained feature encoding branch (i.e., Equation 12) of our Attentional Backprojection Edge Features Module (i.e., ABEM in Equation 13) respectively in different layers of the network. Particularly, all examples show the property of CNN: the shallow layers have a higher impact on simpler features e.g., edges, corners, etc. while deep layers connect those simpler features for more semantically specific parts. As we stated before, prominent feature encoding branch extracts main local context while fine-grained feature encoding branch helps to capture missing details. From the figure, we can observe that fine-grained features complement prominent features as expected.</p><p>As stated in Section III-C, the proposed Channel-wise Affinity Attention (CAA) module aims to refine the feature map along channels. From <ref type="figure" target="#fig_8">Figure 7</ref>(b), we observe the effects of CAA on the feature map. The CAA module enhances the distinct channels based on calculated Channel Affinity (i.e., A d×d in <ref type="figure" target="#fig_3">Equation 14</ref>), which contributes to more attention on semantically meaningful parts. On the other hand, the responses of simple features like edges/corners, are effectively weakened. In general, CAA refines the feature map for more compact and effective representations. 6) Part Segmentation: We further demonstrate the advantages of our approach on the point cloud segmentation task. Specifically, we adopt the ShapeNet Part Segmentation dataset <ref type="bibr" target="#b74">[75]</ref>, which includes 16,881 instance-level point clouds of 16 different classes, with each point being labeled as one of the 50 candidate parts. In terms of the training and test sets, we follow the official data split in <ref type="bibr" target="#b75">[76]</ref>.</p><p>Generally, our architecture for segmentation is in a basic fully convolutional network <ref type="bibr" target="#b78">[79]</ref> form. By using the backbone of our classification network, we concatenate the learned point features from different scales. After attaching the max-pooled global feature and one-hot-labels of objects, we apply fully connected layers to regress the confidence scores for 50 possible part classes of each point. Particularly, we only input the 3D coordinates of 2048 points for each point cloud for training and evaluate the performance based on the Intersection-over-Union (i.e., IoU) metric. The IoU of the shape is calculated by the mean value of IoUs of all parts in that shape, while the Mean Intersection over Union (i.e., mIoU) is the average of IoUs for all testing shapes. <ref type="table" target="#tab_0">Table VII</ref> presents the experimental results of some stateof-the-art methods. In terms of instance mIoU, we outperform most of the competitors while we are slightly behind the latest PointASNL <ref type="bibr" target="#b45">[46]</ref>, which forms an upsampling and downsampling architecture targeting point cloud segmentation tasks. Regarding the mIoU of each class, our basic fully convolutional network for segmentation won 5 out of a total 16 tested classes. Moreover, we can easily find in <ref type="figure" target="#fig_9">Figure 8</ref> that our approach performs better than DGCNN <ref type="bibr" target="#b48">[49]</ref> on the ShapeNet Part Segmentation dataset. 7) Object Detection: In addition, we conduct 3D object detection experiments to validate our network's generalization ability for more downstream tasks. Based on the Hough voting module proposed by VoteNet <ref type="bibr" target="#b77">[78]</ref>, we apply different stateof-the-art models as the network backbone running on SUN RGB-D V2 <ref type="bibr" target="#b76">[77]</ref> datatset.</p><p>Specifically, the SUN RGB-D V2 dataset has 5285 point clouds in the training set and other 5050 samples as the testing set, where the coordinates of points are used as the input. Following the same metric used in <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, we evaluate the performance by calculating the mean average precision (mAP) with an IoU threshold of 0.25. <ref type="table" target="#tab_0">Table VIII</ref> presents the detailed results of different backbone networks. Particularly, our network achieves a higher overall score of 59.3% mAP@0.25 and performs better than others in five out of ten testing categories. In general, our network shows excellent potential in 3D object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose a new point-based CNN network targeting point cloud classification. To leverage more geometric features from scattered point clouds, we explicitly apply the Geometric Point Descriptor to estimate geometric clues in lowlevel space. Accordingly, the Attentional Back-projection Edge Features Module can automatically assist learning a better point cloud representation using local geometric context in high-level space. Besides, the Channel-wise Affinity Attention module further refines the learned feature map by focusing on distinct channels. To compare our method with other state-of-the-art networks, we conduct experiments on synthetic and real-world point cloud datasets. Also, we validate the properties of the proposed modules through necessary ablation studies and visualizations. The results show the effectiveness and robustness of our approach. More tasks, e.g. scene parsing, instance segmentation, will be explored in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>S</head><label></label><figDesc>. Qiu and S. Anwar are with Data61, CSIRO (The Commonwealth Scientific and Industrial Research Organisation) and Research School of Engineering, Australian National University, Canberra, ACT 2601, Australia. (Email: {shi.qiu, saeed.anwar}@data61.csiro.au). N. Barnes is with School of Computing, Australian National University, Canberra, ACT 2601, Australia. (Email: nick.barnes@anu.edu.au).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Our network architecture. The Geometric Point Descriptor offers more low-level geometric clues for subsequent high-level geometric feature learning in cascaded ABEMs, representing the point features in multiple scales of embedding space by aggregating local context. The CAA module refines the learned feature map to avoid channel-wise redundancy. Finally, we use the concatenation of max-pooling and average-pooling results, as well as fully connected layers to regress the class scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The Geometric Point Descriptor module explicitly enhances the lowlevel geometric relations between scattered points using triangular features. The red point is an example of a target, while yellow points are corresponding neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Attentional Back-projection Edge Features Module (ABEM) learns local geometric features in high-level spaces from two branches: one extracts prominent local context, while the other aggregates fine-grained information as a complement. We take the concatenated outputs to form the final feature representations of the point clouds and only pass the prominent features for the next layer of learning. (Note: The ABEM works in a forward manner as introduced in Section III-B.) (a) Channel-wise Affinity Attention module (CAA). (b) Details of Compact Channel-wise Comparator block (CCC) and Channel Affinity Estimator block (CAE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Channel-wise Affinity Attention module (CAA, in 5(a)). Specifically, the Compact Channel-wise Comparator block (CCC, in blue of 5(b)) can approximate the similarity matrix between the channels of the input feature map. Then Channel Affinity Estimator (CAE, in the grey of 5(b)) takes the similarity matrix for the calculation of the affinity matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Examples of ModelNet40 dataset. (b) Examples of ScanObjectNN dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Examples from point cloud classification datasets. ModelNet40 include more categories of point clouds as inFigure 6(a), while the samples shown inFigure 6(b) from ScanObjectNN are more practically challenging due to complex background (points in lighter color), missing parts, and deformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Examples of the features learned by ABEMs in different layers of our network. Shallow layers focus on edges/corners, while deep layers cover semantically meaningful (discriminative) parts that can help to identify the class label of a certain point cloud: e.g., the tail/wings of a plane (1st row in thefigure), the fingerboard of a guitar (2nd row in the figure), or the head/arms/legs of a human (3rd row in the figure), etc. Further, prominent and fine-grained feature encoding branches capture complementary high-level geometric features, which are crucial for comprehensive point clouds representations. (b) Changes to the feature map processed by the Channel-wise Affinity Attention (CAA) module. The CAA module can enhance distinct channels and avoid redundancy by distributing weights along channels according to the calculated Channel Affinity in Equation 14. After CAA processing, the feature map emphasizes the semantically meaningful parts such as the headboard/mattress of a bed (1st column in the figure), the back of a sofa (2nd column in the figure), the hood/chassis of a car (3rd column in the figure), or the keyboard of a piano (4th column in the figure), while the responses of simple edges/corners are reduced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Visualizations of the intermediate feature maps. Figure 7(a) shows the features learned by ABEMs, and Figure 7(b) depicts the CAA effects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Examples of the point cloud segmentation results on ShapeNet Part. The first row shows the results of DGCNN<ref type="bibr" target="#b48">[49]</ref>; the second row exhibits the results of our approach, while the ground truths are in the last row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="5">CLASSIFICATION RESULTS (%) ON ModelNet40 BENCHMARK. (coords:</cell></row><row><cell cols="5">(x, y, z) COORDINATES, norm: POINT NORMAL, voting: MULTI-VOTES</cell></row><row><cell cols="5">EVALUATION STRATEGY, k:×2 10 , -: UNKNOWN)</cell></row><row><cell>method</cell><cell>input type</cell><cell>#points</cell><cell>avg class acc.</cell><cell>overall acc.</cell></row><row><cell>ECC [64]</cell><cell>coords</cell><cell>1k</cell><cell>83.2</cell><cell>87.4</cell></row><row><cell>PointNet [22]</cell><cell>coords</cell><cell>1k</cell><cell>86.0</cell><cell>89.2</cell></row><row><cell>SCN [40]</cell><cell>coords</cell><cell>1k</cell><cell>87.6</cell><cell>90.0</cell></row><row><cell>Kd-Net [65]</cell><cell>coords</cell><cell>1k</cell><cell>-</cell><cell>90.6</cell></row><row><cell>PointCNN [48]</cell><cell>coords</cell><cell>1k</cell><cell>88.1</cell><cell>92.2</cell></row><row><cell>PCNN [66]</cell><cell>coords</cell><cell>1k</cell><cell>-</cell><cell>92.3</cell></row><row><cell>DensePoint [67]</cell><cell>coords</cell><cell>1k</cell><cell>-</cell><cell>92.8</cell></row><row><cell>RS-CNN [23]</cell><cell>coords</cell><cell>1k</cell><cell>-</cell><cell>92.9</cell></row><row><cell>DGCNN [49]</cell><cell>coords</cell><cell>1k</cell><cell>90.2</cell><cell>92.9</cell></row><row><cell>KP-Conv [68]</cell><cell>coords</cell><cell>1k</cell><cell>-</cell><cell>92.9</cell></row><row><cell>PointASNL [46]</cell><cell>coords</cell><cell>1k</cell><cell>-</cell><cell>92.9</cell></row><row><cell>Ours</cell><cell>coords</cell><cell>1k</cell><cell>91.0</cell><cell>93.8</cell></row><row><cell>RGCNN [69]</cell><cell>coords + norm</cell><cell>1k</cell><cell>87.3</cell><cell>90.5</cell></row><row><cell>SO-Net [26]</cell><cell>coords</cell><cell>2k</cell><cell>87.3</cell><cell>90.9</cell></row><row><cell>PointNet++ [47]</cell><cell>coords + norm</cell><cell>5k</cell><cell>-</cell><cell>91.9</cell></row><row><cell>SpiderCNN [25]</cell><cell>coords + norm</cell><cell>5k</cell><cell>-</cell><cell>92.4</cell></row><row><cell>DensePoint [67]</cell><cell>coords + voting</cell><cell>1k</cell><cell>-</cell><cell>93.2</cell></row><row><cell>SO-Net [26]</cell><cell>coords + norm</cell><cell>5k</cell><cell>90.8</cell><cell>93.4</cell></row><row><cell>DGCNN [49]</cell><cell>coords</cell><cell>2k</cell><cell>90.7</cell><cell>93.5</cell></row><row><cell>RS-CNN [23]</cell><cell>coords + voting</cell><cell>1k</cell><cell>-</cell><cell>93.6</cell></row><row><cell cols="5">thoritative because of its clean shapes, well-constructed</cell></row><row><cell cols="5">dataset, etc. However, it is challenging due to limited and</cell></row><row><cell cols="5">unbalanced training data for various categories. Specifi-</cell></row><row><cell cols="5">cally, the original ModelNet40 consists of 12,311 CAD-</cell></row><row><cell cols="5">generated meshes in 40 categories, of which 9,843 are</cell></row><row><cell cols="5">used for training while the remaining 2,468 are reserved</cell></row><row><cell cols="5">for testing. Moreover, the corresponding data points are</cell></row><row><cell cols="5">uniformly sampled from the mesh surfaces (see Fig-</cell></row><row><cell cols="5">ure 6(a)), and then further preprocessed by moving to the</cell></row><row><cell cols="5">origin and scaling into a unit sphere. For our experiments,</cell></row><row><cell cols="5">we only input the 3D coordinates (x, y, z) of 1024 points</cell></row><row><cell cols="3">for each point cloud sample.</cell><cell></cell><cell></cell></row></table><note>• ScanObjectNN. To further prove the effectiveness and robustness of our classification network, we conduct experiments on ScanObjectNN, a newly published real- world object dataset with about 15k objects in 15 cat- egories. Although it has fewer categories than Model- Net40, it is more practically challenging than its synthetic</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CLASSIFICATION</head><label>II</label><figDesc>RESULTS (%) ON ScanObjectNN BENCHMARK.</figDesc><table><row><cell></cell><cell cols="2">overall acc.</cell><cell>avg class acc.</cell><cell>bag</cell><cell>bin</cell><cell>box</cell><cell cols="2">cabinet chair</cell><cell cols="3">desk display door</cell><cell>shelf</cell><cell>table</cell><cell>bed</cell><cell>pillow</cell><cell>sink</cell><cell>sofa</cell><cell>toilet</cell></row><row><cell># shapes</cell><cell></cell><cell>-</cell><cell>-</cell><cell>298</cell><cell>794</cell><cell>406</cell><cell>1344</cell><cell>1585</cell><cell>592</cell><cell>678</cell><cell>892</cell><cell>1084</cell><cell>922</cell><cell>564</cell><cell>405</cell><cell>469</cell><cell>1058</cell><cell>325</cell></row><row><cell cols="2">3DmFV [70]</cell><cell>63</cell><cell>58.1</cell><cell cols="3">39.8 62.8 15.0</cell><cell>65.1</cell><cell>84.4</cell><cell>36.0</cell><cell>62.3</cell><cell>85.2</cell><cell>60.6</cell><cell>66.7</cell><cell>51.8</cell><cell>61.9</cell><cell>46.7</cell><cell>72.4</cell><cell>61.2</cell></row><row><cell cols="2">PointNet [22]</cell><cell>68.2</cell><cell>63.4</cell><cell cols="3">36.1 69.8 10.5</cell><cell>62.6</cell><cell>89.0</cell><cell>50.0</cell><cell>73.0</cell><cell>93.8</cell><cell>72.6</cell><cell>67.8</cell><cell>61.8</cell><cell>67.6</cell><cell>64.2</cell><cell>76.7</cell><cell>55.3</cell></row><row><cell cols="2">SpiderCNN [25]</cell><cell>73.7</cell><cell>69.8</cell><cell cols="3">43.4 75.9 12.8</cell><cell>74.2</cell><cell>89.0</cell><cell>65.3</cell><cell>74.5</cell><cell>91.4</cell><cell>78.0</cell><cell>65.9</cell><cell>69.1</cell><cell>80.0</cell><cell>65.8</cell><cell>90.5</cell><cell>70.6</cell></row><row><cell cols="2">PointNet++ [47]</cell><cell>77.9</cell><cell>75.4</cell><cell cols="3">49.4 84.4 31.6</cell><cell>77.4</cell><cell>91.3</cell><cell>74.0</cell><cell>79.4</cell><cell>85.2</cell><cell>72.6</cell><cell>72.6</cell><cell>75.5</cell><cell>81.0</cell><cell>80.8</cell><cell>90.5</cell><cell>85.9</cell></row><row><cell cols="2">DGCNN [49]</cell><cell>78.1</cell><cell>73.6</cell><cell cols="3">49.4 82.4 33.1</cell><cell>83.9</cell><cell>91.8</cell><cell>63.3</cell><cell>77.0</cell><cell>89.0</cell><cell>79.3</cell><cell>77.4</cell><cell>64.5</cell><cell>77.1</cell><cell>75.0</cell><cell>91.4</cell><cell>69.4</cell></row><row><cell cols="2">PointCNN [48]</cell><cell>78.5</cell><cell>75.1</cell><cell cols="3">57.8 82.9 33.1</cell><cell>83.6</cell><cell>92.6</cell><cell>65.3</cell><cell>78.4</cell><cell>84.8</cell><cell>84.2</cell><cell>67.4</cell><cell>80.0</cell><cell>80.0</cell><cell>72.5</cell><cell>91.9</cell><cell>71.8</cell></row><row><cell cols="2">BGA-DGCNN [12]</cell><cell>79.7</cell><cell>75.7</cell><cell cols="3">48.2 81.9 30.1</cell><cell>84.4</cell><cell>92.6</cell><cell>77.3</cell><cell>80.4</cell><cell>92.4</cell><cell>80.5</cell><cell>74.1</cell><cell>72.7</cell><cell>78.1</cell><cell>79.2</cell><cell>91.0</cell><cell>72.9</cell></row><row><cell cols="2">BGA-PN++ [12]</cell><cell>80.2</cell><cell>77.5</cell><cell>54.2</cell><cell>85.9</cell><cell>39.8</cell><cell>81.7</cell><cell>90.8</cell><cell>76.0</cell><cell>84.3</cell><cell>87.6</cell><cell>78.4</cell><cell>74.4</cell><cell>73.6</cell><cell>80.0</cell><cell>77.5</cell><cell>91.9</cell><cell>85.9</cell></row><row><cell>Ours</cell><cell></cell><cell>80.5</cell><cell>77.8</cell><cell>59.0</cell><cell>84.4</cell><cell>44.4</cell><cell>78.2</cell><cell>92.1</cell><cell>66</cell><cell>91.2</cell><cell>91.0</cell><cell>86.7</cell><cell>70.4</cell><cell>82.7</cell><cell>78.1</cell><cell>72.5</cell><cell>92.4</cell><cell>77.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">F1 Scores</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methhod</cell><cell cols="2">overall F1</cell><cell>avg class F1</cell><cell>bag</cell><cell>bin</cell><cell>box</cell><cell cols="2">cabinet chair</cell><cell cols="3">desk display door</cell><cell>shelf</cell><cell>table</cell><cell>bed</cell><cell>pillow</cell><cell>sink</cell><cell>sofa</cell><cell>toilet</cell></row><row><cell cols="2">DGCNN [49]</cell><cell>0.78</cell><cell>0.75</cell><cell cols="3">0.58 0.80 0.38</cell><cell>0.76</cell><cell>0.87</cell><cell>0.69</cell><cell>0.84</cell><cell>0.89</cell><cell>0.82</cell><cell>0.78</cell><cell>0.68</cell><cell>0.76</cell><cell>0.79</cell><cell>0.85</cell><cell>0.77</cell></row><row><cell cols="2">PointCNN [48]</cell><cell>0.78</cell><cell>0.77</cell><cell cols="3">0.60 0.78 0.45</cell><cell>0.79</cell><cell>0.86</cell><cell>0.68</cell><cell>0.84</cell><cell>0.89</cell><cell>0.79</cell><cell>0.71</cell><cell>0.84</cell><cell>0.75</cell><cell>0.83</cell><cell>0.88</cell><cell>0.82</cell></row><row><cell cols="2">BGA-DGCNN [12]</cell><cell>0.79</cell><cell>0.77</cell><cell cols="3">0.58 0.80 0.37</cell><cell>0.76</cell><cell>0.91</cell><cell>0.76</cell><cell>0.86</cell><cell>0.91</cell><cell>0.83</cell><cell>0.76</cell><cell>0.72</cell><cell>0.80</cell><cell>0.79</cell><cell>0.86</cell><cell>0.78</cell></row><row><cell cols="2">BGA-PN++ [12]</cell><cell>0.80</cell><cell>0.78</cell><cell cols="2">0.64 0.81</cell><cell>0.46</cell><cell>0.78</cell><cell>0.91</cell><cell>0.68</cell><cell>0.86</cell><cell>0.89</cell><cell>0.81</cell><cell>0.73</cell><cell>0.81</cell><cell>0.79</cell><cell>0.80</cell><cell>0.88</cell><cell>0.86</cell></row><row><cell>Ours</cell><cell></cell><cell>0.80</cell><cell>0.79</cell><cell cols="2">0.61 0.77</cell><cell>0.52</cell><cell>0.79</cell><cell>0.87</cell><cell>0.70</cell><cell>0.91</cell><cell>0.91</cell><cell>0.82</cell><cell>0.75</cell><cell>0.88</cell><cell>0.79</cell><cell>0.78</cell><cell>0.87</cell><cell>0.82</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">ABLATION STUDY ABOUT DIFFERENT GEOMETRIC FEATURES ON</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">ModelNet40 (%). (GEOMETRIC POINT DESCRIPTOR: LOW-LEVEL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">GEOMETRIC FEATURES AS IN EQUATION 6. ABEM (ATTENTIONAL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">BACK-PROJECTION EDGE FEATURES MODULE): HIGH-LEVEL GEOMETRIC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">FEATURES AS IN EQUATION 13; PROMINENT: LOCAL PROMINENT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">FEATURES ONLY AS IN EQUATION 11; FINE-GRAINED: LOCAL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">FINE-GRAINED FEATURES ONLY AS IN EQUATION 12)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>model</cell><cell cols="2">Geometric Point Descriptor</cell><cell cols="3">ABEM Prominent Fine-grained</cell><cell cols="2">overall acc.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>92.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>93.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>92.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>92.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>93.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV ABLATION</head><label>IV</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">STUDY ABOUT DIFFERENT GEOMETRIC POINT DESCRIPTOR p i</cell></row><row><cell cols="4">FORMS ON ModelNet40 CLASSIFICATION ACCURACY (%). (p: (x, y, z), n:</cell></row><row><cell></cell><cell>normal, e: edge, l: |edge|.)</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>Geometric Point Descriptor</cell><cell>length</cell><cell>overall acc.</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V CLASSIFICATION</head><label>V</label><figDesc>ACCURACY (%) FOR VARIOUS Attention MODULES ON ModelNet40. (SE AND</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI COMPLEXITY</head><label>VI</label><figDesc>OF CLASSIFICATION NETWORK ON ModelNet40. ( * THE INFERENCE TIME OF OUR MODEL RUNNING ON GEFORCE GTX 2080TI CAN BE REDUCED TO 17.5ms)</figDesc><table><row><cell>method</cell><cell>model size (MB)</cell><cell>time (ms)</cell><cell>overall acc. (%)</cell></row><row><cell>PointNet [22]</cell><cell>40</cell><cell>16.6</cell><cell>89.2</cell></row><row><cell>PointNet++ [47]</cell><cell>12</cell><cell>163.2</cell><cell>90.7</cell></row><row><cell>PCNN [66]</cell><cell>94</cell><cell>117.0</cell><cell>92.3</cell></row><row><cell>DGCNN [49]</cell><cell>21</cell><cell>27.2</cell><cell>92.9</cell></row><row><cell>Ours</cell><cell>34</cell><cell>32.2</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII PART</head><label>VII</label><figDesc>SEGMENTATION RESULTS (MIOU(%)) ON ShapeNet Part DATASET.</figDesc><table><row><cell></cell><cell>air</cell><cell>bag</cell><cell>cap</cell><cell>car</cell><cell>chair</cell><cell>ear</cell><cell>guitar</cell><cell cols="2">knife lamp</cell><cell cols="2">laptop moto</cell><cell>mug</cell><cell cols="2">pistol rocket</cell><cell>skate</cell><cell>table</cell><cell>overall</cell></row><row><cell></cell><cell>plane</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>phone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>bike</cell><cell></cell><cell></cell><cell></cell><cell>board</cell><cell></cell><cell>mIoU</cell></row><row><cell># shapes</cell><cell>2690</cell><cell>76</cell><cell>55</cell><cell>898</cell><cell>3758</cell><cell>69</cell><cell>787</cell><cell>392</cell><cell>1547</cell><cell>451</cell><cell>202</cell><cell>184</cell><cell>283</cell><cell>66</cell><cell>152</cell><cell>5271</cell><cell>16881</cell></row><row><cell>PointNet [22]</cell><cell>83.4</cell><cell cols="3">78.7 82.5 74.9</cell><cell>89.6</cell><cell>73.0</cell><cell>91.5</cell><cell>85.9</cell><cell>80.8</cell><cell>95.3</cell><cell>65.2</cell><cell>93.0</cell><cell>81.2</cell><cell>57.9</cell><cell>72.8</cell><cell>80.6</cell><cell>83.7</cell></row><row><cell>PointNet++ [47]</cell><cell>82.4</cell><cell cols="3">79.0 87.7 77.3</cell><cell>90.8</cell><cell>71.8</cell><cell>91.0</cell><cell>85.9</cell><cell>83.7</cell><cell>95.3</cell><cell>71.6</cell><cell>94.1</cell><cell>81.3</cell><cell>58.7</cell><cell>76.4</cell><cell>82.6</cell><cell>85.1</cell></row><row><cell>A-SCN [40]</cell><cell>83.8</cell><cell cols="3">80.8 83.5 79.3</cell><cell>90.5</cell><cell>69.8</cell><cell>91.7</cell><cell>86.5</cell><cell>82.9</cell><cell>96.0</cell><cell>69.2</cell><cell>93.8</cell><cell>82.5</cell><cell>62.9</cell><cell>74.4</cell><cell>80.8</cell><cell>84.6</cell></row><row><cell>SpiderCNN [25]</cell><cell>83.5</cell><cell cols="3">81.0 87.2 77.5</cell><cell>90.7</cell><cell>76.8</cell><cell>91.1</cell><cell>87.3</cell><cell>83.3</cell><cell>95.8</cell><cell>70.2</cell><cell>93.5</cell><cell>82.7</cell><cell>59.7</cell><cell>75.8</cell><cell>82.8</cell><cell>85.3</cell></row><row><cell>SO-Net [26]</cell><cell>81.9</cell><cell cols="3">83.5 84.8 78.1</cell><cell>90.8</cell><cell>72.2</cell><cell>90.1</cell><cell>83.6</cell><cell>82.3</cell><cell>95.2</cell><cell>69.3</cell><cell>94.2</cell><cell>80.0</cell><cell>51.6</cell><cell>72.1</cell><cell>82.6</cell><cell>84.6</cell></row><row><cell>PCNN [66]</cell><cell>82.4</cell><cell cols="3">80.1 85.5 79.5</cell><cell>90.8</cell><cell>73.2</cell><cell>91.3</cell><cell>86.0</cell><cell>85.0</cell><cell>95.7</cell><cell>73.2</cell><cell>94.8</cell><cell>83.3</cell><cell>51.0</cell><cell>75.0</cell><cell>81.8</cell><cell>85.1</cell></row><row><cell>DGCNN [49]</cell><cell>84.0</cell><cell cols="3">83.4 86.7 77.8</cell><cell>90.6</cell><cell>74.7</cell><cell>91.2</cell><cell>87.5</cell><cell>82.8</cell><cell>95.7</cell><cell>66.3</cell><cell>94.9</cell><cell>81.1</cell><cell>63.5</cell><cell>74.5</cell><cell>82.6</cell><cell>85.2</cell></row><row><cell>RGCNN [69]</cell><cell>80.2</cell><cell>82.8</cell><cell>92.6</cell><cell>75.3</cell><cell>89.2</cell><cell>73.7</cell><cell>91.3</cell><cell>88.4</cell><cell>83.3</cell><cell>96.0</cell><cell>63.9</cell><cell>95.7</cell><cell>60.9</cell><cell>44.6</cell><cell>72.9</cell><cell>80.4</cell><cell>84.3</cell></row><row><cell>PointASNL [46]</cell><cell>84.1</cell><cell>84.7</cell><cell>87.9</cell><cell>79.7</cell><cell>92.2</cell><cell>73.7</cell><cell>91.0</cell><cell>87.2</cell><cell>84.2</cell><cell>95.8</cell><cell>74.4</cell><cell>95.2</cell><cell>81.0</cell><cell>63.0</cell><cell>76.3</cell><cell>83.2</cell><cell>86.1</cell></row><row><cell>Ours</cell><cell>84.5</cell><cell cols="3">82.2 86.8 78.9</cell><cell>91.1</cell><cell>74.5</cell><cell>91.4</cell><cell>89.0</cell><cell>84.5</cell><cell>95.5</cell><cell>69.6</cell><cell>94.2</cell><cell>83.4</cell><cell>57.8</cell><cell>75.5</cell><cell>83.5</cell><cell>85.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII 3D</head><label>VIII</label><figDesc>OBJECT DETECTION RESULTS (%) ON SUN RGB-D V2 [77] DATASET. WE ADOPT THE HOUGH VOTING STRUCTURE FROM VOTENET [78], WHILE REPLACING THE BACKBONE WITH DIFFERENT POINT CLOUD NETWORKS. THE PERFORMANCES ARE EVALUATED USING THE METRIC OF THE AVERAGE PRECISION (MAP) WITH 3D IOU THRESHOLD 0.25<ref type="bibr" target="#b76">[77]</ref>,<ref type="bibr" target="#b77">[78]</ref>.</figDesc><table><row><cell>backbone</cell><cell>bed</cell><cell>table</cell><cell>sofa</cell><cell cols="2">chair toilet</cell><cell>desk</cell><cell>dresser</cell><cell cols="2">night-stand book-shelf</cell><cell>bathtub</cell><cell>mAP</cell></row><row><cell>PointNet [22]</cell><cell>46.2</cell><cell>23.9</cell><cell>28.3</cell><cell>52.1</cell><cell>38.4</cell><cell>9.5</cell><cell>1.4</cell><cell>4.6</cell><cell>3.6</cell><cell>17.8</cell><cell>22.6</cell></row><row><cell>PointNet++ [47]</cell><cell>84.8</cell><cell>51.3</cell><cell>68.5</cell><cell>77.0</cell><cell>88.8</cell><cell>25.3</cell><cell>27.3</cell><cell>60.6</cell><cell>32.2</cell><cell>72.2</cell><cell>58.8</cell></row><row><cell>DGCNN [49]</cell><cell>84.6</cell><cell>46.0</cell><cell>60.0</cell><cell>76.8</cell><cell>89.1</cell><cell>19.5</cell><cell>31.2</cell><cell>54.8</cell><cell>24.6</cell><cell>72.8</cell><cell>55.9</cell></row><row><cell>RS-CNN [23]</cell><cell>85.7</cell><cell>47.3</cell><cell>64.9</cell><cell>77.0</cell><cell>85.2</cell><cell>24.9</cell><cell>27.9</cell><cell>60.5</cell><cell>30.4</cell><cell>77.4</cell><cell>58.1</cell></row><row><cell>Ours</cell><cell>84.8</cell><cell>51.0</cell><cell>64.1</cell><cell>77.0</cell><cell>88.1</cell><cell>24.4</cell><cell>31.2</cell><cell>62.9</cell><cell>32.4</cell><cell>77.5</cell><cell>59.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The codes and models will be available at https://github.com/ShiQiu0419/ GBNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">PB T50 RS, the hardest case of ScanobjectNN dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">pre-trained models in<ref type="bibr" target="#b11">[12]</ref>, downloaded from https://github.com/hkust-vgd/ scanobjectnn</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Point cloud encoding for 3d building model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="345" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and adaptive 3d reconstruction with extensively high completeness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="278" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graphbased static 3d point clouds geometry coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Oliveira Rente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ascenso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning localized representations of point clouds with graph-convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointhop: An explainable machine learning method for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kadam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Review of 20 years of range sensor development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Blais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of electronic imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="231" to="243" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Use of lidar in landslide investigations: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaboyedoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oppikofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abellán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Derron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pedrazzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural hazards</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A network architecture for point cloud classification via automatic depth images generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rahmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4176" to="4184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sampled-point network for classification of deformed building element point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2164" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointaugment: An autoaugmentation framework for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive hierarchical down-sampling for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nezhadarya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient ransac for point-cloud shape detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Registration of point cloud data from a geometric optimization perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing</title>
		<meeting>the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">International archives of photogrammetry remote sensing and spatial information sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vosselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dijkman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">/W4</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
	<note>3d building model reconstruction from point clouds and ground plans</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic sensor-based control of robots with visual feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neuman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="404" to="417" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Computational systems biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">420</biblScope>
			<biblScope unit="issue">6912</biblScope>
			<biblScope unit="page" from="206" to="210" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical back projection network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pu-gan: A point cloud upsampling adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bilinear attention networks for person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="8030" to="8039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real image denoising with feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Point attention network for semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12663</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">L2g auto-encoder: Understanding point clouds by local-to-global reconstruction with hierarchical self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="989" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Estimating surface normals in noisy point cloud data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the nineteenth annual symposium on Computational geometry</title>
		<meeting>the nineteenth annual symposium on Computational geometry</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="322" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Voronoi-based curvature and feature estimation from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mérigot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="743" to="756" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shape matching with 3d shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Körtgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Novotni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 7th central European seminar on computer graphics</title>
		<imprint>
			<publisher>Budmerice</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Dilated point convolutions: On the receptive field size of point convolutions on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attentional pointnet for 3d-object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paigwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Erkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pointgrow: Autoregressively learned point cloud generation with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05591</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pcan: 3d attention map learning using contextual information for point cloud based retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">445</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8778" to="8785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pyramnet: Point cloud pyramid attention network and graph embedding module for classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhiheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03299</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph attention convolution for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">305</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Gapnet: Graph attention based point neural network for exploiting local feature of point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Z</forename><surname>Fragonara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsourdos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08705</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rgcnn: Regularized graph cnn for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="746" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">3dmfv: Threedimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">3d scene understanding benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hkust-Vgd</surname></persName>
		</author>
		<ptr target="https://hkust-vgd.github.io/benchmark/,2020" />
		<imprint>
			<biblScope unit="page" from="2020" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">Shapenet: An informationrich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
