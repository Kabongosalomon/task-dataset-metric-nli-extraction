<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mesh R-CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mesh R-CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes. Project</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The last few years have seen rapid advances in 2D object recognition. We can now build systems that accurately recognize objects <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b61">61]</ref>, localize them with 2D bounding boxes <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b47">47]</ref> or masks <ref type="bibr" target="#b18">[18]</ref>, and predict 2D keypoint positions <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b65">65]</ref> in cluttered, real-world images. Despite their impressive performance, these systems ignore one critical fact: that the world and the objects within it are 3D and extend beyond the XY image plane.</p><p>At the same time, there have been significant advances in 3D shape understanding with deep networks. A menagerie of network architectures have been developed for different 3D shape representations, such as voxels <ref type="bibr" target="#b5">[5]</ref>, pointclouds <ref type="bibr" target="#b8">[8]</ref>, and meshes <ref type="bibr" target="#b69">[69]</ref>; each representation carries its own benefits and drawbacks. However, this diverse and creative set of techniques has primarily been developed on synthetic benchmarks such as ShapeNet <ref type="bibr" target="#b4">[4]</ref> consisting of rendered objects in isolation, which are dramatically less complex than natural-image benchmarks used for 2D object recognition like ImageNet <ref type="bibr" target="#b52">[52]</ref> and COCO <ref type="bibr" target="#b37">[37]</ref>. Input Image 2D Recognition 3D Meshes 3D Voxels <ref type="figure">Figure 1</ref>. Mesh R-CNN takes an input image, predicts object instances in that image and infers their 3D shape. To capture diversity in geometries and topologies, it first predicts coarse voxels which are refined for accurate mesh predictions.</p><p>We believe that the time is ripe for these hitherto distinct research directions to be combined. We should strive to build systems that (like current methods for 2D perception) can operate on unconstrained real-world images with many objects, occlusion, and diverse lighting conditions but that (like current methods for 3D shape prediction) do not ignore the rich 3D structure of the world.</p><p>In this paper we take an initial step toward this goal. We draw on state-of-the-art methods for 2D perception and 3D shape prediction to build a system which inputs a real-world RGB image, detects the objects in the image, and outputs a category label, segmentation mask, and a 3D triangle mesh giving the full 3D shape of each detected object.</p><p>Our method, called Mesh R-CNN, builds on the state-ofthe-art Mask R-CNN <ref type="bibr" target="#b18">[18]</ref> system for 2D recognition, augmenting it with a mesh prediction branch that outputs highresolution triangle meshes.</p><p>Our predicted meshes must be able to capture the 3D structure of diverse, real-world objects. Predicted meshes should therefore dynamically vary their complexity, topology, and geometry in response to varying visual stimuli. However, prior work on mesh prediction with deep networks <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b69">69]</ref> has been constrained to deform from <ref type="bibr">Figure 2</ref>. Example predictions from Mesh R-CNN on Pix3D. Using initial voxel predictions allows our outputs to vary in topology; converting these predictions to meshes and refining them allows us to capture fine structures like tabletops and chair legs.</p><p>fixed mesh templates, limiting them to fixed mesh topologies. As shown in <ref type="figure">Figure 1</ref>, we overcome this limitation by utilizing multiple 3D shape representations: we first predict coarse voxelized object representations, which are converted to meshes and refined to give highly accurate mesh predictions. As shown in <ref type="figure">Figure 2</ref>, this hybrid approach allows Mesh R-CNN to output meshes of arbitrary topology while also capturing fine object structures.</p><p>We benchmark our approach on two datasets. First, we evaluate our mesh prediction branch on ShapeNet <ref type="bibr" target="#b4">[4]</ref>, where our hybrid approach of voxel prediction and mesh refinement outperforms prior work by a large margin. Second, we deploy our full Mesh R-CNN system on the recent Pix3D dataset <ref type="bibr" target="#b60">[60]</ref> which aligns 395 models of IKEA furniture to real-world images featuring diverse scenes, clutter, and occlusion. To date Pix3D has primarily been used to evalute shape predictions for models trained on ShapeNet, using perfectly cropped, unoccluded image segments <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b73">73]</ref>, or synthetic rendered images of Pix3D models <ref type="bibr" target="#b76">[76]</ref>. In contrast, using Mesh R-CNN we are the first to train a system on Pix3D which can jointly detect objects of all categories and estimate their full 3D shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our system inputs a single RGB image and outputs a set of detected object instances, with a triangle mesh for each object. Our work is most directly related to recent advances in 2D object recognition and 3D shape prediction. We also draw more broadly from work on other 3D perception tasks. 2D Object Recognition Methods for 2D object recognition vary both in the type of information predicted per object, and in the overall system architecture. Object detectors output per-object bounding boxes and category labels <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b47">47]</ref>; Mask R-CNN <ref type="bibr" target="#b18">[18]</ref> additionally outputs instance segmentation masks. Our method extends this line of work to output a full 3D mesh per object. Single-View Shape Prediction Recent approaches use a variety of shape representations for single-image 3D reconstruction. Some methods predict the orientation <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b20">20]</ref> or 3D pose <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b66">66]</ref> of known shapes. Other approaches predict novel 3D shapes as sets of 3D points <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b34">34]</ref>, patches <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b70">70]</ref>, or geometric primitives <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b67">67]</ref>; others use deep networks to model signed distance functions <ref type="bibr" target="#b42">[42]</ref>. These methods can flexibly represent complex shapes, but rely on post-processing to extract watertight mesh outputs.</p><p>Some methods predict regular voxel grids <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b72">72]</ref>; while intuitive, scaling to high-resolution outputs requires complex octree <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b62">62]</ref> or nested shape architectures <ref type="bibr" target="#b49">[49]</ref>.</p><p>Others directly output triangle meshes, but are constrained to deform from fixed <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b69">69]</ref> or retrieved mesh templates <ref type="bibr" target="#b51">[51]</ref>, limiting the topologies they can represent.</p><p>Our approach uses a hybrid of voxel prediction and mesh deformation, enabling high-resolution output shapes that can flexibly represent arbitrary topologies.</p><p>Some methods reconstruct 3D shapes without 3D annotations <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b75">75]</ref>. This is an important direction, but at present we consider only the fully supervised case due to the success of strong supervision for 2D perception. Multi-View Shape Prediction There is a broad line of work on multi-view reconstruction of objects and scenes, from classical binocular stereo <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b53">53]</ref> to using shape priors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b21">21]</ref> and modern learning techniques <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b54">54]</ref>. In this work, we focus on single-image shape reconstruction. 3D Inputs Our method inputs 2D images and predicts semantic labels and 3D shapes. Due to the increasing availabilty of depth sensors, there has been growing interest in methods predicting semantic labels from 3D inputs such as RGB-D images <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b58">58]</ref> and pointclouds <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b63">63]</ref>. We anticipate that incorporating 3D inputs into our method could improve the fidelity of our shape predictions. Datasets Advances in 2D perception have been driven by large-scale annotated datasets such as ImageNet <ref type="bibr" target="#b52">[52]</ref> and COCO <ref type="bibr" target="#b37">[37]</ref>. Datasets for 3D shape prediction have lagged their 2D counterparts due to the difficulty of collecting 3D annotations. ShapeNet <ref type="bibr" target="#b4">[4]</ref> is a large-scale dataset of CAD models which are rendered to give synthetic images. The IKEA dataset <ref type="bibr" target="#b33">[33]</ref> aligns CAD models of IKEA objects to real-world images; Pix3D <ref type="bibr" target="#b60">[60]</ref> extends this idea to a larger set of images and models. Pascal3D <ref type="bibr" target="#b74">[74]</ref> aligns CAD models to real-world images, but it is unsuitable for shape reconstruction since its train and test sets share the same small set of models. KITTI <ref type="bibr" target="#b11">[11]</ref> annotates outdoor street scenes with 3D bounding boxes, but does not provide shape annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to design a system that inputs a single image, detects all objects, and outputs a category label, bounding box, segmentation mask and 3D triangle mesh for each detected object. Our system must be able to handle cluttered real-world images, and must be trainable end-to-end. Our output meshes should not be constrained to a fixed topol- <ref type="bibr">Figure 3</ref>. System overview of Mesh R-CNN. We augment Mask R-CNN with 3D shape inference. The voxel branch predicts a coarse shape for each detected object which is further deformed with a sequence of refinement stages in the mesh refinement branch. ogy in order to accommodate a wide variety of complex real-world objects. We accomplish these goals by marrying state-of-the-art 2D perception with 3D shape prediction.</p><p>Specifically, we build on Mask R-CNN <ref type="bibr" target="#b18">[18]</ref>, a state-ofthe-art 2D perception system. Mask R-CNN is an end-toend region-based object detector. It inputs a single RGB image and outputs a bounding box, category label, and segmentation mask for each detected object. The image is first passed through a backbone network (e.g. ResNet-50-FPN <ref type="bibr" target="#b35">[35]</ref>); next a region proposal network (RPN) <ref type="bibr" target="#b47">[47]</ref> gives object proposals which are processed with object classification and mask prediction branches.</p><p>Part of Mask R-CNN's success is due to RoIAlign which extracts region features from image features while maintaining alignment between the input image and features used in the final prediction branches. We aim to maintain similar feature alignment when predicting 3D shapes.</p><p>We infer 3D shapes with a novel mesh predictor, comprising a voxel branch and a mesh refinement branch. The voxel branch first estimates a coarse 3D voxelization of an object, which is converted to an initial triangle mesh. The mesh refinement branch then adjusts the vertex positions of this initial mesh using a sequence of graph convolution layers operating over the edges of the mesh.</p><p>The voxel branch and mesh refinement branch are homologous to the existing box and mask branches of Mask R-CNN. All take as input image-aligned features corresponding to RPN proposals. The voxel and mesh losses, described in detail below, are added to the box and mask losses and the whole system is trained end-to-end. The output is a set of boxes along with their predicted object scores, masks and 3D shapes. We call our system Mesh R-CNN, which is illustrated in <ref type="figure">Figure 3</ref>.</p><p>We now describe our mesh predictor, consisting of the voxel branch and mesh refinement branch, along with its associated losses in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mesh Predictor</head><p>At the core of our system is a mesh predictor which receives convolutional features aligned to an object's bounding box and predicts a triangle mesh giving the object's full 3D shape. Like Mask R-CNN, we maintain correspondence between the input image and features used at all stages of processing via region-and vertex-specific alignment operators (RoIAlign and VertAlign). Our goal is to capture instance-specific 3D shapes of all objects in an image. Thus, each predicted mesh must have instance-specific topology (genus, number of vertices, faces, connected components) and geometry (vertex positions).</p><p>We predict varying mesh topologies by deploying a sequence of shape inference operations. First, the voxel branch makes bottom-up voxelized predictions of each object's shape, similar to Mask R-CNN's mask branch. These predictions are converted into meshes and adjusted by the mesh refinement head, giving our final predicted meshes.</p><p>The output of the mesh predictor is a triangle mesh T = (V, F ) for each object. V = {v i ∈ R 3 } is the set of vertex positions and F ⊆ V ×V ×V is a set of triangular faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Voxel Branch</head><p>The voxel branch predicts a grid of voxel occupancy probabilities giving the course 3D shape of each detected object. It can be seen as a 3D analogue of Mask R-CNN's mask prediction branch: rather than predicting a M × M grid giving the object's shape in the image plane, we instead predict a G × G × G grid giving the object's full 3D shape.</p><p>Like Mask R-CNN, we maintain correspondence between input features and predicted voxels by applying a small fully-convolutional network <ref type="bibr" target="#b39">[39]</ref> to the input feature map resulting from RoIAlign. This network produces a feature map with G channels giving a column of voxel occupancy scores for each position in the input.  <ref type="figure">Figure 4</ref>. Predicting voxel occupancies aligned to the image plane requires an irregularly-shaped voxel grid. We achieve this effect by making voxel predictions in a space that is transformed by the camera's (known) intrinsic matrix K. Applying K −1 transforms our predictions back to world space. This results in frustumshaped voxels in world space.</p><p>Maintaining pixelwise correspondence between the image and our predictions is complex in 3D since objects become smaller as they recede from the camera. As shown in <ref type="figure">Figure 4</ref>, we account for this by using the camera's (known) intrinsic matrix to predict frustum-shaped voxels. Cubify: Voxel to Mesh The voxel branch produces a 3D grid of occupancy probabilities giving the coarse shape of an object. In order to predict more fine-grained 3D shapes, we wish to convert these voxel predictions into a triangle mesh which can be passed to the mesh refinement branch.</p><p>We bridge this gap with an operation called cubify. It inputs voxel occupancy probabilities and a threshold for binarizing voxel occupancy. Each occupied voxel is replaced with a cuboid triangle mesh with 8 vertices, 18 edges, and 12 faces. Shared vertices and edges between adjacent occupied voxels are merged, and shared interior faces are eliminated. This results in a watertight mesh whose topology depends on the voxel predictions.</p><p>Cubify must be efficient and batched. This is not trivial and we provide technical implementation details of how we achieve this in Appendix A. Alternatively marching cubes <ref type="bibr" target="#b40">[40]</ref> could extract an isosurface from the voxel grid, but is significantly more complex. Voxel Loss The voxel branch is trained to minimize the binary cross-entropy between predicted voxel occupancy probabilities and true voxel occupancies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Mesh Refinement Branch</head><p>The cubified mesh from the voxel branch only provides a coarse 3D shape, and it cannot accurately model fine structures like chair legs. The mesh refinement branch processes this initial cubified mesh, refining its vertex positions with a sequence of refinement stages. Similar to <ref type="bibr" target="#b69">[69]</ref>, each refinement stage consists of three operations: vertex alignment, which extracts image features for vertices; graph convolution, which propagates information along mesh edges; and vertex refinement, which updates vertex positions. Each layer of the network maintains a 3D position v i and a feature vector f i for each mesh vertex.</p><p>Vertex Alignment yields an image-aligned feature vector for each mesh vertex 1 . We use the camera's intrinsic matrix to project each vertex onto the image plane. Given a feature map, we compute a bilinearly interpolated image feature at each projected vertex position <ref type="bibr" target="#b22">[22]</ref>.</p><p>In the first stage of the mesh refinement branch, VertAlign outputs an initial feature vector for each vertex. In subsequent stages, the VertAlign output is concatenated with the vertex feature from the previous stage. Graph Convolution <ref type="bibr" target="#b29">[29]</ref> propagates information along mesh edges. Given input vertex features</p><formula xml:id="formula_0">{f i } it computes updated features f i = ReLU W 0 f i + j∈N (i) W 1 f j where N (i)</formula><p>gives the i-th vertex's neighbors in the mesh, and W 0 and W 1 are learned weight matrices. Each stage of the mesh refinement branch uses several graph convolution layers to aggregate information over local mesh regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vertex Refinement computes updated vertex positions</head><formula xml:id="formula_1">v i = v i + tanh(W vert [f i ; v i ])</formula><p>where W vert is a learned weight matrix. This updates the mesh geometry, keeping its topology fixed. Each stage of the mesh refinement branch terminates with vertex refinement, producing an intermediate mesh output which is further refined by the next stage. Mesh Losses Defining losses that operate natively on triangle meshes is challenging, so we instead use loss functions defined over a finite set of points. We represent a mesh with a pointcloud by densely sampling its surface. Consequently, a pointcloud loss approximates a loss over shapes.</p><p>Similar to <ref type="bibr" target="#b57">[57]</ref>, we use a differentiable mesh sampling operation to sample points (and their normal vectors) uniformly from the surface of a mesh. To this end, we implement an efficient batched sampler; see Appendix B for details. We use this operation to sample a pointcloud P gt from the ground-truth mesh, and a pointcloud P i from each intermediate mesh prediction from our model.</p><p>Given two pointclouds P , Q with normal vectors, let Λ P,Q = {(p, arg min q p − q ) : p ∈ P } be the set of pairs (p, q) such that q is the nearest neighbor of p in Q, and let u p be the unit normal to point p. The chamfer distance between pointclouds P and Q is given by <ref type="bibr" target="#b0">(1)</ref> and the (absolute) normal distance is given by</p><formula xml:id="formula_2">L cham (P, Q) = |P | −1 (p,q)∈Λ P,Q p − q 2 + |Q| −1 (q,p)∈Λ Q,P q − p 2</formula><formula xml:id="formula_3">Lnorm(P, Q) = −|P | −1 (p,q)∈Λ P,Q |up · uq| − |Q| −1 (q,p)∈Λ Q,P |uq · up|. (2)</formula><p>The chamfer and normal distances penalize mismatched positions and normals between two pointclouds, but minimizing these distances alone results in degenerate meshes (see <ref type="figure">Figure 5</ref>). High-quality mesh predictions require additional shape regularizers: To this end we use an edge loss</p><formula xml:id="formula_4">L edge (V, E) = 1 |E| (v,v )∈E v − v 2 where E ⊆ V × V 1</formula><p>Vertex alignment is called perceptual feature pooling in <ref type="bibr" target="#b69">[69]</ref> are the edges of the predicted mesh. Alternatively, a Laplacian loss <ref type="bibr" target="#b7">[7]</ref> also imposes smoothness constraints.</p><p>The mesh loss of the i-th stage is a weighted sum of L cham (P i , P gt ), L norm (P i , P gt ) and L edge (V i , E i ). The mesh refinement branch is trained to minimize the mean of these losses across all refinement stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We benchmark our mesh predictor on ShapeNet <ref type="bibr" target="#b4">[4]</ref>, where we compare with state-of-the-art approaches. We then evaluate our full Mesh R-CNN for the task of 3D shape prediction in the wild on the challenging Pix3D dataset <ref type="bibr" target="#b60">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ShapeNet</head><p>ShapeNet <ref type="bibr" target="#b4">[4]</ref> provides a collection of 3D shapes, represented as textured CAD models organized into semantic categories following WordNet <ref type="bibr" target="#b43">[43]</ref>, and has been widely used as a benchmark for 3D shape prediction. We use the subset of ShapeNetCore.v1 and rendered images from <ref type="bibr" target="#b5">[5]</ref>. Each mesh is rendered from up to 24 random viewpoints, giving RGB images of size 137 × 137. We use the train / test splits provided by <ref type="bibr" target="#b69">[69]</ref>, which allocate 35,011 models (840,189 images) to train and 8,757 models (210,051 images) to test; models used in train and test are disjoint. We reserve 5% of the training models as a validation set.</p><p>The task on this dataset is to input a single RGB image of a rendered ShapeNet model on a blank background, and output a 3D mesh for the object in the camera coordinate system. During training the system is supervised with pairs of images and meshes. Evaluation We adopt evaluation metrics used in recent work <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b69">69]</ref>. We sample 10k points uniformly at random from the surface of predicted and ground-truth meshes, and use them to compute Chamfer distance (Equation 1), Normal consistency, (one minus Equation 2), and F1 τ at various distance thresholds τ , which is the harmonic mean of the precision at τ (fraction of predicted points within τ of a ground-truth point) and the recall at τ (fraction of groundtruth points within τ of a predicted point). Lower is better for Chamfer distance; higher is better for all other metrics.</p><p>With the exception of normal consistency, these metrics depend on the absolute scale of the meshes. In <ref type="table">Table 1</ref> we follow <ref type="bibr" target="#b69">[69]</ref> and rescale by a factor of 0.57; for all other results we follow <ref type="bibr" target="#b8">[8]</ref> and rescale so the longest edge of the ground-truth mesh's bounding box has length 10. Implementation Details Our backbone feature extractor is ResNet-50 pretrained on ImageNet. Since images depict a single object, the voxel branch receives the entire conv5 3 feature map, bilinearly resized to 24 × 24, and predicts a 48 × 48 × 48 voxel grid. The VertAlign operator concatenates features from conv2 3, conv3 4, conv4 6, and conv5 3 before projecting to a vector of dimension  <ref type="table">Table 1</ref>. Single-image shape reconstruction results on ShapeNet, using the evaluation protocol from <ref type="bibr" target="#b69">[69]</ref>. For <ref type="bibr" target="#b69">[69]</ref>, † are results reported in their paper and ‡ is the model released by the authors.</p><formula xml:id="formula_5">Chamfer (↓) F1 τ (↑) F1 2τ (↑) N3MR</formula><p>128. The mesh refinement branch has three stages, each with six graph convolution layers (of dimension 128) organized into three residual blocks. We train for 25 epochs using Adam <ref type="bibr" target="#b27">[27]</ref> with learning rate 10 −4 and 32 images per batch on 8 Tesla V100 GPUs. We set the cubify threshold to 0.2 and weight the losses with λ voxel = 1, λ cham = 1, λ norm = 0, and λ edge = 0.2.</p><p>Baselines We compare with previously published methods for single-image shape prediction. N3MR <ref type="bibr" target="#b25">[25]</ref> is a weakly supervised approach that fits a mesh via a differentiable renderer without 3D supervision. 3D-R2N2 <ref type="bibr" target="#b5">[5]</ref> and MVD <ref type="bibr" target="#b56">[56]</ref> output voxel predictions. PSG <ref type="bibr" target="#b8">[8]</ref> predicts point-clouds. Appendix D additionally compares with OccNet <ref type="bibr" target="#b42">[42]</ref>. Pixel2Mesh <ref type="bibr" target="#b69">[69]</ref> predicts meshes by deforming and subdividing an initial ellipsoid. GEOMetrics <ref type="bibr" target="#b57">[57]</ref> extends <ref type="bibr" target="#b69">[69]</ref> with adaptive face subdivision. Both are trained to minimize Chamfer distances; however <ref type="bibr" target="#b69">[69]</ref> computes it using predicted mesh vertices, while <ref type="bibr" target="#b57">[57]</ref> uses points sampled uniformly from predicted meshes. We adopt the latter as it better matches test-time evaluation. Unlike ours, these methods can only predict connected meshes of genus zero.</p><p>The training recipe and backbone architecture vary among prior work. Therefore for a fair comparison with our method we also compare against several ablated versions of our model (see Appendix C for exact details):</p><p>• Voxel-Only: A version of our method that terminates with the cubified meshes from the voxel branch. • Pixel2Mesh + : We reimplement Pixel2Mesh <ref type="bibr" target="#b69">[69]</ref>; we outperform their original model due to a deeper backbone, better training recipe, and minimizing Chamfer on sampled rather than vertex positions. • Sphere-Init: Similar to Pixel2Mesh + , but initializes from a high-resolution sphere mesh, performing three stages of vertex refinement without subdivision. • Ours (light): Uses a smaller nonresidual mesh refinement branch with three graph convolution layers per stage. We will adopt this lightweight design on Pix3D.</p><p>Voxel-Only is essentially a version of our method that omits the mesh refinement branch, while Pixel2Mesh + and Sphere-Init omit the voxel prediction branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Test Set</head><p>Holes Test Set  <ref type="table">Table 2</ref>. We report results both on the full ShapeNet test set (left), as well as a subset of the test set consisting of meshes with visible holes (right). We compare our full model with several ablated version: Voxel-Only omits the mesh refinement head, while Sphere-Init and Pixel2Mesh + omit the voxel head. We show results both for Best models which optimize for metrics, as well as Pretty models that strike a balance between shape metrics and mesh quality (see <ref type="figure">Figure 5</ref>); these two categories of models should not be compared. We also report the number of vertices |V | and faces |F | in predicted meshes (mean±std). ‡ refers to the released model by the authors. (per-instance average)</p><formula xml:id="formula_6">Chamfer(↓) Normal F1 0.1 F1 0.3 F1 0.5 |V | |F | Chamfer(↓) Normal F1 0.1 F1 0.3 F1 0.5 |V | |F |</formula><formula xml:id="formula_7">Input Image Without L edge (best)</formula><p>With L edge (pretty) <ref type="figure">Figure 5</ref>. Training without the edge length regularizer L edge results in degenerate predicted meshes that have many overlapping faces. Adding L edge eliminates this degeneracy but results in worse agreement with the ground-truth as measured by standard metrics such as Chamfer distance.</p><p>Best vs Pretty As previously noted in <ref type="bibr" target="#b69">[69]</ref> (Section 4.1), standard metrics for shape reconstruction are not wellcorrelated with mesh quality. <ref type="figure">Figure 5</ref> shows that models trained without shape regularizers give meshes that are preferred by metrics despite being highly degenerate, with irregularly-sized faces and many self-intersections. These degenerate meshes would be difficult to texture, and may not be useful for downstream applications. Due to the strong effect of shape regularizers on both mesh quality and quantitative metrics, we suggest only quantitatively comparing methods trained with the same shape regularizers. We thus train two versions of all our ShapeNet models: a Best version with λ edge = 0 to serve as an upper bound on quantitative performance, and a Pretty version that strikes a balance between quantitative performance and mesh quality by setting λ edge = 0.2. Comparison with Prior Work <ref type="table">Table 1</ref> compares our Pretty and Best models with prior work on shape prediction from a single image. We use the evaluation protocol from <ref type="bibr" target="#b69">[69]</ref>, Image Pixel2Mesh + Ours <ref type="figure">Figure 6</ref>. Pixel2Mesh + predicts meshes by deforming an initial sphere, so it cannot properly model objects with holes. In contrast our method can model objects with arbitrary topologies. using a 0.57 mesh scaling factor and threshold value τ = 10 −4 on squared Euclidean distances. For Pixel2Mesh, we provide the performance reported in their paper <ref type="bibr" target="#b69">[69]</ref> as well as the performance of their open-source pretrained model. <ref type="table">Table 1</ref> shows that we outperform prior work by a wide margin, validating the design of our mesh predictor. Ablation Study Fairly comparing with prior work is challenging due to differences in backbone networks, losses, and shape regularizers. For a controlled evaluation, we ablate variants using the same backbone and training recipe, shown in <ref type="table">Table 2</ref>. ShapeNet is dominated by simple objects of genus zero. Therefore we evaluate both on the entire test set and on a subset consisting of objects with one or more holes (Holes Test Set) 2 . In this evaluation we remove the ad-hoc scaling factor of 0.57, and we rescale meshes so the longest edge of the ground-truth mesh's bounding box has length 10, following <ref type="bibr" target="#b8">[8]</ref>. We compare the open-source  <ref type="table">Table 3</ref>. Performance on Pix3D S1 &amp; S2. We report mean AP box , AP mask and AP mesh , as well as per category AP mesh . All AP performances are in %. The Voxel-Only baseline outputs the cubified voxel predictions. The Sphere-Init and Pixel2Mesh + baselines deform an initial sphere and thus are limited to making predictions homeomorphic to spheres. Our Mesh R-CNN is flexible and can capture arbitrary topologies. We outperform the baselines consistently while predicting meshes with fewer number of vertices and faces. Pixel2Mesh model against our ablations in this evaluation setting. Pixel2Mesh + (our reimplementation of <ref type="bibr" target="#b69">[69]</ref>) significantly outperforms the original due to an improved training recipe and deeper backbone. We draw several conclusions from <ref type="table">Table 2</ref>: (a) On the Full Test Set, our full model and Pixel2Mesh + perform on par. However, on the Holes Test Set, our model dominates as it is able to predict topologically diverse shapes while Pixel2Mesh + is restricted to make predictions homeomorphic to spheres, and cannot model holes or disconnected components (see <ref type="figure">Figure 6</ref>). This discrepancy is quantitatively more salient on Pix3D (Section 4.2) as it contains more complex shapes. (b) Sphere-Init and Pixel2Mesh + perform similarly overall (both Best and Pretty), suggesting that mesh subdivision may be unnecessary for strong quantitative performance. (c) The deeper residual mesh refinement architecture (inspired by <ref type="bibr" target="#b69">[69]</ref>) performs on-par with the lighter non-residual architecture, motivating our use of the latter on Pix3D. (d) Voxel-Only performs poorly compared to methods that predict meshes, demonstrating that mesh predictions better capture fine object structure. (e) Each Best model outperforms its corresponding Pretty model; this is expected since Best is an upper bound on quantitative performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pix3D</head><p>We now turn to Pix3D <ref type="bibr" target="#b60">[60]</ref>, which consists of 10069 real-world images and 395 unique 3D models. Here the task is to jointly detect and predict 3D shapes for known object categories. Pix3D does not provide standard train/test splits, so we prepare two splits of our own. Our first split, S 1 , randomly allocates 7539 images for training and 2530 for testing. Despite the small number of unique object models compared to ShapeNet, S 1 is challenging since the same model can appear with varying appearance (e.g. color, texture), in different orientations, under different lighting conditions, in different contexts, and with varying occlusion. This is a stark contrast with ShapeNet, where objects appear against blank backgrounds.</p><p>Our second split, S 2 , is even more challenging: we ensure that the 3D models appearing in the train and test sets are disjoint. Success on this split requires generalization not only to the variations present in S 1 , but also to novel 3D shapes of known categories: for example a model may see kitchen chairs during training but must recognize armchairs during testing. This split is possible due to Pix3D's unique annotation structure, and poses interesting challenges for both 2D recognition and 3D shape prediction. Evaluation We adopt metrics inspired by those used for 2D recognition: AP box , AP mask and AP mesh . The first two are standard metrics used for evaluating COCO object detection and instance segmentation at intersection-over-union (IoU) 0.5. AP mesh evalutes 3D shape prediction: it is the mean area under the per-category precision-recall curves for F1 0.3 at 0.5 3 . Pix3D is not exhaustively annotated, so for evaluation we only consider predictions with box IoU &gt; 0.3 with a ground-truth region. This avoids penalizing the model for correct predictions corresponding to unannotated objects.</p><p>We compare predicted and ground-truth meshes in the camera coordinate system. Our model assumes known camera intrinsics for VertAlign. In addition to predicting the box of each object on the image plane, Mesh R-CNN predicts the depth extent by appending a 2-layer MLP head, similar to the box regressor head. As a result, Mesh R-CNN predicts a 3D bounding box for each object. See Appendix E for more details. Implementation details We use ResNet-50-FPN <ref type="bibr" target="#b35">[35]</ref> as the backbone CNN; the box and mask branches are identical to Mask R-CNN. The voxel branch resembles the mask branch, but the pooling resolution is decreased to 12 (vs. 14 for masks) due to memory constraints giving 24 × 24 × 24 voxel predictions. We adopt the lightweight design for the mesh refinement branch from Section 4.1. We train for 12 epochs with a batch size of 64 per image on 8 Tesla V100 GPUs (two images per GPU). We use SGD with momentum, linearly increasing the learning rate from 0.002 to 0.02 over the first 1K iterations, then decaying by a factor of 10 at 8K and 10K iterations. We initialize from a model pretrained for instance segmentation on COCO. We set the cubify threshold to 0.2 and the loss weights to λ voxel = 3, λ cham = 1, λ norm = 0.1 and λ edge = 1 and use weight decay 10 −4 ; detection loss weights are identical to Mask R-CNN. Comparison to Baselines As discussed in Section 1, we are the first to tackle joint detection and shape inference in the wild on Pix3D. To validate our approach we compare with ablated versions of Mesh R-CNN, replacing our full mesh predictor with Voxel-Only, Pixel2Mesh + , and Sphere-Init branches (see Section 4.1). All baselines otherwise use the same architecture and training recipe. <ref type="table">Table 3</ref> (top) shows the performance on S 1 . We observe that: (a) Mesh R-CNN outperforms all baselines, improving over the next-best by 10.6% AP mesh overall and across most categories; Tool and Misc 4 have very few test-set instances (11 and 20 respectively), so their AP is noisy. (b) Mesh R-CNN shows large gains vs. Sphere-Init for objects with complex shapes such as bookcase (+21.6%), table (+16.7%) and chair (+7.3%). (c) Voxel-Only performs very poorlythis is expected due to its coarse predictions. <ref type="table">Table 3</ref> (bottom) shows the performance on the more challenging S 2 split. Here we observe: (a) The overall performance on 2D recognition (AP box , AP mask ) drops significantly compared to S 1 , signifying the difficulty of recognizing novel shapes in the wild. (b) Mesh R-CNN outperforms all baselines for shape prediction for all categories except sofa, wardrobe and tool. (c) Absolute performance on wardrobe, tool and misc is small for all methods due to significant shape disparity between models in train and test and lack of training data. <ref type="bibr" target="#b4">4</ref> Misc consists of objects such as fire hydrant, picture frame, vase, etc.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We propose Mesh R-CNN, a novel system for joint 2D perception and 3D shape inference. We validate our approach on ShapeNet and show its merits on Pix3D. Mesh R-CNN is a first attempt at 3D shape prediction in the wild. Despite the lack of large supervised data, e.g. compared to COCO, Mesh R-CNN shows promising results. Mesh R-CNN is an object centric approach. Future work includes reasoning about the 3D layout, i.e. the relative pose of objects in the 3D scene.</p><p>Algorithm 1 outlines the cubify operation. Cubify takes as input voxel occupancy probabilities V of shape N × D × H × W as predicted from the voxel branch and a threshold value τ . Each occupied voxel is replaced with a cuboid triangle mesh (unit cube) with 8 vertices, 18 edges, and 12 faces. Shared vertices and edges between adjacent occupied voxels are merged, and shared interior faces are eliminated. This results in a watertight mesh T = (V, F ) for each example in the batch whose topology depends on the voxel predictions.</p><p>Algorithm 1 is an inefficient implementation of cubify as it involves nested for loops which in practice increase the time complexity, especially for large batches and large voxel sizes. In particular, this implementation takes &gt; 300ms for N = 32 voxels of size 32 × 32 × 32 on a Tesla V100 GPU. We replace the nested for loops with 3D convolutions and vectorize our computations, resulting in a time complexity of ≈ 30ms for the same voxel inputs. </p><formula xml:id="formula_8">Data: V : [0, 1] N ×D×H×W , τ ∈ [0, 1] unit cube = (V cube , F cube ) for (n, z, y, x) ∈ range(N, D, H, W) do if V [n,</formula><formula xml:id="formula_9">i = (V i , F i )} N −1 i=0</formula><p>Algorithm 1: Cubify</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mesh Sampling</head><p>As described in the main paper, the mesh refinement head is trained to minimize chamfer and normal losses that are defined on sets of points sampled from the predicted and ground-truth meshes.</p><p>Computing these losses requires some method of converting meshes into sets of sampled points. Pixel2Mesh <ref type="bibr" target="#b69">[69]</ref> is trained using similar losses. In their case groundtruth meshes are represented with points sampled uniformly at random from the surface of the mesh, but this sampling is performed offline before the start of training; they represent predicted meshes using their vertex positions. Computing these losses using vertex positions of predicted meshes is very efficient since it avoids the need to sample meshes online during training; however it can lead to degenerate predictions since the loss would not encourage the interior of predicted faces to align with the ground-truth mesh.</p><p>To avoid these potential degeneracies, we follow <ref type="bibr" target="#b57">[57]</ref> and compute the chamfer and normal losses by randomly sampling points from both the predicted and ground-truth meshes. This means that we need to sample the predicted meshes online during training, so the sampling must be efficient and we must be able to backpropagate through the sampling procedure to propagate gradients backward from the sampled points to the predicted vertex positions.</p><p>Given a mesh with vertices V ⊂ R 3 and faces F ⊆ V × V × V , we can sample a point uniformly from the surface of the mesh as follows. We first define a probability distribution over faces where each face's probability is proportional to its area:</p><formula xml:id="formula_10">P (f ) = area(f ) f ∈F area(f )<label>(3)</label></formula><p>We then sample a face f = (v 1 , v 2 , v 3 ) from this distribution. Next we sample a point p uniformly from the interior of f by setting p</p><formula xml:id="formula_11">= i w i v i where w 1 = 1 − √ ξ 1 , w 2 = (1 − ξ 2 )</formula><p>√ ξ 1 , w 3 = ξ 2 √ ξ 1 , and ξ 1 , ξ 2 ∼ U (0, 1) are sampled from a uniform distribution.</p><p>This formulation allows propagating gradients from p backward to the face vertices v i and can be seen as an instance of the reparameterization trick <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mesh R-CNN Architecture</head><p>At a high level we use the same overall architecture for predicting meshes on ShapeNet and Pix3D, but we slightly specialize to each dataset due to memory constraints from the backbone and task-specific heads. On Pix3D Mask R-CNN adds time and memory complexity in order to perform object detection and instance segmentation. ShapeNet. The overall architecture of our ShapeNet model is shown in <ref type="table">Table 6</ref>; the architecture of the voxel branch is shown in <ref type="table">Table 8</ref>.  <ref type="table">Table 5</ref>. Architecture for a single residual mesh refinement stage on ShapeNet. For ShapeNet we follow <ref type="bibr" target="#b69">[69]</ref> and use residual blocks of graph convolutions: ResGraphConv(D1 → D2) consists of two graph convolution layers (each preceeded by ReLU) and an additive skip connection, with a linear projection if the input and output dimensions are different. The output of the refinement stage are the vertex features <ref type="bibr" target="#b15">(15)</ref> and the updated vertex positions <ref type="bibr" target="#b18">(18)</ref>. The first refinement stage does not take input vertex features <ref type="bibr" target="#b5">(5)</ref>, so for this stage <ref type="bibr" target="#b13">(13)</ref> only concatenates <ref type="formula">(6)</ref> and <ref type="bibr" target="#b12">(12)</ref>.  <ref type="table">Table 6</ref>. Overall architecture for our ShapeNet model. Since we do not predict bounding boxes or masks, we feed the conv5 3 features from the whole image into the voxel branch. The architecture for the refinement stage is shown in <ref type="table">Table 5</ref>, and the architecture for the voxel branch is shown in <ref type="table">Table 8</ref>.</p><p>We consider two different architectures for the mesh refinement network on ShapeNet. Our full model as well as our Pixel2Mesh + and Sphere-Init baselines use mesh refinement stages with three residual blocks of two graph convolutions each, similar to <ref type="bibr" target="#b69">[69]</ref>; the architecture of these stages is shown in <ref type="table">Table 5</ref>. We also consider a shallower lightweight design which uses only three graph convolution layers per stage, omitting residual connections and instead concatenating the input vertex positions before each graph convolution layer. The architecture of this lightweight design is shown in <ref type="table">Table 7</ref>.</p><p>As shown in <ref type="table">Table 2</ref>, we found that these two architectures perform similarly on ShapeNet even though the lightweight design uses half as many graph convolution lay-  <ref type="table">Table 7</ref>. Architecture for the nonresidual mesh refinement stage use in the lightweight version of our ShapeNet models. Each GraphConv operation is followed by ReLU. The output of the stage are the vertex features <ref type="bibr" target="#b18">(18)</ref> and updated vertex positions <ref type="bibr" target="#b21">(21)</ref>. The first refinement stage does not take input vertex features <ref type="formula">(5)</ref>, so for this stage <ref type="bibr" target="#b13">(13)</ref> only concatenates <ref type="formula">(6)</ref> and <ref type="bibr" target="#b12">(12)</ref>. <ref type="table">Table 8</ref>. Architecture of our voxel prediction branch. For ShapeNet we use V = 48 and for Pix3D we use V = 24. TConv is a transpose convolution with stride 2. ers per stage. We therefore use the nonresidual design for our Pix3D models. Pix3D. The overall architecture of our full Mesh R-CNN system on Pix3D is shown in <ref type="table" target="#tab_10">Table 9</ref>. The backbone, RPN, box branch, and mask branch are identical to Mask R-CNN <ref type="bibr" target="#b18">[18]</ref>. The voxel branch is the same as in the ShapeNet models (see <ref type="table">Table 8</ref>), except that we predict voxels at a lower resolution (48×48×48 for ShapeNet vs. 24×24×24 for Pix3D) due to memory constraints. <ref type="table">Table 10</ref> shows the exact architecture of the mesh refinement stages for our Pix3D models. Baselines. The Voxel-Only baseline is identical to the full model, except that it omits all mesh refinement branches and terminates with the mesh resulting from cubify. On ShapeNet, the Voxel-Only baseline is trained with a batch size of 64 (vs. a batch size of 32 for our full model); on Pix3D it uses the same training recipe as our full model. The Pixel2Mesh + is our reimplementation of <ref type="bibr" target="#b69">[69]</ref>. This baseline omits the voxel branch; instead all images use an identical inital mesh. The initial mesh is a level-2 icosphere  The RPN produces a bounding box prediction for each of the A anchors at each spatial location in the input feature map; a subset of these candidate boxes are processed by the other branches, but here we show only the shapes resulting from processing a single box for the subsequent task-specific heads. Here C is the number of categories (10 = 9 + background for Pix3D); the box branch produces per-category bounding boxes and classification scores, while the mask branch produces per-category segmentation masks. TConv is a transpose convolution with stride 2. We use a ReLU nonlinearity between all Linear, Conv, and TConv operations. The architecture fo the voxel branch is shown in <ref type="table">Table 8</ref>, and the architecture of the refinement stages is shown in <ref type="table">Table 10</ref>.  <ref type="table">Table 10</ref>. Architecture for a single mesh refinement stage on Pix3D.</p><formula xml:id="formula_12">Index Inputs Operation Output shape (1) Input Image features V /2 × V /2 × D (2) (1) Conv(D → 256, 3 × 3), ReLU V /2 × V /2 × 256 (3) (2) Conv(256 → 256, 3 × 3), ReLU V /2 × V /2 × 256 (4) (3) TConv(256 → 256, 2 × 2, 2), ReLU V × V × 256 (5) (4) Conv(256 → V, 1 × 1) V × V × V</formula><p>with 162 vertices, 320 faces, and 480 edges which results from applying two face subdivision operations to a regular icosahedron and projecting all resulting vertices onto a sphere. For the Pixel2Mesh + baseline, the mesh refinement stages are the same as our full model, except that we apply a face subdivision operation prior to VertAlign in refinement stages 2 and 3. Like Pixel2Mesh + , the Sphere-Init baseline omits the voxel branch and uses an identical initial sphere mesh for all images. However unlike Pixel2Mesh + the initial mesh is a level-4 icosphere with 2562 vertices, 5120 faces, and 7680 edges which results from applying four face subdivivison operations to a regular icosahedron. Due to this large initial mesh, the mesh refinement stages are identical to our full model, and do not use mesh subdivision.</p><p>Pixel2Mesh + and Sphere-Init both predict meshes with the same number of vertices and faces, and with identical topologies; the only difference between them is whether all  <ref type="table">Table 11</ref>. Comparison between our method and Occpancy Networks (OccNet) <ref type="bibr" target="#b42">[42]</ref> on ShapeNet. We use the same evaluation metrics and setup as <ref type="table">Table 2.</ref> subdivision operations are performed before the mesh refinement branch (Sphere-Init) or whether mesh refinement is interleaved with mesh subdivision (Pixel2Mesh + ). On ShapeNet, the Pixel2Mesh + and Sphere-Init baselines are trained with a batch size of 96; on Pix3D they use the same training recipe as our full model.</p><formula xml:id="formula_13">Chamfer(↓) Normal F1 0.1 F1 0.3 F1 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with Occupancy Networks</head><p>Occupancy Networks <ref type="bibr" target="#b42">[42]</ref> (OccNet) also predict 2D meshes with neural networks. Rather than outputing a mesh directly from the neural network as in our approach, they train a neural network to compute a signed distance between a query point in 3D space and the object boundary. At testtime a 3D mesh can be extracted from a set of query points. Like our approach, OccNets can also predict meshes with varying topolgy per input instance. <ref type="table">Table 11</ref> compares our approach with OccNet on the ShapeNet test set. We obtained test-set predictions for Oc-cNet from the authors. Our method and OccNet are trained on slightly different splits of the ShapeNet dataset, so we compare our methods on the intersection of our respective test splits. From <ref type="table">Table 11</ref> we see that OccNets achieve higher normal consistency than our approach; however both the Best and Pretty versions of our model outperform Occ-Nets on all other metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Depth Extent Prediction</head><p>Predicting an object's depth extent from a single image is an ill-posed problem. In an earlier version of our work, we assumed the range of an object in the Z-axis was given at train &amp; test time. Since then, we have attempted to predict the depth extent by training a 2-layer MLP head of similar architecture to the bounding box regressor head. Formally, this head is trained to predict the scale-normalized depth extent (in log space) of the object, as follows</p><formula xml:id="formula_14">dz = dz z c · f h<label>(4)</label></formula><p>Note that the depth extent of an object is related to the size of the object (here approximated by the object's bounding box height h), its location z c in the Z-axis (far away objects need to be bigger in order to explain the image) and the focal length f . At inference time the depth extent dz of the object is recovered from the predicteddz and the predicted height of the object bounding box h, given the focal length f and the center of the object z c in the Z-axis. Note that we assume the center of the object z c since Pix3D annotations are not metric and due to the inherent scale-depth ambiguity. <ref type="figure">Figure 9</ref> shows qualitative comparisons between Pixel2Mesh + and Mesh R-CNN. Pixel2Mesh + is limited to making predictions homeomorphic to spheres and thus cannot capture varying topologies, e.g. holes. In addition, Pixel2Mesh + has a hard time capturing high curvatures, such as sharp table tops and legs. This is due to the large deformations required when starting from a sphere, which are not encouraged by the shape regularizers. On the other hand, Mesh R-CNN initializes its shapes with cubified voxel predictions resulting in better initial shape representations which require less drastic deformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Pix3D: Visualizations and Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. ShapeNet Holes test set</head><p>We construct the ShapeNet Holes Test set by selecting models from the ShapeNet test set that have visible holes from any viewpoint. <ref type="figure">Figure 10</ref> shows several input images for randomly selected models from this subset. This test set is very challenging -many objects have small holes resulting from thin structures; and some objects have holes which are not visible from all viewpoints. <ref type="figure">Figure 10</ref>. Example input images for randomly selected models from the the Holes Test Set on ShapeNet. For each model we show three different input images showing the model from different viewpoints. This set is extremely challenging -some models may have very small holes (such as the holes in the back of the chair in the left model of the first row, or the holes on the underside of the table on the right model of row 2), and some models may have holes which are not visible in all input images (such as the green chair in the middle of the fourth row, or the gray desk on the right of the ninth row).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 .</head><label>7</label><figDesc>Examples of Mesh R-CNN predictions on Pix3D. Mesh R-CNN detects multiple objects per image, reconstructs fine details such as chair legs, and predicts varying and complex mesh topologies for objects with holes such as bookcases and tables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 .</head><label>8</label><figDesc>More examples of Mesh R-CNN predictions on Pix3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>AP box AP mask AP mesh chair sofa table bed desk bkcs wrdrb tool misc 30.9 59.1 40.2 40.5 30.2 50.8 62.4 18.2 26.7 2562 ±0 5120 40.9 75.2 44.2 50.3 28.4 48.6 42.5 26.9 7.0 2562 ±0 5120 ±0 Mesh R-CNN (ours) 94.0 88.4 51.1 48.2 71.7 60.9 53.7 42.9 70.2 63.4 21.6 27.8 2367 ±698 4743 ±1406</figDesc><table><row><cell>Pix3D S 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>|V |</cell><cell>|F |</cell></row><row><cell>Voxel-Only</cell><cell>94.4</cell><cell>88.4</cell><cell>5.3</cell><cell cols="2">0.0 3.5 2.6 0.5 0.7 34.3 5.7 0.0 0.0 2354 ±706 4717 ±1423</cell></row><row><cell>Pixel2Mesh +</cell><cell>93.5</cell><cell>88.4</cell><cell cols="3">39.9 ±0</cell></row><row><cell cols="5">Sphere-Init 40.5 # test instances 94.1 87.5 2530 2530 2530 1165 415 419 213 154 79</cell><cell>54</cell><cell>11 20</cell></row><row><cell>Pix3D S 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Voxel-Only</cell><cell>71.5</cell><cell>63.4</cell><cell>4.9</cell><cell cols="2">0.0 0.1 2.5 2.4 0.8 32.2 0.0 6.0 0.0 2346 ±630 4702 ±1269</cell></row><row><cell>Pixel2Mesh +</cell><cell>71.1</cell><cell>63.4</cell><cell cols="3">21.1 26.7 58.5 10.9 38.5 7.8 34.1 3.4 10.0 0.0 2562 ±0</cell><cell>5120 ±0</cell></row><row><cell>Sphere-Init</cell><cell>72.6</cell><cell>64.5</cell><cell cols="3">24.6 32.9 75.3 15.8 40.1 10.1 45.0 1.5 0.8 0.0 2562 ±0</cell><cell>5120 ±0</cell></row><row><cell cols="2">Mesh R-CNN (ours) 72.2</cell><cell>63.9</cell><cell cols="3">28.8 42.7 70.8 27.2 40.9 18.2 51.1 2.9 5.2 0.0 2358 ±633 4726 ±1274</cell></row><row><cell># test instances</cell><cell cols="2">2356 2356</cell><cell cols="3">2356 777 504 392 218 205 84 134 22 20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>CNN init</cell><cell cols="3"># refine steps AP box AP mask AP mesh</cell></row><row><cell>COCO</cell><cell>3</cell><cell>94.0 88.4</cell><cell>51.1</cell></row><row><cell>IN</cell><cell>3</cell><cell>93.1 87.0</cell><cell>48.4</cell></row><row><cell>COCO</cell><cell>2</cell><cell>94.6 88.3</cell><cell>49.3</cell></row><row><cell>COCO</cell><cell>1</cell><cell>94.2 88.9</cell><cell>48.6</cell></row></table><note>. Ablations of Mesh R-CNN on Pix3D.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 compares</head><label>4</label><figDesc>pretraining on COCO vs ImageNet, and compares different architectures for the mesh predictor. COCO vs. ImageNet initialization improves 2D recognition (AP mask 88.4 vs. 87.0) and 3D shape prediction (AP mesh 51.1 vs. 48.4). Shape prediction is degraded when using only one mesh refinement stage (AP mesh 51.1 vs. 48.6). Figures 2, 7 and 8 show example predictions from Mesh R-CNN. Our method can detect multiple objects per image, reconstruct fine details such as chair legs, and predict varying and complex mesh topologies for objects with holes such as bookcases and desks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>2× downsample, Flatten, Linear(7 * 7 * 256 → 1024), Linear(1024 → 5C)</figDesc><table><row><cell cols="3">Index Inputs Operation</cell><cell>Output shape</cell></row><row><cell>(1)</cell><cell cols="2">Input Input Image</cell><cell>H × W × 3</cell></row><row><cell>(2)</cell><cell>(1)</cell><cell>Backbone: ResNet-50-FPN</cell><cell>h × w × 256</cell></row><row><cell>(3)</cell><cell>(2)</cell><cell>RPN</cell><cell>h × w × A × 4</cell></row><row><cell>(4)</cell><cell cols="2">(2),(3) RoIAlign</cell><cell>14 × 14 × 256</cell></row><row><cell>(5)</cell><cell>(4)</cell><cell cols="2">Box branch: C × 5</cell></row><row><cell>(6)</cell><cell>(4)</cell><cell cols="2">Mask branch: 4× Conv(256 → 256, 3 × 3), TConv(256 → 256, 2 × 2, 2), Conv(256 → C, 1 × 1) 28 × 28 × C</cell></row><row><cell cols="3">(7) (2), (3) RoIAlign</cell><cell>12 × 12 × 256</cell></row><row><cell>(8)</cell><cell>(7)</cell><cell>Voxel Branch</cell><cell>24 × 24 × 24</cell></row><row><cell>(9)</cell><cell>(8)</cell><cell>cubify</cell><cell>|V | × 3, |F | × 3</cell></row><row><cell cols="3">(10) (7), (9) Refinement Stage 1</cell><cell>|V | × 3, |F | × 3</cell></row><row><cell cols="3">(11) (7), (10) Refinement Stage 2</cell><cell>|V | × 3, |F | × 3</cell></row><row><cell cols="3">(12) (7), (11) Refinement Stage 3</cell><cell>|V | × 3, |F | × 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Overall architecture of Mesh R-CNN on Pix3D. The backbone, RPN, box, and mask branches are identical to Mask R-CNN.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We annotated 3075 test set models and flagged whether they contained holes. This resulted in 17% (or 534) of the models being flagged. See Appendix G for more details and examples.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A mesh prediction is considered a true-positive if its predicted label is correct, it is not a duplicate detection, and its F1 0.3 &gt; 0.5</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We would like to thank Kaiming He, Piotr Dollár, Leonidas Guibas, Manolis Savva and Shubham Tulsiani for valuable discussions. We would also like to thank Lars Mescheder and Thibault Groueix for their help.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Implementation of Cubify</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dense object reconstruction with semantic Pixel2Mesh + Mesh R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Each row shows the same example for Pixel2Mesh + (first three columns) and Mesh R-CNN (last three columns), respectively. For each method, we show the input image along with the predicted 2D mask (chair, bookcase, table, bed) and box (in green) superimposed. We show the 3D mesh rendered on the input image and an additional view of the 3D mesh. priors</title>
	</analytic>
	<monogr>
		<title level="m">Qualitative comparisons between Pixel2Mesh + and Mesh R-CNN on Pix3D</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Shapenet: An information-rich 3d model repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR 1512.03012</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D-R2N2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dense reconstruction using 3d object shape priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Dame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Y</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Implicit fairing of irregular meshes using diffusion and curvature flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Mathieu Desbrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">H</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">3d object detection and viewpoint estimation with a deformable 3d cuboid model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Datadriven 3D primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A papier-mâché approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentatio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Class specific 3d object shape priors using surface normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural 3D mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Im-ageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parsing IKEA Objects: Fine Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning efficient point cloud generation for dense 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context. ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Marching cubes: A high resolution 3d surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvey</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3d-lmnet: Latent embedding matching for accurate and diverse 3d point cloud reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyanka</forename><surname>Mandikal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneet</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Konstantinos G. Derpanis, and Kostas Daniilidis. 6-dof object pose from semantic keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Matryoshka networks: Predicting 3d geometry via nested shape layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Completing 3d object shape from one depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Thorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>JunYoung Gwak, Daeyun Shin, and Derek Hoiem</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Selfsupervised visual descriptor learning for dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multiview silhouette and depth decomposition for high resolution 3d object representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">GEOMetrics: Exploiting geometric structure for graph-encoded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep Sliding Shapes for amodal 3D object detection in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pix3d: Dataset and methods for single-image 3d shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning to infer and execute 3d shape programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning shape abstractions by assembling volumetric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: Generating 3D mesh models from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Adaptive O-CNN: a patch-based deep representation of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Marrnet: 3d shape reconstruction via 2.5 d sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning 3D Shape Priors for Shape Completion and Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Perspective transformer nets: Learning singleview 3d object reconstruction without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning to Reconstruct Shapes from Unseen Classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
