<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><forename type="middle">Karnewar</forename><surname>Tomtom</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
							<email>owang@adobe.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CelebA-HQ FFHQ <ref type="figure">Figure 1</ref>: Results of our proposed MSG-GAN technique where the generator synthesizes images at all resolutions simultaneously and gradients flow directly to all levels from a single discriminator. The first column has a resolution of 4x4 which increases towards the right reaching the final output resolution of 1024x1024. Best viewed zoomed in on screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>While Generative Adversarial Networks (GANs) have seen huge successes in image synthesis tasks, they are notoriously difficult to adapt to different datasets, in part due to instability during training and sensitivity to hyperparameters. One commonly accepted reason for this instability is that gradients passing from the discriminator to the generator become uninformative when there isn't enough overlap in the supports of the real and fake distributions. In this work, we propose the Multi-Scale Gradient Generative Adversarial Network (MSG-GAN), a simple but effective technique for addressing this by allowing the flow of gradients from the discriminator to the generator at multiple scales. This technique provides a stable approach for high resolution image synthesis, and serves as an alternative to the commonly used progressive growing technique. We show that MSG-GAN converges stably on a variety of image datasets of different sizes, resolutions and domains, as well as different types of loss functions and architectures, all with the same set of fixed hyperparameters. When compared to state-of-the-art GANs, our approach matches or exceeds the performance in most of the cases we tried.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since their introduction by Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref>, Generative Adversarial Networks (GANs) have become the de facto standard for high quality image synthesis. The success of GANs comes from the fact that they do not require manually designed loss functions for optimization, and can therefore learn to generate complex data distributions without the need to be able to explicitly define them. While flow-based models such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18]</ref> and autoregressive models such as <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref> allow training generative models directly using Maximum Likelihood Estimation (explicitly and implicitly respectively), the fidelity of the generated images has not yet been able to match that of the state-ofthe-art GAN models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b2">3]</ref>. However, GAN training suffers from two prominent problems: (1) mode collapse and (2) training instability.</p><p>The problem of mode collapse occurs when the generator network is only able to capture a subset of the variance present in the data distribution. Although numerous works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref> have been proposed to address this problem, it remains an open area of study. In this work, however, we address the problem of training instability. This is a fundamental issue with GANs, and has been widely reported by previous works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b24">25]</ref>. We propose a method to address training instability for the task of image generation by investigating how gradients at multiple scales can be used to generate high resolution images (typically more challenging due to the data dimensionality) without relying on previous greedy approaches, such as the progressive growing technique <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. MSG-GAN allows the discriminator to look at not only the final output (highest resolution) of the generator, but also at the outputs of the intermediate layers ( <ref type="figure" target="#fig_0">Fig. 2)</ref>. As a result, the discriminator becomes a function of multiple scale outputs of the generator and importantly, passes gradients to all the scales simultaneously (more details in section 1.1 and section 2). Furthermore, our method is robust to different loss functions (we show results on WGAN-GP and Non-saturating GAN loss with 1-sided gradient penalty), datasets (we demonstrate results on a wide range of commonly used datasets and a newly created Indian Celebs dataset), and architectures (we integrate the MSG approach with both Pro-GANs and StyleGAN base architectures). Much like progressive growing <ref type="bibr" target="#b14">[15]</ref>, we note that multi-scale gradients account for a considerable improvement in FID score over the vanilla DCGAN architecture. However, our method achieves better performance with comparable training time to state-of-the-art methods on most existing datasets without requiring the extra hyperparameters that progressive growing introduces, such as training schedules and learning rates for different generation stages (resolutions). This robustness allows the MSG-GAN approach to be easily used "out-of-the-box" on new datasets. We also show the importance of the multi-scale connections on multiple generation stages (coarse, medium, and fine), through ablation experiments on the high resolution FFHQ dataset.</p><p>In summary, we present the following contributions. First, we introduce a multiscale gradient technique for image synthesis that improves the stability of training as defined in prior work. Second, we show that we can robustly generate high quality samples on a number of commonly used datasets, including CIFAR10, Oxford102 flowers, CelebA-HQ, LSUN Churches, Flickr Faces HQ and our new Indian Celebs all with the same fixed hyperparameters. This makes our method easy to use in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivation</head><p>Arjovsky and Bottou <ref type="bibr" target="#b0">[1]</ref> pointed out that one of the reasons for the training instability of GANs is due to the passage of random (uninformative) gradients from the discriminator to the generator when there is insubstantial overlap between the supports of the real and fake distributions. Since the inception of GANs, numerous solutions have been proposed to this problem. One early example proposes adding instance noise to the real and the fake images so that the supports minimally overlap <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref>. More recently, Peng et al. <ref type="bibr" target="#b24">[25]</ref> proposed a mutual information bottleneck between input images and the discriminator's deepest representation of those input images called the variational discriminator bottleneck (VDB) <ref type="bibr" target="#b24">[25]</ref>, and Karras et al. <ref type="bibr" target="#b14">[15]</ref> proposed a progressive growing technique to add continually increasing resolution layers. The VDB solution forces the discriminator to focus only on the most discerning features of the images for classification, which can be viewed as an adaptive variant of instance noise. Our work is orthogonal to the VDB technique, and we leave an investigation into a combination of MSG-GAN and VDB to future work.</p><p>The progressive growing technique tackles the instability problem by training the GAN layer-by-layer by gradually doubling the operating resolution of the generated images. Whenever a new layer is added to the training it is slowly faded in such that the learning of the previous layers are retained. Intuitively, this technique helps with the support overlap problem because it first achieves a good distribution match on lower resolutions, where the data dimensionality is lower, and then partially-initializes (with substantial support overlap between real and fake distributions) higher resolution training with these previously trained weights, focusing on learning finer details.</p><p>While this approach is able to generate state-of-the-art results, it can be hard to train, due to the addition of hyperparameters to be tuned per resolution, including different iteration counts, learning rates (which can be different for the Generator and Discriminator <ref type="bibr" target="#b11">[12]</ref>) and the fade-in iterations. In addition, a concurrent submission <ref type="bibr" target="#b16">[17]</ref> discovered that it leads to phase artifacts where certain generated features are attached to specific spatial locations. Hence our main motivation lies in addressing these problems by providing a simpler alternative that leads to high quality results and stable training.</p><p>Although the current state-of-the-art in class conditional image generation on the Imagenet dataset, i.e. BigGAN <ref type="bibr" target="#b3">[4]</ref>, doesn't employ multi-scale image generation, note that the highest resolution they operate on is 512x512. All high resolution state-of-the-art methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref> use some or the other form of multi-scale image synthesis. Multi-scale image generation is a well established technique, with methods existing well before deep networks became popular for this task <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref>. More recently, a number of GAN-based methods break the process of high resolution image synthesis into smaller subtasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref>. For example, LR-GAN <ref type="bibr" target="#b35">[36]</ref> uses separate generators for synthesizing the background, foreground and compositor masks for the final image. Works such as GMAN and StackGAN employ a single generator and multiple discriminators for variation in teaching and multi-scale generation respectively <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref>. MAD-GAN <ref type="bibr" target="#b8">[9]</ref>, instead uses multiple generators to address mode-collapse by training a multi-agent setup in such a way that different generators capture different modalities in the training dataset. LapGAN <ref type="bibr" target="#b4">[5]</ref> models the difference between the generated multi-scale components of a Laplacian pyramid of the images using a single generator and multiple discriminators for different scales. Pix2PixHD <ref type="bibr" target="#b33">[34]</ref> uses three architecturally similar discriminators acting upon three different resolutions of the images obtained by downsampling the real and the generated images.</p><p>Our proposed method draws architectural inspiration from all these works and builds upon their teachings and ideologies, but has some key differences. In MSG-GAN, we use a single discriminator and a single generator with multi-scale connections, which allows for the gradients to flow at multiple resolutions simultaneously. There are several advantages (driven largely by the simplicity) of the proposed approach. If multiple discriminators are used at each resolution <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34]</ref>, the total parameters grow exponentially across scales, as repeated downsampling layers are needed, whereas in MSG-GAN the relationship is linear. In addition, multiple discriminators with different effective fields <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref> are not able to share information across scales, which could make the task easier. Besides having fewer parameters and design choices required, our approach also avoids the need for an explicit color consistency regularization term across images generated at multiple scales, which was necessary, e.g. in StackGAN <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multi-Scale Gradient GAN</head><p>We conduct experiments with the MSG-GAN framework applied to two base architectures, ProGANs <ref type="bibr" target="#b14">[15]</ref> and Style-GAN <ref type="bibr" target="#b15">[16]</ref>. We call these two methods MSG-ProGAN and MSG-StyleGAN respectively. Despite the name, there is no progressive growing used in any of the MSG variants, and we note that ProGANs without progressive growing is essentially the DCGAN <ref type="bibr" target="#b25">[26]</ref> architecture. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of our MSG-ProGAN architecture, which we define in more detail in this section, and include the MSG-StyleGAN model details in the supplemental material.</p><p>Let the initial block of the generator function g gen be defined as g gen : Z → A begin , such that sets Z and A begin are respectively defined as Z = R 512 , where z ∼ N (0, I) such that z ∈ Z and A begin = R 4×4×512 contains [4x4x512] dimensional activations. Let g i be a generic function which acts as the basic generator block, which in our implementation consists of an upsampling operation followed by two conv layers.</p><formula xml:id="formula_0">g i : A i−1 → A i (1) where, A i = R 2 i+2 ×2 i+2 ×ci<label>(2)</label></formula><p>and, i ∈ N;</p><formula xml:id="formula_1">A 0 = A begin<label>(3)</label></formula><p>where c i is the number of channels in the i th intermediate activations of the generator. We provide the sizes of c i in all layers in the supplementary material. The full generator GEN (z) then follows the standard format, and can be defined as a sequence of compositions of k such g functions followed by a final composition with g gen :</p><formula xml:id="formula_2">y = GEN (z) = g k • g k−1 • ...g i • ...g 1 • g gen (z). (4)</formula><p>We now define the function r which generates the output at different stages of the generator (red blocks in <ref type="figure" target="#fig_0">Fig. 2)</ref>, where the output corresponds to different downsampled versions of the final output image. We model r simply as a (1x1) convolution which converts the intermediate convolutional activation volume into images.</p><formula xml:id="formula_3">r i : A i → O i (5) where, O i = R 2 i+2 ×2 i+2 ×3 [0−1] (6) hence, r i (g i (z)) = r i (a i ) = o i (7) where, a i ∈ A i and o i ∈ O i<label>(8)</label></formula><p>In other words, o i is an image synthesized from the output of the i th intermediate layer of the generator a i . Similar to the idea behind progressive growing <ref type="bibr" target="#b14">[15]</ref>, r can be viewed as a regularizer, requiring that the learned feature maps are able to be projected directly into RGB space. Now we move on to defining the discriminator. Because the discriminator's final critic loss is a function of not only the final output of the generator y , but also the intermediate outputs o i , gradients can flow from the intermediate layers of the discriminator to the intermediate layers of the generator. We denote all the components of the discriminator function with the letter d. We name the final layer of the discriminator (which provides the critic score) d critic (z ), and the function which defines the first layer of the discriminator d 0 (y) or d 0 (y ), taking the real image y (true sample) or the highest resolution synthesized image y (fake sample) as the input. Similarly, let d j represent the intermediate layer function of the discriminator. Note that i and j are always related to each other as j = k−i. Thus, the output activation volume a j of any j th intermediate layer of the discriminator is defined as:</p><formula xml:id="formula_4">a j = d j (φ(o k −j , a j −1 )) (9) = d j (φ(o i , a j −1 )),<label>(10)</label></formula><p>where φ is a function used to combine the output o i of the (i) th intermediate layer of the generator (or correspondingly downsampled version of the highest resolution real image y) with the corresponding output of the (j − 1) th intermediate layer in the discriminator. In our experiments, we experimented with three different variants of this combine function:</p><formula xml:id="formula_5">φ simple (x 1 , x 2 ) = [x 1 ; x 2 ] (11) φ lin cat (x 1 , x 2 ) = [r (x 1 ); x 2 ] (12) φ cat lin (x 1 , x 2 ) = r ([x 1 ; x 2 ])<label>(13)</label></formula><p>where, r is yet another (1x1) convolution operation similar to r and [; ] is a simple channelwise concatenation operation. We compare these different combine functions in Sec 4. The final discriminator function is then defined as:</p><formula xml:id="formula_6">DIS (y , o 0 , o 1 , ...o i , ...o k−1 ) = (14) d critic • d k (., o 0 ) • d k−1 (., o 1 ) • ...d j (., o i ) • ...d 0 (y )<label>(15)</label></formula><p>We experimented with two different loss functions for the d critic function namely, WGAN-GP <ref type="bibr" target="#b10">[11]</ref> which was used by ProGAN <ref type="bibr" target="#b14">[15]</ref> and Non-saturating GAN loss with 1-sided GP <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> which was used by StyleGAN <ref type="bibr" target="#b15">[16]</ref>. Please note that since the discriminator is now a function of multiple input images generated by the generator, we modified the gradient penalty to be the average of the penalties over each input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>While evaluating the quality of GAN generated images is not a trivial task, the most commonly used metrics today are the Inception Score (IS, higher is better) <ref type="bibr" target="#b27">[28]</ref> and Fréchet Inception Distance (FID, lower is better) <ref type="bibr" target="#b11">[12]</ref>. In order to compare our results with the previous works, we use the IS for the CIFAR10 experiments and the FID for the rest of the experiments, and report the "number of real images shown" as done in prior work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>New Indian Celebs Dataset In addition to existing datasets, we also collect a new dataset consisting of Indian celebrities. To this end, we collected the images using a process similar to CelebA-HQ. First, we downloaded images for Indian celebrities by scraping the web for related search queries. Then, we detected faces using an off the shelf face-detector and cropped and resized all the images to 256x256. Finally, we manually cleaned the images by filtering out low-quality, erroneous, and low-light images. In the end, the dataset contained only 3K samples, an order of magnitude less than CelebA-HQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>We evaluate our method on a variety of datasets of different resolutions and sizes (number of images); CIFAR10 (60K images at 32x32 resolution); Oxford flowers (8K images at 256x256), LSUN churches (126K images at 256x256), Indian Celebs (3K images at 256x256 resolution), CelebA-HQ (30K images at 1024x1024) and FFHQ (70K images at 1024x1024 resolution).</p><p>For each dataset, we use the same initial latent dimensionality of 512, drawn from a standard normal distribution N (0, I) followed by hypersphere normalization <ref type="bibr" target="#b14">[15]</ref>. For all experiments, we use the same hyperparameter settings for MSG-ProGAN and MSG-StyleGAN (lr=0.003), with   <ref type="table">Table 1</ref>: Experiments on mid-level resolution (i.e. 256x256) datasets. We use author provided scores where possible, and otherwise train models with the official code and recommended hyperparameters (denoted " * ") the only differences being the number of upsampling layers (fewer for lower resolution datasets).</p><p>All models were trained with RMSprop (lr=0.003) for generator and discriminator. We initialize parameters according to the standard normal N (0, I) distribution. To match previously published work, StyleGAN and MSG-StyleGAN models were trained with Non-saturating GAN loss with 1-sided GP while ProGANs and MSG-ProGAN models were trained with the WGAN-GP loss function.</p><p>We also extend the MinBatchStdDev technique <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, where the average standard deviation of a batch of activations is fed to the discriminator to improve sample diversity, to our multiscale setup. To do this, we add a separate Min-BatchStdDev layer at the beginning of each block in the discriminator. This way, the discriminator obtains batchstatistics of the generated samples along with the straightpath activations at each scale, and can detect some degree of mode collapse by the generator.</p><p>When we trained the models ourselves, we report training time and GPUs used. We use the same machines for corresponding set of experiments so that direct training time comparisons can be made. Please note that the variation in numbers of real images shown and training time is because, as is common practice, we report the best FID score obtained in a fixed number of iterations, and the time that it took achieve that score. All the code and the trained models required for reproducing our work are made available for research purposes at https://github.com/ akanimax/msg-stylegan-tf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>Quality <ref type="table">Table 1</ref> shows quantitative results of our method on various mid-level resolutions datasets. Both our MSG-ProGAN and MSG-StyleGAN models achieve better FID   <ref type="table">Table 2</ref>: Experiments on high resolution (1024x1024) datasets. We use author provided scores where possible, and otherwise train models with the official code and recommended hyperparameters (denoted " * ").</p><p>scores than the respective baselines of ProGANs and Style-GAN on the (256x256) resolution datasets of Oxford Flowers, LSUN Churches and Indian Celebs. While each iteration of MSG-GAN is slower than the initial lower resolution iterations of progressive growing, due to all layers being trained together, MSG-GAN tends to converge in fewer iterations, requiring fewer total hours of GPU training time to achieve these scores. <ref type="figure">Figure 3</ref> shows random samples generated on these datasets for qualitative evaluation.</p><p>For high-resolution experiments <ref type="table">(Table 2)</ref>, the MSG-ProGAN model trains in comparable amount of time and gets similar scores on the CelebA-HQ and the FFHQ datasets (8.02 vs 7.79) and (8.36 vs 8.04) respectively. We note a small difference in the author reported scores and what we were able to achieve with the author provided code. This could be due to subtle hardware differences or variance between runs. Our MSG-StyleGAN model was unable to beat the FID score of StyleGAN on the CelebA-HQ dataset (6.37 vs 5.17) and the FFHQ dataset (5.8 vs 4.40). We discuss some hypotheses for why this might be in Sec 4, but note that our method does have other advantages, namely that it seems to be easier to generalize to different datasets as shown in our other experiments. Also, our generated images do not show any traces of the phase artifacts <ref type="bibr" target="#b16">[17]</ref> which are prominently visible in progressively grown GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stability during training</head><p>To compare the stability of MSG-ProGAN with ProGANs during training, we measure the changes in the generated samples for the same fixed latent points as iterations progress (on CelebA-HQ dataset). This method was introduced by <ref type="bibr" target="#b36">[37]</ref> as a way to measure stability during training, which we quantify by calculat- <ref type="figure">Figure 5</ref>: During training, all the layers in the MSG-GAN synchronize across the generated resolutions fairly early in the training and subsequently improve the quality of the generated images at all scales simultaneously. Throughout the training the generator makes only minimal incremental improvements to the images generated from fixed latent points. ing the mean squared error between two consecutive samples. <ref type="figure" target="#fig_3">Figure 6</ref> shows that while ProGANs tends towards convergence (making less changes) for lower resolutions only, MSG-ProGAN shows the same convergence trait for all the resolutions. The training epochs for the ProGANs take place in sequence over each resolution, whereas for the MSG-ProGAN they are simultaneous <ref type="figure">(Fig. 5)</ref>. While not necessary for generating good results, methods with high stability can be advantageous in that it is easier to get a rea-sonable estimate for how the final result will look by visualizing snapshots during training, which can help when training jobs take on the order of days to weeks.</p><p>Robustness to learning rate It has been observed by prior work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref> and also our experience, that convergence of GANs during training is very heavily dependant on the choice of hyperparameters, in particular, learning rate. To validate the robustness of MSG-ProGAN,    we trained our network with four different learning rates (0.001, 0.003, 0.005 and 0.01) for the CIFAR-10 dataset <ref type="table">(Table.</ref> 3). We can see that all of our four models converge, producing sensible images and similar inception scores, even with large changes in learning rate. Robust training schemes are significant as they indicate how easily a method can be generalized to unseen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Ablation Studies We performed two types of ablations on the MSG-ProGAN architecture. <ref type="table" target="#tab_4">Table 4</ref> summarizes our experiments on applying ablated versions of the Multi-Scale Gradients, where we only add subsets of the connections from the generator to the discriminator at differ-ent scales. We can see that adding multi-scale gradients at any level to the ProGANs/DCGAN architecture improves the FID score. Interestingly, adding only mid-level connections performs slightly better than adding only coarse or fine-level connections, however the overall best performance is achieved with the connections at all levels. <ref type="table" target="#tab_5">Table 5</ref> presents our experiments with the different variants of the combine function φ on the MSG-ProGAN and the MSG-StyleGAN architectures. φ simple (Eq 11) performed best on the MSG-ProGAN architecture while the φ cat lin (Eq 13) has the best FID score on the MSG-StyleGAN architecture. All results shown in this work employ these respective combine functions. We can see through these experiments that the combine function also plays an important role in the generative performance of the model, and it is possible that a more advanced combine function such as multi-layer densenet or AdaIN <ref type="bibr" target="#b12">[13]</ref> could improve the results even further.</p><p>Limitations and Future Work Our method is not without limitations. We note that using progressive training, the first set of iterations at lower resolutions take place much faster, whereas each iteration of MSG-GAN takes the same amount of time. However, we observe that MSG-GAN requires fewer total iterations to reach the same FID, and often does so after a similar length of total training time.</p><p>In addition, because of our multi-scale modification in MSG-StyleGAN, our approach cannot take advantage of the mixing regularization trick <ref type="bibr" target="#b15">[16]</ref>, where multiple latent vectors are mixed and the resulting image is forced to be realistic by the discriminator. This is done to allow the mixing of different styles at different levels at test time, but also improves overall quality. Interestingly, even though we do not explicitly enforce mixing regularization, our method is still able to generate plausible mixing results (see supplementary material).</p><p>Conclusion Although huge strides have been made towards photo-realistic high resolution image synthesis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, true photo-realism has yet to be achieved, especially with regards to domains with substantial variance in appearance. In this work, we presented the MSG-GAN technique which contributes to these efforts with a simple approach to enable high resolution multi-scale image generation with GANs.</p><p>6. Appendix <ref type="table" target="#tab_8">Tables 6 and 7</ref> provide the detailed configurations of the generator and the discriminator of MSG-ProGAN respectively. After every block in the generator, a 1 x 1 conv layer is used to convert the output activation volume into an RGB image which is passed onto the discriminator. On the discriminator's side, these RGB images are combined with straight path activation volumes using the combine function φ. In case of φ simple , a simple channelwise concatenation operation is used (see <ref type="table" target="#tab_8">Table 7</ref>). For the φ lin cat variant of the combine function, a 1 x 1 conv layer is used to project the RGB images into activation space which is then followed by channelwise concatenation operation. The number of channels output by the 1 x 1 conv layer is equal to half of the output channels in that block of the discriminator, e.g. for block 3 (see <ref type="table" target="#tab_8">Table 7</ref>), the output of the 1 x 1 conv layer is 32 x 256 x 256 and the output of φ lin cat operation is 96 x 256 x 256 (32 + 64). Finally, for the φ cat lin , the RGB images are first concatenated with the straight path activation volumes followed by a 1 x 1 conv layer. The number of channels output by this 1 x 1 conv layer is again equal to the prevalent number of channels in that block (e. g. 64 for block 3  <ref type="table">Table 6</ref>: Generator architecture for the MSG-ProGAN models used in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Architecture Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSG-ProGAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSG-StyleGAN</head><p>The MSG-StyleGAN model uses all the modifications proposed by StyleGAN <ref type="bibr" target="#b15">[16]</ref> to the ProGANs <ref type="bibr" target="#b14">[15]</ref> architecture except the mixing regularization. Similar to MSG-ProGAN, we use a 1 x 1 conv layer to obtain the RGB images output from every block of the Style-GAN generator leaving everything else (mapping network, non-traditional input and style adaIN) untouched. The discriminator architecture is same as the ProGANs (and consequently MSG-ProGAN, Tab. 7) discriminator.  spectively. <ref type="figure" target="#fig_7">Figure 10</ref> and <ref type="figure" target="#fig_8">Fig 11 shows</ref> samples generated by the MSG-StyleGAN model at all resolutions on the Oxford Flowers and Cifar-10 datasets respectively. <ref type="figure" target="#fig_2">Figure 14</ref> shows additional qualitative results (random samples) from the CelebA-HQ dataset, trained using our Model full architecture at 1024 x 1024 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Additional Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Observations</head><p>In this section, we present some of our observations and hypotheses about the differences in results generated by our method and StyleGAN. We show an overview of randomly selected samples from both models in <ref type="figure" target="#fig_4">Fig 7.</ref> In our analysis of the results, we find that while the actual resulting image quality is very close, StyleGAN samples exhibit slightly higher variation in terms of pose. In contrast, MSG-StyleGAN results are slightly more globally consistent and more realistic. This trade-off between diversity and result quality is widely reported <ref type="bibr" target="#b40">[41]</ref>, and may explain some of the difference in FID score. Further investigation into methods to control either axis (realism vs diversity), and the impact this has on the FID score, would be an interesting avenue for future work.</p><p>We also conducted experiments investigating the role that the pixelwise noise added to each block of the Style-GAN generator plays in image generation. We found that on non-face datasets, these noise layers model semantic aspects of the images and not just stochastic variations, as was their initial intent <ref type="bibr" target="#b15">[16]</ref> (see <ref type="figure" target="#fig_5">Fig 8)</ref>. We observed that MSG-StyleGAN also shows this type of effect, although to a slightly less degree. We conjecture that this disentanglement between the stochastic and semantic features is more straightforward for the face modelling task (e.g., on CelebA-HQ and FFHQ datasets), and the different models sensitivity to this noise could contribute to some of the the performance differences we observe as well, on face vs nonface datasets.</p><p>As mentioned in the discussion section of the main paper, we do not use the mixing regularization technique described in the StyleGAN <ref type="bibr" target="#b15">[16]</ref> work (the question of how to integrate such a regularization is an interesting direction for future work). However, we note that in spite of not using it, the model still learns to disentangle high level semantic features of the images due to the scale based constraint (see <ref type="figure" target="#fig_6">Fig. 9</ref>). As apparent from the figure, the high level mixing is much more coherent and generates more visually realistic results; while lower level mixing often generates incorrect visual cues, such as improper lighting and unbalanced hair. This shows that performance gains might be possible by ensuring proper style-based mixing at the low (coarse-grained) level of generation.   : Images generated by mixing the styles coming from two different latent vectors at different levels (granularity) of generation. As in StyleGAN <ref type="bibr" target="#b15">[16]</ref>, the first column images are source 1 and first row are source 2. Rows numbered 2, 3, and 4 have the mixing at resolutions (4 x 4 and 8 x 8), while rows 5 and 6 at (16 x 16 and 32 x 32), and the row 6 images are generated by swapping the source 2 latents at resolutions (64 x 64 till 1024 x 1024).     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of MSG-GAN, shown here on the base model proposed in ProGANs [15]. Our architecture includes connections from the intermediate layers of the generator to the intermediate layers of the discriminator. Multi-scale images sent to the discriminator are concatenated with the corresponding activation volumes obtained from the main path of convolutional layers followed by a combine function (shown in yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Random, uncurated samples generated by MSG-StyleGAN on high resolution (1024x1024) datasets. Best viewed zoomed in on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Image stability during training. These plots show the MSE between images generated from the same latent code at the beginning of sequential epochs (averaged over 36 latent samples) on the CelebA-HQ dataset. MSG-ProGAN converges stably over time while ProGANs<ref type="bibr" target="#b14">[15]</ref> continues to vary significantly across epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Random generated samples for qualitative comparison between StyleGAN<ref type="bibr" target="#b15">[16]</ref> and MSG-StyleGAN. All the samples were generated without truncating the input latent space for both because the FID calculation is done on nontruncated latent spaces. Best viewed zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>LSUN Church images generated by StyleGAN (top) and MSG-StyleGAN (bottom) using different realizations of the per-pixel noise while keeping the input latent vectors constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9</head><label>9</label><figDesc>Figure 9: Images generated by mixing the styles coming from two different latent vectors at different levels (granularity) of generation. As in StyleGAN [16], the first column images are source 1 and first row are source 2. Rows numbered 2, 3, and 4 have the mixing at resolutions (4 x 4 and 8 x 8), while rows 5 and 6 at (16 x 16 and 32 x 32), and the row 6 images are generated by swapping the source 2 latents at resolutions (64 x 64 till 1024 x 1024).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Random samples generated at all 7 resolutions for the Oxford102 flowers dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Random samples generated at all 4 resolutions for the CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Random generated CelebA Faces at resolution 128 x 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Random generated LSUN bedrooms at resolution 128 x 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Random generated CelebA-HQ Faces at resolution 1024 x 1024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Robustness to learning rate on CIFAR-10. We see that our approach converges to similar IS scores over a range of learning rates.</figDesc><table><row><cell cols="2">Level of Multi-scale connections FID (↓)</cell></row><row><cell>No connections (DC-GAN)</cell><cell>14.20</cell></row><row><cell>Coarse Only</cell><cell>10.84</cell></row><row><cell>Middle Only</cell><cell>9.17</cell></row><row><cell>Fine Only</cell><cell>9.74</cell></row><row><cell>All (MSG-ProGAN)</cell><cell>8.36</cell></row><row><cell>ProGAN  *</cell><cell>9.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Ablation experiments for varying degrees of</cell></row><row><cell cols="3">multiscale gradient connections on the high resolution</cell></row><row><cell cols="3">(1024x1024) FFHQ dataset. Coarse contains connections</cell></row><row><cell cols="3">at (4x4) and (8x8), middle at (16x16) and (32x32); and</cell></row><row><cell cols="2">fine at (64x64) till (1024x1024).</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Combine function FID (↓)</cell></row><row><cell>MSG-ProGAN</cell><cell>φ lin cat</cell><cell>11.88</cell></row><row><cell></cell><cell>φ cat lin</cell><cell>9.63</cell></row><row><cell></cell><cell>φ simple</cell><cell>8.36</cell></row><row><cell cols="2">MSG-StyleGAN φ simple</cell><cell>6.46</cell></row><row><cell></cell><cell>φ lin cat</cell><cell>6.12</cell></row><row><cell></cell><cell>φ cat lin</cell><cell>5.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Experiments with different combine functions on the high resolution (1024x1024) FFHQ dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Here we include additional results for further empirical validation. We show full resolution results from MSG-StyleGAN for the 256 x 256 Oxford102 flower dataset, and the MSG-ProGAN architecture for the 128 x 128 CelebA and LSUN bedroom datasets. The CelebA model was trained for 28M real images and obtained an FID of 8.86. Because of the huge size of the LSUN bedrooms dataset (30M), we trained it for 150M real images (roughly 5 epochs) which resulted in an FID of 18.32.Figures 12 and 13show the 128 x 128 (highest resolution) samples generated for the CelebA and LSUN bedrooms datasets re-</figDesc><table><row><cell>Block</cell><cell>Operation</cell><cell>Act.</cell><cell>Output Shape</cell></row><row><cell></cell><cell cols="2">Model full ↓</cell><cell></cell></row><row><cell></cell><cell>Raw RGB images 0</cell><cell>-</cell><cell>3 x 1024 x 1024</cell></row><row><cell></cell><cell>FromRGB 0</cell><cell>-</cell><cell>16 x 1024 x 1024</cell></row><row><cell>1.</cell><cell>MinBatchStd</cell><cell>-</cell><cell>17 x 1024 x 1024</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>16 x 1024 x 1024</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>32 x 1024 x 1024</cell></row><row><cell></cell><cell>AvgPool</cell><cell>-</cell><cell>32 x 512 x 512</cell></row><row><cell></cell><cell>Raw RGB images 1</cell><cell>-</cell><cell>3 x 512 x 512</cell></row><row><cell></cell><cell>Concat/φ simple</cell><cell>-</cell><cell>35 x 512 x 512</cell></row><row><cell>2.</cell><cell>MinBatchStd</cell><cell>-</cell><cell>36 x 512 x 512</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>32 x 512 x 512</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>64 x 512 x 512</cell></row><row><cell></cell><cell>AvgPool</cell><cell>-</cell><cell>64 x 256 x 256</cell></row><row><cell></cell><cell cols="2">Model 3 ↓</cell><cell></cell></row><row><cell></cell><cell>Raw RGB images 2</cell><cell>-</cell><cell>3 x 256 x 256</cell></row><row><cell></cell><cell>Concat/φ simple</cell><cell>-</cell><cell>67 x 256 x 256</cell></row><row><cell>3.</cell><cell>MinBatchStd</cell><cell>-</cell><cell>68 x 256 x 256</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>64 x 256 x 256</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>128 x 256 x 256</cell></row><row><cell></cell><cell>AvgPool</cell><cell>-</cell><cell>128 x 128 x 128</cell></row><row><cell></cell><cell cols="2">Model 2 ↓</cell><cell></cell></row><row><cell></cell><cell>Raw RGB images 3</cell><cell>-</cell><cell>3 x 128 x 128</cell></row><row><cell></cell><cell>Concat/φ simple</cell><cell>-</cell><cell>131 x 128 x 128</cell></row><row><cell>4.</cell><cell>MinBatchStd</cell><cell>-</cell><cell>132 x 128 x 128</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>128 x 128 x 128</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>256 x 128 x 128</cell></row><row><cell></cell><cell>AvgPool</cell><cell>-</cell><cell>256 x 64 x 64</cell></row><row><cell></cell><cell>Raw RGB images 4</cell><cell>-</cell><cell>3 x 64 x 64</cell></row><row><cell></cell><cell>Concat/φ simple</cell><cell>-</cell><cell>259 x 64 x 64</cell></row><row><cell>5.</cell><cell>MinBatchStd</cell><cell>-</cell><cell>260 x 64 x 64</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>256 x 64 x 64</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>512 x 64 x 64</cell></row><row><cell></cell><cell>AvgPool</cell><cell>-</cell><cell>512 x 32 x 32</cell></row><row><cell></cell><cell cols="2">Model 1 ↓</cell><cell></cell></row><row><cell></cell><cell>Raw RGB images 5</cell><cell>-</cell><cell>3 x 32 x 32</cell></row><row><cell></cell><cell>Concat/φ simple</cell><cell>-</cell><cell>515 x 32 x 32</cell></row><row><cell>6.</cell><cell>MinBatchStd</cell><cell>-</cell><cell>516 x 32 x 32</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>512 x 32 x 32</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>512 x 32 x 32</cell></row><row><cell></cell><cell>AvgPool</cell><cell>-</cell><cell>512 x 16 x 16</cell></row><row><cell></cell><cell>Raw RGB images 6</cell><cell>-</cell><cell>3 x 16 x 16</cell></row><row><cell></cell><cell>Concat/φ simple</cell><cell>-</cell><cell>515 x 16 x 16</cell></row><row><cell>7.</cell><cell>MinBatchStd</cell><cell>-</cell><cell>516 x 16 x 16</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>512 x 16 x 16</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>512 x 16 x 16</cell></row><row><cell></cell><cell>AvgPool</cell><cell>-</cell><cell>512 x 8 x 8</cell></row><row><cell></cell><cell>Raw RGB images 7</cell><cell>-</cell><cell>3 x 8 x 8</cell></row><row><cell></cell><cell>Concat/φ simple</cell><cell>-</cell><cell>515 x 8 x 8</cell></row><row><cell>8.</cell><cell>MinBatchStd</cell><cell>-</cell><cell>516 x 8 x 8</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>512 x 8 x 8</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>512 x 8 x 8</cell></row><row><cell></cell><cell>AvgPool</cell><cell>-</cell><cell>512 x 4 x 4</cell></row><row><cell></cell><cell>Raw RGB images 7</cell><cell>-</cell><cell>3 x 4 x 4</cell></row><row><cell></cell><cell>Concat/φ simple</cell><cell>-</cell><cell>515 x 4 x 4</cell></row><row><cell>9.</cell><cell>MinBatchStd</cell><cell>-</cell><cell>516 x 4 x 4</cell></row><row><cell></cell><cell>Conv 3 x 3</cell><cell>LReLU</cell><cell>512 x 4 x 4</cell></row><row><cell></cell><cell>Conv 4 x 4</cell><cell>LReLU</cell><cell>512 x 1 x 1</cell></row><row><cell></cell><cell>Fully Connected</cell><cell>Linear</cell><cell>1 x 1 x 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Discriminator Architecture for the MSG-ProGAN and MSG-StyleGAN Models used in training.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head><p>We would like to thank Alexia Jolicoeur-Martineau (Ph.D. student at MILA) for her guidance over Relativism in GANs and for proofreading the paper. Finally we extend special thanks to Michael Hoffman (Sr. Mgr. Software Engineering, TomTom) for his support and motivation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">NICE: non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Mahadevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01673</idno>
		<title level="m">Generative multi-adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-agent diverse generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viveka</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The relativistic discriminator: a key element missing from standard GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07215</idno>
		<title level="m">On convergence and stability of GANs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parallel controllable texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2005" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PacGAN: The power of two samples in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-class generative adversarial networks with the l2 loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno>abs/1611.04076</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<title level="m">Unrolled generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational discriminator bottleneck: Improving imitation learning, inverse RL, and GANs by constraining information flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Toyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PixelCNN++: A PixelCNN implementation with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Amortised MAP inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Casper Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszár</surname></persName>
		</author>
		<idno>abs/1610.04490</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1601.06759</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hyung Jin Chang, and Yiannis Demiris. Magan: Margin adaptation for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Cully</surname></persName>
		</author>
		<idno>abs/1704.03817</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spacetime completion of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Lr-gan: Layered recursive generative adversarial networks for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The unusual effectiveness of averaging in GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yazıcı</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Sheng</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10916</idno>
		<title level="m">Xiaolei Huang, and Dimitris Metaxas. Stack-gan++: Realistic image synthesis with stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Photographic text-to-image synthesis with a hierarchically-nested adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanpu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
