<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
							<email>shikhar@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
							<email>soumyasanyal@iisc.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
							<email>vikram.nitin@columbia.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing knowledge graphs suffer from incompleteness, which can be alleviated by inferring missing links based on known facts. One popular way to accomplish this is to generate low-dimensional embeddings of entities and relations, and use these to make inferences. ConvE, a recently proposed approach, applies convolutional filters on 2D reshapings of entity and relation embeddings in order to capture rich interactions between their components. However, the number of interactions that ConvE can capture is limited. In this paper, we analyze how increasing the number of these interactions affects link prediction performance, and utilize our observations to propose InteractE. InteractE is based on three key ideas -feature permutation, a novel feature reshaping, and circular convolution. Through extensive experiments, we find that InteractE outperforms state-of-the-art convolutional link prediction baselines on FB15k-237. Further, InteractE achieves an MRR score that is 9%, 7.5%, and 23% better than ConvE on the FB15k-237, WN18RR and YAGO3-10 datasets respectively. The results validate our central hypothesis -that increasing feature interaction is beneficial to link prediction performance. We make the source code of InteractE available to encourage reproducible research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) are structured representations of facts, where nodes represent entities and edges represent relationships between them. This can be represented as a collection of triples (s, r, o), each representing a relation r between a "subject-entity" s and an "object-entity" o. Some real-world knowledge graphs include Freebase <ref type="bibr" target="#b0">(Bollacker et al. 2008)</ref>, WordNet <ref type="bibr" target="#b7">(Miller 1995)</ref>, YAGO <ref type="bibr" target="#b14">(Suchanek, Kasneci, and Weikum 2007)</ref>, and NELL <ref type="bibr" target="#b9">(Mitchell et al. 2018)</ref>. KGs find application in a variety of tasks, such as relation extraction <ref type="bibr" target="#b8">(Mintz et al. 2009</ref>), question answering <ref type="bibr" target="#b2">(Bordes, Chopra, and Weston 2014;</ref><ref type="bibr" target="#b2">Bordes, Weston, and Usunier 2014)</ref>, recommender systems <ref type="bibr" target="#b16">(Zhang et al. 2016</ref>) and dialog systems <ref type="bibr" target="#b6">(Ma et al. 2015)</ref>.</p><p>However, most existing KGs are incomplete <ref type="bibr" target="#b2">(Dong et al. 2014)</ref>. The task of link prediction alleviates this drawback by inferring missing facts based on the known facts in a KG. A popular approach for solving this problem involves learning a low-dimensional representation for all entities and relations and utilizing them to predict new facts. In general, most existing link prediction methods learn to embed KGs by optimizing a score function which assigns higher scores to true facts than invalid ones. These score functions can be classified as translation distance based <ref type="bibr" target="#b1">(Bordes et al. 2013;</ref><ref type="bibr" target="#b16">Xiao, Huang, and Zhu 2016;</ref><ref type="bibr" target="#b16">Wang et al. 2014a)</ref> or semantic matching based <ref type="bibr" target="#b11">(Nickel, Rosasco, and Poggio 2016;</ref><ref type="bibr" target="#b6">Liu, Wu, and Yang 2017)</ref>.</p><p>Recently, neural networks have also been utilized to learn the score function <ref type="bibr" target="#b13">(Socher et al. 2013;</ref><ref type="bibr" target="#b12">Ravishankar, Chandrahas, and Talukdar 2017;</ref><ref type="bibr" target="#b2">Dettmers et al. 2018)</ref>. The motivation behind these approaches is that shallow methods like TransE <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref> and <ref type="bibr">DistMult (Yang et al. 2014)</ref> are limited in their expressiveness. As noted in <ref type="bibr" target="#b2">(Dettmers et al. 2018)</ref>, the only way to remedy this is to increase the size of their embeddings, which leads to an enormous increase in the number of parameters and hence limits their scalability to larger knowledge graphs.</p><p>Convolutional Neural Networks (CNN) have the advantage of using multiple layers, thus increasing their expressive power, while at the same time remaining parameterefficient. <ref type="bibr" target="#b2">(Dettmers et al. 2018)</ref> exploit these properties and propose ConvE -a model which applies convolutional filters on stacked 2D reshapings of entity and relation embeddings. Through this, they aim to increase the number of interactions between components of these embeddings.</p><p>In this paper, we conclusively establish that increasing the number of such interactions is beneficial to link prediction performance, and show that the number of interactions that ConvE can capture is limited. We propose InteractE, a novel CNN based KG embedding approach which aims to further increase the interaction between relation and entity embeddings. Our contributions are summarized as follows: 1. We propose InteractE, a method that augments the expressive power of ConvE through three key ideas -feature permutation, "checkered" feature reshaping, and circular convolution. 2. We provide a precise definition of an interaction, and theoretically analyze InteractE to show that it increases  <ref type="figure">Figure 1</ref>: Overview of InteractE. Given entity and relation embeddings (e s and e r respectively), InteractE generates multiple permutations of these embeddings and reshapes them using a "Checkered" reshaping function (φ chk ). Depth-wise circular convolution is employed to convolve each of the reshaped permutations (C i ), which are then flattened (Ĉ i ) and fed to a fullyconnected layer to generate the predicted object embedding ( e o ). Please refer to Section 5 for details.</p><p>interactions compared to ConvE. Further, we establish a correlation between the number of heterogeneous interactions (refer to Def. 4.2) and link prediction performance. 3. Through extensive evaluation on various link prediction datasets, we demonstrate InteractE's effectiveness (Section 9).</p><p>We have made available the source code of InteractE and datasets used in the paper as a supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Non-Neural: Starting with TransE <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref>, there have been multiple proposed approaches that use simple operations like dot products and matrix multiplications to compute a score function. Most approaches embed entities as vectors, whereas for relations, vector <ref type="bibr" target="#b1">(Bordes et al. 2013;</ref><ref type="bibr" target="#b11">Nickel, Rosasco, and</ref><ref type="bibr">Poggio 2016), matrix (Yang et al. 2014;</ref><ref type="bibr" target="#b6">Liu, Wu, and Yang 2017)</ref> and tensor <ref type="bibr" target="#b5">(Lin et al. 2015)</ref> representations have been explored. For modeling uncertainty of learned representations, Gaussian distributions <ref type="bibr" target="#b4">(He et al. 2015;</ref><ref type="bibr" target="#b16">Xiao, Huang, and Zhu 2016)</ref>   <ref type="bibr">Nguyen et al. 2018</ref>) is another convolution based method which applies convolutional filters of width 1 on the stacked subject, relation and object embeddings for computing score. As noted in <ref type="bibr" target="#b12">(Shang et al. 2019)</ref>, although ConvKB was claimed to be superior to ConvE, its performance is not consistent across different datasets and metrics. Further, there have been concerns raised about the validity of its evaluation procedure 1 Hence, we do not compare against it in this paper. A survey of all variants of existing KG embedding techniques can be found in ).</p><p>3 Background KG Link Prediction: Given a Knowledge Graph (KG) G = (E, R, T ), where E and R denote the set of entities and relations, and T denotes the triples (facts) of the form {(s, r, o)} ⊂ E × R × E, the task of link prediction is to predict new facts (s , r , o ) such that s , o ∈ E and r ∈ R, based on the existing facts in KG. Formally, the task can be modeled as a ranking problem, where the goal is to learn a function ψ(s, r, o) : E × R × E → R which assigns higher scores to true or likely facts than invalid ones.</p><p>Most existing KG embedding approaches define an encoding for all entities and relations, i.e., e s , e r ∀s ∈ E, r ∈ R. Then, a score function ψ(s, r, o) is defined to measure the validity of triples.  representations, an optimization problem is solved for maximizing the plausibility of the triples T in the KG. ConvE: In this paper, we build upon ConvE <ref type="bibr" target="#b2">(Dettmers et al. 2018)</ref>, which models interaction between entities and relations using 2D Convolutional Neural Networks (CNN). The score function used is defined as follows:</p><formula xml:id="formula_0">ψ(s, r, o) = f (vec(f ([e s ; e r w]))W )e o , where, e s ∈ R dw×d h , e r ∈ R dw×d h denote 2D reshapings of e s ∈ R dwd h ×1 , e r ∈ R dwd h ×1</formula><p>, and ( ) denotes the convolution operation. The 2D reshaping enhances the interaction between entity and relation embeddings which has been found to be helpful for learning better representations <ref type="bibr" target="#b11">(Nickel, Rosasco, and Poggio 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Notation and Definitions</head><p>Let e s = (a 1 , ..., a d ), e r = (b 1 , ..., b d ), where a i , b i ∈ R ∀i, be an entity and a relation embedding respectively, and let w ∈ R k×k be a convolutional kernel of size k. Further, we define that a matrix M k ∈ R k×k is a k-submatrix of another matrix N ∈ R m×n if ∃ i, j such that M k = N i:i+k, j:j+k . We denote this by M k ⊆ N . Definition 4.1. (Reshaping Function) A reshaping function φ : R d × R d → R m×n transforms embeddings e s and e r into a matrix φ(e s , e r ), where m × n = 2d. For conciseness, we abuse notation and represent φ(e s , e r ) by φ. We define three types of reshaping functions.</p><p>• Stack (φ stk ) reshapes each of e s and e r into a matrix of shape (m/2) × n, and stacks them along their height to yield an m × n matrix ( <ref type="figure" target="#fig_0">Fig. 2a</ref>). This is the reshaping function used in <ref type="bibr" target="#b2">(Dettmers et al. 2018</ref>).</p><p>• Alternate (φ τ alt ) reshapes e s and e r into matrices of shape (m/2) × n, and stacks τ rows of e s and e r alternately. In other words, as we decrease τ , the "frequency" with which rows of e s and e r alternate increases. We denote φ 1 alt (τ = 1) as φ alt for brevity ( <ref type="figure" target="#fig_0">Fig. 2b</ref>). • Chequer (φ chk ) arranges e s and e r such that no two adjacent cells are occupied by components of the same embedding ( <ref type="figure" target="#fig_0">Fig. 2c</ref>).   An interaction (x, y, M k ) is called heterogeneous if x and y are components of e s and e r respectively, or viceversa. Otherwise, it is called homogeneous. We denote the number of heterogeneous and homogeneous interactions as N het (φ, k) and N homo (φ, k) respectively. For example, in a 3 × 3 matrix M 3 , if there are 5 components of e s and 4 of e r , then the number of heterogeneous and homogeneous interactions are: N het = 2(5 × 4) = 40, and N homo = 2 5 2 + 4 2 = 32. Please note that the sum of total number of heterogenous and homogenous interactions in a reshaping function is constant and is equal to 2 k 2 2 , i.e., N het (φ, k) + N homo (φ, k) = 2 k 2 2 .</p><formula xml:id="formula_1">b 3 b 1 b 2 b 4 b 6 b 7 b 5 b 8 a 2 a 1 b 1 b 2 a 3 b 4 b 3 a 4 a 6 a 5 b 5 b 6 a 7 b 8 b 7 a 8 (a) Stack (b) Alternate (c) Chequer</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">InteractE Overview</head><p>Recent methods <ref type="bibr" target="#b16">(Yang et al. 2014;</ref><ref type="bibr" target="#b11">Nickel, Rosasco, and Poggio 2016)</ref> have demonstrated that expressiveness of a model can be enhanced by increasing the possible interactions between embeddings. ConvE (Dettmers et al. 2018) also exploits the same principle albeit in a limited way, using convolution on 2D reshaped embeddings. InteractE extends this notion of capturing entity and relation feature interactions using the following three ideas:</p><p>• Feature Permutation: Instead of using one fixed order of the input, we utilize multiple permutations to capture more possible interactions.</p><p>• Checkered Reshaping: We substitute simple feature reshaping of ConvE with checked reshaping and prove its superiority over other possibilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">InteractE Details</head><p>In this section, we provide a detailed description of the various components of InteractE. The overall architecture is depicted in <ref type="figure">Fig. 1</ref>. InteractE learns a d-dimensional vector representation (e s , e r ∈ R d ) for each entity and relation in the knowledge graph, where d = d w d h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Feature Permutation</head><p>To capture a variety of heterogeneous interactions, InteractE first generates t-random permutations of both e s and e r , denoted by P t = [(e 1 s , e 1 r ); ...; (e t s , e t r )]. Note that with high probability, the sets of interactions within φ(e i s , e i r ) for different i are disjoint. This is evident because the number of distinct interactions across all possible permutations is very large. So, for t different permutations, we can expect the total number of interactions to be approximately t times the number of interactions for one permutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Checkered Reshaping</head><p>Next, we apply the reshaping operation φ chk (e i s , e i r ), ∀i ∈ {1, ..., t}, and define φ(P t ) = [φ(e 1 s , e 1 r ); ...; φ(e t s , e t r )]. ConvE (Dettmers et al. 2018) uses φ stk (·) as a reshaping function which has limited interaction capturing ability. On the basis of Proposition 7.3, we choose to utilize φ chk (·) as the reshaping function in InteractE, which captures maximum heterogeneous interactions between entity and relation features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Circular Convolution</head><p>Motivated by our analysis in Proposition 7.4, InteractE uses circular convolution, which further increases interactions compared to the standard convolution. This has been successfully applied for tasks like image recognition . Circular convolution on a 2-dimensional input I ∈ R m×n with a filter w ∈ R k×k is defined as:</p><formula xml:id="formula_2">[I w] p,q = k/2 i=− k/2 k/2 j=− k/2 I [p−i]m,[q−j]n w i,j ,</formula><p>where, [x] n denotes x modulo n and · denotes the floor function. <ref type="figure">Figure 3</ref> and Proposition 7.4 show how circular convolution captures more interactions compared to standard convolution with zero padding.</p><p>InteractE stacks each reshaped permutation as a separate channel. For convolving permutations, we apply circular convolution in a depth-wise manner (Chollet 2017). Although different sets of filters can be applied for each permutation, in practice we find that sharing filters across channels works better as it allows a single set of kernel weights to be trained on more input instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Score Function</head><p>The output of each circular convolution is flattened and concatenated into a vector. InteractE then projects this vector to the embedding space (R d ). Formally, the score function used in InteractE is defined as follows:</p><formula xml:id="formula_3">ψ(s, r, o) = g(vec(f (φ(P k ) w))W )e o ,</formula><p>x 13</p><p>x 23</p><p>x <ref type="bibr">31</ref> x <ref type="bibr">32</ref> x <ref type="bibr">33</ref> x 34</p><p>x 43</p><p>x <ref type="bibr">13</ref> x 14</p><p>x <ref type="bibr">23</ref> x 24</p><p>x <ref type="bibr">31</ref> x <ref type="bibr">32</ref> x <ref type="bibr">33</ref> x 34</p><p>x <ref type="bibr">41</ref> x <ref type="bibr">42</ref> x <ref type="bibr">43</ref> x 44 (a) Standard Convolution (b) Circular Convolution</p><p>x 22</p><p>x 12 x 11</p><p>x 21</p><p>x <ref type="bibr">11</ref> x 12</p><p>x 22 x 21</p><p>x <ref type="bibr">41</ref> x <ref type="bibr">42</ref> x 44</p><p>x 24</p><p>x 14 <ref type="figure">Figure 3</ref>: Circular convolution induces more interactions than standard convolution. Here, X is a 4 × 4 input matrix with components x ij . The shaded region depicts where the filter is applied. Please refer to Section 6.3 for more details.</p><p>where denotes depth-wise circular convolution, vec(·) denotes vector concatenation, e o represents the object entity embedding matrix and W is a learnable weight matrix. Functions f and g are chosen to be ReLU and sigmoid respectively. For training, we use the standard binary cross entropy loss with label smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Theoretical Analysis</head><p>In this section, we analyze multiple variants of 2D reshaping with respect to the number of interactions they induce. We also examine the advantages of using circular padded convolution over the standard convolution.</p><p>For simplicity, we restrict our analysis to the case where the output of the reshaping function is a square matrix, i.e., m = n. Note that our results can be extended to the general case as well. Proofs of all propositions herein are included in the supplementary material.</p><p>Proposition 7.1. For any kernel w of size k, for all n ≥ 5k 3 − 1 if k is odd and n ≥ (5k+2)(k−1) 3k if k is even, the following statement holds:</p><formula xml:id="formula_4">N het (φ alt , k) ≥ N het (φ stk , k)</formula><p>Proposition 7.2. For any kernel w of size k and for all τ &lt; τ (τ, τ ∈ N), the following statement holds:</p><formula xml:id="formula_5">N het (φ τ alt , k) ≥ N het (φ τ alt , k)</formula><p>Proposition 7.3. For any kernel w of size k and for all reshaping functions φ : R d × R d → R n×n , the following statement holds:</p><formula xml:id="formula_6">N het (φ chk , k) ≥ N het (φ, k)</formula><p>Proposition 7.4. Let Ω 0 , Ω c : R n×n → R (n+p)×(n+p) denote zero padding and circular padding functions respectively, for some p &gt; 0. Then for any reshaping function φ,   <ref type="bibr" target="#b7">(Miller 1995)</ref>, with deleted inverse relations similar to FB15k-237. • YAGO3-10 is a subset of YAGO3 <ref type="bibr" target="#b14">(Suchanek, Kasneci, and Weikum 2007)</ref> constitutes entities with at least 10 relations. Triples consist of descriptive attributes of people.</p><formula xml:id="formula_7">N het (Ω c (φ), k) ≥ N het (Ω 0 (φ), k)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Evaluation protocol</head><p>Following <ref type="bibr" target="#b1">(Bordes et al. 2013)</ref>, we use the filtered setting, i.e., while evaluating on test triples, we filter out all the valid triples from the candidate set, which is generated by either corrupting the head or tail entity of a triple. The performance is reported on the standard evaluation metrics: Mean Reciprocal Rank (MRR), Mean Rank (MR) and Hits@1, and Hits@10. We report average results across 5 runs. We note that the variance is substantially low on all the metrics and hence omit it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Baselines</head><p>In our experiments, we compare InteractE against a variety of baselines which can be categorized as:</p><p>•  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Results</head><p>In this section, we attempt to answer the questions below: Q1. How does InteractE perform in comparison to the existing approaches? (Section 9.1) Q2. What is the effect of different feature reshaping and circular convolution on link prediction performance? (Section 9.2) Q3. How does the performace of our model vary with number of feature permutations? (Section 9.3) Q4. What is the performance of InteractE on different relation types? (Section 9.4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Performance Comparison</head><p>In order to evaluate the effectiveness of InteractE, we compare it against the existing knowledge graph embedding methods listed in Section 8.3. The results on three standard link prediction datasets are summarized in   <ref type="figure">Figure 4</ref>: Performance with different feature reshaping and convolution operation on validation data of FB15k-237 and WN18RR. Stack and Alt denote Stacked and Alternate reshaping as defined in Section 4. As we decrease τ the number of heterogeneous interactions increases (refer to Proposition 7.2). The results empirically verify our theoretical claim in Section 7 and validate the central thesis of this paper that increasing heterogeneous interactions improves link prediction performance. Please refer to Section 9.2 for more details. YAGO3-10. On an average, InteractE obtains an improvement of 9%, 7.5%, and 23% on FB15k-237, WN18RR, and YAGO3-10 on MRR over ConvE. This validates our hypothesis that increasing heterogeneous interactions help improve performance on link prediction. For YAGO3-10, we observe that the MR obtained from InteractE is worse than ConvE although it outperforms ConvE on all other metrics. Simliar trend has been observed in <ref type="bibr" target="#b2">(Dettmers et al. 2018;</ref><ref type="bibr" target="#b15">Sun et al. 2019)</ref>.</p><p>Compared to other baseline methods, InteractE outperforms them on FB15k-237 across all the metrics and on 3 out of 4 metrics on YAGO3-10 dataset. The below-par performance of InteractE on WN18RR can be attributed to the fact that this dataset is more suitable for shallow models as it has very low average relation-specific in-degree. This is consistent with the observations of (Dettmers et al. 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Effect of Feature Reshaping and Circular Convolution</head><p>In this section, we empirically test the effectiveness of different reshaping techniques we analyzed in Section 7. For this, we evaluate different variants of InteractE on validation data of FB15k-237 and WN18RR with the number of feature permutations set to 1. We omit the analysis on YAGO3-10 given its large size. The results are summarized in <ref type="figure">Figure 4</ref>. We find that the performance with Stacked reshaping is the worst, and it improves when we replace it with alternate reshaping. This observation is consistent with our findings in Proposition 7.1. Further, we find that MRR improves on decreasing the value of τ in alternate reshaping, which empirically validates Proposition 7.2. Finally, we observe that checkered reshaping gives the best performance across all reshaping functions for most scenarios, thus justifying Proposition 7.3. We also compare the impact of using circular and standard convolution on link prediction performance. The MRR scores are reported in <ref type="figure">Figure 4</ref>. The results show that circu-  <ref type="figure">Figure 5</ref>: Performance on the validation data of FB15k-237, WN18RR, and YAGO3-10 with different numbers of feature permutations. We find that although increasing the number of permutations improves performance, it saturates as we exceed a certain limit. Please see Section 9.3 for details. lar convolution is consistently better than the standard convolution. This also verifies our statement in Proposition 7.4. Overall, we find that increasing interaction helps improve performance on the link prediction task, thus validating the central thesis of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Effect of Feature Permutations</head><p>In this section, we analyze the effect of increasing the number of feature permutations on InteractE's performance on validation data of FB15k-237, WN18RR, and YAGO3-10. The overall results are summarized in <ref type="figure">Figure 5</ref>. We observe that on increasing the number of permuations although on FB15k-237, MRR remains the same, it improves on WN18RR and YAGO3-10 datasets. However, it degrades as the number of permutations is increased beyond   <ref type="bibr">et al., 2014b)</ref>, the relations are categorized into one-to-one (1-1), one-to-many (1-N), many-to-one (N-1), and many-to-many (N-N). We observe that InteractE is effective at capturing complex relations compared to RotatE. Refer to Section 9.4 for details.</p><p>a certain limit. We hypothesize that this is due to overparameteralization of the model. Moreover, since the number of relevant interactions are finite, increasing the number of permutations could become redundant beyond a limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Evaluation on different Relation Types</head><p>In this section, we analyze the performance of InteractE on different relation categories of FB15k-237. We chose FB15k-237 for analysis over other datasets because of its more and diverse set of relations. Following (Wang et al. 2014b), we classify the relations based on the average number of tails per head and heads per tail into four categories: one-to-one, one-to-many, many-to-one, and many-to-many. The results are presented in <ref type="table" target="#tab_11">Table 4</ref>. Overall, we find that In-teractE is effective at modeling complex relation types like one-to-many and many-to-many whereas, RotatE captures simple relations like one-to-one better. This demonstrates that an increase in interaction allows the model to capture more complex relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>In this paper, we propose InteractE, a novel knowledge graph embedding method which alleviates the limitations of ConvE by capturing additional heterogeneous feature interactions. InteractE is able to achieve this by utilizing three central ideas, namely feature permutation, checkered feature reshaping, and circular convolution. Through extensive experiments, we demonstrate that InteractE achieves a consistent improvement on link prediction performance on multiple datasets. We also theoretically analyze the effectiveness of the components of InteractE, and provide empirical validation of our hypothesis that increasing heterogeneous feature interaction is beneficial for link prediction with ConvE. This work demonstrates a possible scope for improving existing knowledge graph embedding methods by leveraging rich heterogenous interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Propositions</head><p>Proposition G.1. For any kernel w of size k, for all n ≥ 5k 3 − 1 if k is odd and n ≥ (5k+2)(k−1) 3k if k is even, the following statement holds:</p><formula xml:id="formula_8">N het (φ alt , k) ≥ N het (φ stk , k) Proof. Any M k ∈ R k×k , M k ⊆ φ alt contains k</formula><p>2 rows of elements of e s , and k+1 2 rows of elements of e r , or vice-versa.</p><p>For a single fixed M k , the total number of triples</p><formula xml:id="formula_9">(a i , b j , M k ) and (b j , a i , M k ) is 2 × k k 2 × k k + 1 2</formula><p>The number of possible M k matrices is (n − k + 1) 2 . Hence the total number of heterogeneous interactions is</p><formula xml:id="formula_10">N het (φ alt , k) = (n − k + 1) 2 k 2 × 2 k 2 k + 1 2</formula><p>Any M k ∈ R k×k , M k ⊆ φ stk contains l rows of elements of e s , and k − l rows of elements of e r , where 0 ≤ l ≤ k.</p><p>For a fixed l, the number of different possible M k matrices is (n − k + 1). Hence, the number of heterogeneous interactions is</p><formula xml:id="formula_11">N het (φ stk , k) = (n − k + 1) k l=0 2 × kl × k(k − l) = (n − k + 1) · k 2 · k 2 (k + 1) − k(k + 1)(2k + 1) 3 = (n − k + 1) · k 2 · k(k + 1)(k − 1) 3<label>(1)</label></formula><p>We need to check whether,</p><formula xml:id="formula_12">N het (φ alt , k) ≥ N het (φ stk , k) =⇒ (n − k + 1) k 2 k + 1 2 ≥ k(k + 1)(k − 1) 6</formula><p>For odd k, this becomes For even k,</p><formula xml:id="formula_13">(n − k + 1) k − 1 2 k + 1 2 ≥ k(k + 1)(k − 1) 6 n − k + 1 ≥ 2k 3 n ≥ 5k 3 − 1</formula><formula xml:id="formula_14">(n − k + 1) k 2 2 ≥ k(k + 1)(k − 1) 6 nk − k(k − 1) ≥ 2(k + 1)(k − 1) 3 n ≥ (5k + 2)(k − 1) 3k</formula><p>Proposition G.2. For any kernel w of size k and for all τ &lt; τ (τ, τ ∈ N), the following statement holds:</p><formula xml:id="formula_15">N het (φ τ alt , k) ≥ N het (φ τ alt , k)</formula><p>Proof. For simplicity, let us assume that α = n/(2τ ) ∈ N, i.e., φ τ alt is composed of exactly α blocks of τ rows of e s and e r stacked alternately. Also, when τ &lt; k, we assume that k/τ ∈ N. Now, for any M k ∈ R k×k , M k ⊆ φ τ alt , we consider the following two cases:</p><p>Case 1. τ ≥ k − 1: It is easy to see that this case can be split into n/τ subproblems, each of which is similar to φ stk . Hence,</p><formula xml:id="formula_16">N het (φ τ alt , k) = n τ N het (φ stk , k)</formula><p>Clearly, N het (φ τ alt , k) is monotonically decreasing with increasing τ .</p><p>Case 2. τ &lt; k − 1: As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, let T a , T b ∈ R τ ×k denote a submatrix formed by components of e s , e r respectively. Note that if k is even, then for any M k ⊆ φ τ alt , the number of components of e s and e r are always equal to k 2 /2 each. For odd k, the number of T a 's and T b 's are (k/τ )+1 2 and (k/τ )−1 2 in some order. Now, if we move M k down by i rows (i ≤ τ ), the total number of heterogeneous interactions across all such positions is:</p><formula xml:id="formula_17">n τ τ −1 i=0 k 2 k/τ + 1 2 τ − i k/τ − 1 2 τ + i = nk 2 τ τ −1 i=0 (k + τ − 2i)(k − τ + 2i) 4 = nk 2 4τ τ −1 i=0 k 2 − (τ 2 + 4i 2 − 4iτ ) = nk 2 4 (k 2 − τ 2 ) − 4(τ − 1)(2τ − 1) 6 + 4τ (τ − 1) 2 = C k 2 − τ 2 3 − 2 3</formula><p>We can see that this is also monotonically decreasing with increasing τ . It is also evident that the above expression is maximum at τ = 1 (since τ ∈ N).</p><p>Proposition G.3. For any kernel w of size k and for all reshaping functions φ : R d × R d → R n×n , the following statement holds:</p><formula xml:id="formula_18">N het (φ chk , k) ≥ N het (φ, k)</formula><p>Proof. For any φ, and for any M k ∈ R k×k such that M k ⊆ φ, let x, y be the number of components of e s and e r in M k respectively. Then N het (M k , k) = 2xy. Also, since total number of elements in M k is fixed, we have x + y = k 2 . Using the AM-GM inequality on x, y we have,</p><formula xml:id="formula_19">xy ≤ x + y 2 2 = k 4 4</formula><p>If k is odd, since x, y ∈ N, xy ≤ k 4 − 1 4</p><p>Therefore, the maximum interaction occurs when x = y = k 2 2 (for even k), or x = k 2 +1 2 , y = k 2 −1 2 (for odd k). It can be easily verified that this property holds for all M k ⊆ φ chk . Hence,</p><formula xml:id="formula_20">N het (φ, k) = M k 2xy ≤ M k 2k 4 4 = N het (φ chk , k)</formula><p>Proof. If M k contains x components of e s and y components of e r , then N het (M k , k) = 2xy, and N het (M k , k) = 2(x − l)(y − (p − l)) for some l ≤ p and l ≤ x. We observe that N het (M k , k) = N het (M k , k) − 2(x − l)(p − l) − 2ly ≤ N het (M k , k) Proposition G.4. Let Ω 0 , Ω c : R n×n → R (n+p)×(n+p) denote zero padding and circular padding functions respectively, for some p &gt; 0. Then for any reshaping function φ, N het (Ω c (φ), k) ≥ N het (Ω 0 (φ), k)</p><p>Proof.</p><p>Given Ω c (φ)), we know that we can obtain Ω 0 (φ) by replacing certain components of Ω c (φ)) with 0. So for every M k ⊆ Ω c (φ), there is a corresponding M k ⊆ Ω 0 (φ) which is obtained by replacing some p components (p ≥ 0) of M k with 0.</p><p>Using the above Lemma, we can see that</p><formula xml:id="formula_21">N het (Ω c (φ), k)) = M k ⊆Ωc(φ) N het (M k , k)) ≥ M k ⊆Ω0(φ) N het (M k , k))</formula><p>= N het (Ω 0 (φ), k))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters</head><p>We use the standard training, validation and test splits provided with the datasets. A detailed description of the datasets is included in the main paper. We select the best model using the validation data on the hyperparameters listed in <ref type="table">Ta</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Different types of reshaping functions we analyze in this paper. Here, e s = (a 1 , ..., a 8 ), e r = (b 1 , ..., b 8 ), and m = n = 4. Please refer to Section 4 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Definition 4.2. (Interaction) An interaction is defined as a triple (x, y, M k ), such that M k ⊆ φ(e s , e r ) is a k-submatrix of the reshaped input embeddings; x, y ∈ M k and are distinct components of e s or e r . The number of interactions N (φ, k) is defined as the cardinality of the set of all possible triples. Note that φ can be replaced with Ω(φ) for some padding function Ω.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>The figure depicts a k × k matrix M k . T a , T b are reshaped matrices each containing τ k components of e s , e r respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>lists some of the commonly used score functions. Finally, to learn the entity and relation Model Scoring Function ψ(e s , e r , e o ) TransE e s + e r − e o p DistMult e s , e r , e o HolE e r , e s * e o ComplEx Re( e s , e r , e o )</figDesc><table><row><cell>ConvE</cell><cell cols="2">f (vec(f ([e s ; e r ] w))W)e o</cell></row><row><cell>RotatE</cell><cell>− e s • e r − e o</cell><cell>2</cell></row><row><cell>InteractE</cell><cell cols="2">g(vec(f (φ(P k ) w))W )e o</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: The scoring functions ψ(s, r, o) of various knowl-</cell></row><row><cell>edge graph embedding methods. Here, e s , e r , e o ∈ R d ex-</cell></row><row><cell>cept for ComplEx and RotatE, where they are complex vec-</cell></row><row><cell>tors (C d ),  *  denotes circular-correlation, denotes convo-</cell></row><row><cell>lution, • represents Hadamard product and denotes depth-</cell></row><row><cell>wise circular convolution operation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Link prediction results of several models evaluated on FB15k-237, WN18RR and YAGO3-10. We find that InteractE outperforms all other methods across metrics on FB15k-237 and in 3 out of 4 settings on YAGO3-10. Since InteractE generalizes ConvE, we highlight performance comparison between the two methods specifically in the table above. Please refer to Section 9.1 for more details.</figDesc><table><row><cell>8 Experimental Setup</cell></row><row><cell>8.1 Datasets</cell></row><row><cell>In our experiments, following (Dettmers et al. 2018; Sun et</cell></row><row><cell>al. 2019), we evaluate on the three most commonly used link</cell></row><row><cell>prediction datasets. A summary statistics of the datasets is</cell></row><row><cell>presented in Table 3.</cell></row><row><cell>• FB15k-237 (Toutanova and Chen 2015) is a improved</cell></row><row><cell>version of FB15k (Bordes et al. 2013) dataset where all</cell></row><row><cell>inverse relations are deleted to prevent direct inference of</cell></row><row><cell>test triples by reversing training triples.</cell></row><row><cell>• WN18RR (Dettmers et al. 2018) is a subset of WN18</cell></row><row><cell>(Bordes et al. 2013) derived from WordNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Details of the datasets used. Please see Section 8.1 for more details.</figDesc><table><row><cell>This includes R-GCN (Schlichtkrull et al. 2017), ConvE</cell></row><row><cell>(Dettmers et al. 2018), ConvTransE (Shang et al. 2019),</cell></row><row><cell>and SACN (Shang et al. 2019).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Link prediction results by relation category on FB15k-237 dataset for RotatE, ConvE, and InteractE. Following (Wang</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>ble 5. Most of the hyperparamters are adopted from ConvE (Dettmers et al. 2018) model. In this paper, we explore both 1-1 (Bordes et al. 2013) and 1-N (Dettmers et al. 2018) scoring techniques. In 1-N scoring, each (s, r) pair is scored against all the entities o ∈ E simultaneously. For training, we use Adam optimizer (Kingma and Ba 2014) and Xavier initialization (Glorot and Bengio 2010) for initializing parameters.</figDesc><table><row><cell>Hyperparameter</cell><cell>Values</cell></row><row><cell>Learning rate</cell><cell>{0.001, 0.0001}</cell></row><row><cell>Label smoothening</cell><cell>{0.0, 0.1}</cell></row><row><cell>Batch size</cell><cell>{16, 64, 256}</cell></row><row><cell>Negative Samples</cell><cell>{100, 500, 1000, 4000}</cell></row><row><cell>l2 regularization</cell><cell>{0, 10 −5 }</cell></row><row><cell>Hidden dropout</cell><cell>{0, 0.3, 0.5}</cell></row><row><cell>Feature dropout</cell><cell>{0, 0.2, 0.5}</cell></row><row><cell>Input dropout</cell><cell>{0, 0.2}</cell></row><row><cell>kw</cell><cell>{10}</cell></row><row><cell>k h</cell><cell>{20}</cell></row><row><cell>Number of convolutional filters</cell><cell>{32, 48, 64, 96}</cell></row><row><cell>Convolutional kernal size k</cell><cell>{3, 5, 7, 9, 11}</cell></row><row><cell cols="2">Number of feature permutations t {1, 2, 3, 4, 5}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>Details of hyperparameters used. Please refer to Section B for more details.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://openreview.net/forum?id=HkgEQnRqYQ&amp;noteId= HklyVUAX2m</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">• Circular Convolution: Compared to the standard convolution, circular convolution allows to capture more feature interactions as depicted inFigure 3. The convolution is performed in a depth-wise manner (Chollet 2017) on different input permutations.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bollacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kblrn: End-to-end learning of knowledge base representations with latent, relational, and numerical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chopra</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename><forename type="middle">;</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pasquale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Monterey, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
	<note>Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Teh, Y. W., and Titterington, M.</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Chia Laguna Resort</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to represent knowledge graphs with gaussian embedding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15<address><addrLine>New York, NY, USA; Ba</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analogical inference for multi-relational embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th International Conference on Machine Learning</title>
		<editor>Precup, D., and Teh, Y. W.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
	<note>Knowledge graph inference for spoken dialog systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual Meeting of the ACL</title>
		<meeting>the 47th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter</title>
		<editor>Nguyen et al. 2018] Nguyen, D. Q.</editor>
		<editor>Nguyen, T. D.</editor>
		<editor>Nguyen, D. Q.</editor>
		<editor>and Phung, D</editor>
		<meeting>North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
	<note>Never-ending learning</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosasco</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Conference on Artificial Intelligence, AAAI&apos;16</title>
		<meeting>the Thirtieth Conference on Artificial Intelligence, AAAI&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting simple neural networks for learning representations of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3060" to="3067" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Modeling relational data with graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Yago: A Core of Semantic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kasneci</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on the World Wide Web</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trouillon</surname></persName>
		</author>
		<idno>abs/1412.6575</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>KDD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
