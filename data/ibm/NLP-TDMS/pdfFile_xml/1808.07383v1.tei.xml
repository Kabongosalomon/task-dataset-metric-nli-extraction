<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Self-Attention: Computing Attention over Words Dynamically for Sentence Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deunsol</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbok</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangkeun</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Korea University Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Korea University Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Korea University Seoul</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Self-Attention: Computing Attention over Words Dynamically for Sentence Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose Dynamic Self-Attention (DSA), a new self-attention mechanism for sentence embedding. We design DSA by modifying dynamic routing in capsule network (Sabour et al., 2017) for natural language processing. DSA attends to informative words with a dynamic weight vector. We achieve new state-of-the-art results among sentence encoding methods in Stanford Natural Language Inference (SNLI) dataset with the least number of parameters, while showing comparative results in Stanford Sentiment Treebank (SST) dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In Natural Language Process (NLP), most neural network-based models contain a sentence encoder to map a sequence of words into a vector. The vector is then used for various downstream tasks, e.g., sentiment analysis, natural language inference, etc. The key part of a sentence encoder is a computation across a variable-length input sequence for a fixed size vector. One of the common approaches is the max-pooling in CNN or RNN <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b3">Conneau et al., 2017)</ref>.</p><p>Self-attention is another approach for a fixed size vector. Self-attention derived from the attention mechanism, originally designed for neural machine translation <ref type="bibr">(Bahdanau et al.)</ref>, is utilized in various tasks <ref type="bibr" target="#b9">(Liu et al., 2016;</ref><ref type="bibr" target="#b9">Yang et al., 2016;</ref><ref type="bibr" target="#b16">Shen and Huang, 2016)</ref>. Self-attention computes attention weights by the inner product between words and the learnable weight vector. The weight vector is important in that it detects informative words, yet it is static during inference. The significance of the role of the weight vector casts doubt on whether its being static is an optimal status. * Equal Contribution.</p><p>In parallel, <ref type="bibr" target="#b13">Sabour et al. (2017)</ref> recently proposed capsule network for image classification. In capsule network, dynamic routing iteratively computes weights over inputs by the inner product between inputs and a weighted sum of inputs. Varying with the inputs, the weighted sum detects informative inputs; therefore it can be interpreted as a dynamic weight vector from the perspective of self-attention. We expect the dynamic weight vector to give rise to flexibility in self-attention since it can adapt to given sentences even after training.</p><p>Motivated by dynamic routing <ref type="bibr" target="#b13">(Sabour et al., 2017)</ref>, we propose a new self-attention mechanism for sentence embedding, namely Dynamic Self-Attention (DSA). To this end, we modify dynamic routing such that it functions as self-attention with the dynamic weight vector. DSA, which is stacked on CNN with Dense Connection <ref type="bibr" target="#b5">(Huang et al., 2017)</ref>, achieves new state-of-the-art results among the sentence encoding methods in Stanford Natural Language Inference (SNLI) dataset with the least number of parameters, while obtaining comparative results in Stanford Sentiment Treebank (SST) dataset. It also outperforms recent models in terms of time efficiency due to its simplicity and highly parallelized computations.</p><p>Our technical contributions are as follows:</p><p>• We design and implement Dynamic Self-Attention (DSA), a new self-attention mechanism for sentence embedding.</p><p>• We devise the dynamic weight vector with which DSA computes attention weights.</p><p>• We achieve new state-of-the-art results in SNLI dataset, while showing comparative results in SST dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>In self-attention <ref type="bibr" target="#b9">(Liu et al., 2016;</ref><ref type="bibr" target="#b9">Yang et al., 2016)</ref>, attention weights are computed as follows: <ref type="figure">Figure 1</ref>: Overall architecture of our approach. z 1 , ..., z m are dynamic weight vectors. The final sentence embedding, i.e. z, is generated by concatenating z 1 , ..., z m .</p><formula xml:id="formula_0">a = Softmax(v T Tanh(W X))<label>(1)</label></formula><p>where X ∈ R dw×n is an input sequence, W ∈ R dv×dw is a projection matrix and v ∈ R dv is the learnable weight vector of self-attention. The weight vector v plays an important role, since attention weights are computed by the inner product between v and the projection of the input sequence X. The weight vector v is static with respect to the input sequence X during inference. Replacing the weight vector v with a weight matrix enables multiple attentions <ref type="bibr" target="#b8">(Lin et al., 2017;</ref><ref type="bibr" target="#b14">Shen et al., 2018a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Our architecture, shown in <ref type="figure">Figure 1</ref>, is built on CNN with Dense Connection <ref type="bibr" target="#b5">(Huang et al., 2017)</ref>. Dynamic Self-Attention (DSA), which is stacked on CNN with Dense Connection, computes attention weights over words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CNN with Dense Connection</head><p>The goal of this module is to encode each word into a meaningful representation space while capturing local information. We do not add any positional encoding, as suggested by <ref type="bibr" target="#b4">Gehring et al. (2017)</ref>; deep convolution layers capture relative position information. We also enforce every output of layers to have the same number of columns by using appropriate zero padding. We denote a sequence of word embeddings as</p><formula xml:id="formula_1">X 0 ∈ R d 0 ×n , where X 0 = [x 0 1 , x 0 2 , ..., x 0 n ]. h l (·)</formula><p>is a composite function of the l th layer, composed of 1D Convolution, dropout <ref type="bibr" target="#b17">(Srivastava et al., 2014)</ref>, and leaky rectified linear unit (Leaky ReLU). We feed a sequence of word embeddings into h 1 (·) with kernel size 1:</p><formula xml:id="formula_2">X 1 = h 1 (X 0 )<label>(2)</label></formula><p>where X 1 ∈ R d 1 ×n . We add Dense Connection in every layer h l (·) with the same kernel size:</p><formula xml:id="formula_3">X l = h l (concat[X l−1 , X l−2 , ..., X 1 ])<label>(3)</label></formula><p>where X l ∈ R d l ×n , and l ∈ [2, L]. We concatenate outputs of all h l (·), and denote it as a single function:</p><formula xml:id="formula_4">Φ k (X 0 ) = concat[X L , X L−1 , ..., X 1 ]<label>(4)</label></formula><p>where kernel sizes of all h l (·) in Φ k (·) are the same number k, except for h 1 (·). We then feed outputs of two different functions Φ k 1 (·), Φ k 2 (·), and a sequence of word embeddings X 0 into a compression layer:</p><formula xml:id="formula_5">X c = h c (concat[Φ k 1 (X 0 ), Φ k 2 (X 0 ), X 0 ]) (5)</formula><p>where h c (·) is the composite function with kernel size 1. It compresses the first dimension of input (i.e., 2 L l=1 d l + d 0 ) into d c to represent a word compactly. Finally, L 2 norm of every column vector x c i in the X c is normalized, which is found to help our model to converge fast and stably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Self-Attention (DSA)</head><p>Dynamic Self-Attention (DSA) iteratively computes attention weights over words with the dynamic weight vector, which varies with inputs. DSA enables multiple attentions in parallel by multiplying different projection matrices to X c , the output from CNN with Dense Connection. For the j th attention, DSA projects the compact representation of every word x c i with LeakyReLU activation:x</p><formula xml:id="formula_6">j|i = LeakyReLU(W j x c i + b j )<label>(6)</label></formula><p>where W j ∈ R do×dc is a projection matrix, b j ∈ R do is a bias term for the j th attention. Given the number of attentions m, i.e., j ∈ [1, m], attention weights of words are computed by following Algorithm 1:</p><p>Algorithm 1 Dynamic Self-Attention for all i, j : q ij = q ij +x T j|i z j 8:</p><p>return all z j r is the number of iterations, and a ij is the attention weight for the i th word in the j th attention. z j is the output for the j th attention of DSA at the r th iteration, and also the dynamic weight vector for the j th attention of DSA before r th iteration. The final sentence embedding z is the concatenation of z 1 , ..., z m :</p><formula xml:id="formula_7">z = concat[z 1 , ..., z m ]<label>(7)</label></formula><p>where z ∈ R mdo is used for downstream tasks. We modify dynamic routing <ref type="bibr" target="#b13">(Sabour et al., 2017)</ref> to make it function as self-attention with the dynamic weight vector. We remove capsulization layer in capsule network which transforms scalar neurons to capsules, multi-dimensional neurons. A single word is then not decomposed into multiple capsules, but represented as a single vector x c i in Eq. 6. Squashing function is a nonlinear function for capsules. We replace it with Tanh nonlinear function for scalar neurons in Line 6 of Algorithm 1. We also force all the words in the j th attention to share a projection matrix W j in Eq. 6, as an input is a variable-length sequence. By contrast, each capsule in capsule network has its own projection matrix W ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dynamic Weight Vectors</head><p>The weight vector v of self-attention in Eq. 1 is static during inference. In DSA, however, the dynamic weight vector z j in Line 6 of Algorithm 1, varies with an input sequencex j|1 , ...,x j|n , even after training. In order to show how the dynamic weight vectors vary, we perform dimensionality reduction on them, z 1 at the (r − 1) th iteration of Algorithm 1, by Principal Component Analysis (PCA). We randomly select 1,000 sentences from Stanford Natural Language Inference (SNLI)  <ref type="figure" target="#fig_0">Figure 2</ref> shows that dynamic weight vectors are scattered in all directions. Thus, DSA adapts the dynamic weight vector with respect to each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our sentence embedding method with two different tasks: natural language inference and sentiment analysis. We implement single DSA, multiple DSA and self-attention in Eq. 1 as a baseline. Both DSA and self-attention are stacked on CNN with Dense Connection for fair comparison.</p><p>For our implementations, we initialize word embeddings by 300D GloVe 840B pretrained vectors <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref>, and fix them during training. We use cross-entropy loss as an objective function for both tasks. We set d o = 600, m = 1 for single DSA and d o = 300, m = 8 for multiple DSA. In Appendix, we provide details for training our implementations, hyperparameter settings, and visualization of attention maps of DSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Natural Language Inference Results</head><p>Natural language inference is a task of classifying the semantic relationship between two sentences, i.e., a premise and a hypothesis. We conduct experiments on Stanford Natural Language Inference (SNLI) dataset, consisting of human-written 570k pairs of English sentences labeled with one of three classes: Entailment, Contradiction and Neutral. As the task considers the semantic relationship, SNLI is used as a benchmark for evaluating the performance of a sentence encoder.</p><p>We follow a conventional approach, called heuristic matching <ref type="bibr" target="#b10">(Mou et al., 2016)</ref>, to classify the relationship of two sentences. The sentences are encoded by our proposed model. Given encoded sentences s h , s p for hypothesis and premise respectively, an input of the classifier Model Train (%) Test (%) Parameters (m) T(s)/epoch 600D BiLSTM with self-attention <ref type="bibr" target="#b9">(Liu et al., 2016)</ref> 84.5 84.2 2.8 -300D Directional self-attention network <ref type="bibr" target="#b14">(Shen et al., 2018a)</ref> 91.1 85.6 2.4 587 600D Gumbel TreeLSTM <ref type="bibr" target="#b2">(Choi et al., 2018)</ref> 93.1 86.0 10.0 -600D Residual stacked encoders <ref type="bibr" target="#b11">(Nie and Bansal, 2017)</ref> 91.0 86.0 29.0 -300D Reinforced self-attention network <ref type="bibr" target="#b15">(Shen et al., 2018b)</ref> 92.6 86.3 3.1 622 1200D Distance-based self-attention network <ref type="bibr" target="#b6">(Im and Cho, 2017)</ref> 89 Model SST-2 SST-5 BiLSTM <ref type="bibr" target="#b1">(Cho et al., 2014)</ref> 87.5 49.5 CNN-non-static <ref type="bibr" target="#b7">(Kim, 2014)</ref> 87   <ref type="formula" target="#formula_0">(135s)</ref> is significantly faster than recent models because of its simple structure and highly parallelized computations. With tradeoffs in terms of parameters and learning time per epoch, multiple DSA outperforms other models by a large margin (+1.1%).</p><p>In comparison to the baseline, single DSA shows better performance than self-attention (+2.2%). This confirms that the dynamic weight vector is more effective for sentence embedding. Note that our implementation of the baseline, selfattention stacked on CNN with Dense Connection, shows better performance (+0.4%) than the one stacked on BiLSTM <ref type="bibr" target="#b9">(Liu et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentiment Analysis Results</head><p>Sentiment analysis is a task of classifying sentiment in sentences. We use Stanford Sentiment Treebank (SST) dataset, consisting of 10k English sentences, to evaluate our model in singlesentence classification. We experiment SST-2 and SST-5 dataset labeled with binary sentiment labels and five fine-grained labels, respectively.</p><p>The SST results are summarized in <ref type="table" target="#tab_3">Table 2</ref>. We compare single DSA with four baseline models: 1 https://nlp.stanford.edu/projects/snli/ BiLSTM <ref type="bibr" target="#b1">(Cho et al., 2014)</ref>, CNN <ref type="bibr" target="#b7">(Kim, 2014)</ref> and self-attention with BiLSTM or CNN with dense connection. Single DSA outperforms all the baseline models in SST-2 dataset, and achieves comparative results in SST-5, which again verifies the effectiveness of the dynamic weight vector. In contrast to the distinguished results in SNLI dataset (+2.2%), in SST dataset, only marginal differences in the performance between DSA and the previous self-attentive models are found. We conclude that DSA exhibits a more significant improvement for large and complex datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>Our work differs from early self-attention for sentence embedding <ref type="bibr" target="#b9">(Liu et al., 2016;</ref><ref type="bibr" target="#b9">Yang et al., 2016;</ref><ref type="bibr" target="#b8">Lin et al., 2017;</ref><ref type="bibr" target="#b14">Shen et al., 2018a)</ref> in that the dynamic weight vector is not static. Independently, there have recently been an approach to capsule network-based NLP. <ref type="bibr" target="#b19">Zhao et al. (2018)</ref> applied whole capsule network to text classification task. However, we only utilize an algorithm, dynamic routing from capsule network, and modify it into self-attention with the dynamic weight vector, without unnecessary concepts, e.g., capsule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed Dynamic Self-Attention (DSA), which computes attention weights over words with the dynamic weight vector. With the dynamic weight vector, the self attention mechanism can be furnished with flexibility. Our experiments show that DSA achieves new state-of-the-art results in SNLI dataset, while showing comparative results in SST dataset. <ref type="figure">Figure 3</ref>: We visualize 4 out of 8 attentions from multiple DSA, which are human interpretable. Each attention in multiple DSA attends different aspects in the given sentences. 1 th attention only attends words related to a verb, 2 th attends related to a place, 3 th attends related to adjective, and 4 th attends related to an organism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Dynamic weight vectors visualization dataset and plot each dynamic weight vector for the sentences in 2D vector space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1: procedure ATTENTION(x j|i , r) 2:for all i th word, j th attention : q ij = 0</figDesc><table><row><cell>3: 4:</cell><cell>for r iterations do for all i, j : a ij =</cell><cell>exp(q ij ) k exp(q kj )</cell></row><row><cell>5:</cell><cell cols="2">for all j : s j = i a ijxj|i</cell></row><row><cell>6:</cell><cell cols="2">for all j : z j = Tanh(s j )</cell></row><row><cell>7:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>SNLI Results. The values in T(s)/epoch come from original papers and are experimented on the same graphic card to ours (single Nvidia GTX 1080Ti). Word embedding is not counted in parameters.</figDesc><table><row><cell>.6</cell><cell>86.3</cell><cell>4.7</cell><cell>693</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy with SST dataset. is concat[s h , s p , |s h − s p |, s h s p ].The results from the official SNLI leader board 1 are summarized inTable 1. Single DSA achieves new state-of-the-art results with test accuracy (86.8%) and the number of parameters (2.1m). Besides, our learning time per epoch</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SNLI</head><p>We minimize cross-entropy loss with Adam optimizer. We apply m = 1, d o = 600 for single DSA and m = 8, d o = 300 for multiple DSA. We use two-hidden layer multilayer perceptron (MLP) with leaky rectified linear unit (Leaky ReLU) activation as a classifier, where the number of hidden layer neurons are 300 (single) or 512 (multiple). Batch normalization and dropout are used for an input for a hidden layer in the MLP. Dropout rate is set to 0.3 (single) or 0.4 (multiple) in the MLP. For the both models, we use 0.00001 L2 regularization. We use dropout with rate of 0.2 in every h l (·), and h c (·). We also use dropout with rate of 0.3 for word embeddings. We initialize parameters in every layer with He initialization and multiply them by square root of the dropout rate of its layer. We initialize out-of-vocabulary words with U(−0.005, 0.005). starting with default value of Adam, we halve learning rate if training loss is not reduced for five times with a patience of 0.001. The size of mini-batch is set to 256.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Detailed Experimental Settings</head><p>For training our implementations in both Standford Natural Language Inference (SNLI) dataset and Stanford Sentiment Treebank (SST) dataset, we initialize word embeddings by 300D GloVe 840B pretrained vectors, and fix them during training. We use cross-entropy loss as an objective function for both tasks. For both tasks, we set r = 2, k 1 = 3, k 2 = 5, L = 4, d 1 = 150, d l = 75, where l ∈ [2, L], and d c = 300. All models are implemented via PyTorch. Details of the hyperparameters for each task are introduced in each section. Note that we followed data preprocessing of SST as <ref type="bibr" target="#b7">(Kim, 2014)</ref> 2 and SNLI as <ref type="bibr">(Conneau et al., 2017) 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SST</head><p>We minimize cross-entropy loss with Adadelta optimzier. We apply d o = 600, m = 1. We use onehidden layer MLP with Leaky ReLU activation as a classifier, where the number of hidden layer neurons is 300. Batch normalization and dropout with rate of 0.4 are used for an input for a hidden layer 2 https://github.com/yoonkim/CNN_sentence 3 https://github.com/facebookresearch/InferSent in MLP. For regularization, we use 0.00001 L2 regularization. We use dropout with rate of 0.2 in every h l (·) and h c (·). We also use dropout with rate of 0.4 for word embeddings. We initialize parameters in every layer with He initialization and multiply them by square root of the dropout rate of each layer. We initialize out-of-vocabulary words with U(−0.05, 0.05). Starting with default value of Adadelta, we halve learning rate halved learning rate if training loss is not reduced for two times with a patience of 0.001. The size of mini-batch is set to 128. <ref type="figure">Figure 4</ref>: Single DSA attends only informative words in the given sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473.Version7</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to compose task-specific tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distancebased self-attention network for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbae</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02047</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning natural language inference using bidirectional LSTM model and inner-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09090</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shortcut-stacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Evaluating Vector Space Representations for NLP</title>
		<meeting>the Second Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforced selfattention network: A hybrid of hard and soft attention for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10296</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Version 1</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attentionbased convolutional neural network for semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Computational Linguistics, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2526" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Investigating capsule networks with dynamic routing for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soufei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00857</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Version 1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
