<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyyed</forename><forename type="middle">Hossein</forename><surname>Hasanpour</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rouhani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
						</author>
						<title level="a" type="main">Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning</term>
					<term>Convolutional Neural Networks (CNNS)</term>
					<term>Simple Network</term>
					<term>Classification</term>
					<term>Efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Major winning Convolutional Neural Networks (CNNs), such as VGGNet, ResNet, DenseNet, etc., include tens to hundreds of millions of parameters, which impose considerable computation and memory overheads. This limits their practical usage in training and optimizing for real-world applications. On the contrary, light-weight architectures, such as SqueezeNet, are being proposed to address this issue. However, they mainly suffer from low accuracy, as they have compromised between the processing power and efficiency. These inefficiencies mostly stem from following an ad-hoc designing procedure. In this work, we discuss and propose several crucial design principles for an efficient architecture design and elaborate intuitions concerning different aspects of the design procedure. Furthermore, we introduce a new layer called SAF-pooling to improve the generalization power of the network while keeping it simple by choosing best features. Based on such principles, we propose a simple architecture called SimpNet. We empirically show that SimpNet provides a good trade-off between the computation/memory efficiency and the accuracy solely based on these primitive but crucial principles. SimpNet outperforms the deeper and more complex architectures such as VGGNet, ResNet, WideResidualNet etc., on several well-known benchmarks, while having 2 to 25 times fewer number of parameters and operations. We obtain state-of-theart results (in terms of a balance between the accuracy and the number of involved parameters) on standard datasets, such as CIFAR10, CIFAR100, MNIST and SVHN. The implementations are available at https://github.com/Coderx7/SimpNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S INCE the resurgence of neural networks, deep learning methods have been gaining huge success in diverse fields of applications, including semantic segmentation, classification, object detection, image annotation and natural language processing <ref type="bibr">[1]</ref>. Convolutional Neural Network (CNN), as a powerful tool for representation learning, is able to discover complex structures in the given data and represent them in a hierarchical manner <ref type="bibr">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr">[4]</ref>. Therefore, there are few parameters to be manually engineered, among which is the network architecture. What all of the recent architectures have in common is the increasing depth and complexity of the network that provides better accuracy for the given task. The winner of the ImageNet Large Scale Visual Recognition Competition 2015 (ILSVRC) <ref type="bibr" target="#b4">[5]</ref> achieved its success using a very deep architecture of 152 layers <ref type="bibr">[2]</ref>. The runner up also deployed a deep architecture <ref type="bibr">[3]</ref>. This trend has been followed ever since <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><p>As networks get deeper, aiming to improve their discrimination power, the computations and memory usage cost and overhead become critically expensive, which makes it very hard to apply or expand them for a variety of real-world applications. Despite the existence of various techniques for improving the learning algorithms, such as different initialization algorithms <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, normalization and regularization <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>, nonlinearities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> and data-augmentation tricks <ref type="bibr">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>, they are most beneficial when utilized in an already well performing architecture. In addition, some of these techniques may even impose more computational and memory usage overheads <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Therefore, it is highly desirable to design efficient (less complex) architectures with smaller number of layers and parameters, which are as good as their deeper and more complex counterparts. Such architectures can then be further tweaked using novel tricks from the literature.</p><p>The key to devising a well performing architecture is to have basic principles that each aim at achieving a specific goal, which together enhance different aspects of a network. Following a principled approach, rather than an ad-hoc one, has the advantage of allowing granular tuning and automated architecture design approaches. In other words, established proven designing principles can be used for achieving a custom architecture that suits a specific goal by the architect or even be made automatically by maneuvering in different design space aspects introduced by such principles. In addition, such maneuvering in the design space effectively exhibits which aspects are more influential and can better result in rectifying existing issues without diverging altogether or working on aspects that are less influential. One of such influential use cases can be the related research on creating an architecture using evolutionary optimization algorithms. With better recognized intuitions, the domain knowledge can be transferred into the algorithm. For instance, Google AutoML aims at automating the design of machine learning models <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this paper, we introduce a set of designing principles to create a road-map for designing efficient networks tailored to the desired goals. We elaborate detailed explanation about different aspects in design, such as different pooling operations, kernel sizes, depth, etc.and provide a good insight into the underlying components in an architecture. Following our defined principles, we propose a new operational layer called "SAF-pooling", which enhances the network discrimination power. In this layer, we enforce the network to learn more robust features by first pooling the highest activations, which denote strong features related to the specific class, and then randomly turning off some of them. During this process, we simulate the cases where not all features are present in the input due to occlusion, viewpoint changes, illumination variation and the likes, and thus the network is forced to adapt itself to these new situations by developing new feature detectors.</p><p>Considering all these introduced principles, we propose a new simple and efficient architecture denoted as SimpNet. To show the effectiveness of such principles, a series of experiments are conducted on 4 major benchmark datasets (CIFAR10/100, SVHN, and MNIST), results of which show that the our architecture outperforms all deeper and heavier architectures, while using 2 to 25 times fewer parameters. Apart from these, each principle is also tested extensively to show their necessity. Simple yet efficient architectures, such as SimpNet, signify the importance of considering such principles. They result in efficient and ideal architectures for many scenarios, especially for deploying in mobile devices (such as drones, cell phones), embedded systems (for Internet of Things, IoT, applications), and in general for Artificial Intelligence (AI) and Deep Learning (DL) applications at the edge (i.e., bringing AI and DL from cloud to the end point devices). They can be further compressed using Compression methods such as <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> and thus their memory and processing power consumption can be further decreased drastically. We intentionally employ simple and basic methods, in order to avoid any unnecessary cluttered design spaces. Doing this allows us to specifically focus on and identify the main underlying principles and aspects in the design space that greatly influence the performance. Failing to do so brings unnecessary complexity and many side challenges specific to each new method that would ultimately result in either a prolonged process or failing to identify truly important principles. This can ultimately lead to overlooking or mistaking main contributing factors with not-so-relevant criterion, which would be a deviation from the initial goal. Therefore, complementary methods from the literature can, then, be separately investigated. In this way, when the resulting model performs reasonably well, relaxing the constraints on the model (inspired by the above recent complementary methods) can further boost the performance with little to no effort. This performance boost has direct correlation with how well an architecture is designed. A fundamentally clumsily designed architecture would not be able to harness the advantages, because of its inherent flawed design.</p><p>The rest of the paper is organized as follows: Section II presents the most relevant works. In Section III, we present our set of designing principles and in Section IV we present SimpNet, derived from such principles. In Section V the experimental results are presented on 4 major datasets (CIFAR10, CIFAR100, SVHN, and MNIST), followed by more details on the architecture and different changes pertaining to each dataset. Finally, conclusions and directions for future works are summarized in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In this section, we review the latest trends and related works, categorized into two subsections. However, to the best of our knowledge there is no work investigating the network design principles in a general framework. There are previous works such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29</ref>] that proposed some principles or strategies that suited their specific use-cases, although some of them can be applied to any architecture, they were mostly aimed at a specific architecture and therefore did not contain tests or experiments to show how effective they are in other scenarios. Works such as <ref type="bibr" target="#b29">[30]</ref> only listed a few previously used techniques without specifically talking about their effectiveness or validity/usability in a broader sense. It was mere a report on what techniques are being used without any regards on how effective they are in different situations. Strategy-wise, the most similar work to ours is <ref type="bibr" target="#b27">[28]</ref>. In the following, we briefly talk about the general trends that have been used in recent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Complex Networks</head><p>Designing more effective networks were desirable and attempted from the advent of neural networks <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. This desire manifested itself in the form of creating deeper and more complex architectures <ref type="bibr">[2-4, 6, 9, 22, 34-37]</ref>. This was first attempted and popularized by Ciresan et al. <ref type="bibr" target="#b33">[34]</ref> training a 9 layer multi-layer precenteron (MLP) on a GPU, which was then practiced by other researchers <ref type="bibr">[2-4, 6, 9, 21, 34-38]</ref>. Among different works, some played an important role in defining a defacto standard in creating and designing architectures. In 2012 Krizhevsky et al. <ref type="bibr" target="#b21">[22]</ref> created a deeper version of LeNet5 <ref type="bibr" target="#b38">[39]</ref> with 8 layers called AlexNet, unlike LeNet5, It had a new normalization layer called local contrast normalization, and used rectified linear unit (ReLU) <ref type="bibr" target="#b20">[21]</ref> nonlinearity instead of the hyperbolic tangent (i.e., T anh), and also a new regularization layer called Dropout <ref type="bibr" target="#b39">[40]</ref>. This architecture achieved stateof-the-art on ILSVRC 2012 <ref type="bibr" target="#b4">[5]</ref>. In 2013 Lin et al. <ref type="bibr" target="#b40">[41]</ref> introduced a new concept into the literature and deviated from the previously established trend in the designing networks. They proposed a concept named network-in-network (NIN). they built micro neural networks into convolutional neural networks using 1 × 1 filters and also used global pooling instead of fully connected layers at the end, acting as a structural regularizer that explicitly enforces feature maps to be confidence maps of concepts and achieved state-of-the-art results on CIFAR10 dataset. In 2014, VGGNet by Simonyan et al. <ref type="bibr">[4]</ref> introduced several architectures, with increasing depth from 11 to 19 layers, which denoted deeper ones perform better. They also used 3 × 3 convolution (conv) filters, and showed that stacking smaller filters results in better nonlinearity and yields better accuracy. In the same year, a NIN inspired architecture named GoogleNet <ref type="bibr">[3]</ref> was released, 56 convolutional layers making up a 22 modular layered network. The building block was a block made up of conv layers with 1×1, 3×3, and 5×5 filters, named an inception module. This allowed to decrease the number of parameters drastically compared to former architectures and yet achieve state-of-the-art in ILSVRC. In a newer version, each 5 × 5 kernel was replaced with two consecutive 3 × 3. A technique called batch-normalization <ref type="bibr" target="#b15">[16]</ref> was also incorporated into the network for reducing internal covariate shift, which proved to be an essential part in training deep architectures and achieving state-of-the-art results in the ImageNet challenge.</p><p>In 2015, He et al. <ref type="bibr" target="#b10">[11]</ref> achieved state-of-the-art results in ILSVRC using VGGNet19 <ref type="bibr">[4]</ref> with ReLU replaced with a new variant, called Parametric ReLU (PReLU), to improve model fitting. An accompanying new initialization method was also introduced to enhance the new nonlinearity performance. Inter-layer connectivity was a new concept aimed at enhancing the gradient and information flow throughout the network. The concept was introduced by <ref type="bibr">[2]</ref> and <ref type="bibr" target="#b36">[37]</ref> independently. In <ref type="bibr">[2]</ref>, a residual block was introduced, in which layers are let to fit a residual mapping. Accompanied by previous achievements in <ref type="bibr" target="#b10">[11]</ref>, they could train very deep architectures ranging from 152 to 1000 layers successfully and achieve state-of-the-art on ILSVRC. Similarly, <ref type="bibr" target="#b36">[37]</ref> introduced a solution inspired by Long Short Term Memory (LSTM) recurrent networks (i.e., adaptive gating units to regulate the information flow throughout the network). They trained 100 to 1000 layer networks successfully.</p><p>In 2016, Szegedy et al. <ref type="bibr" target="#b6">[7]</ref> investigated the effectiveness of combining residual connections with their Inception-v3 architecture. They gave empirical evidence that training with residual connections accelerates the training of Inception networks significantly, and reported that residual Inception networks outperform similarly expensive Inception networks by a thin margin. With these variations, the single-frame recognition performance on the ILSVRC 2012 classification task <ref type="bibr" target="#b4">[5]</ref> improves significantly. Zagoria et al. <ref type="bibr" target="#b8">[9]</ref> ran a detailed experiment on residual nets <ref type="bibr">[2]</ref> called Wide Residual Net (WRN), where instead of a thin deep network, they increased the width of the network in favor of its depth (i.e., decreased the depth). They showed that the new architecture does not suffer from the diminishing feature reuse problem <ref type="bibr" target="#b36">[37]</ref> and slow training time. Huang et al. <ref type="bibr" target="#b5">[6]</ref> introduced a new form of inter-layer connectivity called DenseBlock, in which each layer is directly connected to every other layer in a feed-forward fashion. This connectivity pattern alleviates the vanishing gradient problem and strengthens feature propagation. Despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters. The models tend to generalize surprisingly well, and obtain state-of-the-art in several benchmarks, such as CIFAR10/100 and SVHN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Light Weight Architectures</head><p>Besides more complex networks, some researchers investigated the opposite direction. Springenberg et al. <ref type="bibr" target="#b41">[42]</ref> investigated the effectiveness of simple architectures. They intended to come up with a simplified architecture, not necessarily shallower, that would perform better than, more complex networks. They proposed to use strided convolutions instead of pooling and mentioned that downsampling is enough for achieving good performance, therefore no pooling is necessary. They tested different versions of their architecture, and using a 17 layer version, they achieved a result very close to the state-of-the-art on CIFAR10 with intense data-augmentation. In 2016, Iandola et al. <ref type="bibr" target="#b28">[29]</ref> proposed a novel architecture called, SqueezeNet, a lightweight CNN architecture that achieves AlexNet-level <ref type="bibr" target="#b21">[22]</ref> accuracy on ImageNet, with 50 times fewer parameters. They used a previously attempted technique called bottleneck and suggested that the spatial correlation does not matter much and thus 3 × 3 conv filters can be replaced with 1 × 1 ones. In another work, <ref type="bibr" target="#b42">[43]</ref> proposed a simple 13-layer architecture, avoiding excessive depth and large number of parameters, and only utilizing a uniform architecture using 3 × 3 convolutional and 2 × 2 pooling layers. Having 2 to 25 times less parameters, it could outperform much deeper and heavier architectures such as ResNet on CIFAR10, and achieve state-of-the-art result on CIFAR10 without data-augmentation. It also obtained very competitive results on other datasets.</p><p>In 2017, Howard et al. <ref type="bibr" target="#b43">[44]</ref>, proposed a novel class of architectures with 28 layers called MobileNets, which are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. They tested their architecture on ImageNet and achieved VGG16 level accuracy while being 32 times smaller and 27 times less computational intensive. Several month later, Zhang et al. <ref type="bibr" target="#b44">[45]</ref> proposed their architecture called ShuffleNet, designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). Using the idea of depth-wise separable convolution, the new architecture proposed two operations, point-wise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy.</p><p>In this work, we introduce several fundamental designing principles, which can be used in devising efficient architectures. Using these principles, we design a simple 13-layer convolutional network that performs exceptionally well on several highly competitive benchmark datasets and achieves state-ofthe-art results in a performance per parameter scheme. The network outperforms nearly all deeper and heavier architectures with several times fewer parameters. The network has much fewer parameters (2 to 25 times less) and computation overhead compared to all previous deeper and more complex architectures. Compared to architectures like SqueezeNet or FitNet (which have less number of parameters than ours while being deeper), our network performs far superior in terms of accuracy. This shows the effectiveness and necessity of the proposed principles in designing new architectures. Such simple architectures can further be enhanced with novel improvements and techniques in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DESIGN INTUITIONS</head><p>Designing an optimal architecture requires a careful consideration and compromise between the computational complexity and the overall performance of the framework. There are several different factors contributing to each of these two. Previous works often neglected one or more of such factors based on their applications. Here, we study these factors, and clearly categorize and itemize them as principles to take into consideration when designing a network. Then, based on these principles we propose an architecture and illustrate that we can obtain comparable or even better results, while requiring much less computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gradual Expansion with Minimum Allocation</head><p>The very first thought striking to mind, when designing a network, is that it should be a very deep architecture. It is a widely accepted belief in the literature that deeper networks perform better than their shallower counterparts. One reason for such a wide spread belief is the success of deeper models in recent years <ref type="bibr">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr">[4]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref>. Romero et al. <ref type="bibr" target="#b51">[52]</ref> also showed how a deeper and thinner architecture performs better. It makes sense that by adding more layers, we are basically providing more capability of learning various concepts and relations between them. Furthermore, it is shown that early layers learn lower level features while deeper ones learn more abstract and domain specific concepts. Therefore, a deeper hierarchy of such features would yield better results <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref>. Furthermore, More complex datasets seem to benefit more from deeper networks, while simpler datasets work better with relatively shallower ones <ref type="bibr">[2-4, 22, 48]</ref>. However, while deeper architectures do provide better accuracy compared to a shallower counterpart, after certain depth, their performance starts to degrade <ref type="bibr" target="#b27">[28]</ref>, and they perform inferior to their shallower counterpart, indicating a shallower architecture may be a better choice. Therefore, there have been attempts to show that wider and shallower networks can perform like a deep one <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43]</ref>. The depth of the network has a higher priority among other aspects in achieving better performance. Based on these results, it seems a certain depth is crucial to attain a satisfactory result. However, the ratio between depth, width and number of parameters are unknown, and there are conflicting results against and in support of depth. Based on the above discussions, to utilize depth, memory and parameters more efficiently, it is better to design the architecture in a gradual fashion, i.e., instead of creating a network with a random yet great depth, and random number of neurons, beginning with a small and thin network then gradually proceeding with deepening and then widening the network is recommended. This helps to prevent excessive allocation of processing units, which imposes unnecessary overhead and causes overfitting. Furthermore, employing a gradual strategy helps in managing how much entropy a network would provide. The more parameters a network withholds, the faster it can converge and the more accuracy it can achieve, However it will also overfit more easily. A model with fewer parameters, which provides better results or performs comparable to its heavier counterpart indicates the network has learned better features for the task. In other words, by imposing more constraints on the entropy of a network, the network is implicitly forced to find and learn much better and more robust features. This specifically manifests itself in the generalization power, since the network decisions are based on more important, more discriminative, and less noisy features. By allocating enough capacity to the network, a shallower and wider network can perform much better than when it is randomly made deeper and thinner, as shown in our experiments and also pointed out by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref>. It is also shown that widening an existing residual architecture significantly improves its performance compared to making it deeper <ref type="bibr" target="#b8">[9]</ref>. Thus, instead of going any deeper with thinner layers, a wider and comparatively shallower "but still deep enough" is a better choice. Additionally, it is computationally more effective to widen the layers, rather than having thousands of small kernels, as GPUs are often much more efficient in parallel computations on large tensors <ref type="bibr" target="#b8">[9]</ref>.</p><p>Failing to give enough care at this stage will give rise to issues concerning very deep architectures (such as weakened gradient flow, degradation issue <ref type="bibr">[2]</ref> and computation usage overhead), while it is usually unnecessary to maintain such a depth in many applications. It is recommended to expand the network to reach a pyramid-shaped form, which means a progressive reduction of spatial resolution of the learned feature maps with the increase in the number of feature maps to keep the representational expressiveness. A Large degree of invariance to geometric transformations of the input can be achieved with this gradual reduction of spatial resolution compensated by a progressive increase of the richness of the representation (the number of feature maps) <ref type="bibr" target="#b38">[39]</ref>. It is important to note that, one may find out that with the same number of parameters, a deeper version might not achieve as good accuracy as a shallower counterpart and this looks like a contradiction to what we have previously discussed. The improved performance in the shallower architecture can be attributed to the better allocation of layers processing capacity. Each layer in an architecture needs a specific number of processing units in order to be able to carry out its underlying task properly. Hence, with the exact same number of parameters, these parameters will be scattered among the shallower layers better than a much deeper architecture. It is evident that in the deeper counterpart, with the same processing budget, these fewer units in each layer will bear less processing capacity and, hence, a decreased and degraded performance will result. In such a circumstance, we can say that we have an underfitting or low-capacity issue at the layer level. The decreased processing capacity will not let the network take advantage of the available depth, and hence the network is unable to perform decently. A much deeper architecture also exhibits a higher chance of ill-distribution of processing units to the other extreme. Furthermore, properly distributing neurons between a shallower and a deeper architecture, using the same processing budget, is harder for the deeper counter part. This issue even gets more pronounced, when the architecture is made further deeper. In other words, as the difference between the depth of the two increases, the job of properly allocating neurons to all layers becomes even harder and ultimately at some point will be impossible with the given budget. When a deeper architecture is deprived of the needed processing capacity, a Processing Level Deprivation (PLD) phenomena occurs, in which the architecture fails to develop simple but necessary functions to represent the data. This causes the deeper network to perform inferior to the shallower one. That is one of the main issues that arises when a shallower and a deeper architecture are being compared performance-wise. Accordingly, when the processing budget increases, the shallower architecture starts to perform inferior to the deeper counterpart, and this gets more pronounced as the budget is further increased. This is because the shallower architecture has saturated and fails to properly utilize the existing capacity, and thus a phenomena called Processing Level Saturation (PLS) occurs, where more processing power would not yield in increased representational power. Meanwhile, the deeper architecture can now utilize the increased processing power and develop more interesting functions, resulting in PLD vanishing and further improvements in model performance. As the number of parameters increases, this difference is even more vigorously noticed. This is in turn one of the reasons, deeper architectures are usually larger in the number of parameters (compared to shallower ones). Note that if we consider an increased budget (suited for a deeper architecture so that all layers can be capacitated properly to carry on their task), the same number of parameters will over-saturate the shallower network and result only in unnecessary and wasted processing power (and vice versa). This also shows why gradual expansion and minimum allocation are key concepts, as they prevent first from choosing a very deep architecture and second from allocating too much neurons, thus, preventing the PLD and PLS issues effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Homogeneous Groups of Layers</head><p>The design process of typical architectures has conventionally been treated as simply putting together a stack of several types of layers such as Convolution, Pooling, Normalization and the likes. Instead of viewing the design process as a simple process of stacking a series of individual layers, it is better to thoughtfully design the architecture in groups of homogeneous layers. The idea is to have several homogeneous groups of layers, each responsible for a specific task (achieving a single goal). This symmetric and homogeneous design, not only allows to easily manage the number of parameters a network will withhold while providing better information pools for each semantic level, but will it also provide the possibility of further granular fine-tuning and inspection, in a group-wise fashion. This technique has been in use implicitly, since the success of <ref type="bibr" target="#b21">[22]</ref> and is also being used by all recent major architectures more profoundly (accentuated) than before, including <ref type="bibr">[2,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b53">54]</ref>. However, the previous use of this technique has been only to showcase a new concept rather than to fully take advantage of such building block. Therefore, other aspects and advantages of such feature has not yet been fully harnessed. In other words, almost all former use-cases have been following an ad-hoc formation <ref type="bibr" target="#b6">[7]</ref> in utilizing such concept inside the network. Nevertheless, this scheme greatly helps in managing network topology requirements and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Local Correlation Preservation</head><p>It is very important to preserve locality throughout the network, especially by avoiding 1 × 1 kernels in early layers. The corner stone of CNN success lies in the local correlation preservation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b54">55]</ref>. However, <ref type="bibr" target="#b28">[29]</ref> has a contrary idea and reported that using more 1 × 1 (as opposed to more 3 × 3) in their architecture had a better result, and thus spatial resolution in CNN is not as important as it may have looked. Our extensive experiments along with others <ref type="bibr">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39]</ref> show the contrary and we argue this is the result of following an ad-hoc procedure in evaluating the hypothesis and more thorough experiments could yield a different outcome. For instance, the distribution of filters throughout the network does not follow a principled strategy, the same thing applies to the ad-hoc creation of the architecture, which would prevent the network from fully utilizing the capacity provided by bigger kernels. One reason can be the excessive use of bottlenecks to compress the representations that yielded such results. Further experiments showed indications of our claim, where a much shallower architecture utilizing only 3 × 3 kernels with fewer parameters could outperform SqueezeNet with twice the number of parameters. Based on our extensive experiments, it is recommended not to use 1×1 filters or fully connected layers, where locality of information matters the most. This is exclusively crucial for the early layers in the network. 1 × 1 kernels bear several desirable characteristics, such as increasing networks nonlinearity and feature fusion <ref type="bibr" target="#b40">[41]</ref> and also decreasing number of parameters by factorization and similar techniques <ref type="bibr" target="#b7">[8]</ref>. However, on the other hand, they ignore any local correlation in the input. Since they do not consider any neighborhood in the input and only take channel information into account, they distort valuable local information. Nevertheless, the 1 × 1 kernels are preferable in the later layers of the network. It is suggested to replace 1 × 1 filters with 2 × 2 if one plans on using them in places other than the ending layers of the network. Using 2 × 2 filters reduces the number of parameters, while retaining the neighborhood information. It should also be noted that excessive shrinkage in bottleneck strategy using 1 × 1 filters should be avoided or it will harm the representational expressiveness <ref type="bibr" target="#b7">[8]</ref>.</p><p>While techniques, such as aggregation and factorization heavily utilized in <ref type="bibr" target="#b7">[8]</ref>, provide a good way for replacing larger kernels by smaller ones and decreasing computation massively, we specifically do not use them for several reasons: (1) To intentionally keep everything as simple as possible, evaluate the basic elements, and identify crucial ones, which can then be incorporated in other schemes including inceptionlike modules. (2) Trying to replace all kernels into a sequence of smaller ones without knowing their effectiveness would increase the depth of the network unnecessarily (e.g., one 5 × 5 would need two 3 × 3 or four 2 × 2 kernels), and would therefore quickly provide difficulties in managing the network efficiency. (3) Knowing the effectiveness of different kernel sizes helps utilizing this technique judiciously, and thus, improve the network performance. (4) They have been used in multi-path designs such as inception <ref type="bibr">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, where several convolutional layers are concatenated with various filter sizes or numbers, and there is not enough evidence of how they work individually <ref type="bibr" target="#b27">[28]</ref>. It seems they work well in cases where complementary kernels are also being incorporated alongside. So, their effectiveness in isolation requires more research and experimentation. Furthermore, the principle of choosing hyperparameters concerning such multi-path designs requires further investigation. As an example, the influence of each branch remains unclear <ref type="bibr" target="#b27">[28]</ref>, and therefore we only consider "single-path" designs with no parallel convolutional layers (already faced with abundance of choices). (5) While modules such as inception do reduce the number of parameters <ref type="bibr" target="#b7">[8]</ref>, they impose computation overhead and their complex nature makes it even harder to customize it for arbitrary scenarios, since there are many variables to account for. Therefore, we resorted to single path design. At the very minimum, using these principles improves the results.</p><p>Finally It is noteworthy to avoid shrinking feature-map size at ending layers too much and at the same time, allocate excessive amount of neurons to them, while specifically earlier layers have much fewer numbers. very small feature-map sizes at the end of the network (e.g., 1 × 1) leads to a minuscule information gain and allocating too much neuron would only result in wasted capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Maximum Information Utilization</head><p>It is very important to avoid rapid downsampling or pooling, especially in early layers. Similar suggestion has also been given by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. To increase the network's discriminability power, more information needs to be made available. This can be achieved either by a larger dataset (i.e., collecting more data, or using augmentation) or utilizing available information more efficiently in the form of larger feature-maps and better information pools. Techniques, such as Inception <ref type="bibr">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>, inter layer connectivity including residual connections <ref type="bibr">[2]</ref>, Dense connections <ref type="bibr" target="#b5">[6]</ref>, and pooling fusion <ref type="bibr" target="#b55">[56]</ref>, are examples of more complex ways of providing better information pools. However, at the very least in its most basic form, an architecture can achieve better information pool without such complex techniques as well. If larger dataset is not available or feasible, the existing training samples must be efficiently utilized. Larger feature-maps, especially in the early layers, provide more valuable information in the network compared to the smaller ones. With the same depth and number of parameters, a network that utilizes larger feature-maps achieves a higher accuracy <ref type="bibr">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28]</ref>. Therefore, instead of increasing the complexity of a network by increasing its depth and its number of parameters, one can leverage better results by simply using larger input dimensions or avoiding rapid early downsampling. This is a good technique to keep the complexity of the network in check and to increase the accuracy. Similar observation has been reported by <ref type="bibr">[3,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b7">8]</ref> as well. Utilizing larger feature-maps to create more information pools can result in large memory consumption despite having a small number of parameters. A good example of such practice can be observed in DenseNet <ref type="bibr" target="#b5">[6]</ref>, from which a model (DenseNet-BC, L = 100, k = 12, with 0.8M parameters) takes more than 8 gigabytes of video memory (or VRAM). Although such memory consumption in part can be attributed to the inefficient implementation, still a great deal of that is present even after intensive optimization. Other instances can be seen in <ref type="bibr">[2]</ref> and <ref type="bibr" target="#b8">[9]</ref> that utilize many layers with large feature-maps. This shows that part of these architectures success is because of the information pool that is being provided for each semantic level using an increased number of layers with feature maps of the same size (without being downsampled). Moreover, other than data-augmentation, which has the most influential role in achieving translation invariance <ref type="bibr" target="#b56">[57]</ref>, (max-)pooling plays an important role in reducing the sensitivity of the output to shift and distortions <ref type="bibr" target="#b38">[39]</ref>. It also helps achieving translation invariance to some extend <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b57">58]</ref>. Scherner et al. <ref type="bibr" target="#b57">[58]</ref> show that max-pooling operation is vastly superior for capturing invariances in imagelike data, compared to a subsampling operations. Therefore, it is important to have reasonably enough number of pooling layers  in the architecture. Since the use of max-pooling will distort the exact location, excessive appliance of such layer would negatively impact the performance in some applications, such as semantic segmentation and object detection. Proper application of pooling both results in obtaining translation invariance, and imposes less memory and computation overheads. It also needs to be noted that the pooling operation, in its essence is different than simple downsampling. It is usually theorized that a convolutional layer can "learn" the pooling operation better. The notion that a pooling operation can be learned in a network is correct and have already been tried many times <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, however, examples such as strided convolutions <ref type="bibr" target="#b41">[42]</ref> which are convolutions with bigger strides, are far from doing that. Techniques such as strided convolution thus do not yield the same effect as a pooling operation, since a simple convolution layer with rectified linear activation cannot by itself implement a p-norm computation <ref type="bibr" target="#b41">[42]</ref>. Furthermore It is argued <ref type="bibr" target="#b41">[42]</ref> that the improvement caused by pooling is solely because of the downsampling and thus, one can replace a pooling operation with a strided convolution. However, our empirical results and further intuitions, which result in improved performance by pooling (explained later), prove otherwise. This is more visible in our tests, where with the same number of parameters, the architecture with simple maxpooling always outperforms the strided convolution counterpart. This is thoroughly addressed in the experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Maximum Performance Utilization</head><p>Considering implementation details and recent improvements in the underlying libraries, one can simply design better performing and more efficient architectures. For instance, using 3 × 3 filters, besides its already known benefits <ref type="bibr">[4]</ref>, allows to achieve a substantial boost in performance, when using NVIDIA's Cudnnv5.x library or higher. This is a speed up of about 2.7× compared to the former v4 version. The performance boost can be witnessed in newer versions such as Cudnnv7.x as well. This is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="table" target="#tab_0">Table I</ref>. The ability to harness every amount of performance is crucial, when it comes to production and industry. Whereas, using larger kernels such as 5 × 5 and 7 × 7 tend to be disproportionally more expensive in terms of computation. For example, a 5 × 5 convolution with n filters over a grid with m filters is 25 /9 = 2.78 times more computationally expensive than a 3 × 3 convolution with the same number of filters. Of course, a 5 × 5 filter can capture dependencies between signals activation of units further away in the earlier layers, so a reduction of the geometric size of the filters comes at a larger cost <ref type="bibr" target="#b7">[8]</ref>. Besides the performance point of view, on one hand larger kernels do not provide the same efficiency per parameter as a 3 × 3 kernel does. It may be interpreted that since larger kernels capture a larger area of neighborhood in the input, they may help suppressing noise to some extend and thus capturing better features. However, in practice the overhead they impose in addition to the loss in information they cause makes them not an ideal choice. Furthermore, larger kernels can be factorized into smaller ones <ref type="bibr" target="#b7">[8]</ref>, and therefore using them makes the efficiency per parameter to decline, causing unnecessary computational burden. Substituting larger kernels with smaller ones is also previously investigated in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Smaller kernels, on the other hand, do not capture local correlations as well as 3 × 3 kernels. Detecting boundaries and orientations are better done using a 3×3 kernel in earlier layers. A cascade of 3×3 can also replace any larger one and yet achieve similar effective receptive field. In addition, as discussed, they lead to better performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Balanced Distribution Scheme</head><p>Typically, allocating neurons disproportionally throughout the network is not recommended, because the network would face information shortage in some layers and operate ineffectively. For example, if not enough capacity is given to early layers to provide low-level features, or middle layers to provide middle-level features, the final layers would not have access to sufficient information to build on. This applies to all semantic levels in a network, i.e., if there is not enough capacity in the final layers, the network cannot provide higher level abstractions needed for accurate deduction. Therefore, to increase the network capacity (i.e., number of neurons) it is best to distribute the neurons throughout the whole network, rather than just fattening one or several specific layers <ref type="bibr" target="#b7">[8]</ref>. The degradation problem that occurs in deep architectures stems from several causes, including the ill-distributed neurons. As the network is deepened, properly distributing the processing capacity throughout the network becomes harder and this often is the cause for some deeper architectures under-performing against their shallower counterparts. This is actually what arises to PLD and PLS issues. we address this in the experiments section in details. Using this scheme, all semantic levels in the network will have increased capacity and will contribute accordingly to the performance, whereas the other way around will only increase some specific levels of capacity, which will most probably be wasted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Rapid Prototyping In Isolation</head><p>It is very beneficial to test the architecture with different learning policies before altering it. Most of the times, it is not the architecture that needs to be changed, rather it is the optimization policy that does. A badly chosen optimization policy leads to inefficient convergence, wasting network resources. Simple things, such as learning rates and regularization methods, usually have an adverse effect if not tuned correctly. Therefore, it is first suggested to use an automated optimization policy to run quick tests and when the architecture is finalized, the optimization policy is carefully tuned to maximize network performance. It is essential to note that when testing a new feature or applying a change, everything else (i.e., all other settings) remain unchanged throughout the whole experimental round. For example, when testing 5 × 5 vs. 3 × 3, the overall network entropy must remain the same. It is usually neglected in different experiments and features are not tested in isolation or better said, under a fair and equal condition which ultimately results in a not-accurate or, worse, a wrong deduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Dropout Utilization</head><p>Using dropout has been an inseparable part of nearly all recent deep architectures, often considered as an effective regularizer. Dropout is also interpreted as an ensemble of several networks, which are trained on different subsets of the training data. It is believed that the regularization effect of dropout in convolutional layers is mainly influential for robustness to noisy inputs. To avoid overfitting, as instructed by <ref type="bibr" target="#b39">[40]</ref>, half of the feature detectors are usually turned off. However, this often causes the network to take a lot more to converge or even underfit, and therefore in order to compensate for that, additional parameters are added to the network, resulting in higher overhead in computation and memory usage. Towards better utilizing this technique, there have been examples such as <ref type="bibr" target="#b41">[42]</ref> that have used dropout after nearly all convolutional layers rather than only fully connected layers, seemingly to better fight overfitting of their large (at the time) network. Recently <ref type="bibr" target="#b5">[6]</ref> also used dropout with all convolutional layers with less dropout ratio. Throughout our experiments and also in accordance to the findings of <ref type="bibr" target="#b60">[61]</ref>, we found that applying dropout to all convolutional layers improves the accuracy and generalization power to some extent. This can be attributed to the behavior of neurons in different layers. One being the Dead ReLU issue, where a percentage of the whole network never gets activated and thus a substantial network capacity is wasted. The dead ReLU issue can be circumvented by using several methods (e.g., by using other variants of ReLU nonlinearity family <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>). Another way to avoid this issue is to impose sparsity by randomly turning off some neurons, so that their contribution that may cause incoming input to the next neurons get negative, cancel out and thus avoid the symptom. The dropout procedure contributes to this by randomly turning off some neurons, and thus helps in avoiding the dead ReLU issue (see <ref type="figure" target="#fig_1">Figures 2 and 3)</ref>. Additionally, for other types of nonlinearities incorporated in deep architectures, dropout causes the network to adapt to new feature combinations and thus improves its robustness (see <ref type="figure" target="#fig_5">Figure 4</ref>). It also improves the distributed representation effect <ref type="bibr" target="#b39">[40]</ref> by preventing complex co-adaptions. This can be observed in early layers, where neurons have similar mean values, which again is a desirable result, since early layers in a CNN architecture capture common features. This dropout procedure improves the generalization power by allowing better sparsity. This can be seen by visualizing the neurons activation in higher layers, which shows more neurons are activated around zero <ref type="bibr" target="#b60">[61]</ref> (see <ref type="figure">Figure 5</ref>).    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Simple Adaptive Feature Composition Pooling</head><p>Max-and average-pooling are two methods for pooling operation that have been widely in use by nearly all architectures. While different architectures achieved different satisfactory results by using them and also many other variants <ref type="bibr" target="#b55">[56]</ref>, we (a) layer 13, using dropout.</p><p>(b) layer 13, using no dropout. <ref type="figure">Fig. 5</ref>: More neurons are activated around 0 when dropout is used which means more sparsity.</p><p>propose a variation, denoted as SAF-pooling, which provides the best performance compared to conventional max-, averagepooling operations. SAF-pooling is essentially the max-pooling operation carried out before a dropout operation. If we consider the input as an image, pooling operation provides a form of spatial transformation invariance, as well as reducing the computational complexity for the upper layers. Furthermore, different images of the same class often do not share the same features due to the occlusion, viewpoint changes, illumination variation, and so on. Therefore, a feature which plays an important role in identification of a class in an image may not appear in different images of the same class. When maxpooling is used alongside (before) dropout, it simulates these cases by dropping high activations deterministically(since high activations are already pooled by max-pooling), acting as if these features were not present This procedure of dropping the maximum activations simulates the case where some important features are not present due to occlusion or other types of variations. Therefore, turning off high activations helps other less prominent but as important features to participate better. The network also gets a better chance of adapting to new and more diverse feature compositions. Furthermore in each feature-map, there may be different instances of a feature, and by doing so, those instances can be further investigated, unlike average pooling that averages all the responses feature-mapwise. This way, all those weak responses get the chance to be improved and thus take a more prominent role in shaping the more robust feature detector. This concept is visualized and explained in more details in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Final Regulation Stage</head><p>While we try to formulate the best ways to achieve better accuracy in the form of rules or guidelines, they are not necessarily meant to be aggressively followed in all cases. These guidelines are meant to help achieve a good compromise between performance and the imposed overhead. Therefore, it is better to start by designing the architecture according to the above principles, and then altering the architecture gradually to find the best compromise between performance and overhead. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMPNET</head><p>Based on the principles mentioned above, we build a simple convolutional network with 13 layers. The network employs a homogeneous design utilizing 3 × 3 filters for convolutional layers and 2×2 filters for pooling operations. <ref type="figure" target="#fig_6">Figure 6</ref> illustrates the proposed architecture. To design the network based on the above principles, we first specified the maximum depth allowed, to assess the architecture characteristics in comparisons with other deeper and wider counterparts. Following the first rule, we started by creating a thin but deeper architecture with 10 layers, then gradually increased its width to the point where no improvements were witnessed. The depth is then increased, and at the same time the last changes related to the width are reverted back. The experiments are reiterated and the width is increased gradually (related to the first rule and rapid prototyping). During this process, we kept the width of all layers proportional and avoided any excessive allocation or lack thereof (balanced distribution scheme). In order to manage this criteria, we designed layers in groups with similar characteristics, i.e., the same number of featuremaps and feature-map sizes (homogeneous groups). To enable the extraction and preservation of as much information as possible, we used pooling operation sparingly, thus placing a pooling layer after each 5 layers. This lets the network to preserve the essential information with increased nonlinearity (maximum information utilization). The exact locations where pooling should be placed can be easily determined through a few tests. Since groups of homogeneous layers are used, placing poolings after each group makes it an easy experiment. We used 3 × 3 filters to preserve local information maximally, and also to utilize underlying performance optimizations in cuDNN library (maximum performance). We then ran several experiments in order to assess different network characteristics, and in order to avoid sweeping unintentional details affecting the final result, we ran experiments in isolation, i.e., when an experiment is being run, all criteria are locked. Finally the network is revised in order to resolve specific internal limits, i.e., memory consumption and number of parameters (final regulation stage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In this section, several experiments are setup to show the significance of the introduced principles. To this end, results are generated without any hyperparameter tuning. These experiments are conducted using variants of our simple architecture. The overall architecture is the same as the one introduced in Section IV with slight changes in the architecture for two reasons, first to make it easier to conduct fair tests between different usecases, and second to provide more diversity in terms of the architecture essence. For example, we tried architectures with different depth and different number of parameters. Therefore, in each experiment, a different version of this network is used with an emphasis on the principle in question. Also, different cases within an experiment use the same architecture and optimization policy so that the test reflects only the changes required by the principle. We report the number of layers and parameters when evaluating each architecture, to see the effect of varying depth with fixed number of parameters for the same network. We present our experimental results in the following subsections, in which two different types of experiments are designed: first to evaluate the principles outlined in Section III, and then to assess the performance of the proposed architecture based on those principles (i.e., SimpNet) compared to several leading architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>The experiments concerning the design principles use CIFAR10 benchmark dataset, however, the second set of experiments are run on 4 major benchmarking datasets namely, CIFAR10, CIFAR100, SVHN and MNIST. CIFAR10/100 datasets <ref type="bibr" target="#b61">[62]</ref> include 60,000 color images, of which 50,000 belong to the training set and the rest are reserved for testing (validation), with 10 and 100 distinct classes, respectively. The classification performance is evaluated using top-1 error. The SVHN dataset <ref type="bibr">[</ref>  handwritten digits, 0 to 9, with 60,000 images used for training and 10,000 for testing. We used Caffe framework <ref type="bibr" target="#b63">[64]</ref> for training our architecture and ran our experiments on a system with Intel Corei7 4790K CPU, 20 Gigabytes of RAM and NVIDIA GTX1080 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Examining the Design Intuitions</head><p>The first set of experiments involve examining the design principles, ran on CIFAR10 <ref type="bibr" target="#b61">[62]</ref> dataset.</p><p>Gradual Expansion and Minimum Allocation: <ref type="table" target="#tab_0">Table II</ref> shows how gradually expanding the network helps obtaining better performance. Increasing the depth up to a certain point improves accuracy (up to 10 layers) and then after that it starts to degrade the performance. It is interesting to note that a 10-layer network is outperforming the same network with 13 layers. While in the next experiment(s), a deeper network is outperforming a shallower one with much fewer parameters. As already explained in the principles, namely in Subsections III-A and III-F, one of the root causes that affects deeper architectures is an ill-distribution of processing capacity and more specifically the PLD/PLS issues, which can be seen in <ref type="table" target="#tab_0">Table II</ref> and III respectively. These issues are indications of how a specific architecture manifests its needs, including more processing or representational capacity. It may be argued that the shallower architectures can still accommodate more capacity until they meet their true saturation point and, therefore, the difference in accuracy may be attributed to illdistribution, rather than saturation (i.e., PLD/PLS). PLD/PLS are themselves manifestation of issues in proper distribution of processing units. Moreover, saturation is a process, which starts at some point and, as continues, it becomes more prominent. Our point is to give such impression and thus do not get into the specifics of finding the upper limit of an architecture saturation point. The networks used for II experiments are variants of SimpNet with different depths, denoted by Arch1 in the table. More information concerning architectures and their overall topology is given in the supplementary material.</p><p>In addition to the first test, <ref type="table" target="#tab_0">Table III</ref> shows how a deeper network with fewer number of parameters works better than its shallower but wider counterparts. A deeper architecture can develop more interesting functions and thus the composition of many simpler functions can yield better inference, when properly capaciated. This is why despite the difference in the number of parameters, the deeper one is performing better. This experiment shows the importance of minimum allocation in the gradual expansion scheme. The networks used in III are variants of Arch1. <ref type="table" target="#tab_0">Table IV</ref> demonstrates the results concerning balanced distribution of processing units throughout the architecture and how it excels against the ill-distributed counterpart. Networks used for IV use variants of <ref type="bibr" target="#b42">[43]</ref>, which  we call Arch2 from which The first 10 layers are used for the 10-layer architecture and the 13-layer architecture is slimmed to have 128K parameters for the second test. Maximum Information Utilization: The Results summarized in the <ref type="table" target="#tab_5">Table V</ref> show the effect of delayed pooling in an architecture, and demonstrate how utilizing more information in the form of bigger feature-maps can help the network to achieve higher accuracy. This table shows a variant of SimpNet with 13 layers and only 53K parameters, along with its performance with and without delayed pooling. We call this architecture Arch3. To assess the effect in question, one pooling operation is applied each time at different layers and everything else is kept fixed. L5, L3 and L7 refer to layer 5, 3 and 7, respectively. The results indicate an improvement compared to the initial design.</p><p>Strided convolution vs. Maxpooling: <ref type="table" target="#tab_0">Table VI</ref> demonstrates how a pooling operation regardless of architecture, dataset and number of parameters, outperforms the strided convolution. This proves our initial claim and thus nullifies the theory which implies 1. downsampling is what that matters, 2. strided convolution performs better than pooling. Apart from these, form the design point of view, strided convolution imposes more unwanted and unplanned overhead to the architecture and thus contradicts the principled design and notably gradual expansion and minimum allocation. To be more precise, it introduces adhoc allocation strategy, which in-turn introduces PLD issues into the architecture. This is further explained in supplementary material.</p><p>Correlation Preservation: <ref type="table" target="#tab_0">Table VII</ref> shows the effect of correlation preservation, and how a 3 × 3 architecture is better than its counterpart utilizing a bigger kernel size but with the same number of parameters. <ref type="table" target="#tab_0">Table VIII</ref> illustrates the same concept on different architectures and demonstrates how 1 × 1, 2 × 2 and 3 × 3 compare with each other at different layers. As it can be seen, applying 1 × 1 on earlier layers has a bad effect on the performance, while in the middle layers it is not as bad. It is also clear that 2 × 2 performs better than 1 × 1 in all cases, while 3 × 3 filters outperform both of them.</p><p>Here we ran several tests using different kernel sizes, with two different variants, one having 300K and the other  1.6M parameters. Using 7 × 7 in an architecture results in more parameters, therefore in the very same architecture with 3 × 3 kernels, 7 × 7 results in 1.6M parameters. Also, to decrease the number of parameters, layers should contain fewer neurons. Therefore, we created two variants of 300K parameter architecture for the 7 × 7 based models to both demonstrate the effect of 7 × 7 kernel and to retain the computation capacity throughout the network as close as possible between the two counter parts (i.e., 3 × 3 vs. 7 × 7). On the other hand, we increased the base architecture from 300K to 1.6M for testing 3 × 3 kernels as well. <ref type="table" target="#tab_0">Table VII</ref> shows the results for these architectures, in which the 300K.v1 architecture is an all 7 × 7 kernel network with 300K parameters (i.e., the neurons per layer are decreased to match the 300K limit). The 300K.v2 is the same architecture with 7 × 7 kernels only in the first two layers, while the rest use 3 × 3 kernels. This is done, so that the effect of using 7 × 7 filters are demonstrated, while most layers in the network almost has the same capacity as their counterparts in the corresponding 3 × 3-based architecture. Finally, the 7×7 1.6M architecture is the same as the 3×3 300K architecture, in which the kernels are replaced by 7 × 7 ones and thus resulted in increased number of parameters (i.e., 300K became 1.6M). Therefore, we also increased the 3 × 3-based architecture to 1.6, and ran a test for an estimate on how each of these work in different scenarios. We also used a 5 × 5 1.6M parameter network with the same idea and reported the results. The network used for this test is the SimpNet architecture, with only 8 layers, denoted by Arch4. The networks used in <ref type="table" target="#tab_0">Table VIII</ref> are also variants of SimpNet with slight changes to the depth or number of parameters, denoted by Arch5.</p><p>Experiment Isolation: <ref type="table" target="#tab_0">Table IX</ref> shows an example of invalid assumption, when trying different hyperparameters. In the following, we test to see whether 5 × 5 filters achieve better accuracy against 3 × 3 filters. Results show a higher accuracy for the network that uses 5 × 5 filters. However, looking at the number of parameters in each architecture, it is clear that the higher accuracy is due to more parameters in the second architecture. The first architecture, while cosmetically the same as the second one, has 300K parameters, whereas the second one has 1.6M. A second example can be seen in <ref type="table" target="#tab_11">Table X</ref>, where  VIII: Different kernel sizes applied on different parts of a network affect the overall performance, i.e., the kernel sizes that preserve the correlation the most yield the best accuracy. Also, the correlation is more important in early layers than it is for the later ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Properties Params Accuracy (%)</head><p>Arch5, <ref type="bibr" target="#b12">13</ref>     again the number of parameters and thus network capacity is neglected. The example test was to determine whether placing 5 × 5 filters at the beginning of the network is better than placing it at the end. Again here the results suggest that the second architecture should be preferred, but at a closer inspection, it is cleared that the first architecture has only 412K parameters, while the second one uses 640K parameters. Thus, the assumption is not valid because of the difference between the two networks capacities. SAF-pooling: In order to assess the effectiveness of our intuition concerning Section III-I, we further ran a few tests using different architectures with and without SAF-pooling operation on CIFAR10 dataset. We used SqueezeNetv1.1 and our slimmed version of the proposed architecture to showcase the improvement. Results can be found in <ref type="table" target="#tab_0">Table XI</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SimpNet Results on Different Datasets</head><p>SimpNet performance is reported on CIFAR-10/100 <ref type="bibr" target="#b61">[62]</ref>, SVHN <ref type="bibr" target="#b62">[63]</ref>, and MNIST <ref type="bibr" target="#b38">[39]</ref> datasets to evaluate and compare our architecture against the top ranked methods and deeper  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error rate Batch-normalized Max-out NIN <ref type="bibr" target="#b67">[68]</ref> 0.24% Max-out network (k=2) <ref type="bibr" target="#b14">[15]</ref> 0.45% Network In Network <ref type="bibr" target="#b40">[41]</ref> 0.45% Deeply Supervised Network <ref type="bibr" target="#b66">[67]</ref> 0.39% RCNN-96 <ref type="bibr" target="#b69">[70]</ref> 0.31% SimpNet 0.25% models that also experimented on these datasets. We only used simple data augmentation of zero padding, and mirroring on CIFAR10/100. Other experiments on MNIST <ref type="bibr" target="#b38">[39]</ref>, SVHN <ref type="bibr" target="#b62">[63]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref> datasets are conducted without dataaugmentation. In our experiments we used one configuration for all datasets and did not fine-tune anything except for CIFAR10. We did this to see how this configuration can perform with no or slightest changes in different scenarios. 1) CIFAR10/100: <ref type="table" target="#tab_0">Table XII</ref> shows the results achieved by different architectures. We tried two different configurations for CIFAR10 experiment, one with no data-augmentation, i.e., no zero-padding and normalization and another one using data-augmentation, we achieved 95.26% accuracy with no zeropadding and normalization and achieved 95.56% with zero-padding. By just naively adding more parameters to our architecture without further fine-tuning or extensive changes to the architecture, we could easily surpass all WRN results ranging from 8.9M (with the same model complexity) to 11M, to 17M and also 36M parameters on CIFAR10/100 and get very close to its state-of-the-art architecture with 36M parameters on CIFAR100. This shows that the architecture, although with fewer number of parameters and layers, is still capable beyond what we tested with a limited budget of 5M parameters and, thus, by just increasing the number of parameters, it can match or even exceed the performance of much more complex architectures.</p><p>2) MNIST: On this dataset, no data-augmentation is used, and yet we achieved the second highest score even without fine-tuning. <ref type="bibr" target="#b17">[18]</ref> achieved state-of-the-art with extreme dataaugmentation and an ensemble of models. We also slimmed our architecture to have only 300K parameters and achieved 99.73%. <ref type="table" target="#tab_0">Table XIII shows the result.</ref> 3) SVHN: Like <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b65">66]</ref>, we only used the training and testing sets for our experiments and did not use any dataaugmentation. Best results are presented in <ref type="table" target="#tab_0">Table XIV</ref>. Our slimmed version with only 300K parameters could achieve an error rate of 1.95%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error rate Network in Network <ref type="bibr" target="#b40">[41]</ref> 2.35 Deeply Supervised Net <ref type="bibr" target="#b66">[67]</ref> 1.92 ResNet <ref type="bibr">[2]</ref> (reported by <ref type="bibr" target="#b65">[66]</ref>  <ref type="table">(2016))</ref> 2.01 ResNet with Stochastic Depth <ref type="bibr" target="#b65">[66]</ref> 1.75 DenseNet <ref type="bibr" target="#b5">[6]</ref> 1.79-1.59 Wide ResNet <ref type="bibr" target="#b8">[9]</ref> 2.08-1.64 SimpNet 1.648</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>Architectures with fewer number of parameters: Some architectures cannot scale well, when their processing capacity is decreased. This shows the design is not robust enough to efficiently use its processing capacity. We tried a slimmed version of our architecture, which has only 300K parameters to see how it performs and whether it is still efficient. <ref type="table" target="#tab_5">Table  XV</ref> shows the results for our architecture with only 300K and 600K parameters, in comparison to other deeper and heavier architectures with 2 to 20 times more parameters. As it can be seen, our slimmed architecture outperforms ResNet and WRN with fewer and also the same number of parameters on CIFAR10/100. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed a set of architectural design principles, a new pooling layer, detailed insight about different parts in design, and finally introduced a new lightweight architecture, SimpNet. SimpNet outperforms deeper and more complex architectures in spite of having considerably fewer number of parameters and operations, although the intention was not to set a new state-of-the-art, rather, showcasing the effectiveness of the introduced principles. We showed that a good design should be able to efficiently use its processing capacity and that our slimmed version of the architecture with much fewer number of parameters outperforms deeper and heavier architectures as well. Intentionally, limiting ourselves to a few layers and basic elements for designing an architecture allowed us to overlook the unnecessary details and concentrate on the critical aspects of the architecture, keeping the computation in check and achieving high efficiency, along with better insights about the design process and submodules that affect the performance the most. As an important direction for the future works, there is a calling need to study the vast design space of deep architectures in an effort to find better guidelines for designing more efficient networks. S1 Table VI (in the main paper) demonstrates how a pooling operation, regardless of architecture, dataset, and number of parameters, outperforms the strided convolution. This goes along our initial claim and also contradicts with the assumption which implies <ref type="bibr">(1)</ref> Downsampling is what that matters; <ref type="bibr">(2)</ref> Strided convolution performs better than pooling. Furthermore, from the design point of view, strided convolution imposes unwanted and unplanned overhead to the architecture and therefore contradicts the principled design and notably gradual expansion and minimum allocation. It introduces ad hoc allocation which in-turn causes PLD issues in the architecture. Suppose an optimal depth is X with a given budget Y , using gradual expansion, one reaches to the depth X using pooling operation, now replacing pooling with strided convolution, introduces K layers to the architecture depth, (K is the number of pooling layers). In the limited budget, the network will face PLD, since the balanced distribution is not optimal for the new depth. Furthermore, in order to cope with the PLD issue, the capacity needs to be increased by a large margin to accommodate K new layers. The resulting architecture, then, ultimately under-performs against its counterpart using pooling operations, since strided convolution does not approximate a pooling layer and can only simulate the downsampling part of a pooling layer. This can be clearly seen from the obtained results. For the first part of the argument, it can be seen that the 13-layer architecture, which uses pooling with the same number of parameters , outperforms the strided version with 15 layers by a relatively large margin. The 15-layer version also outperforms the strided counterpart, although both show signs of PLD, the pooling based architecture performs much better. For the second part of the argument, a ResNet architecture with 32 layers is tested. ResNet uses strided convolutions and when it is altered to use pooling instead it performs much better. With the same depth, number of parameters and distribution scheme, the pooling-based architecture outperforms the strided convolution version. Therefore it is recommended not to use strided convolution instead of pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Simple Adaptive Feature Composition Pooling</head><p>Max-and average-pooling are the two methods for pooling operation that have been widely in use by nearly all architectures. While different architectures achieved different satisfactory results by using them and also many other variants <ref type="bibr">[1]</ref>, we propose a variation, denoted as SAF-Pooling, which provides the best performance compared to conventional max-, average-pooling operations. SAF-Pooling is essentially the max-pooling operation carried out before a dropout operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SAF-Pooling Intuition</head><p>If the input is considered as an image, pooling operation provides a form of spatial transformation invariance as well as reducing the computational complexity for the upper layers. However, by considering the input to the pooling layer as a series of different features, which is actually the case after appliance of several layers and feature representation possibly getting shrunk, it would be a mechanism which selects prominent features that describe an input identity. In this sense, an ideal pooling method is expected to preserve task-related information, while discarding irrelevant details. It is noteworthy that for the first intuition which considers input as an image, an overlapping pooling would be desired since the nonoverlapping pooling would lose considerably more spatial information in the input than its counterpart. However, when this assumption doesn't hold, nonoverlapped pooling would be a better choice.</p><p>In our experiments, we noted that the nonoverlapping version outperforms the overlapped version by a relatively high margin, two intuitions may explain this improvement: (1) Due to the distributed representation and hierarchical nature of the network, and the fact that the neurons at higher level capture high-level and abstract features unlike the ones at earlier layers which are concerned with more low-level features and also <ref type="bibr">(2)</ref> Deeper in the architecture, the essence of the input image changes and it is no more the same input image/representation rather, it is a feature-map, each pixel of which denotes a feature being present or not. Aafter some stages, the notion of image would not make sense at all, since the spatial dimension of the input may have shrunk so much that the identity inside image can no more be recognized (consider a 8×8 grid from a 32×32 pixel image of CIFAR10 dataset). Taking these two points into consideration, we are truly dealing with feature-maps in a network especially in mid to high layers. Thus, the second intuition seems more plausible, at least when pooling is carried out in the later layers as opposed to earlier ones, where the nonoverlapping may matter more because of more image like properties being still available. Furthermore, the first intuition may only apply to images in applications such as semantic segmentation and object detection based on current practices, and for other applications such as classification or other strategies in semantic   <ref type="figure" target="#fig_0">Fig. S1</ref>: SAF-Pooling: In this scheme, the network is forced to adapt itself to several different feature combinations through deterministically deactivating some feature responses, so that in cases some features are absent, due to occlusion, viewpoint changes, noise, etc., the network can still carry out its task using other available features. The process happens in several stages. After some mid/high level features are available (Stage 1), most prominent features (responses) are pooled (Stage 2). Then, some of them are deterministically turned off. The network now has to adapt to the absence of some prominent features and therefore develop new feature compositions (Stage 3). After several iterations, less relevant (or irrelevant) features will be gradually replaced by more informative ones as weaker features are more frequently given the chance to be further developed. As the cycle continues, more feature compositions are developed which ultimately results in the network maximizing the capacity utilization, by creating more diverse feature detectors, as more neurons will be sparsely activated and independently learn patterns. segmentation-or detection-based applications, it may not hold. Moreover, drastic loss of information that is claimed to be attributed to nonoverlapped pooling is usually caused by either drastic usage of pooling operations or frequent uses of them in the architecture, such as the cases that can be encountered in very deep architectures with several pooling layers, to take the computational burden as low as possible. Therefore, in the absence of such preconceptions/preconditions, the nonoverlapping pooling would not only pose an issue but would also be beneficial, as we experienced in our results as well. Therefore, we followed the second intuition. Another form of pooling is average-pooling, but how does it fare with maxpooling in general? Average-pooling takes all activations in a pooling region into consideration with "equal contributions". This may downplay high activations as many low activations are averagely included. Max-pooling, however, only captures the strongest activations, and disregards all other units in the pooling region <ref type="bibr">[2]</ref>. We noted that the usage of average-pooling specifically in early layers adversely affects the accuracy. However, in upper layers, it seems the difference between the two methods (max and average) is much less severe. Furthermore, different images of the same class often do not share the same features due to the occlusion, viewpoint changes, illumination variation, and so on. Therefore, a feature which plays an important role in identification of a class in an image may not appear in different images of the same class. When max-pooling is used alongside (before) dropout, it simulates these cases by dropping high activations deterministically (since high activations are already pooled by max-pooling), acting as if these features were not present. Placing pooling after dropout, simulates the stochastic pooling operation, which turns off any neurons randomly and does not yield the same improvement as often. In our experiments, we found that it performs inferior to its counterpart. This procedure of dropping the maximum activations simulates the case where some important features are not present due to occlusion or other types of variations. Therefore, turning off high activations helps other less prominent but also as important features to also participate better and network gets a better chance of adapting to new and more diverse feature compositions. Furthermore, in each featuremap, there may be different instances of a feature, and by doing so, those instances can be further investigated. Unlike average pooling that averages all the responses feature-map wise, this way, all those weak responses get the chance to be improved and thus take a more prominent role in shaping the more robust feature detector. Moreover, in case those responses were false positives, they will be treated accordingly and suppressed in the back-propagation step, resulting in more robust feature detector development. Therefore, this results in more robust feature detectors and also better learning the class-specific characteristics <ref type="bibr">[3]</ref>, and feature composition adaptability by the network. Next, we present examples on how this improves the generalization power of the network greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. SAF Pooling vs. Related Methods</head><p>SAF-Pooling operation looks very similar to the work of <ref type="bibr">[3]</ref>, however, there is a difference in the way the two operations are carried out. In <ref type="bibr">[3]</ref>, in the first step, the highest activations are set to zero. This scheme turns off the most informative features within the feature-map. However, in our case, we first pool the highest activations, and then randomly turn off some of them. Furthermore, we use the dropout ratio instead of a different probabilistic selection scheme as in <ref type="bibr">[3]</ref>. This ensures that there are still enough features available not to hinder the speed of convergence. Also, at the same time by turning off some of the other highly activated values, the network is forced to learn more robust features to compensate for their absence, apart from that we avoid introducing new probabilistic scheme and making the procedure any further complex. Also <ref type="bibr">[2]</ref> introduced a similar layer named max-pooling dropout which looks identical to the work presented in <ref type="bibr">[3]</ref> with the difference that they use different probabilistic selection scheme. Wu et al. <ref type="bibr">[2]</ref> also applied dropout before max-pooling and therefore performs some kind of stochastic pooling. On the contrary, the way SAF-Pooling is constructed and behaves is driven by the intuition we explained earlier and in the main paper (i.e., SAF-Pooling), and also the fact that we are intentionally keeping everything simple. <ref type="figure" target="#fig_0">Figure S1</ref> demonstrates our idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualizing SAF-Pooling</head><p>In order to better demonstrate the underlying idea, and how it affects the performance of the network, we have run some experiments. In the following set of figures we show how the the notion of image is drastically changed through a network especially in higher layers and how this seemingly minor observation plays an important role in developing better intuitions about the network and how to improve its performance. This observation, in addition to the fact that poolings should not be applied early in the network, signifies that using overlapped pooling to keep image's spatial information does not make much sense, and if they are treated simply as feature-maps, it can lead to simple yet surprising ways of improvements (i.e., SAF). In <ref type="figure" target="#fig_1">Figure S2</ref>, a head/neck detector is being demonstrated. Followed by a max-pooling operation, the most prominent responses are captured. This mechanism can then be viewed as a way to filter useful features for the rest of the pipeline and ultimately lead to the creation of robust feature compositions. Visualizations are created using DeepVis toolbox <ref type="bibr">[4]</ref>.</p><p>This simple observation has critical consequences in networks performance and the way it is constructed. Knowing what one deals with in an architecture, greatly helps in designing more intuitive and well-performing architectures. It seems logical when we are dealing with features, and we can simply filter prominent ones, we can enhance the feature composition part by explicitly creating new compositions on the fly. This hopefully will result in much better flexibility in the network, which would account for a lot of changes in the input (when some features are not present but others are). such intuitions mentioned so far, lead to the simple adaptive feature composition pooling. In <ref type="figure" target="#fig_3">Figure S3</ref>, one can see examples of how a network trained on CIFAR10, generalizes on hard to recognize samples. The network used Simple Adaptive feature composition pooling.</p><p>The following example also demonstrates how well the network has learned robust features to identify a class. We created a random image, that resembles several classes existing in CIFAR10, to see if the network can, identify several classes that make sense (i.e., all 5 predictions need to be somehow relevant). <ref type="figure" target="#fig_5">Figure S4</ref> shows the example image. Interestingly the network successfully identifies similar classes. Some features are valid for several classes and indeed network could use them to identify the corresponding identity. Not only a robust set of features need to have been learned, but also useful combinations of such features needed to exist. This helps the network to enjoy better generalization and avoid relying heavily on one feature as much as possible.</p><p>It should be noted that the image is not a dog, neither a cat nor a deer, or basically any specific creature/object. It was created in a way that it could express some features, thus resembling more than 1 class. The reason behind having such drawing, was initially to see if the network has a logical/reasonable predictions. For instance, it should predict similar classes rather than any random ones. If it does random predictions, it implies the network has failed to learned proper features to use. Please note that, by predictions, we refer to S4 <ref type="figure" target="#fig_1">Fig. S2</ref>: Throughout the network, the notion of input changes into a feature-map, in which each response denotes the presence of a specific feature. Using Max-pooling, it is possible to filter out responses and capture the most prominent ones and use them to form interesting feature compositions later in the pipeline.  the top 5 predictions. And by a logical/reasonable prediction, we mean, we would like the network to predict similar classes based on what exists in the image. The similarity between different classes are based on abstract attributes in those objects. A dog is different than a cat, so is a horse or a deer, but when we see a not clear image of a dog, we might mistake it for a cat, but we most probably never mistake it for a frog or a tin can! This kind of discrimination between different classes and yet be able to recognize and use inter-class features plays a critical role in our generalization capability. By looking at prominent features we can deduce the the actual object. As an example, by looking at a stickman drawing we can instantly say it is a depiction of a human, while a lot of coarse and fine-grained features related to humans and how they look are not even available, by looking at a very simplified drawing of a car we can identify it correctly. Now when we are faced with such cases, that are not clear from the start, such as when we have not seen an object before hand ( the first encounter), or the shape/features are different what has already been observed, we use a similarity measure which is based completely on abstract attributes found in the objects we have thus far observed. Using this similarity measure we are able to say a picture resembles a cat and a bit less like a dog, but its definitely not a frog, a shark or an airplane. This information is crucial for improved generalization, because it needs to know prominent yet highly abstract features and then build on top of such information, the new similarity measure. Such information are vital for techniques such as Knowledge Distillation as well, which directly use them to train student networks. The more such information is accurate, the better the network performs on unseen data and the more useful it becomes for knowledge transfer. Such information is considered hidden knowledge which a network with holds and use to carry on its task unlike networks parameter values which are visible yet not interpretable normally. Therefore improving upon this subject can greatly benefit network performance, since we are implicitly providing the kind of information it mostly needs better. This is also a good indication of how well a network has learned and performs on unseen data. This is a very interesting observation, and clearly shows the network has learned exceptionally good features and their combinations to predict a class. This is again to point attention to the distributed nature of the neural network and thus treat feature-maps as they are and not simply as images. Having this insight lets utilizing the underlying learned features much efficiently.</p><p>The following images ( <ref type="figure" target="#fig_4">Figures S5 and S6</ref>) also show how the network has developed robust features and how it is robust to drastic changes in the input. Note that this is trained on CIFAR10, without any fine-tuning, or heavy data-augmentation. It was simply trained on CIFAR10 with the same configuration explained before. S5 <ref type="figure">Fig. S5</ref>: The image that we created to see the network responses and see if its random or based on legitimately well learned features, here we can see the network could successfully list all classes that are close to each other can also can exists in the image  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Maximum Performance Utilization</head><p>Table S1 demonstrates the performance and elapsed time when different kernels are used. 3 × 3 has the best performance among the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Correlation Preservation</head><p>SqueezeNet test on CIFAR10 vs. SimpNet (slim version), which use 3 × 3 convs everywhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. EXPERIMENTS OF THE NETWORK ARCHITECTURES</head><p>For different tests we used a variety of network typologies based on main SimpNet (plain) architecture. Here we give general configuration concerning each architecture used in different tests reported in the main paper. The full implementation details along with other information is available at our Github repository. In what follows, Cx{y} stands for a convolution layer with x feature-map, which is repeated y times. When       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Balanced Distribution</head><p>The configurations used in <ref type="table" target="#tab_0">table IV are shown in table S5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Maximum Information Utilization</head><p>The configurations used in <ref type="table" target="#tab_0">table V are shown in table S6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Correlation Preservation</head><p>The configurations used in <ref type="table" target="#tab_0">table VII are shown in table S7</ref>. <ref type="table" target="#tab_0">Table S8 contains the configurations pertaining to the Table  VIII</ref> . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Comparisons of the speedup of different architectures. This plot shows 2.7× faster training when using 3 × 3 kernels using cuDNN v5.x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Layer 1 neuron activations with and without dropout. More neurons are frequently activated when dropout is used, which means more filters are better developed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) With dropout.(b) Without dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>When dropout is used, all neurons are forced to take part and thus less dead neurons occur. It also results in the development of more robust and diverse filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )</head><label>a</label><figDesc>Filters learned in the first layer, when dropout is used.(b) Filters learned in the first layer, when dropout is not used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>When dropout is not used, filters are not as vivid and more dead units are encountered, which is an indication of the presence of more noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>SimpNet base architecture with no dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. S3 :</head><label>S3</label><figDesc>At the absence of some features, network has learned to use other features and successfully and confidently recognize the cats.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. S4 :</head><label>S4</label><figDesc>Made up image to test network generalization and feature robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. S6 :</head><label>S6</label><figDesc>Robust features are learned which is directly manifested in the very well generalization capability of the network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Improved performance by utilizing cuDNN v7.x.</figDesc><table><row><cell>2.5x + CNN 3x + LSTM</cell><cell>k80+cuDNN 6 P100+ cuDNN 6 v100+cuDNN 7 100 200 600 1x 2x 6x</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>63] is a real-world image dataset, obtained from house numbers in Google Street View images. It consists of 630,420 32 × 32 color images, 73,257 of which are used for training, 26,032 images are used for testing and the other 531,131 images are used for extra training. The MNIST dataset [39] also consists of 70,000 28 × 28 grayscale images of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Gradual expansion of the number of layers.</figDesc><table><row><cell>Network Properties Parameters Accuracy (%) Arch1, 8 Layers 300K 90.21 Arch1, 9 Layers 300K 90.55 Arch1, 10 Layers 300K 90.61 Arch1, 13 Layers 300K 89.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Shallow vs. Deep (related to Minimum Allocations), showing how a gradual increase can yield better performance with fewer number of parameters.</figDesc><table><row><cell>Network Properties Parameters Accuracy (%) Arch1, 6 Layers 1.1M 92.18 Arch1, 10 Layers 570K 92.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Balanced distribution scheme is demonstrated by using two variants of SimpNet architecture with 10 and 13 layers, each showing how the difference in allocation results in varying performance and ultimately improvements for the one with balanced distribution of units.</figDesc><table><row><cell>Network Properties Arch2, 10 Layers (wide end) Arch2, 10 Layers (balanced width) 8M Parameters Accuracy (%) 8M 95.19 95.51 Arch2, 13 Layers (wide end) 128K 87.20 Arch2, 13 Layers (balanced width) 128K 89.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>The effect of using pooling at different layers. Applying pooling early in the network adversely affects the performance.</figDesc><table><row><cell>Network Properties Arch3, L5 default Arch3, L3 early pooling Arch3, L7 delayed pooling 53K Parameters Accuracy (%) 53K 79.09 53K 77.34 79.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Effect of using strided convolution ( †) vs. Maxpooling ( * ). Max-pooling outperforms the strided convolution regardless of specific architecture. First three rows are tested on CIFAR100 and two last on CIFAR10.</figDesc><table><row><cell>Network Properties Depth Parameters Accuracy (%) SimpNet *  13 360K 69.28 SimpNet *  15 360K 68.89 SimpNet † 15 360K 68.10 ResNet *  32 460K 93.75 ResNet † 32 460K 93.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Accuracy for different combinations of kernel sizes and number of network parameters, which demonstrates how correlation preservation can directly effect the overall accuracy.</figDesc><table><row><cell>Network Properties Parameters Accuracy (%) Arch4, 3 × 3 300K 90.21 Arch4, 3 × 3 1.6M 92.14 Arch4, 5 × 5 1.6M 90.99 Arch4, 7 × 7 300K.v1 86.09 Arch4, 7 × 7 300K.v2 88.57 Arch4, 7 × 7 1.6M 89.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX :</head><label>IX</label><figDesc>The importance of experiment isolation using the same architecture once using 3×3 and then using 5×5 kernels.</figDesc><table><row><cell>Network Properties Use of 3 × 3 filters Use of 5 × 5 instead of 3 × 3</cell><cell>Accuracy (%) 90.21 90.99</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X :</head><label>X</label><figDesc>Wrong interpretation of results when experiments are not compared in equal conditions (Experimental isolation).</figDesc><table><row><cell>Network Properties Use of 5 × 5 filters at the beginning Use of 5 × 5 filters at the end</cell><cell>Accuracy (%) 89.53 90.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XI :</head><label>XI</label><figDesc>Using SAF-pooling operation improves architecture performance. Tests are run on CIFAR10.</figDesc><table><row><cell>Network Properties Accuracy (%) With-without SAF Pooling SqueezeNetv1.1 88.05(avg)-87.74(avg) SimpNet 94.76-94.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XII :</head><label>XII</label><figDesc>Top CIFAR10-100 results.</figDesc><table><row><cell>Method VGGNet(16L) [65]/Enhanced ResNet-110L / 1202L [2] * SD-110L / 1202L [66] WRN-(16/8)/(28/10) [9] DenseNet [6] Highway Network [37] FitNet [52] FMP* (1 tests) [14] Max-out(k=2) [15] Network in Network [41] DSN [67] Max-out NIN [68] LSUV [69] SimpNet SimpNet</cell><cell>#Params 138m 1.7/10.2m 93.57 / 92.07 74.84/72.18 CIFAR10 CIFAR100 91.4 / 92.45 -1.7/10.2m 94.77 / 95.09 75.42 / -11/36m 95.19 / 95.83 77.11/79.5 27.2m 96.26 80.75 N/A 92.40 67.76 1M 91.61 64.96 12M 95.50 73.61 6M 90.62 65.46 1M 91.19 64.32 1M 92.03 65.43 -93.25 71.14 N/A 94.16 N/A 5.48M 95.49/95.56 78.08 8.9M 95.89 79.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XIII :</head><label>XIII</label><figDesc>MNIST results without data-augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XIV :</head><label>XIV</label><figDesc>Comparisons of performance on SVHN dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XV :</head><label>XV</label><figDesc>Slimmed version results on CIFAR10/100 datasets.</figDesc><table><row><cell>Model Ours Maxout [15] DSN [67] ALLCNN [42] dasNet [71] ResNet [2] (Depth32, tested by us) WRN [9] NIN [41]</cell><cell>Param 300K -600K 93.25 -94.03 68.47 -71.74 CIFAR10 CIFAR100 6M 90.62 65.46 1M 92.03 65.43 1.3M 92.75 66.29 6M 90.78 66.22 475K 93.22 67.37-68.95 600K 93.15 69.11 1M 91.19 -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE S1 :</head><label>S1</label><figDesc>Maximum performance utilization using Caffe, cuDNNv6, networks have 1.6M parameters and the same depth.</figDesc><table><row><cell>Network Properties Accuracy (higher is better) Elapsed time(min)(lower is better)</cell><cell>3×3 92.14 41.32</cell><cell>5×5 90.99 89.22 7×7 45.29 64.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE S2 :</head><label>S2</label><figDesc>Correlation Preservation: SqueezeNet vs. SimpNet on CIFAR10. By optimized we mean, we added Batch-Normalization to all layers and used the same optimization policy we used to train SimpNet.</figDesc><table><row><cell>Network SqueezeNet1.1_default SqueezeNet1.1_optimized SimpNet_Slim SimpNet_Slim</cell><cell>Params Accuracy (%) 768K 88.60 768K 92.20 300K 93.25 600K 94.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE S3 :</head><label>S3</label><figDesc>Gradual Expansion: Network Configurations. , braces are not used. We use this form to demonstrate the overall architectures for the rest of the experiments.</figDesc><table><row><cell>Network 8 Layers 9 Layers 10 Layers 13 Layers</cell><cell>Params 300K 300K 300K 300K</cell><cell>Config C41, C43, C70{4}, C85, C90 C44{2}, C57{4}, C75, C85, C96 C40{3}, C55{4},75, C85, C95 C30, C40{3}, C50{5}, C58{2}, C70, C90</cell></row><row><cell>y = 1</cell><cell></cell><cell></cell></row></table><note>A. Gradual Expansion The configurations used in table II are shown in table S3.B. Minimum Allocation The configurations used in table III are shown in table S4.S6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE S4 :</head><label>S4</label><figDesc>Minimum Allocation: Network Configurations</figDesc><table><row><cell>Network 6 Layers 10 Layers</cell><cell>Params 1.1M 570K</cell><cell>Config C64{2}, C128{2}, C256{2} C32, C48{2}, C96{7}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE S5 :</head><label>S5</label><figDesc>Balanced Distribution: Network Configurations. WD stands for Wide End and BW stands for Balanced Width.</figDesc><table><row><cell>Network 10 Layers.WE 10 Layers.BW 13 Layers.WE 13 Layers.BW</cell><cell>Params 8M 8M 128K 128K</cell><cell>Config C64, C128{2}, C256{3}, C384{2},C512{2} C150, C200{2}, C300{5},C512{2} C19, C16{7}, C32{3}, C252,C300 C19, C25{7}, C56{3},C110{2}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>TABLE S6 :</head><label>S6</label><figDesc>Maximum Information Utilization: Network Configuration.</figDesc><table><row><cell>Network Params Arch 53K</cell><cell>Config C6, C12{3}, C19{5}, C28{2}, C35, C43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>TABLE S7 :</head><label>S7</label><figDesc>Correlation Preservation: Network Configurations</figDesc><table><row><cell>Kernel Size 3×3 3×3 5×5 7×7.v1 7×7.v2 7×7</cell><cell>Params 300K 1.6M 1.6M 300K 300K 1.6M</cell><cell>Config C41, C43, C70{4}, C85, C90 C66, C96, C128{4}, C215, C384 C53, C55, C90{4}, C110, C200 C20{2}, C30{4}, C35, C38 C35, C38, C65{4}, C74, C75 C41, C43, C70{4}, C85, C90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>TABLE S8 :</head><label>S8</label><figDesc>Correlation Preservation 2: Network Configurations.</figDesc><table><row><cell>Kernel Size 1×1.L1 →L3 2×2.L1 →L3 1×1.L4 →L8 2×2.L4 →L8 1×1.End 3×3.End 2×2.End 3×3.End</cell><cell>Params 128K 128K 128K 128K 128K 128K 128K 128K</cell><cell>Config C21, C32{8}, C37, C48,c49, C50 C20, C32{8}, C38, C46{3} C20, C32{2}, C46{6}, C48{3}, C49 C20, C32, C33, C41{2}, C40{4}, C41, C43,44, C45 C19, C25{7}, C64, C53{2}, C108{2} C19, C25{7}, C26, C51{3},52 C19, C257, C91, C922 C20, C257, C643</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>TABLE S9 :</head><label>S9</label><figDesc>Using Different base architectures. All kernels are 3 × 3, if anything else is used it is specified inside parentheses (GP = Global Pooling).</figDesc><table><row><cell>2014.</cell><cell></cell></row><row><cell>Arch1 Conv1 Conv2 Conv3 Conv4 Conv5 Pool Conv6 Conv7 Conv8 Conv9 Conv10 Conv9 Arch2 Conv1 Conv2 Conv3 Conv4 Pool Conv5 Conv6 Conv7 Pool Conv8 Pool Pool Conv11 Conv10 Conv12 Conv11(1x1) Conv13 Conv12(1x1) GP Pool Conv13 Pool</cell><cell>Arch3 Conv1 Conv2 Conv3 Conv4 Conv5 Pool Conv6 Conv7 Conv8 Conv9 Conv10 Conv10 Pool Arch4 Arch5 Conv1 Conv1 Conv2 Conv2 Conv3 Conv3 Conv4 Conv4 Conv5 Conv5 Pool Pool Conv6 Conv6 Conv7 Conv7 Conv8 Conv8 Conv9 Conv9 Pool Pool Conv10 Conv11 Conv11 Conv11 Conv12 Conv12 Conv12 Conv13 Conv13 Conv13 GP GP GP</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Dr. Ali Diba, CTO of Sensifai, for his invaluable help and cooperation, and Dr. Hamed Pirsiavash, Assistant Professor at University of Maryland-Baltimore County (UMBC), for his insightful comments on strengthening this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for visual understanding: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oerlemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge,&quot; in ICCV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06422</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fractional max-pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Geoffrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep image: Scaling up image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1501.02876</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>abs/1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional neural networks at constrained time cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5353" to="5360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep convolutional neural network design patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00847</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural network model for a mechanism of pattern recognition unaffected by shift in position-neocognitron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ELECTRON. &amp; COMMUN. JAPAN</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="11" to="18" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Polynomial theory of complex systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Ivakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="364" to="378" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3207" to="3220" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cirecsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-column deep neural network for traffic sign classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cirecsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="333" to="338" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>abs/1412.6806</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Lets keep it simple: using simple architectures to outperform deeper architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Hasanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rouhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06037</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Understanding deep architectures using a recursive convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.1847</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Blending lstms into cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06433</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Conversational speech transcription using contextdependent deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ISCA</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<title level="m">Do deep convolutional nets really need to be deep</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>or even convolutional)?,&quot; in ICLR</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="818" to="833" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Densenet: Implementing efficient convnet descriptor pyramids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1869</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Quantifying translation-invariance in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kauderer-Abrams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="92" to="101" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learned-norm pooling for deep feedforward and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids: Receptive field learning for pooled image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3370" to="3377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Analysis on the dropout effect in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="189" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">92.45% on cifar-10 in torch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09382</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deeply supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Batch-normalized maxout network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><forename type="middle">C</forename><surname>Jia-Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02583v1</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Dmytro Mishkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="3367" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep networks with internal selective attention through feedback connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<biblScope unit="page" from="3545" to="3553" />
		</imprint>
	</monogr>
	<note>REFERENCES</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Max-pooling dropout for regularization of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICNIP</title>
		<imprint>
			<biblScope unit="page" from="46" to="54" />
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Analysis on the dropout effect in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="189" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop, International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
