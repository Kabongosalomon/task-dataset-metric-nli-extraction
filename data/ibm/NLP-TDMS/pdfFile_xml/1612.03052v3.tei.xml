<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ActionFlowNet: Learning Motion Representation for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Yue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
							<email>jonghyunc@allenai.orgjanneumann@cable.comcast.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Comcast Labs</orgName>
								<address>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ActionFlowNet: Learning Motion Representation for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a data-efficient representation learning approach to learn video representation with small amount of labeled data. We propose a multitask learning model Ac-tionFlowNet to train a single stream network directly from raw pixels to jointly estimate optical flow while recognizing actions with convolutional neural networks, capturing both appearance and motion in a single model. Our model effectively learns video representation from motion information on unlabeled videos. Our model significantly improves action recognition accuracy by a large margin (23.6%) compared to state-of-the-art CNN-based unsupervised representation learning methods trained without external large scale data and additional optical flow input. Without pretraining on large external labeled datasets, our model, by well exploiting the motion information, achieves competitive recognition accuracy to the models trained with large labeled datasets such as ImageNet and Sport-1M.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks have demonstrated great success to multiple visual recognition tasks. With the help of large amount of annotated data like ImageNet, the network learns multiple layers of complex visual features directly from raw pixels in an end-to-end manner without relying on hand-crafted features. Unlike image labeling, manual video annotation often involves frame-by-frame inspection and temporal trimming of videos that are expensive and time consuming. This prohibits the technique to be applied to other problem domains like medical imaging where data collection is difficult.</p><p>We focus on effectively learning video motion representation for action recognition without large amount of external annotated video data. Following previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6]</ref> that leverages spatio-temporal structure in videos for unsupervised or self-supervised representation learning, we are interested in learning video representation from motion information encoded in videos in addition to semantic la-bels. Learning motion representation on videos from raw pixels is challenging. With large scale datasets such as Sports-1M <ref type="bibr" target="#b9">[10]</ref> and Kinetics <ref type="bibr" target="#b10">[11]</ref>, one could train a high capacity classifier to learn complex motion signatures for action recognition by extending image based CNN architectures with 3D convolutions for video action recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2]</ref>. However, while classification loss is an excellent generic appearance learner for image classification, it is not necessarily the most effective supervision for learning motion features for action recognition. As shown in <ref type="bibr" target="#b1">[2]</ref>, even with large amount of labeled video data, the model still benefits from additional optical flow input stream. This suggests that the model is ineffective in learning motion representation for action recognition from video frames, and thus alternative approach should be explored for learning video representation.</p><p>Two-stream convolutional neural networks, which separately learn appearance and motion by two convolutional networks on static images and optical flow respectively, show impressive results on action recognition <ref type="bibr" target="#b21">[22]</ref>. The separation, however, fails to learn the interaction between the motion and the appearance of objects, and introduces additional complexity of computing the flow to the classification pipeline. In addition, human visual system does not take optical flow as front end input signals but infer the motion from raw intensities internally. Therefore, we focus to learn both motion features and appearance directly from raw pixels without hand-crafted flow input.</p><p>Encouraged by the success on estimating optical flow with convolutional neural networks <ref type="bibr" target="#b6">[7]</ref>, we train a single stream feed-forward convolutional neural network -ActionFlowNet -for jointly recognizing actions and estimating optical flow. Specifically, we formulate the learning problem as multitask learning, which enables the network to learn both appearance and motion in a single network from raw pixels. The proposed architecture is illustrated in <ref type="figure">Figure</ref> 1. With the auxiliary task of optical flow learning, the network effectively learns useful representations from motion modeling without a large amount of human annotation. Based on the already learned motion modeling, the model then only requires action annotations as supervision to learn action class specific details, which results in requiring less annotation to perform well for action recognition.</p><p>Our experiments and analyses show that our model successfully learns motion features for action recognition and provide insights on how the learned optical flow quality affects action classification. We demonstrate the effectiveness of our learned motion representation on two standard action recognition benchmarks -UCF101 and HMDB51. Without providing external training data or fine-tuning from already well-trained models with millions of samples, we show that jointly learning action and optical flow significantly boosts action recognition accuracy compared to state-ofthe-art representation learning methods trained without external labeled data. Remarkably, our model outperforms the models trained with large datasets Sports-1M pretrained C3D by 1.6% on UCF101 dataset, showing the importance of feature learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Over the past few years, action recognition accuracy has been greatly improved by learned features and various learning models utilizing deep networks. Two-stream network architecture was proposed to recognize action using both appearance and motions separately <ref type="bibr" target="#b21">[22]</ref>. A number of follow up methods have been proposed based on twostream networks that further improved action recognition accuracies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>. Our work is motivated by their success in incorporating optical flow for action recognition, but we focus on learning from raw pixels instead of relying on hand-crafted representations.</p><p>Optical flow encodes motion between frames and is highly related to action recognition. Our model is motivated by the success of FlowNet <ref type="bibr" target="#b6">[7]</ref> and 3D convolutions for optical flow estimation in videos <ref type="bibr" target="#b26">[27]</ref>, but emphasizes on improving action recognition.</p><p>Pre-training the network with a large dataset helps to learn appearance signatures for action recognition. Karpathy et al. proposed a "Slow Fusion" network for large scale video classification <ref type="bibr" target="#b9">[10]</ref>. Tran et al. trained a 3D convo-lutional neural network (C3D) with a large amount of data and showed the learned features are generic for different tasks <ref type="bibr" target="#b25">[26]</ref>. Recently, Carreira and Zisserman trained I3D models <ref type="bibr" target="#b1">[2]</ref> on the Kinetics dataset <ref type="bibr" target="#b10">[11]</ref> and achieved strong action recognition performance. In contrast, since training networks on such large scale datasets is extremely computationally expensive, we focus on learning from small amounts of labeled data. With only small amount of labeled data, we show that our model performs competitive to models trained with large datasets.</p><p>Leveraging videos as a source for unsupervised learning has been suggested to learn video representations without large labeled data. Different surrogate tasks have been proposed to learn visual representations from videos without any labels. Wang et al. trained a network to learn visual similarity for patches obtained from visual tracking in videos <ref type="bibr" target="#b31">[32]</ref>. Misra et al. trained a network to differentiate the temporal order of different frames from a video <ref type="bibr" target="#b16">[17]</ref>. Jacob et al. learned apperance features by predicting the future trajectories in videos <ref type="bibr" target="#b28">[29]</ref>. Fernando et al. proposed Odd-One-Out networks (O3N) to identify video sequences that are out of order for self-supervised learning <ref type="bibr" target="#b5">[6]</ref>. Our work, similarly, uses video as an additional source for learning visual representation. However, in contrast to previous work which focused on learning visual representations for a single image, we learn motion representations for videos which models more than a single frame. Vondrick et al. used a Generatie Adversarial Network to learn a generative model for video <ref type="bibr" target="#b27">[28]</ref>. We focus on learning motion representations but not video generation.</p><p>Independent to our work, Diba et al. trained a two stream network with flow estimation <ref type="bibr" target="#b2">[3]</ref>. They based their network on C3D with a two-stream architecture. Our work employs a single stream network to learn both appearance and motion. While we both estimate motion and recognize actions in the same model, we focus on learning motion representations without pretraining on large labeled datasets and provide more analysis to learn flow representations for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We propose a single end-to-end model to learn both motions and action classes simultaneously. Our primary goal is to improve action classification accuracy with the help of motion information; we use optical flow as a motion signature. Unlike previous methods that utilize externally computed optical flow as the input to their models, we only use the video frames for input and simultaneously learn the flow and class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-frame Optical Flow with 3D-ResNet</head><p>Fischer et al. proposed FlowNet <ref type="bibr" target="#b6">[7]</ref> that is based on convolutional neural networks to estimate high quality optical flow. Tran et al. proposed to use 3D convolution and deconvolution layers to learn multi-frame optical flow from videos <ref type="bibr" target="#b26">[27]</ref>. In addition, He et al. introduced residual networks (ResNet) to train a deeper convolutional neural network model by adding shortcut connections <ref type="bibr" target="#b7">[8]</ref>.</p><p>In addition to the benefit of easy training, ResNet is fully convolutional, so is easily applied to pixel-wise prediction of optical flow, unlike many architectures with fully connected layers including AlexNet <ref type="bibr" target="#b12">[13]</ref> and VGG-16 <ref type="bibr" target="#b22">[23]</ref>. In contrast to other classification architectures like AlexNet and VGG-16, which contains multiple max pooling layers that may harm optical flow estimation, the ResNet architecture only contains one pooling layer right after conv1. We believe the reduced number of pooling layers makes ResNet more suitable for optical flow estimation where spatial details need to be preserved. Specifically, we use an 18 layers ResNet, which is computationally efficient with good classification performance <ref type="bibr" target="#b7">[8]</ref>.</p><p>Taking advantage of ResNet for flow estimation, we extend ResNet-18 to 3D-ResNet-18 for multi-frame optical flow estimation by replacing all k × k 2D convolutional kernels with extra temporal dimension k × k × 3, inspired by <ref type="bibr" target="#b26">[27]</ref>. The deconvolution layers in the decoder are extended similarly. Skip connections from encoder to decoder are retained as in <ref type="bibr" target="#b6">[7]</ref> to obtain higher resolution information in the decoder. Unlike <ref type="bibr" target="#b6">[7]</ref>, we only use the loss on the highest resolution to avoid downsampling in the temporal dimension. We do not apply temporal max pooling suggested in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, but use only strided convolutions to preserve temporal details. After the third residual block, the temporal resolution is reduced by half when the spatial resolution is reduced.</p><p>Future Prediction. In addition to computing the optical flow between the T input frames, we train the model to predict the optical flow on the last frame, which is the optical flow between the T th and (T + 1) st frames. There are two benefits of training the model to predict the optical flow of the last frame: 1) It is practically easier to implement a model with the same input and output sizes, since the output sizes of deconvolution layers are usually multiples of the inputs; and 2) Semantic reasoning is required for the model to extrapolate the future optical flow given the previous frames. This possibly trains the model to learn better motion features for action recognition, as also suggested by previous work <ref type="bibr" target="#b28">[29]</ref>, which learned appearance feature by predicting the future.</p><p>Following <ref type="bibr" target="#b6">[7]</ref>, the network is optimized over the endpoint error (EPE), which is the sum of L 2 distance between the ground truth optical flow and the obtained flow over all pixels. The total loss for the multiple frame optical flow model is the EPE of T output optical flow frames:</p><formula xml:id="formula_0">T t=1 p o j,t,p − o j,t,p 2 ,<label>(1)</label></formula><p>where o j,t,p is 2-dimensional optical flow vector of the t th and the (t + 1) st frame in the j th video at pixel p.</p><p>Note that the T th optical flow frame o j,t is the future optical flow for the T th and (T + 1) st input frames, where the (T + 1) st frame is not given to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ActionFlowNet</head><p>Knowledge Transfer by Finetuning. Finetuning a pretrained network is a common practice to transfer knowledge from different datasets and tasks. Unlike previous work, where knowledge transfer has been accomplished between very similar tasks (image classification and detection or semantic segmentation), knowledge transfer in our model is challenging since the goals of pixel-wise optical flow and action classification are not obviously compatible. We transfer the learned motion by initializing the classification network using a network trained for optical flow estimation. Since the network was trained to predict optical flow, it should encode motion information in intermediate levels which support action classification. However, finetuning a pretrained network is known to have the problem of catastrophic forgetting. Specifically, when training the network for action recognition, the originally initialized flow information could be destroyed when the network adapts the appearance information. We prevent catastrophic forgetting by using the multitask learning framework. ActionFlowNet. To force the model to learn motion features while training for action recognition, we propose a multitask model ActionFlowNet, which simultaneously learns to estimate optical flow, together with predicting the future optical flow of the last frame, and action classification to avoid catastrophic forgetting. With optical flow as supervision, the model can effectively learn motion features while not relying on explicit optical flow computation.</p><p>In our implementation, we take 16 consecutive frames as input to our model. In the last layer of the encoder, global average pooling across the spatial-temporal feature map, with size 512 × 2 × 7 × 7, is employed to obtain a single 512 dimensional feature vector, followed by a linear softmax classifier for action recognition. The architecture is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The multitask loss is given as follows:</p><formula xml:id="formula_1">MT-Loss j = −1(y j = y j ) log p( y j ) Classification Loss + λ T t=1 p o j,t,p − o j,t,p 2 Flow Loss ,<label>(2)</label></formula><p>where 1(·) is a indicator function, y j and y j are the groundtruth and predicted action labels respectively of the j th video. λ is a hyper-parameter balancing the classification loss and the flow loss, where optical flow estimation can be seen as a regularizer for the model to learn motion feature for classification.</p><p>Although previous work on multitask learning <ref type="bibr" target="#b15">[16]</ref> suggests that sharing parameters of two different tasks may hurt performance, this architecture performs well since optical flow is known empirically to improve video action recognition significantly. In addition, our architecture contains multiple skip connections from lower convolutional layers to decoder. This allows higher layers in the encoder to focus on learning more abstract and high level features, without constraining them to remembering all spatial details for predicting optical flow, which is beneficial for action recognition. This idea is central to Ladder Networks <ref type="bibr" target="#b19">[20]</ref> which introduced lateral connections to learn denoising functions and significantly improved classification performance.</p><p>It is worth noting that this is a very general architecture and requires minimal architectural engineering. Thus, it can be trivially extended to learn more tasks jointly to adapt knowledge from different domains. ActionFlowNet Inference. During inference for action classification, optical flow estimation is not required since the motion information is already learned in the encoder. Therefore, the decoder can be removed and only the forward pass of the encoder and the classifier are computed. If the same backbone architecture is used, our model runs at the same speed as a single-stream RGB network without extra computational overhead. Since the optical flow estimation and flow-stream CNN are not needed, it is more efficient than two-stream counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Two-Frame Based Models</head><p>In this section, we propose various models that take two consecutive input frames. Experimenting with two-frame models has three benefits. First, when there are multiple frames in the input, it is difficult to determine whether the performance improvement comes from motion modeling or aggregating long term appearance information. Thus for better analysis, it is desirable to use the two frame input. Second, training two-frame models is computationally much more efficient than multi-frame models which take N video frames and output N − 1 optical flow images. Third, we can measure the effectiveness of external large scale optical flow datasets, such as the FlyingChairs dataset <ref type="bibr" target="#b6">[7]</ref>, which provide ground-truth flow on only two consecutive frames, for action recognition. Learning Optical Flow with ResNet. Similarly, we use ResNet-18 as our backbone architecture and learn optical flow. Like FlowNet-S <ref type="bibr" target="#b6">[7]</ref>, we concatenate two consecutive frames to produce a 6(ch) × 224(w) × 224(h) input for our two frames model. At the decoder, there are four outputs with different resolutions. The total optical flow loss is the weighted sum of end-point error at multiple resolutions per the following equation:</p><formula xml:id="formula_2">4 r=1 α r p o (r) j,t,p − o (r) j,t,p 2 ,<label>(3)</label></formula><p>where o (r) j,t,p is the optical flow vector of the r th layer output and α r is the weighting coefficient of the r th optical flow output. We refer to this pre-trained optical flow estimation network as FlowNet.</p><p>We first propose an architecture to classify actions on top of the optical flow estimation network, which we call the Stacked Model. Then, we present the two-frame version of ActionFlowNet to classify the actions and estimate the optical flow, which we call the ActionFlowNet-2F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Stacked Model</head><p>A straightforward way to use the trained parameters from FlowNet is to take the output of FlowNet and learn a CNN on top of the output, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. This is reminiscence of the temporal stream in <ref type="bibr" target="#b21">[22]</ref> which learns a CNN on precomputed optical flow. If the learned optical flow has high quality, it should give similar performance to learning a network on optical flow. Since the output of FlowNet has 4 times lower resolution than the original image, we remove the first two layers of the CNN (conv1 and pool1) and stack the network on top of it. We also tried to upsample the flow to the original resolution and use the original architecture including conv1 and pool1, but this produces slightly worse results and is computationally more expensive.</p><p>The stacked model introduces about 2x number of parameters compared to the original ResNet, and is also 2x more expensive for inference. It learns motion features by explicitly including optical flow as an intermediate representation, but cannot model appearance and motion simultaneously, similar to learning a CNN on precomputed optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">ActionFlowNet-2F</head><p>The multitask ActionFlowNet-2F architecture, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, is based on the two-frame FlowNet with additional classifier. Similar to ActionFlowNet, classification is performed by average pooling the last convolutional layer in the encoder followed by a linear classsifier.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use two publicly available datasets, UCF101 and HMDB51, to evaluate action classification accuracy. The UCF101 dataset contains 13,320 videos with 101 action classes <ref type="bibr" target="#b23">[24]</ref>. The HMDB51 contains 6,766 videos with 51 action categories <ref type="bibr" target="#b13">[14]</ref>. As the number of training videos in HMDB51 is small, we initialized our models trained on UCF101 and fine-tuned for HMDB51 similar to <ref type="bibr" target="#b21">[22]</ref> The UCF101 and HMDB51 do not have groundtruth optical flow annotation. Similar to <ref type="bibr" target="#b26">[27]</ref>, we use EpicFlow <ref type="bibr" target="#b20">[21]</ref> as a psuedo-groundtruth optical flow to train the motion part of the network.</p><p>To experiment models with better learned the motion signature, we also use FlyingChairs dataset <ref type="bibr" target="#b6">[7]</ref> as it has groundtruth optical flow since it is a synthetic dataset. The FlyingChairs dataset contains 22,872 image pairs and ground truth flow from synthetically generated chairs on real images. We use the Sintel dataset <ref type="bibr" target="#b0">[1]</ref>, which provides dense groundtruth optical flow, to validate the quality of optical flow models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>Overfitting Prevention. We use different data augmentations on different datasets and tasks. On the FlyingChairs dataset for optical flow estimation, we augment the data using multi-scale cropping, horizontal flipping, translation and rotation following <ref type="bibr" target="#b6">[7]</ref>. On the UCF101 dataset for optical flow estimation, we use multi-scale cropping and horizontal flipping, but do not use translation and rotation in order to maintain the original optical flow distribution in the data. On UCF101 dataset for action recognition, we use color jittering <ref type="bibr" target="#b24">[25]</ref>, multi-scale cropping and horizontal flipping. Dropout is applied to the output of the average pooling layer before the linear classifier with probability 0.5. Optimization and Evaluation. The models are trained using Adam <ref type="bibr" target="#b11">[12]</ref> for 40,000 iterations with batch size 128 and learning rate 1 × 10 −4 . For evaluation, we sample 25 random video segments from a video and run a forward pass to the network on the 10-crops (4 corners + center with their horizontal reflections) and average the prediction scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Improving Action Recognition</head><p>We first evaluate the action recognition accuracy by the various proposed two-frame models described in Section 3.3, and then the multi-frame models in Section 3.2, on both UCF101 and HMDB51 datasets. All models take RGB inputs only without external optical flow inputs. The recognition accuracies are summarized in <ref type="table">Table 1</ref>.  <ref type="bibr" target="#b25">[26]</ref> 82.3 53.5 Kinetics pretrained I3D <ref type="bibr" target="#b1">[2]</ref> 95.6 74.8 <ref type="table">Table 1</ref>: Action recognition accuracies of our models on UCF101 and HMDB51 datasets (split 1). FlCh denotes FlyingChairs dataset. "ActionFlowNet-2F (UCF101)" denotes its FlowNet part is pretrained on UCF101, and "ActionFlowNet-2F (FlCh+UCF101)" denotes its FlowNet part is pretrained on FlyingChairs dataset. All Action-FlowNets are then learned on UCF101 dataset for action and flow. For reference, we additionally show the results trained with large scale datasets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref>, but it is not directly comparable since our models are trained with significantly less annotation.</p><p>Two-frame Models. 'Scratch' is a ResNet-18 model that is trained from scratch (random initialization) using UCF101 without any extra supervision, which represents the baseline performance without motion modeling. 'FlowNet fine-tune' is a model that is pretrained from UCF101 for optical flow only, and then fine-tuned with action classification, which captures motion information by initialized FlowNet. 'Stacked' is a stacked classification model on top of optical flow output depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. Its underlying FlowNet is trained with UCF101 and is fixed to predict optical flow, so only the CNN classifier on top is learned. 'ActionFlowNet-2F' is the multitask model depicted in <ref type="figure" target="#fig_2">Figure 3</ref>, which is trained for action recognition and optical flow estimation to learn both motion and appearance. We trained two versions of ActionFlowNet-2F: one with FlowNet pretrained on UCF101 and one on Fly-ingChairs dataset.</p><p>As shown in the table, all proposed models -'FlowNet fine-tune', 'Stacked' and 'ActionFlowNet-2F' significantly outperform 'Scratch' . This implies that our models can take advantage of the learned motion for action recognition, which is difficult to learn implicitly from action labels. Both the Stacked model and two ActionFlowNet-2Fs outperform the finetuning models by a large margin (up to 5.0% in UCF101 and up to 13.5% in HMDB51). As all models are pretrained from the high quality optical flow model, the results show that the knowledge learned from previous task is prone to be forgotten when learning new task without multitask learning. With extra supervision from optical flow estimation, multitask models regularize the action recognition with the effort of learning the motion features.</p><p>While the Stacked model performs similarly to ActionFlowNet-2F when trained only on UCF101, ActionFlowNet-2F is much more compact than the Stacked model, containing only approximately half the number of parameters of the Stacked model. When ActionFlowNet-2F is first pretrained with FlyingChairs, which predicts better quality optical flow in EPE, and finetuned with the UCF101 dataset, it further improves accuracy by 1%. This implies that our multitask model is capable of transferring general motion information from other datasets to improve recognition accuracy further.</p><p>Our ActionFlowNet-2F still performs inferior compared to ResNet pretrained on ImageNet, especially in UCF101 (71.0% vs 80.7%) because of the rich background context appearance in the dataset. When evaluated on HMDB51, where the backgrounds are less discriminative, our ActionFlowNet-2F is only slightly behind the ImageNet pretrained model (42.6% vs 47.1%), indicating that our model learns strong motion features for action recognition.</p><p>Multi-frame Models. We train 16-frame Action-FlowNet on UCF101. The results are shown in the lower part of <ref type="table">Table 1</ref>. By taking more frames per model, our multi-frame models significantly improve two-frame models (83.9% vs 70.0%). This confirms previous work <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref> that taking more input frames in the model is important.</p><p>Remarkably, without pretraining on large amounts of labeled data, our ActionFlowNet outperforms the ImageNet pretrained single frame model and Sports-1M pretrained C3D. Our ActionFlowNet gives 1.6% and 2.9% improvements over C3D on UCF101 and HMDB51 repsectively. The recently published I3D models <ref type="bibr" target="#b1">[2]</ref> achieved strong performance by training on the newly released Kinetics dataset <ref type="bibr" target="#b10">[11]</ref> with large amount of clean and trimmed labeled video data and performing 3D convolutions on 64 input frames instead of 16 frames. Although the I3D model achieved better results compared to previous work, their RGB model could still benefit from optical flow inputs, which indicates that even with large amount of labeled data the I3D model does not learn motion features effectively.</p><p>It should be noted that there is prior work that gives better results with the use of large scale datasets like ImageNet and Kinetics dataset <ref type="bibr" target="#b1">[2]</ref>, or with the help of external optical flow input <ref type="bibr" target="#b21">[22]</ref>. Those results are not directly comparable to us because we are using a significantly smaller amount of labeled data -only UCF101 and HMDB51. Nevertheless, our method shows promising results for learning motion representations from videos. Even with only a small amount of labeled data, our action recognition network outperforms methods trained with a large amount of labeled data with the exception of the recently trained I3D models <ref type="bibr" target="#b1">[2]</ref> which used ImageNet and Kinetics dataset <ref type="bibr" target="#b10">[11]</ref>. We envision the performance of ActionFlowNet would further improve when trained on larger datasets like Kinetics and taking more input frames in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>UCF101 Accuracy ResNet-18 Scratch 51.3 VGG-M-2048 Scratch <ref type="bibr" target="#b21">[22]</ref> 52.9 Sequential Verification <ref type="bibr" target="#b16">[17]</ref> 50.9 VGAN <ref type="bibr" target="#b27">[28]</ref> 52.1 O3N <ref type="bibr" target="#b5">[6]</ref> 60.3 OPN <ref type="bibr" target="#b14">[15]</ref> 59.8 FlowNet fine-tuned (ours) 66.0 ActionFlowNet-2F (ours) 70.0 ActionFlowNet (ours) 83.9 <ref type="table">Table 2</ref>: Results on UCF101 (split 1) from single stream networks with raw pixel input and without pretraining on large labeled dataset.</p><p>Comparison to state-of-the-arts. We compare our approach to previous work that does not perform pretraining with external large labeled datasets in <ref type="table">Table 2</ref> on UCF101. All models are trained only with UCF101 labels with different unsupervised learning methods. Our models significantly outperform previous work that use videos for unsupervised feature learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref>. Specifically, even with only our two-frame fine-tuned model on UCF101, the model obtain more than 5.9% improvement compared to Sequential Verification, VGAN and O3N, indicating the importance of motion in learning video representations. When combined with multitask learning, the performance improves to 70.0%. Finally, when extending our model to 16 frames by 3D convolutions, the performance of Action-FlowNet further boost to 83.9%, giving a 23.6% improve-  ment over the best previous work. This shows that explicitly learning motion information is important for learning video representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Learning Motions for Discriminative Regions</head><p>We visualize what is learned from the multitask network by using the method from <ref type="bibr" target="#b32">[33]</ref> by using a black square to occlude the frames at different spatial locations and compute the relative difference between classification confidence before and after occlusion. We visualize the two-frame based ActionFlowNet-2F for more straightforward visualization.</p><p>We compare the discriminative regions discovered by our multitask network with ones by the ImageNet pretrained ResNet-18, which only models the discriminative appearances without motion. <ref type="figure" target="#fig_4">Figure 4</ref> shows example results. The visualization reveals that our model focuses more on mo-tion, while the ImageNet pretrained network relies more on background appearance, which may not directly relate to the action itself. However, when appearance is discriminative -for example the writing on the board in the last example -our model can also focus on appearance, which is not possible for models that learn from optical flow only. <ref type="figure" target="#fig_5">Figure 5</ref> shows the optical flow estimation and prediction results from our multi-frame model. Although the model does not have accurate optical flow groundtruth for training, the optical flow quality is fairly good. The model predicts reasonable future optical flow, which shows semantic understanding from the model to the frames in addition to simply performing matching between input frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Optical Flow and Future Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Classes Improved By Learning Motions</head><p>We compare the per class accuracy for ActionFlowNet, Im-ageNet pretrained model and C3D. Not all action classes are motion-centric -objects and their contextual (background) appearances provide more discriminative information for some classes <ref type="bibr" target="#b8">[9]</ref>, which can greatly benefit from large amounts of labeled data. As shown in <ref type="figure">Figure 6</ref>, our model better recognizes action classes with simple and discriminative motion like WallPushups and ApplyEyeMakeup, while C3D and ImageNet models perform better on classes with complex appearance like MoppingFloor and BaseballPitch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Recognition and Optical Flow Quality</head><p>In this section, we study the effects of different optical flow models for action recognition based on the twoframe models. We train our optical flow models on Fly-ingChairs or UCF101 and evaluate their accuracies on the Sintel dataset (similar to <ref type="bibr" target="#b6">[7]</ref> that trains the model on Fly-ingChairs but tests on other datasets).</p><p>We investigate how the quality of the learned optical flow affects action recognition. Since optical flow in the multitask model is collaboratively learned with the recognition task, the quality of optical flow in the multitask model does not directly affect recognition accuracy. Thus, we use our Stacked model learned with different datasets, fix the optical flow part and train the classification part in the network shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We compare the end-point-error of different optical flow learners and the corresponding classification accuracy in <ref type="table">Table 3</ref>  <ref type="table">Table 3</ref>: Comparison between End-Point-Error (EPE, lower is better) and the classification accuracy. Interestingly, better optical flow does not always result in better action recognition accuracy. Refer to the text for discussion.</p><p>Action Recognition with Learned Flow. Surprisingly, even with lower end-point-error the Stacked model pretrained on FlyingChairs performs significantly worse than the one pretrained on UCF101 dataset (51.7% vs 69.6%), as shown in <ref type="table">Table 3</ref>. Compared to the model directly taking high quality optical flow as input (77.7%), our models are still not as good as training directly on optical flow. We believe this is because the quality of learned optical flow is not high enough.</p><p>To understand how the learned optical flow affects action recognition, we qualitatively observe the optical flow performance in <ref type="figure" target="#fig_6">Figure 7</ref>. Even though the end-point error on Sintel of the FlowNet pretrained on FlyingChairs is low, the estimated optical flow has lots of artifacts in the background and the recognition accuracy on top of that is correspondingly low. We believe the reason is that the FlyingChairs dataset mostly consists of large displacement flow, and therefore the model performs badly on estimating small optical flow, which contributes less in the EPE metric when averaged over the whole dataset. This is in contrast to traditional optimization based optical flow algorithms that can predict small displacements well but have difficulties for large displacements. In addition, traditional optical flow algorithms such as TV-L1 and EpicFlow explicitly enforce smoothness and constancy. They are able to preserve object shape information when the flow displacements are small, which is important for action recognition. While our models perform comparably to traditional optical flow algorithms in terms of endpoint error, our model is not optimized for preserving flow smoothness. This shows that end-point-error of optical flow in public dataset may not be a good indicator of action classification performance, since shape preservation is not accounted for in the metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a multitask framework for learning action with motion flow, named ActionFlowNet. By using optical flow as supervision for classification, our model captures motion information while not requiring explicit optical flow computation as input. Our model significantly outperforms previous feature learning methods trained without external large scale data and additional optical flow input.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>ActionFlowNet for jointly estaimting optical flow and recognizing actions. Orange and blue blocks represent ResNet modules, where blue blocks represents strided convolution. Channel dimension is not shown in thefigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Network structure of the 'Stacked Model'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Network structure of the ActionFlowNet-2F Just as with the stacked model, the loss function is defined for each frame. For the t th frame in the j th video the loss is defined as a weighted sum of classification loss and optical flow loss:MT-Loss j,t = −1(y j = y j ) log p( y j ) j,t,p − o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of important regions for action recognition. Our ActionFlowNet-2F discovers the regions where the motions are happening to be important while 'Appearance Only' captures discriminative regions based on the appearance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Optical flow and future prediction outputs from our multi-frame model. The 1st and 3rd row shows an example of input videos, and the 2nd and 4th row shows the corresponding optical flow outputs. The last optical flow output frames (in red border) are extrapolated rather than computed within input frames. Only last 8 frames are shown per sample due to space limit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>FlowNet on FlyingChairs (d) FlowNet on UCF101 Qualitative comparison of flow outputs. It shows an example of small motion, where the maximum magnitude of displacement estimated from EpicFlow is only about 1.6px. FlowNet trained on FlyingChairs dataset fails to estimate small motion, since the FlyingChairs dataset consists of large displacement flow.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by funds provided from the Office of Naval Research under grant N000141612713 entitled "Visual Common Sense Reasoning for Multi-agent Activity Prediction and Recognition".</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient twostream motion and appearance 3d cnns for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08851</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FlowNet: Learning Optical Flow with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crossstitch Networks for Multi-task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal difference networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond Short Snippets: Deep Networks for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning with Ladder Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep End2End Voxel2Voxel Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW DeepVision Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Actions˜transforma-tions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
