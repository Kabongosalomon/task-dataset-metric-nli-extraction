<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON MEDICAL IMAGING 1 H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Hao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Qi</forename><surname>Dou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON MEDICAL IMAGING 1 H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-CT</term>
					<term>liver tumor segmentation</term>
					<term>deep learning</term>
					<term>hybrid features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Liver cancer is one of the leading causes of cancer death. To assist doctors in hepatocellular carcinoma diagnosis and treatment planning, an accurate and automatic liver and tumor segmentation method is highly demanded in the clinical practice. Recently, fully convolutional neural networks (FCNs), including 2D and 3D FCNs, serve as the back-bone in many volumetric image segmentation. However, 2D convolutions can not fully leverage the spatial information along the third dimension while 3D convolutions suffer from high computational cost and GPU memory consumption. To address these issues, we propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of a 2D DenseUNet for efficiently extracting intra-slice features and a 3D counterpart for hierarchically aggregating volumetric contexts under the spirit of the auto-context algorithm for liver and tumor segmentation. We formulate the learning process of H-DenseUNet in an end-to-end manner, where the intra-slice representations and inter-slice features can be jointly optimized through a hybrid feature fusion (HFF) layer. We extensively evaluated our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge and 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the segmentation results of tumors and achieved very competitive performance for liver segmentation even with a single model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Liver cancer is one of the most common cancer diseases in the world and causes massive deaths every year <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The accurate measurements from CT, including tumor volume, shape, location and further functional liver volume, can assist doctors in making accurate hepatocellular carcinoma evaluation and treatment planning. Traditionally, the liver and liver lesion are delineated by radiologists on a slice-by-slice basis, which is time-consuming and prone to inter-and intrarater variations. Therefore, automatic liver and liver tumor segmentation methods are highly demanded in the clinical practice.</p><p>Automatic liver segmentation from the contrast-enhanced CT volumes is a very challenging task due to the low intensity contrast between the liver and other neighboring organs (see the first row in <ref type="figure" target="#fig_0">Figure 1</ref>). Moreover, radiologists usually enhance CT scans by an injection protocol for clearly observing This work was supported by the grants from the Research Grants Council of the Hong Kong Special Administrative Region (Project Nos. GRF 14202514 and GRF 14203115).</p><p>X. M. <ref type="bibr">Li</ref>  tumors, which may increase the noise inside the images on the liver region <ref type="bibr" target="#b2">[3]</ref>. Compared with liver segmentation, liver tumor segmentation is considered to be a more challenging task. First, the liver tumor has various size, shape, location and numbers within one patient, which hinders the automatic segmentation, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Second, some lesions do not have clear boundaries, limiting the performance of solely edge based segmentation methods (see the lesions in the third row of <ref type="figure" target="#fig_0">Figure 1</ref>). Third, many CT scans consist of anisotropic dimensions with high variations along the z-axis direction (the voxel spacing ranges from 0.45mm to 6.0mm), which further poses challenges for automatic segmentation methods.</p><p>To tackle these difficulties, many segmentation methods have been proposed, including intensity thresholding, region growing, and deformable models. These methods, however, rely on hand-crafted features, and have limited feature representation capability. Recently, fully convolutional neural networks (FCNs) have achieved great success on a broad array of recognition problems <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. Many researchers advance this stream using deep learning methods in the liver and tumor segmentation problem and the literature can be classified into two categories broadly. (1) 2D FCNs, such arXiv:1709.07330v3 [cs.CV] 3 Jul 2018 as UNet architecture <ref type="bibr" target="#b14">[15]</ref>, the multi-channel FCN <ref type="bibr" target="#b15">[16]</ref>, and the FCN based on VGG-16 <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b1">(2)</ref> 3D FCNs, where 2D convolutions are replaced by 3D convolutions with volumetric data input <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>In the clinical diagnosis, the experienced radiologist usually observes and segments tumors according to many adjacent slices along the z-axis. However, 2D FCN based methods ignore the contexts on the z-axis, which would lead to limited segmentation accuracy. To be specific, single or three adjacent slices cropped from volumetric images are fed into 2D FCNs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and the 3D segmentation volume is generated by simply stacking the 2D segmentation maps. Although adjacent slices are employed, it is still not enough to probe the spatial information along the third dimension, which may degrade the segmentation performance. To solve this problem, some researchers proposed to use tri-planar schemes or RNN to probe the 3D contexts <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. For example, Prasoon et al. <ref type="bibr" target="#b3">[4]</ref> applied three 2D FCNs on orthogonal planes (e.g., the xy, yz, and xz planes) and voxel prediction results are generated by the average of these probabilities. Compared to 2D FCNs, 3D FCNs suffer from high computational cost and GPU memory consumption. The high memory consumption limits the depth of the network as well as the filter's field-ofview, which are the two key factors for performance gains <ref type="bibr" target="#b21">[22]</ref>. The heavy computation of 3D convolutions also impedes the application in training a large-scale dataset. Moreover, many researchers have demonstrated the effectiveness of knowledge transfer (the knowledge learnt from one source domain efficiently transferred to another domain) for boosting the performance <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Unfortunately, only a dearth of 3D pretrained model exists, which restricts the performance and also the adoption of 3D FCNs.</p><p>To address the above problems, we proposed a novel endto-end system, called hybrid densely connected UNet (H-DenseUNet), where intra-slice features and 3D contexts are effectively probed and jointly optimized for accurate liver and lesion segmentation. Our H-DenseUNet has the following two technical achievements:</p><p>Deep and efficient network. First, to fully extract highlevel intra-slice features, we design a very deep and efficient network based on the pre-defined design principles by 2D convolutions, called 2D DenseUNet, where the advantages of both densely connected path <ref type="bibr" target="#b24">[25]</ref> and UNet connections <ref type="bibr" target="#b4">[5]</ref> are fused together. Densely connected path is derived from densely connected network (DenseNet), where the improved information flow and parameters efficiency alleviate the difficulty for training the deep network. Different from DenseNet <ref type="bibr" target="#b24">[25]</ref>, we add the UNet connections, i.e., long-range skip connections, between the encoding part and the decoding part in our architecture; hence, the network can enable low-level spatial feature preservation for better intra-slice context exploration.</p><p>Hybrid feature exploration. Second, to explore the volumetric feature representation, we design an end-to-end training system, called H-DenseUNet, where intra-slice and inter-slice features are effectively extracted and then jointly optimized through the hybrid feature fusion (HFF) layer. Specifically, 3D DenseUNet is integrated with the 2D DenseUNet by the way of auto-context <ref type="bibr" target="#b25">[26]</ref> mechanism, which is a general form of stacked generality <ref type="bibr" target="#b26">[27]</ref>. With the guidance of semantic probabilities from 2D DenseUNet, the optimization burden in the 3D DenseUNet can be well alleviated, which contributes to the training efficiency for 3D contexts extraction. Moreover, with the end-to-end system, the hybrid feature, consisting of volumetric features and the high-level representative intra-slice features, can be automatically fused and jointly optimized together for better liver and tumor recognition. In summary, this work has the following achievements:</p><p>• We design a DenseUNet to effectively probe hierarchical intra-slice features for liver and tumor segmentation, where the densely connected path and UNet connections are carefully integrated based on pre-defined design principles to improve the liver tumor segmentation performance. <ref type="bibr">•</ref> We propose a H-DenseUNet framework to explore hybrid (intra-slice and inter-slice) features for liver and tumor segmentation. The hybrid feature learning architecture well sidesteps the problems that 2D networks neglect the volumetric contexts and 3D networks suffer from heavy computational cost, and can be served as a new paradigm for effectively exploiting 3D contexts. • Our method ranked the 1st on lesion segmentation, achieved very competitive performance on liver segmentation in the 2017 LiTS Leaderboard, and also achieved the state-of-the-art results on the 3DIRCADb Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A. Hand-crafted feature based methods</p><p>In the past decades, a lot of algorithms, including thresholding <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, region growing, deformable model based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> and machine learning based methods <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> have been proposed to segment liver and liver tumor. Thresholdbased methods classified foreground and background according to whether the intensity value is above a threshold. Variations of region growing algorithms were also popular in the liver and lesion segmentation task. For example, Wong et al. <ref type="bibr" target="#b29">[30]</ref> segmented tumors by a 2D region growing method with knowledge-based constraints. Level set methods also attracted attentions from researchers with the advantages of numerical computations involving curves and surfaces <ref type="bibr" target="#b36">[37]</ref>. For example, Jimenez-Carretero et al. <ref type="bibr" target="#b30">[31]</ref> proposed to classify tumors by a multi-resolution 3D level set method coupled with adaptive curvature technique. A large variety of machine learning based methods have also been proposed for liver tumor segmentation. For example, Huang et al. <ref type="bibr" target="#b31">[32]</ref> proposed to employ the random feature subspace ensemble-based extreme learning machine (ELM) for liver lesion segmentation. Vorontsov et al. <ref type="bibr" target="#b32">[33]</ref> proposed to segment tumors by support vector machine (SVM) classifier and then refined the results by the omnidirectional deformable surface model. Similarly, Kuo et al. <ref type="bibr" target="#b34">[35]</ref> proposed to learn SVM classifier with texture feature vector for liver tumor segmentation. Le et al. <ref type="bibr" target="#b33">[34]</ref> employed the fast marching algorithm to generate initial regions and then classified tumors by training a noniterative single hidden layer feedforward network (SLFN). To speed up the segmentation algorithm, Chaieb et al. <ref type="bibr" target="#b37">[38]</ref> adopted a bootstrap sampling approach for efficient liver tumor segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep learning based methods</head><p>Convolutional neural networks (CNNs) have achieved great success in many object recognition problems in computer vision community. Many researchers followed this trend and proposed to utilize various CNNs for learning feature representations in the application of liver and lesion segmentation. For example, Ben-Cohen et al. <ref type="bibr" target="#b16">[17]</ref> proposed to use a FCN for liver segmentation and liver-metastasis detection in CT examinations. Christ et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39]</ref> proposed a cascaded FCN architecture and dense 3D conditional random fields (CRFs) to automatically segment liver and liver lesions. In the meanwhile, Sun et al. <ref type="bibr" target="#b15">[16]</ref> designed a multi-channel FCN to segment liver tumors from CT images, where the probability maps were generated by the feature fusion from different channels.</p><p>Recently, during the 2017 ISBI LiTS challenge, Han <ref type="bibr" target="#b39">[40]</ref>, proposed a 2.5D 24-layer FCN model to segment liver tumors, where the residual block was employed as the repetitive building blocks and the UNet connection was designed across the encoding part and decoding part. 2.5D refers to using 2D convolutional neural network with the input of adjacent slices from the volumetric images. Both Vorontsov et al. <ref type="bibr" target="#b40">[41]</ref> and Chlebus et al. <ref type="bibr" target="#b41">[42]</ref> achieved the second place in the ISBI challenge. Vorontsov et al. <ref type="bibr" target="#b40">[41]</ref> also employed ResNet-like residual blocks and UNet connections with 21 convolutional layers, which is a bit shallower and has fewer parameters compared to that proposed by Han <ref type="bibr" target="#b39">[40]</ref>. Chlebus et al. <ref type="bibr" target="#b41">[42]</ref> designed a 28-layer UNet architecture in two individual models and subsequently filtered the false positives of tumor segmentation results by a random forest classifier. Instead of using 3D FCNs, all of the top results employed 2D FCNs with different network depths, showing the efficacy of 2D FCNs regarding the underlying volumetric segmentation problem. However, all these networks are shallow and ignore the 3D contexts, which limit the high-level feature extraction capability and restrict the recognition performance.</p><p>III. METHOD <ref type="figure" target="#fig_2">Figure 2</ref> shows the pipeline of our proposed method for liver and tumor segmentation. We employed the cascaded learning strategy to reduce the overall computation time, which has also been adopted in many recognition tasks <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref>. First, a simple ResNet architecture <ref type="bibr" target="#b39">[40]</ref> is trained to get a quick but coarse segmentation of liver. With the region of interest (ROI), our proposed H-DenseUNet efficiently probes intraslice and inter-slice features through a 2D DenseUNet f 2d and a 3D counterpart f 3d , followed by jointly optimizing the hybrid features in the hybrid feature fusion (HFF) layer for accurate liver and lesion segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep 2D DenseUNet for Intra-slice Feature Extraction</head><p>The intra-slice feature extraction part follows the structure of DenseNet-161 <ref type="bibr" target="#b24">[25]</ref>, which is composed of repetitive densely connected building blocks with different output dimensions. In each densely connected building block, there are direct connections from any layer to all subsequent layers, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>(c). Each layer produces k feature maps and k is called growth rate. One advantage of the dense connectivity between layers is that it has fewer output dimensions than traditional networks, avoiding learning redundant features. Moreover, the densely connected path ensures the maximum information flow between layers, which improves the gradient flow, and thus alleviates the burden in searching for the optimal solution in a very deep neural network.</p><p>However, the original DenseNet-161 <ref type="bibr" target="#b24">[25]</ref> is designed for the object classification task while our problem belongs to the segmentation topics. Moreover, a deep FCN network for segmentation tasks actually contains several max-pooling and upsampling operations, which may lead to the information loss of low-level (i.e., high resolution) features. Given above two considerations, we develop a 2D DenseUNet, which inherits both advantages of densely connected path and UNet-like connections <ref type="bibr" target="#b4">[5]</ref>. Specifically, the dense connection between layers is employed within each micro-block to ensure the maximum information flow while the UNet long range connection links the encoding part and the decoding part to preserve low-level information.</p><p>Let I ∈ R n×224×224×12×1 denote the input training samples (for 224 × 224 × 12 input volumes) with ground-truth labels Y ∈ R n×224×224×12×1 , where n denotes the batch size of the input training samples and the last dimension denotes the channel. Y i,j,k = c since each pixel (i, j, k) is tagged with class c (background, liver and tumor). Let function F denote the transformation from the volumetric data to three adjacent slices. Specifically, every three adjacent slices along z-axis are stacked together and the number of groups can be transformed to the batch dimension. For example, I 2d = F (I), where I 2d ∈ R 12n×224×224×3 denotes the input samples of 2D DenseUNet. The detailed transformation process is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(d). Because of the transformation, the 2D and 3D DenseUNet can be jointly trained, which will be described in detail in section B. For convenience, F −1 denotes the inverse transformation from three adjacent slices to the volumetric data. The 2D DenseUNet conducts liver and tumor segmentation,</p><formula xml:id="formula_0">X 2d = f 2d (I 2d ; θ 2d ), X 2d ∈ R 12n×224×224×64 , y 2d = f 2dcls (X 2d ; θ 2dcls ),ŷ 2d ∈ R 12n×224×224×3 (1)</formula><p>where X 2d is the feature map from layer "upsampling layer 5" (see <ref type="table" target="#tab_1">Table I</ref>) andŷ 2d is the corresponding pixel-wise probabilities for input I 2d .</p><p>The illustration and detailed structure of 2D DenseUNet are shown in <ref type="figure" target="#fig_2">Figure 2</ref>(c) and <ref type="table" target="#tab_1">Table I</ref>, respectively. The depth of 2D DenseUNet is extended to 167 layers, referred as 2D DenseUNet-167, which consists of 167 convolution layers, pooling layers, dense blocks, transition layers and upsampling layers. The dense block denotes the cascade of several micro-blocks, in which all layers are directly connected, see <ref type="figure" target="#fig_2">Figure 2</ref>(c). To change the size of feature-maps, the transition layer is employed, which consists of a batch normalization layer and a 1 × 1 convolution layer followed by (c) The illustration of the 2D DenseUNet. Dense block 1</p><formula xml:id="formula_1">1 1 1 1 3 3</formula><p>Transition layer 1 <ref type="bibr">3 3</ref> Upsampling layer 3 <ref type="bibr">7 7</ref> Upsampling layer 4 an average pooling layer. A compression factor is included in the transition layer to compress the number of feature-maps, preventing the expanding of feature-maps (set as 0.5 in our experiments). The upsampling layer is implemented by the bilinear interpolation, followed by the summation with lowlevel features (i.e., UNet connections) and a 3×3 convolutional layer. Before each convolution layer, the batch normalization and the Rectified Linear Unit (ReLU) are employed in the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. H-DenseUNet for Hybrid Feature Exploration</head><p>2D DenseUNet with deep convolutions can produce highlevel representative in-plane features but neglect the spatial information along the z dimension while 3D DenseUNet has large GPU computational cost and limited kernel's field-ofview as well as the network depth. To address these issues, we propose H-DenseUNet to jointly fuse and optimize the learned intra-slice and inter-slice features for better liver tumor segmentation.</p><p>To fuse hybrid features from the 2D and 3D network, the feature volume size should be aligned. Therefore, the feature maps and score maps from 2D DenseUNet are transformed to the volumetric shape as follows:</p><formula xml:id="formula_2">X 2d = F −1 (X 2d ), X 2d ∈ R n×224×224×12×64 , y 2d = F −1 (ŷ 2d ),ŷ 2d ∈ R n×224×224×12×3 ,<label>(2)</label></formula><p>Then the 3D DenseUNet distill the visual features with 3D contexts by concatenating the original volumes I with the contextual informationŷ 2d from the 2D network. Specifically, the detectors in the 3D counterpart trained based not only on the features probed from the original images, but also on the probabilities of a large number of context pixels from 2D DenseUNet. With the guidance from the supporting contexts pixels, the burden in searching for the optimal solution in the 3D counterpart has also been well alleviated, which significantly improves the learning efficiency of the 3D network. The learning process of 3D DenseUNet can be described as:</p><formula xml:id="formula_3">X 3d = f 3d (I,ŷ 2d ; θ 3d ), Z = X 3d + X 2d ,<label>(3)</label></formula><p>where X 3d denotes the feature volume from layer "upsampling layer 5" in 3D DenseUNet-65. Z denotes the hybrid feature, which refers to the sum of intra-slice and inter-slice features from 2D and 3D network, respectively. Then the hybrid feature is jointly learned and optimized in the HFF layer,</p><formula xml:id="formula_4">H = f HF F (Z; θ HF F ), y H = f HF F cls (H; θ HF F cls )<label>(4)</label></formula><p>where H denotes the optimized hybrid features andŷ H refers to the pixel-wise predicted probabilities generated from the HFF layer f HF F cls (·). In our experiments, the 3D counterpart of H-DenseUNet cost only 9 hours to converge, which is significantly faster than training the 3D counterpart with original data solely (63 hours). The detailed structure of the 3D counterpart is shown in the Table I, called 3D DenseUNet-65, which consists of 65 convolutional layers and the growth rate is 32. Compared with 2D DenseUNet counterpart, the number of micro-blocks in each dense block is decreased due to the high memory consumption of 3D convolutions and the limited GPU memory. The rest of the network setting is the same with the 2D counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Function, Training and Inference Schemes</head><p>In this section, we present more details regarding the loss function, training and the inference schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Loss Function:</head><p>To train the networks, we employed weighted cross-entropy function as the loss function, which is described as:</p><formula xml:id="formula_5">L(y,ŷ) = − 1 N N i=1 3 c=1 w c i y c i logŷ i c<label>(5)</label></formula><p>whereŷ i c denotes the probability of voxel i belongs to class c (background, liver or lesion), w c i denotes the weight and y c i indicates the ground truth label for voxel i.</p><p>2) Training Scheme: We first train the ResNet in the same way with Han <ref type="bibr" target="#b39">[40]</ref> to get the coarse liver segmentation results. The parameters of the encoder part in 2D DenseUNet f 2d are initialized with DenseNet's weights (object classificationtrained) <ref type="bibr" target="#b24">[25]</ref> while the decoder part are trained with the random initialization. Since the weights are initialized with a random distribution in the decoder part, we first warm up the network without UNet connections. After several iterations, the UNet connections are added to jointly fine tune the model.</p><p>To effectively train the H-DenseUNet, we first optimize f 2d (·) and f 2dcls (·) with cross entropy loss L(y,ŷ 2d ) on our dataset. Secondly, we fix parameters in f 2d (·) and f 2dcls (·), and focus on training f 3d (·), f HF F (·) and f HF F cls (·) with cross entropy loss L(y,ŷ H ), where parameters are all randomly initialized. Finally, The whole network is jointly finetuned with following combined loss:</p><formula xml:id="formula_6">L total = λL(y,ŷ 2d ) + L(y,ŷ H )<label>(6)</label></formula><p>where λ is the balanced weight and set as 0.5 in our experiments empirically.</p><p>3) Inference Scheme: In the test stage, we first get the coarse liver segmentation result. Then H-DenseUNet can generate accurate liver and tumor predicted probabilities within the ROI. The thresholding is applied to get the liver tumor segmentation result. To avoid the holes in the liver, a largest connected component labeling is performed to refine the liver result. After that, the final lesion segmentation result is obtained by removing lesions outside the final liver region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Pre-processing</head><p>We tested our method on the competitive dataset of MICCAI 2017 LiTS Challenge and 3DIRCADb Dataset. The LiTS dataset contains 131 and 70 contrast-enhanced 3D abdominal CT scans for training and testing, respectively. The dataset was acquired by different scanners and protocols from six different clinical sites, with a largely varying in-plane resolution from 0.55 mm to 1.0 mm and slice spacing from 0.45 mm to 6.0 mm. The 3DIRCADb dataset contains 20 venous phase enhanced CT scans, where 15 volumes have hepatic tumors in the liver.</p><p>For image preprocessing, we truncated the image intensity values of all scans to the range of <ref type="bibr">[-200,250</ref>] HU to remove the irrelevant details. For coarse liver segmentation in the first stage, we trained a simple network from resampled images with the same resolution 0.69 × 0.69 × 1.0 mm 3 . In the test stage, we also employ the resampled images for coarse liver segmentation. For lesion segmentation in the second stage, the network is trained on the images with the original resolution. This is because in some training cases liver lesions are notably small, thus we use images with the original resolution to avoid possible artifacts from image resampling. In this test stage, we also employ the images with original resolution for accurate liver and lesion segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>According to the evaluation of 2017 LiTS challenge, we employed Dice per case score and Dice global score to evaluate the liver and tumor segmentation performance respectively. Dice per case score refers to an average Dice score per volume while Dice global score is the Dice score evaluated by combining all datasets into one. Root mean square error (RMSE) is also adopted to measure the tumor burden.</p><p>In the 3DIRCADb dataset, five metrics are used to measure the accuracy of segmentation results, including the volumetric overlap error (VOE), relative volume difference (RVD), average symmetric surface distance (ASD), root mean square symmetric surface distance (RMSD) and DICE. For the first four evaluation metrics, the smaller the value is, the better the segmentation result. The value of DICE refers to the same measurement as Dice per case in the LiTS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>In this section, we present more details regarding the implementation environment and data augmentation strategies. The model was implemented using Keras package <ref type="bibr" target="#b46">[47]</ref>. The initial learning rate was 0.01 and decayed according to the equation lr = lr * (1 − iterations/total_iterations) 0.9 . We used stochastic gradient descent with momentum. For data augmentation, we adopted random mirror and scaling between 0.8 and 1.2 for all training data to alleviate the overfitting problem. The training of 2D DenseUNet model took about 21 hours using two NVIDIA Titan Xp GPUs with 12 GB memory while the end-to-end system fine-tuning cost approximately 9 hours. In other words, the total training time for H-DenseUNet took about 30 hours. In the test phase, the total processing time of one subject depends on the number of slices, ranging from 30 seconds to 200 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Analysis of H-DenseUNet on LiTS dataset</head><p>In this section, we conduct comprehensive experiments to analyze the effectiveness of our proposed H-DenseUNet. <ref type="figure" target="#fig_3">Figure 3</ref> shows the training losses of 2D DenseUNet with and without pre-trained model, 2D DenseNet with pre-trained model, 3D DenseUNet without pre-trained model as well as H-DenseUNet. Note that 3D DenseUNet costs around 60 hours, nearly 3 times than 2D networks. H-DenseUNet costs nearly 30 hours, where 21 hours are spent for 2D DenseUNet training and 9 hours are used to fine-tune the whole architecture in the end-to-end manner. It is worth mentioning that all of the models are run with NVIDIA Titan Xp GPUs with full memory.</p><p>1) Effectiveness of the Pre-trained Model: One advantage in the proposed method is that we can train the network by transfer learning with the pre-trained model, which is crucial in finding an optimal solution for the network. Here, we analyze the learning behaviors of 2D DenseUNet with and without the pre-trained model. Both two experiments were conducted under the same experimental settings. From <ref type="figure" target="#fig_3">Figure 3</ref>, it is clearly observed that with the pre-trained model, 2D Dense-UNet can converge faster and achieve lower loss value, which shows the importance of utilizing the pre-trained model with transfer learning. The test results in <ref type="table" target="#tab_1">Table II</ref> demonstrated that the pre-trained model can help the network achieve better  Note: -denotes that the team participated in ISBI competition and the measurement was not evaluated.</p><p>performance consistently. Our proposed H-DenseUNet inherits this advantage, which plays an important role in achieving the promising results.</p><p>2) Comparison of 2D and 3D DenseUNet: We compare the inherent performance of 2D DenseUNet and 3D DenseUNet to validate that using 3D network solely maybe defective. The number of parameters is one of key elements in measuring the model representation capability, thus both 2D DenseUNet-167 and 3D DenseUNet-65 are designed with the same level of model complexity (around 40M parameters).</p><p>We compare the learning behaviors of two experiments without using the pre-trained model. From <ref type="figure" target="#fig_3">Figure 3</ref>, it shows that the 2D DenseUNet achieves better performance than the 3D DenseUNet, which highlights the effectiveness and efficiency of 2D convolutions with the deep architecture. This is because the 3D kernel consumes large GPU memory so that the network depth and width are limited, leading to weak representation capability. In addition, 3D DenseUNet took much more training time (approximately 60 hours) to converge compared to 2D DenseUNet (around 20 hours).</p><p>Except for the heavy computational cost of the 3D network, another defective is that only a dearth of pre-trained model exists for the 3D network. From <ref type="table" target="#tab_1">Table II,</ref>  that DenseUNet contains long range connections between the encoding part and the decoding part to preserve highresolution features. As the results shown in <ref type="figure" target="#fig_3">Figure 3</ref>, it is obvious that DenseUNet achieves lower loss value than DenseNet, demonstrating the UNet connections actually help the network converge to a better solution. The experimental results in <ref type="table" target="#tab_1">Table II</ref> consistently demonstrated that the lesion segmentation performance can be boosted by a large margin with UNet connections embedded in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Effectiveness of Hybrid Feature Fusion:</head><p>To validate the effectiveness of the hybrid architecture, we compare the learning behaviors of H-DenseUNet and 2D DenseUNet. It is observed that the loss curve for H-DenseUNet begins around 0.04. This is because we fine tune the H-DenseUNet on the 2D DenseUNet basis, which serves as a good initialization. Then the loss value decreases to nearly 0.02, which is attributed to the hybrid feature fusion learning. <ref type="figure" target="#fig_3">Figure 3</ref> shows that H-DenseUNet can converge to the smaller loss value than the 2D DenseUNet, which indicates that the hybrid architecture can contribute to the performance gains. Compared with 2D DenseUNet, our proposed H-DenseUNet advances the segmentation results on both two measurements for liver and tumor segmentation consistently, as shown in <ref type="table" target="#tab_1">Table II</ref>. The performance gains indicate that contextual information along the z dimension, indeed, contributes to the recognition of lesion and liver, especially for lesions that have much more blurred boundary and considered to be difficult to recognize. <ref type="figure" target="#fig_4">Figure 4</ref> shows some segmentation results achieved by 2D DenseUNet and H-DenseUNet on the validation dataset. It is observed that H-DenseUNet can achieve much better results than 2D DenseUNet. Moreover, we trained H-DenseUNet in an end-to-end manner, where the 3D contexts can also help extract more representative in-plane features. The end-to-end system jointly optimizes the 2D and 3D networks, where the hybrid feature can be fully explored. <ref type="figure" target="#fig_5">Figure 5</ref> presents some examples of liver and tumor segmentation results of our H-DenseUNet on the test dataset. We can observe that most small targets as well as large objects can be well segmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with Other Methods on LiTS dataset</head><p>There were more than 50 submissions in 2017 ISBI and MICCAI LiTS challenges. Both challenges employed the same training and test datasets for fair performance comparison. Different from the ISBI challenge, more evaluation metrics have been added in the MICCAI challenge for comprehensive comparison. The detailed results of top 15 teams on the leaderboard 1 , including both ISBI and MICCAI challenges, are listed in <ref type="table" target="#tab_1">Table III</ref>. Our method (team name: xjqi, entry date: Nov. 17, 2017) outperformed other state-of-the-arts on the segmentation results of tumors and achieved very competitive performance for liver segmentation. For tumor burden evaluation, our method achieved the lowest estimation error and ranked the 1st place among all the teams. It is worth mentioning that we used ten entries on the test dataset for ablation analysis of our method. Since there is no validation set provided by challenge organizers, the ablation experiments were performed on test dataset for fair comparison. Please note that the final result is just one of these entries, instead of multiple entries averages.</p><p>Most of the top teams in the challenges employed deep learning based methods, demonstrating the effectiveness of CNN based methods in medical image analysis. For example, Han <ref type="bibr" target="#b39">[40]</ref>, Vorontsov et al. <ref type="bibr" target="#b40">[41]</ref> and Bi et al. <ref type="bibr" target="#b48">[49]</ref> all adopted 2D deep FCNs, where ResNet-like residual blocks were employed as the building blocks. In addition, Chlebus et al. <ref type="bibr" target="#b41">[42]</ref> trained the UNet architecture in two individual models, followed by a random forest classifier. In comparison, our method with a 167-layer network consistently outperformed these methods, which highlighted the efficacy of 2D DenseUNet with pretrained model. Our proposed H-DenseUNet further advanced the segmentation accuracy for both liver and tumor, showing the effectiveness of the hybrid feature learning process.</p><p>Our method achieved the 1st place among all state-of-thearts in the lesion segmentation and very competitive result to DeepX <ref type="bibr" target="#b47">[48]</ref> for liver segmentation. Note that our method surpassed DeepX by a significant margin in the Dice per case evaluation for lesion, which is considered to be notoriously challenging and difficult. Moreover, our result was produced by the single model while DeepX <ref type="bibr" target="#b47">[48]</ref> employed multi-model combination strategy to improve the results, showing the efficiency of our method in the clinical practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison with Other Methods on 3DIRCADb Dataset</head><p>To validate the effectiveness and robustness of our method, we also conduct experiments on 3DIRCADb dataset <ref type="bibr" target="#b55">[56]</ref>, which is publicly available and offers a higher variety and complexity of livers and lesions. <ref type="table" target="#tab_1">Table IV and Table V</ref> show the comparison of the tumor and liver segmentation performance on the 3DIRCADb dataset. We compared our method with the state-of-the-art method <ref type="bibr" target="#b38">[39]</ref> on the 3DIRCADb dataset by running experiments through cross-validation, as the way used in <ref type="bibr" target="#b38">[39]</ref>. We can see that our method achieved the better performance than <ref type="bibr" target="#b38">[39]</ref> on both lesion and liver segmentation accuracy, with 9.0% and 0.4% improvement on DICE, respectively. To further validate the effectiveness of our method, we ran experiments with methods of Unet <ref type="bibr" target="#b41">[42]</ref> and ResNet architecture <ref type="bibr" target="#b39">[40]</ref> respectively, where the training setting keeps the same with Christ et al. <ref type="bibr" target="#b38">[39]</ref>. <ref type="table" target="#tab_1">From Table IV</ref> and <ref type="table" target="#tab_5">Table V</ref>, we can see that our method still outperforms Unet <ref type="bibr" target="#b41">[42]</ref> and ResNet <ref type="bibr" target="#b39">[40]</ref> on the 3DIRCADb dataset, with 14.0% and 5.0% improvement on DICE for tumor segmentation respectively. The experimental comparison validated the superiority of our proposed method in comparison with other methods.</p><p>To have a comprehensive comparison with liver tumor segmentation methods, we listed the reported tumor and liver segmentation results on the 3DIRCADb dataset below the bold line in <ref type="table" target="#tab_1">Table IV and Table V</ref>, respectively. Note that except Note: * denotes the semi-automatic methods; † denotes the method use additional datasets; -denotes the result is not reported. Actually, these methods cannot be compared directly with each other due to the differences in the training dataset and whether is fully-automatic or not. However, to some extent, the reported results on the 3DIRCADb dataset can reflect the state-of-theart performance for the lesion and liver segmentation task.</p><p>Here, we employed the LiTS dataset as the additional dataset. Specifically, we directly tested the well-trained model from 2017 LiTS dataset on the 3DIRCADb dataset. As shown in <ref type="table" target="#tab_1">Table IV and Table V</ref>, our method achieves the best tumor and liver segmentation results on the 3DIRCADb dataset, surpassing the state-of-the-art result largely, with 10.7% and 7.1% improvement on DICE for tumor and liver segmentation respectively. The promising result indicates the effectiveness and good generalization capability of our method. On the other hand, such a good result is also attributed to the LiTS dataset, which contains a huge amount of training data with large variations, and the ability of our method to extract discriminative features from this dataset. <ref type="figure">Figure 6</ref> shows some examples of the results on the 3DIRCADb dataset. It is obvious that our method can well segment the liver and liver lesions from challenging raw CT scans.</p><p>test image ground truth our result <ref type="figure">Figure 6</ref>: Examples of our segmentation results on the 3DIRCADb dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>Automatic liver and tumor segmentation plays an important role in clinical diagnosis. It provides the precise contour of the liver and any tumors inside the anatomical segments of the liver, which assists doctors in the diagnosis process. In this paper, we present an end-to-end training system to explore hybrid features for automatic liver lesion segmentation, where the 3D contexts are effectively probed under the auto-context mechanism. Through the hybrid fusion learning of intraslice and inter-slice features, the segmentation performance for liver lesion has been improved, which demonstrates the effectiveness of our H-DenseUNet. Moreover, compared with other 3D networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, our method probes 3D contexts efficiently. This is crucial in the clinical practice, especially when huge amount of 3D images, containing large image size and a number of slices, are increasingly accumulated in the clinical sites. To show the generalization capability of our method in the clinical practice, we tested our trained model from the LiTS dataset on the 3DIRCADb dataset, and it achieved the stateof-the-art results on both liver and tumor segmentation, with 98.2% and 93.7% on DICE. The promising results achieved on the 3DIRACDb dataset also validated that our method is not simple overtraining, but actually is effective to generalize to different dataset under different data collection conditions.</p><p>To have a better understanding about the performance gains, we analyze the effectiveness of our method regarding the liver tumor size in each patient. <ref type="figure" target="#fig_7">Figure 7</ref> shows the tumor size value of 40 CT volume data in our validation dataset, where the tumor size is obtained by summing up tumor voxels in each ground-truth image. It is observed that the dataset has large variations of the tumor size. For comparison, we divide the dataset into the large-tumor group and the small-tumor group by the orange line in <ref type="figure" target="#fig_7">Figure 7</ref>. From Table VI, we can observe that our method improves the segmentation accuracy by 1.48 (Dice:%) in the whole validation dataset. We can also observe that the large-tumor group achieves 2.35 (Dice:%) accuracy improvements while the score for the small-tumor group is slightly advanced, with 1.1 (Dice:%). From the comparison, we claim that the performance gain is mainly attributed to the improvement of the large-tumor data segmentation results. This is mainly because that the H-DenseUNet mimics the diagnosis process of radiologists, where tumors are delineated by observing several adjacent slices, especially for tumors have blurred boundaries. Once the blurred boundaries are well segmented, the segmentation accuracy for the large-tumor data can be improved by a large margin. Although the hybrid feature still contributes to the segmentation of small tumors, the improvement is limited since small tumors usually occur in fewer slices. In the future, we will focus on the segmentation for small liver tumors. Several potential directions will be taken into considerations for tackling small liver tumor problem, i.e., multi-scale representation structure <ref type="bibr" target="#b56">[57]</ref> and deep supervision <ref type="bibr" target="#b17">[18]</ref>. Recently, perceptual generative adversarial networks (GANs) have been proposed for small object detection and classification <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>. For example, Li et al. <ref type="bibr" target="#b57">[58]</ref> generated superresolved representations for small objects by discovering the intrinsic structural correlations between smallscale and large-scale objects, which may also be a potential direction for handling this challenging problem.</p><p>Another key that should be explored in the future study is the potential depth for the H-DenseUNet. In our experiments, we trained the network using data parallel training, which is an effective technique to speed up the gradient descent by paralleling the computation of the gradient for a mini-batch across mini-batch elements. However, the model complexity is restricted by the GPU memory. In the future, to exploit the potential depth of the H-DenseUNet, we can train the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We present an end-to-end training system H-DenseUNet for liver and tumor segmentation from CT volumes, which is a new paradigm to effectively probe high-level representative intra-slice and inter-slice features, followed by optimizing the features through the hybrid feature fusion layer. The architecture gracefully addressed the problems that 2D convolutions ignore the volumetric contexts and 3D convolutions suffer from heavy computational cost. Extensive experiments on the dataset of 2017 LiTS and 3DIRCADb dataset demonstrated the superiority of our proposed H-DenseUNet. With a singlemodel basis, our method excelled others by a large margin on lesion segmentation and achieved very competitive result on liver segmentation on the LiTS Leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGMENTS</head><p>This work was supported by the grants from the Research Grants Council of the Hong Kong Special Administrative Region (Project Nos. GRF 14202514 and GRF 14203115).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of contrast-enhanced CT scans showing the large variations of shape, size, location of liver lesion. Each row shows a CT scan acquired from individual patient. The red regions denote the liver while the green ones denote the lesions (see the black arrows above).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>H-DenseUNet for accurate liver and tumor segmentation , Input: 224 224 12 Concatenation (b) The illustration of transformation process padding The illustration of the pipeline for liver and lesion segmentation. Each 3D input volume is sliced into adjacent slices through transformation process F and then fed into 2D DenseUNet; Concatenated with the prediction volumes from 2D network, the 3D input volumes are fed into the 3D network for learning inter-slice features; Then, the HFF layer fused and optimized the intra-slice and inter-slice features for accurate liver and tumor segmentation. (a) The structure of H-DenseUNet, including the 2D DenseUNet and the 3D counterpart. (b) The transformation of the volumetric data to three adjacent slices. (c) The network structure of the 2D DenseUNet. The structure in the orange block is a micro-block and k denotes the growth-rate. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>pre-trained model 2D DenseUNet without pre-trained model 2D DenseNet with pre-trained model 2D DenseUNet with pre-trained model H-DenseUNet Training losses of 2D DenseUNet with and without pre-trained model, 2D DenseNet with pre-trained model, 3D DenseUNet without pre-trained model as well as H-DenseUNet (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>compared with the results generated by 3D DenseUNet, 2D DenseUNet with pretrained model achieved 8.9 and 3.0 (Dice: %) improvements on the lesion segmentation results by the measurement of Dice per case and Dice global score, respectively.3) Effectiveness of UNet Connections: We analyze the effectiveness of UNet connections in our proposed framework. Both 2D DenseNet and DenseUNet are trained with the same pre-trained model and training strategies. The difference is Examples of segmentation results by 2D DenseUNet and H-DenseUNet on the validation dataset. The red regions denote the segmented liver while the green ones denote the segmented lesions. The gray regions denote the true liver while the white ones denote the true lesions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Examples of liver and tumor segmentation results of H-DenseUNet from the test dataset. The red regions denote the liver and the green ones denote the tumors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>small tumor group boundary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Tumor size (tumor voxels number) in each patient of our validation dataset. We define the orange line to seperate the large-tumor and the small-tumor group. network using model parallel training, where different portions of the model computation are done on distributed computing infrastructures for the same batch of examples. This strategy maybe another possible direction to further improve the liver tumor segmentation performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Heng are with the Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong. H.Chen is also with Imsight Medical Technology, Inc. (e-mail: xmli@cse.cuhk.edu.hk; hchen@cse.cuhk.edu.hk). Corresponding author: Hao Chen</figDesc><table><row><cell>Raw image</cell><cell>Ground truth</cell><cell>3D display</cell></row></table><note>, H. Chen, X. J. Qi, Q. Dou, C. W. Fu and P. A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Architectures of the proposed H-DenseUNet, consisting of the 2D DenseUNet and the 3D counterpart. The symbol -[ ] denotes the long range UNet summation connections with the last layer of the dense block. The second and forth column indicate the output size of the current stage in two architectures, respectively. Note that "1 × 1, 192 conv" corresponds to the sequence BN-ReLU-Conv layer with convolutional kernel size of 1 × 1 and 192 features. "[ ]×d" represents the dense block is repeated for d times.</figDesc><table><row><cell></cell><cell>Feature size</cell><cell cols="2">2D DenseUNet-167 (k=48)</cell><cell>Feature size</cell><cell cols="2">3D DenseUNet-65 (k=32)</cell></row><row><cell>input</cell><cell>224 × 224</cell><cell>-</cell><cell></cell><cell>224 × 224 × 12</cell><cell>-</cell><cell></cell></row><row><cell>convolution 1</cell><cell>112 × 112</cell><cell cols="2">7 × 7, 96, stride 2</cell><cell>112 × 112 × 6</cell><cell>7 × 7 × 7, 96, stride 2</cell><cell></cell></row><row><cell>pooling</cell><cell>56 × 56</cell><cell cols="2">3 × 3 max pool, stride 2</cell><cell>56 × 56 × 3</cell><cell cols="2">3 × 3 × 3 max pool, stride 2</cell></row><row><cell>dense block 1</cell><cell>56 × 56</cell><cell>1 × 1, 192 conv 3 × 3, 48 conv</cell><cell>× 6</cell><cell>56 × 56 × 3</cell><cell>1 × 1 × 1, 128 conv 3 × 3 × 3, 32 conv</cell><cell>× 3</cell></row><row><cell>transition layer 1</cell><cell>56 × 56 28 × 28</cell><cell cols="2">1 × 1 conv 2 × 2 average pool</cell><cell>56 × 56 × 3 28 × 28 × 3</cell><cell cols="2">1 × 1 × 1 conv 2 × 2 × 1 average pool</cell></row><row><cell>dense block 2</cell><cell>28 × 28</cell><cell>1 × 1, 192 conv 3 × 3, 48 conv</cell><cell>× 12</cell><cell>28 × 28 × 3</cell><cell>1 × 1 × 1, 128 conv 3 × 3 × 3, 32 conv</cell><cell>× 4</cell></row><row><cell>transition layer 2</cell><cell>28 × 28 14 × 14</cell><cell cols="2">1 × 1 conv 2 × 2 average pool,</cell><cell>28 × 28 × 3 14 × 14 × 3</cell><cell cols="2">1 × 1 × 1 conv 2 × 2 × 1 average pool</cell></row><row><cell>dense block 3</cell><cell>14 × 14</cell><cell>1 × 1, 192 conv 3 × 3, 48 conv</cell><cell>× 36</cell><cell>14 × 14 × 3</cell><cell>1 × 1 × 1, 128 conv 3 × 3 × 3, 32 conv</cell><cell>× 12</cell></row><row><cell>transition layer 3</cell><cell>14 × 14 7 × 7</cell><cell cols="2">1 × 1 conv 2 × 2 average pool</cell><cell>14 × 14 × 3 7 × 7 × 3</cell><cell cols="2">1 × 1 × 1 conv 2 × 2 × 1 average pool</cell></row><row><cell>dense block 4</cell><cell>7 × 7</cell><cell>1 × 1, 192 conv 3 × 3, 48 conv</cell><cell>× 24</cell><cell>7 × 7 × 3</cell><cell>1 × 1 × 1, 128 conv 3 × 3 × 3, 32 conv</cell><cell>× 8</cell></row><row><cell>upsampling layer 1</cell><cell>14 × 14</cell><cell cols="2">2 × 2 upsampling -[dense block 3], 768, conv</cell><cell>14 × 14 × 3</cell><cell cols="2">2 × 2 × 1 upsampling -[dense block 3], 504, conv</cell></row><row><cell>upsampling layer 2</cell><cell>28 × 28</cell><cell cols="2">2 × 2 upsampling -[dense block 2], 384, conv</cell><cell>28 × 28 × 3</cell><cell cols="2">2 × 2 × 1 upsampling -[dense block 2], 224, conv</cell></row><row><cell>upsampling layer 3</cell><cell>56 × 56</cell><cell cols="2">2 × 2 upsampling -[dense block 1], 96, conv</cell><cell>56 × 56 × 3</cell><cell cols="2">2 × 2 × 1 upsampling -[dense block 1], 192, conv</cell></row><row><cell>upsampling layer 4</cell><cell>112 × 112</cell><cell cols="2">2 × 2 upsampling -[convolution 1], 96, conv</cell><cell>112 × 112 × 6</cell><cell cols="2">2 × 2 × 2 upsampling -[convolution 1], 96, conv</cell></row><row><cell>upsampling layer 5</cell><cell>224 × 224</cell><cell cols="2">2 × 2 upsampling, 64, conv</cell><cell>224 × 224 × 12</cell><cell cols="2">2 × 2 × 2 upsampling, 64, conv</cell></row><row><cell>convolution 2</cell><cell>224 × 224</cell><cell>1 × 1, 3</cell><cell></cell><cell>224 × 224 × 12</cell><cell>1 × 1 × 1, 3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II :</head><label>II</label><figDesc>Segmentation results by ablation study of our methods on the test dataset (Dice: %).</figDesc><table><row><cell>Model</cell><cell cols="2">Lesion Dice per case Dice global</cell><cell>Liver Dice per case</cell><cell>Dice global</cell></row><row><cell>3D DenseUNet without pre-trained model</cell><cell>59.4</cell><cell>78.8</cell><cell>93.6</cell><cell>92.9</cell></row><row><cell>UNet [42]</cell><cell>65.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet [40]</cell><cell>67.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>2D DenseUNet without pre-trained model</cell><cell>67.7</cell><cell>80.1</cell><cell>94.7</cell><cell>94.7</cell></row><row><cell>2D DenseNet with pre-trained model</cell><cell>68.3</cell><cell>81.8</cell><cell>95.3</cell><cell>95.9</cell></row><row><cell>2D DenseUNet with pre-trained model</cell><cell>70.2</cell><cell>82.1</cell><cell>95.8</cell><cell>96.3</cell></row><row><cell>H-DenseUNet</cell><cell>72.2</cell><cell>82.4</cell><cell>96.1</cell><cell>96.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table III :</head><label>III</label><figDesc>Leaderboard of 2017 Liver Tumor Segmentation (LiTS) Challenge (Dice: %, until 1st Nov. 2017)</figDesc><table><row><cell>Team</cell><cell cols="2">Lesion Dice per case Dice global</cell><cell>Liver Dice per case</cell><cell>Dice global</cell><cell>Tumor Burden RMSE</cell></row><row><cell>our</cell><cell>72.2</cell><cell>82.4</cell><cell>96.1</cell><cell>96.5</cell><cell>0.015</cell></row><row><cell>IeHealth</cell><cell>70.2</cell><cell>79.4</cell><cell>96.1</cell><cell>96.4</cell><cell>0.017</cell></row><row><cell>hans.meine</cell><cell>67.6</cell><cell>79.6</cell><cell>96.0</cell><cell>96.5</cell><cell>0.020</cell></row><row><cell>superAI</cell><cell>67.4</cell><cell>81.4</cell><cell>0.0</cell><cell>0.0</cell><cell>1251.447</cell></row><row><cell>Elehanx [40]</cell><cell>67.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>medical</cell><cell>66.1</cell><cell>78.3</cell><cell>95.1</cell><cell>95.1</cell><cell>0.023</cell></row><row><cell>deepX [48]</cell><cell>65.7</cell><cell>82.0</cell><cell>96.3</cell><cell>96.7</cell><cell>0.017</cell></row><row><cell>Njust768</cell><cell>65.5</cell><cell>76.8</cell><cell>4.10</cell><cell>13.5</cell><cell>0.920</cell></row><row><cell>Medical [41]</cell><cell>65.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Gchlebus [42]</cell><cell>65.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>predible</cell><cell>64.0</cell><cell>77.0</cell><cell>95.0</cell><cell>95.0</cell><cell>0.020</cell></row><row><cell>Lei [49]</cell><cell>64.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ed10b047</cell><cell>63.0</cell><cell>77.0</cell><cell>94.0</cell><cell>94.0</cell><cell>0.020</cell></row><row><cell>chunliang</cell><cell>62.5</cell><cell>78.8</cell><cell>95.8</cell><cell>96.2</cell><cell>0.016</cell></row><row><cell>yaya</cell><cell>62.4</cell><cell>79.2</cell><cell>95.9</cell><cell>96.3</cell><cell>0.016</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table IV :</head><label>IV</label><figDesc>Comparsion of tumor segmentation results on 3DIRCADb dataset.</figDesc><table><row><cell>Model</cell><cell>Year</cell><cell>VOE(%)</cell><cell>RVD(%)</cell><cell>ASD(mm)</cell><cell>RMSD(mm)</cell><cell>DICE</cell></row><row><cell>Unet [42]</cell><cell>2017</cell><cell>62.55 ± 22.36</cell><cell>0.380 ± 1.95</cell><cell>11.11 ± 12.02</cell><cell>16.71 ± 13.81</cell><cell>0.51 ± 0.25</cell></row><row><cell>Christ et al. [39]</cell><cell>2017</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.56 ± 0.26</cell></row><row><cell>ResNet [40]</cell><cell>2017</cell><cell>56.47 ± 13.62</cell><cell>-0.41 ± 0.21</cell><cell>6.36 ± 3.77</cell><cell>11.69 ± 7.60</cell><cell>0.60 ± 0.12</cell></row><row><cell>ours</cell><cell></cell><cell>49.72 ± 5.2</cell><cell>-0.33 ± 0.10</cell><cell>5.293 ± 6.15</cell><cell>11.11 ± 29.14</cell><cell>0.65 ± 0.02</cell></row><row><cell>Foruzan and Chen [50]*</cell><cell>2016</cell><cell>30.61 ± 10.44</cell><cell>15.97 ± 12.04</cell><cell>4.18 ± 9.60</cell><cell>5.09 ± 10.71</cell><cell>0.82 ± 0.07</cell></row><row><cell>Wu et al. [51]*</cell><cell>2017</cell><cell>29.04 ± 8.16</cell><cell>-2.20 ± 15.88</cell><cell>0.72 ± 0.33</cell><cell>1.10 ± 0.49</cell><cell>0.83 ± 0.06</cell></row><row><cell>Li et al. [52]  †</cell><cell>2013</cell><cell>14.4 ± 5.3</cell><cell>-8.1 ± 2.1</cell><cell>2.4 ± 0.8</cell><cell>2.9 ± 0.7</cell><cell>-</cell></row><row><cell>Moghbel et al. [53]  †</cell><cell>2016</cell><cell>22.78 ± 12.15</cell><cell>8.59 ± 18.78</cell><cell>-</cell><cell>-</cell><cell>0.75 ± 0.15</cell></row><row><cell>Sun et al. [16]  †</cell><cell>2017</cell><cell>15.6 ± 4.3</cell><cell>5.8 ± 3.5</cell><cell>2.0 ± 0.9</cell><cell>2.9 ± 1.5</cell><cell>-</cell></row><row><cell>ours  †</cell><cell></cell><cell>11.68 ± 4.33</cell><cell>-0.01 ± 0.05</cell><cell>0.58 ± 0.46</cell><cell>1.87 ± 2.33</cell><cell>0.937 ± 0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table V :</head><label>V</label><figDesc>Comparsion of liver segmentation results on 3DIRCADb dataset. Note: † denotes the method use additional datasets. -denotes the result is not reported.experiments<ref type="bibr" target="#b41">[42]</ref> and<ref type="bibr" target="#b39">[40]</ref>, all other experiment results are the reported values in the original papers. It is worth noting that most liver tumor segmentation methods[16, 19, 52- 55]  utilized additional datasets for training and tested on the 3DIRCADb dataset. For example, Li et al.<ref type="bibr" target="#b51">[52]</ref>, Sun et al.<ref type="bibr" target="#b15">[16]</ref> and Lu et al.<ref type="bibr" target="#b18">[19]</ref> collected additional clinical data from hospitals as the training set. Moghbel et al.<ref type="bibr" target="#b52">[53]</ref> utilized additional the MIDAS dataset while Li et al.<ref type="bibr" target="#b53">[54]</ref> used the SLIVER07 dataset in the training, respectively. In addition, Foruzan and Chen<ref type="bibr" target="#b49">[50]</ref> and Wu et al.<ref type="bibr" target="#b50">[51]</ref> achieved good results on tumor segmentation by semi-automatic methods.</figDesc><table><row><cell>Model</cell><cell>Year</cell><cell>VOE(%)</cell><cell>RVD(%)</cell><cell>ASD(mm)</cell><cell>RMSD(mm)</cell><cell>DICE</cell></row><row><cell>Unet [42]</cell><cell>2017</cell><cell>14.21 ± 5.71</cell><cell>-0.05 ± 0.10</cell><cell>4.33 ± 3.39</cell><cell>8.35 ± 7.54</cell><cell>0.923 ± 0.03</cell></row><row><cell>ResNet [40]</cell><cell>2017</cell><cell>11.65 ± 4.06</cell><cell>-0.03 ± 0.06</cell><cell>3.91 ± 3.95</cell><cell>8.11 ± 9.68</cell><cell>0.938 ± 0.02</cell></row><row><cell>Christ et al. [39]</cell><cell>2017</cell><cell>10.7</cell><cell>-1.4</cell><cell>1.5</cell><cell>24.0</cell><cell>0.943</cell></row><row><cell>ours</cell><cell></cell><cell>10.02 ± 3.44</cell><cell>-0.01 ± 0.05</cell><cell>4.06 ± 3.85</cell><cell>9.63 ± 10.41</cell><cell>0.947 ± 0.01</cell></row><row><cell>Li et al. [54]  †</cell><cell>2015</cell><cell>9.15 ± 1.44</cell><cell>-0.07 ± 3.64</cell><cell>1.55 ± 0.39</cell><cell>3.15 ± 0.98</cell><cell>-</cell></row><row><cell>Moghbel et al. [55] †</cell><cell>2016</cell><cell>5.95</cell><cell>7.49</cell><cell>-</cell><cell>-</cell><cell>0.911</cell></row><row><cell>Lu et al. [19]  †</cell><cell>2017</cell><cell>9.36 ± 3.34</cell><cell>0.97 ± 3.26</cell><cell>1.89 ± 1.08</cell><cell>4.15 ± 3.16</cell><cell>-</cell></row><row><cell>ours  †</cell><cell></cell><cell>3.57 ± 1.66</cell><cell>0.01 ± 0.02</cell><cell>1.28 ± 2.02</cell><cell>3.58 ± 6.58</cell><cell>0.982 ± 0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table VI :</head><label>VI</label><figDesc>Effectiveness of our method regarding to the tumor size (Dice: %). Baseline is the 2D DenseUNet with pre-trained model.</figDesc><table><row><cell></cell><cell>Total</cell><cell>Large-tumor group</cell><cell>Small-tumor group</cell></row><row><cell>Baseline</cell><cell>43.56</cell><cell>58.24</cell><cell>41.08</cell></row><row><cell>H-DenseUNet</cell><cell>45.04 (+1.48)</cell><cell>60.59 (+2.35)</cell><cell>42.18 (+1.1)</cell></row><row><cell>Note:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/competitions/17094#results</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimates of worldwide burden of cancer in 2008: Globocan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-R</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mathers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Parkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of cancer</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2893" to="2917" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Liver tumor volume estimation by semi-automatic segmentation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marziliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Thng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Engineering in Medicine and Biology Society, 2005. IEEE-EMBS 2005. 27th Annual International Conference of the</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3296" to="3299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Review of liver segmentation and computer assisted detection/diagnosis methods in computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moghbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mashohor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I B</forename><surname>Saripan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prasoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lauze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2998" to="3006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="556" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detection of glands and villi by collaboration of domain knowledge and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-scale and modality dropout learning for intervertebral disc localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Computational Methods and Clinical Applications for Spine Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d u-net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voxresnet: Deep voxelwise residual networks for brain segmentation from 3d mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Liver segmentation from ct images using a sparse priori statistical shape model (sp-ssm)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">185249</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d multi-scale fcn with random modality voxel dropout learning for intervertebral disc localization and segmentation from multi-modality mr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Belavỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Armbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Felsenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic liver and lesion segmentation in ct using cascaded fully convolutional neural networks and 3d conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E A</forename><surname>Elshaer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ettlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tatavarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rempfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Armbruster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="415" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic segmentation of liver tumors from multiphase contrast-enhanced ct images based on fcns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional network for liver segmentation and lesions detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d deeply supervised network for automatic liver segmentation from ct volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="149" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic 3d liver location and segmentation via convolutional neural network and graph cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="182" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Joint sequence learning and cross-modality convolution for 3d biomedical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-L</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07754</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improving deep pancreas segmentation in ct and mri images via recurrent neural contextual learning and direct loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04912</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Standard plane localization in fetal ultrasound via domain transferred deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1627" to="1636" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for medical image analysis: Full training or fine tuning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1312" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully automatic anatomical, pathological, and functional segmentation from ct scans for hepatic surgery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malandain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Montagnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dourthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Malassagne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Aided Surgery</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segmentation of liver metastases in ct scans by adaptive thresholding and morphological processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Moltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bornemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dicken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peitgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI workshop</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">195</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A semi-automated method for liver tumor segmentation based on 2d region growing with knowledge-based constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fengshou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI workshop</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">159</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimal multiresolution 3d level-set method for liver segmentation incorporating local curvature constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez-Carretero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fernandez-De Manuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pascau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tellado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Desco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Ledesma-Carbayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Engineering in medicine and biology society, EMBC, 2011 annual international conference of the IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3419" to="3422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Random feature subspace ensemble based extreme learning machine for liver tumor detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4675" to="4678" />
		</imprint>
	</monogr>
	<note>36th Annual International Conference of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Metastatic liver tumor segmentation using texture-based omni-directional deformable surface models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abi-Jaoudeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Workshop on Computational and Clinical Challenges in Abdominal Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Liver tumor segmentation from mr images using 3d fast marching algorithm and single hidden layer feedforward neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed research international</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Texture-based treatment prediction by automatic liver tumor segmentation on computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-F</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer, Information and Telecommunication Systems (CITS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="128" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scale-adaptive supervoxelbased random forests for liver tumor segmentation in dynamic contrast-enhanced ct scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Conze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Noblet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pessaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="223" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive local window for level set segmentation of ct and mri liver lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hoogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cunha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Sirlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Napel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accelerated liver tumor segmentation in four-phase computed tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chaieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mabrouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ghorbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Real-Time Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="133" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Automatic liver and tumor segmentation of ct and mri volumes using cascaded fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ettlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grün</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E A</forename><surname>Elshaera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lipkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schlecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tatavarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05970</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Automatic liver lesion segmentation using a deep convolutional neural network method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07239</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Liver lesion segmentation informed by joint liver segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Neural network-based automatic liver tumor segmentation with random forest-based candidate filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chlebus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Moltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schenk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00842</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A fixed-point model for pancreas segmentation in abdominal ct scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A bottom-up approach for pancreas segmentation using cascaded superpixels and (deep) image patch labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="386" to="399" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep supervision for pancreatic cyst segmentation in abdominal ct scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="222" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00045</idno>
		<title level="m">Spatial aggregation of holistically-nested convolutional neural networks for automated pancreas localization and segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Hierarchical convolutional-deconvolutional neural networks for automatic liver and tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04540</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Automatic liver lesion detection using cascaded deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02703</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improved segmentation of low-contrast lesions using sigmoid edge model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Foruzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1267" to="1283" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3d liver tumor segmentation in ct images using improved fuzzy c-means and graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed research international</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A likelihood and local constraint level set model for liver tumor segmentation from ct volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eberl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2967" to="2977" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Automatic liver tumor segmentation on computed tomography for patient treatment planning and monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moghbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mashohor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I B</forename><surname>Saripan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EXCLI journal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">406</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automatic liver segmentation based on shape constraints and deformable graph cut in ct images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5315" to="5329" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Automatic liver segmentation on computed tomography using random walkers for treatment planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moghbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mashohor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I B</forename><surname>Saripan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EXCLI journal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">500</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">3d image reconstruction for comparison of algorithm database: a patient-specific anatomical and medical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hostettler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Agnus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Charnoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fasquel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bouhadjar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient multi-scale 3d cnn with fully connected crf for accurate brain lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="61" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Perceptual generative adversarial networks for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Gan-based data augmentation for improved liver lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frid-Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
