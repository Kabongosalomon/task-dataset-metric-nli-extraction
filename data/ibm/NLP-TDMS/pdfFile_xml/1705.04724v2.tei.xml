<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Person Re-Identification by Deep Joint Learning of Multi-Loss Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<email>w.li@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<email>xiatian.zhu@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<email>s.gong@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Person Re-Identification by Deep Joint Learning of Multi-Loss Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing person re-identification (re-id) methods rely mostly on either localised or global feature representation alone. This ignores their joint benefit and mutual complementary effects. In this work, we show the advantages of jointly learning local and global features in a Convolutional Neural Network (CNN) by aiming to discover correlated local and global features in different context. Specifically, we formulate a method for joint learning of local and global feature selection losses designed to optimise person re-id when using only generic matching metrics such as the L2 distance. We design a novel CNN architecture for Jointly Learning Multi-Loss (JLML) of local and global discriminative feature optimisation subject concurrently to the same re-id labelled information. Extensive comparative evaluations demonstrate the advantages of this new JLML model for person re-id over a wide range of state-of-the-art re-id methods on five benchmarks (VIPeR, GRID, CUHK01, CUHK03, Market-1501).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (re-id) is about matching identity classes in detected person bounding box images from nonoverlapping camera views over distributed open spaces. This is an inherently challenging task because person visual appearance may change dramatically in different camera views from different locations due to unknown changes in human pose, illumination, occlusion, and background clutter . Existing person re-id studies typically focus on either feature representation <ref type="bibr" target="#b8">[Gray and Tao, 2008;</ref><ref type="bibr" target="#b7">Farenzena et al., 2010;</ref><ref type="bibr">Kviatkovsky et al., 2013;</ref><ref type="bibr" target="#b10">Liao et al., 2015;</ref><ref type="bibr" target="#b12">Matsukawa et al., 2016a;</ref><ref type="bibr" target="#b11">Ma et al., 2017]</ref> or matching distance metrics <ref type="bibr" target="#b9">[Koestinger et al., 2012;</ref><ref type="bibr" target="#b22">Xiong et al., 2014;</ref><ref type="bibr" target="#b23">Zheng et al., 2013;</ref><ref type="bibr" target="#b19">Wang et al., 2014b;</ref><ref type="bibr" target="#b14">Paisitkriangkrai et al., 2015;</ref><ref type="bibr" target="#b21">Wang et al., 2016b;</ref><ref type="bibr" target="#b21">Wang et al., 2016c;</ref><ref type="bibr" target="#b22">Wang et al., 2016d;</ref><ref type="bibr" target="#b4">Chen et al., 2017b]</ref> or their combination in deep learning framework <ref type="bibr" target="#b0">Ahmed et al., 2015;</ref><ref type="bibr" target="#b20">Wang et al., 2016a;</ref><ref type="bibr">Subramaniam et al., 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2017a]</ref>. Regardless, the overall objective is to obtain a view-and location-invariant (cross-domain) representation. We consider that learning any matching distance metric is intrinsically learning a global feature transformation across domains (two disjoint camera views) therefore obtaining a "normalised" feature representation for matching.</p><p>Most re-id features are typically hand-crafted to encode local topological and/or spatial structural information, by different image decomposition schemes such as horizontal stripes <ref type="bibr" target="#b8">[Gray and Tao, 2008;</ref><ref type="bibr">Kviatkovsky et al., 2013]</ref>, body parts <ref type="bibr" target="#b7">[Farenzena et al., 2010]</ref>, and patches <ref type="bibr" target="#b12">Matsukawa et al., 2016a;</ref><ref type="bibr" target="#b10">Liao et al., 2015]</ref>. These localised features are effective for mitigating the person pose and detection misalignment in re-id matching. More recent deep re-id models <ref type="bibr" target="#b20">Wang et al., 2016a;</ref><ref type="bibr" target="#b3">Chen et al., 2017a;</ref><ref type="bibr" target="#b0">Ahmed et al., 2015]</ref> benefit from the availability of larger scale datasets such as CUHK03  and Market-1501 <ref type="bibr" target="#b24">[Zheng et al., 2015]</ref> and from lessons learned on other vision tasks <ref type="bibr" target="#b9">[Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b9">Girshick et al., 2014]</ref>. In contrast to local hand-crafted features, deep models, in particular Convolutional Neural Networks (CNN) <ref type="bibr" target="#b9">[LeCun et al., 1998]</ref>, favour intrinsically in learning global feature representations with a few exceptions. They have been shown to be effective for re-id.</p><p>We consider that either local or global feature learning alone is suboptimal. This is motivated by the human visual system that leverages both global (contextual) and local (saliency) information concurrently <ref type="bibr" target="#b13">[Navon, 1977;</ref><ref type="bibr" target="#b15">Torralba et al., 2006]</ref>. This intuition for joint learning aims to extract correlated complementary information in different context whilst satisfying the same learning constraint 1 therefore achieving more reliable recognition. To that end, we need to address a number of non-trivial problems: (i) the model learning behaviour in satisfying the same label constraint may be different at the local and global levels; (ii) any complementary correlation between local and global features is unknown and may vary among individual instances, therefore must be learned and optimised consistently across data; (iii) People's appearance in public scenes is diverse in both pattens and configurations. This makes it challenging to learn correlations between local and global features for all appearances.</p><p>This work aims to formulate a deep learning model for jointly optimising local and global feature selections concurrently and to improve person re-id using only generic matching metrics such as the L2 distance. We explore a deep learning approach for its potential superiority in learning from large scale data <ref type="bibr" target="#b3">Chen et al., 2017a]</ref>. For the bounding box image based person re-id, we consider the entire person in the bounding box as a global scene context and body parts of the person as local information sources, both are subject to the surrounding background clutter within a bounding box, and potentially also misalignment and partial occlusion from bounding box detection. In this setting, we wish to discover and optimise jointly correlated complementary feature selections in the local and global representations, both subject to the same label constraint concurrently. Whilst the former aims to address pose/detection misalignment and occlusion by localised fine-grained saliency information, the latter exploits holistic coarse-grained context for more robust global matching.</p><p>To that end, we formulate a deep two-branch CNN architecture, with one branch for learning localised feature selection (local branch) and the other for learning global feature selection (global branch). Importantly, the two branches are not independent but synergistically correlated and jointly learned concurrently. This is achieved by: (i) imposing interbranch interaction between the local and global branches, and (ii) enforcing a separate learning objective loss function to each branch for learning independent discriminative capabilities, whilst being subject to the same class label constraint. Under such balancing between interaction and independence, we allow both branches to be learned concurrently for maximising their joint optimal extraction and selection of different discriminative features for person re-id. We call this model the Joint Learning Multi-Loss (JLML) CNN model. To minimise poor learning due to inherent noise and potential covariance, we introduce a structured feature selective and discriminative learning mechanism into both the local and global branches subject to a joint sparsity regularisation.</p><p>The contributions of this work are: (I) We propose the idea of learning concurrently both local and global feature selections for optimising feature discriminative capabilities in different context whilst performing the same person re-id tasks. This is currently under-studied in the person re-id literature to our best knowledge. (II) We formulate a novel Joint Learning Multi-Loss (JLML) CNN model for not only learning both global and local discriminative features in different context by optimising multiple classification losses on the same person label information concurrently, but also utilising their complementary advantages jointly in coping with local misalignment and optimising holistic matching criteria for person re-id. (III) We introduce a structured sparsity based feature selection learning mechanism for improving multiloss joint feature learning robustness w.r.t. noise and data covariance between local and global representations. Extensive comparative evaluations demonstrate the superiority of the proposed JLML model over a wide range of existing state-ofthe-art re-id models on five benchmark datasets VIPeR <ref type="bibr" target="#b8">[Gray and Tao, 2008]</ref>, <ref type="bibr">GRID [Loy et al., 2009</ref><ref type="bibr">], CUHK01 [Li et al., 2012</ref>, <ref type="bibr">CUHK03 [Li et al., 2014]</ref>, and Market-1501 <ref type="bibr" target="#b24">[Zheng et al., 2015]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>The proposed JLML model considers learning both local and global feature selections jointly for optimising their correlated complementary advantages. This goes beyond existing methods mostly relying on only one level of feature representation. Specifically, the JLML method is related to the saliency learning based models <ref type="bibr" target="#b18">Wang et al., 2014a]</ref> in terms of modelling localised part importance. However, these existing methods consider only the patch appearance statistics within individual locations but no global feature representation learning, let alone the correlation and complementary information discovery between local and global features as modelled by the JLML.</p><p>Whilst the more recent Spatially Constrained Similarity (SCS) model  and Multi-Channel Parts (MCP) network <ref type="bibr" target="#b4">[Cheng et al., 2016]</ref> consider both levels of representation, the JLML model differs significantly from them: (i) The SCS method focuses on supervised metric learning, whilst the JLML aims at joint discriminative feature learning and needs only generic metrics for re-id matching. Also, hand-crafted local and global features are extracted separately in SCS without any inter-feature interaction and correlation learning involved, as opposite to the joint learning of global and local feature selections concurrently subject to the same supervision information in the JLML; (ii) The local and global branches of the MCP model are supervised and optimised by a triplet ranking loss, in contrast to the proposed multiple classification loss design (Sec. 3.2). Critically, this one-loss model learning is likely to impose negative influence on the discriminative feature learning behaviour for both branches due to potential over-low pre-branch independence and over-high inter-branch correlation. This may lead to suboptimal joint learning of local and global feature selections in model optimisation, as suggested by our evaluation in Section 4.3. (iii) In addition, the JLML is capable of performing structured feature sparsity regularisation along with the multi-loss joint learning of local and global feature selections for providing additional benefits (Sec. 4.3). Whilst similar in theory to the sparsity constraint on the supervised SCS metric learning, we perform differently sparse generic feature learning without the need for supervised metric optimisation.</p><p>In terms of loss function, the HER model <ref type="bibr" target="#b21">[Wang et al., 2016b]</ref> similarly does not exploit pair-wise re-id labels but defines a single identity label per training person for regression loss (vs. the classification loss in the JLML) based reid feature embedding optimisation. Importantly, HER relies on the pre-defined feature (mostly hand-crafted local feature) without the capability of jointly learning global and local feature representations and discovering their correlated complementary advantages as specifically designed in JLML. Also, the DGD  model uses the classification loss for model optimisation. However, this model considers only the global feature representation learning of one-loss classification as opposite to the proposed joint global and local feature learning of multi-loss classification concurrently subject to maximising the same person identity matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>We assume a set of n training images I = {I i } n i=1 with the corresponding identity labels as Y = {y i } n i=1 . These training images capture the visual appearance of n id (where y i ∈ [1, · · · , n id ]) different people under non-overlapping camera views. We formulate a Joint Learning Multi-Loss (JLML) CNN model that aims to discover and capture concurrently complementary discriminative information about a person image from both local and global visual features of the image in order to optimise person re-id under significant viewing condition changes across locations. This is in contrast to most existing re-id methods typically depending only on either local or global features alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Learning Multi-Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling-256</head><p>Pooling-512 The overall design of the proposed JLML model is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. This JLML model consists of a twobranches CNN network: (1) One local branch of m streams of an identical structure with each stream learning the most discriminative local visual features for one of m local image regions of a person bounding box image; (2) Another global branch responsible for learning the most discriminative global level features from the entire person image. For concurrently optimising per-branch discriminative feature representations and discovering correlated complementary information between local and global feature selections, a joint learning scheme that subjects both local and global branches to the same identity label supervision is considered with two underlying principles: (I) Shared low-level features. We construct the global and local branches on a shared lower conv layer, in particular the first conv layer 2 , for facilitating inter-branch common learning. The intuition is that, the lower conv layers capture lowlevel features such as edges and corners which are common to all patterns in the same images. This shared learning is similar in spirit to multi-task learning <ref type="bibr" target="#b1">[Argyriou et al., 2007]</ref>, where the local and global feature learning branches are two related learning tasks. Sharing the low-level conv layer reduces the model parameter size therefore model overfitting <ref type="bibr">2</ref> We found empirically no clear benefits from increasing the number of shared conv layers in our implementation. risks. This is especially critical in learning person re-id models when labelled training data is limited. (II) Multi-task independent learning subject to shared label constraints. To maximise the learning of complementary discriminative features from local and global representations, the remaining layers of the two branches are learned independently subject to given identity labels. That is, the JLML model aims to learn concurrently multiple identity feature representations for different local image regions and the entire image, all of which aim to maximise the same identity matching both individually and collectively at the same time. Independent multi-task learning aims to preserve both local saliency in feature selection and global robustness in image representation. To that end, the JLML model is designed to perform multi-task independent learning subject to shared identity label constraints by allocating each branch with a separate objective loss function. By doing so, the per-branch learning behaviour is conditioned independently on the respective feature representation. We call this branch-wise loss formulation as the MultiLoss design.  . Our idea is to have a competing-to-survive mechanism in feature learning that discourages irrelevant features whilst encourages discriminative features concurrently in different local and global context to maximise a shared identity matching objective. To that end, we sparsify the global feature representation with a group LASSO <ref type="bibr" target="#b17">[Wang et al., 2013]</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC-512</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Conv 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Class Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Class Labels</head><formula xml:id="formula_0">2,1 = W G 2,1 = dg i=1 w i g 2 (1) where W G = [w 1 g , · · · , w</formula><p>dg g ] ∈ R cg×dg is the parameter matrix of the global branch feature layer taking as input d g dimensional vectors from the previous layer and outputting c g dimensional (512-D) feature representation. Specifically, with the 1 norm applied on the 2 norm of w i g , our aim is to learn (tune) selectively feature dimension importance subject to both the sparsity principle and the identity label constraint simultaneously.</p><p>Similarly, we also enforce a local feature sparsity constraint by an exclusive group <ref type="bibr">LASSO [Kong et al., 2014]</ref>:</p><formula xml:id="formula_1">1,2 = W L 1,2 = c l i=1 m j=1 w i l,j 2 1 (2) where W L =   w 1 l,1 · · · w 1 l,m · · · · · · · · · w c l l,1 · · · w c l l,m   =   w 1 l · · · w c l l  </formula><p>( <ref type="formula">3)</ref> is the parameter matrix of the local branch feature layer with m×d l and c l (512) as the input and output dimensions (m the image stripe number). The w i l,j ∈ R d l ×1 defines the parameter vector for contributing the i-th output feature dimension from the j-th local input feature vector, j ∈ [1, 2, · · · , m]. In particular, the 2,1 regulariser performs sparse feature selection for individual image regions as below: (1) We perform feature selective learning at the local region level by enforcing the 1 norm directly on w i l,j , conceptually similar to the group LASSO at the global level. (2) We then apply a nonsparse smooth fusion with the 2 norm to combine the effects of different local features weighted by the sparse w i l,j . (3) Lastly, we exploit the 1 norm again at the level of w k l (k ∈ [1, 2, · · · , c l ]) to learn the local 512-D feature representation selection. <ref type="figure" target="#fig_1">Figure 2</ref> shows our structured sparsity regularisations for both local and global feature selections. Loss Function. For model training, we utilise the crossentropy classification loss function for both global and local branches so to optimise person identity classification given training labels of multiple person classes extracted from pairwise labelled re-id dataset. Formally, we predict the posterior  probabilityỹ i of image I i over the given identity label y i :</p><formula xml:id="formula_2">p(ỹ i = y i |I i ) = exp(w yi x i ) |nid| k=1 exp(w k x i )<label>(4)</label></formula><p>where x i refers to the feature vector of I i from the corresponding branch, and W k the prediction function parameter of training identity class k. The training loss on a batch of n bs images is computed as:</p><formula xml:id="formula_3">l = − 1 n bs nbs i=1 log p(ỹ i = y i |I i )<label>(5)</label></formula><p>Combined with the group sparsity based feature selection regularisations, we have the final loss function for the global and local branch sub-networks as:</p><formula xml:id="formula_4">l global = l + λ global W G 2,1 , l local = l + λ local W L 1,2 (6)</formula><p>where λ global and λ local control the balance between the identity label loss and the feature selection sparsity regularisation. We empirically set λ local = λ global = 5×10 −4 by crossvalidation in our evaluations.</p><p>Choice of Loss Function. Our JLML model learning deploys a classification loss function. This differs significantly from the contrastive loss functions used by most existing deep re-id methods designed to exploit pairwise re-id labels defined by both positive and negative pairs, such as the pairwise verification <ref type="bibr" target="#b16">[Varior et al., 2016;</ref><ref type="bibr">Subramaniam et al., 2016;</ref><ref type="bibr" target="#b0">Ahmed et al., 2015;</ref>  <ref type="bibr" target="#b7">[Edelman, 1998]</ref>. We consider that reid tasks are about model generalisation to unseen test identity classes given training data on independent seen identity classes. Our JLML model learning exploits this general classification learning principle beyond the strict pair-wise relative verification loss in existing re-id models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>We adopt the standard Stochastic Gradient Descent (SGD) optimisation algorithm <ref type="bibr" target="#b9">[Krizhevsky et al., 2012]</ref> to perform the batch-wise joint learning of local and global branches. Note that, with SGD we can naturally synchronise the optimisation processes of the two branches by constraining their learning behaviours subject to the same identity label information at each update. This is likely to avoid representation learning divergence between two branches and help enhance the correlated complementary learning capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Re-Id by Generic Distance Metrics</head><p>Once the JLML model is learned, we obtain a 1,024-D joint representation by concatenating the local (512-D) and global (512-D) feature vectors (the fc layer in <ref type="table" target="#tab_0">Table 1</ref>). For person re-id, we deploy this 1,024-D deep feature representation using only a generic distance metric without camera-pair specific distance metric learning, e.g. L2 distance. Specifically, given a test probe image I p from one camera view and a set of test gallery images {I g i } from other non-overlapping camera views: (1) We first compute their corresponding 1,024-D feature vectors by forward-feeding the images to the trained JLML model, denoted as x p = [x p g ; x p l ] and {x g i = [x g g ; x g l ]}.</p><p>(2) We then compute L2 normalisation on the global and local features, separately. (3) Lastly we compute the cross-camera matching distances between x p and x g i by some generic matching metric, e.g. L2 distance. We then rank all gallery images in ascendant order by their L2 distances to the probe image. The probabilities of true matches of probe person images in Rank-1 and among the higher ranks indicate the goodness of the learned JLML deep features for person re-id tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. For evaluation, we used five benchmarking re-id datasets, VIPeR <ref type="bibr" target="#b8">[Gray and Tao, 2008]</ref> These datasets present a wide range of re-id evaluation scenarios with different population sizes under different challenging viewing conditions <ref type="table" target="#tab_3">(Table 2)</ref>.   <ref type="bibr" target="#b24">[Zheng et al., 2015]</ref>. We used the cumulative matching characteristic (CMC) to measure re-id accuracy on all benchmarks, except on Market-1501 we also used in addition the recall measure of multiple truth matches by mean Average Precision (mAP), i.e. first computing the area under the Precision-Recall curve for each probe, then calculating the mean of Average Precision over all probes <ref type="bibr" target="#b24">[Zheng et al., 2015]</ref>.</p><p>Competitors. We compared the JLML model against 10 existing state-of-the-art methods as listed in   <ref type="table" target="#tab_6">Table 4</ref>) for pre-training and training the JLML model on all datasets. We also adopted the stepped learning rate policy, e.g. dropping the learning rate by a factor of 10 every 100K iterations for JLML pre-training and every 20K iterations for JLML training. We utilised the L2 distance as the default matching metric, unless stated otherwise. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Conventional Intra-Domain Re-Id Evaluations</head><p>We conducted extensively comparative evaluations on conventional supervised learning based person re-id tasks.</p><p>(I) Evaluation on CUHK03. <ref type="table" target="#tab_7">Table 5</ref> shows the comparisons of JLML against 8 existing methods on CUHK03. It is evident that JLML outperforms existing methods in all categories on both labelled and detected bounding boxes, surpassing the 2nd best performers DGD and X-Corr on corresponding labelled and detected images in Rank-1 by 7.9%(83.2-75.3) and 8.6%(80.6-72.0) respectively. X-Corr/GOG/JLML also suffer the least from auto-detection misalignment, indicating the robustness and competitiveness of the joint learning approach to mining complementary local and global discriminative features.</p><p>(II) Evaluation on Market-1501.</p><p>We evaluated JLML against four existing models on Market-1501. <ref type="table" target="#tab_8">Table 6</ref> shows the clear performance superiority of JLML over all state-ofthe-arts with more significant Rank-1 advantages over other methods compared to CUHK03, giving 19.3%(85.1-65.8) (SQ) and 13.7%(89.7-76.0) (MQ) gains over the 2nd best S-CNN. This further validates the advantages of our joint learning of multi-loss classification for optimising re-id especially when the re-id test population size increases (751 people on Market-1501 vs. 100 people on CUHK03). (III) Evaluation on CUHK01. We compared our JLML model with 8 state-of-the-art methods on CUHK01. <ref type="table" target="#tab_9">Table  7</ref> shows that JLML surpasses clearly all compared models under both training/test splits in single-and multi-short settings. Moreover, JLML outperforms in Rank-1 (76.7%) the best hand-crafted feature method NFST (R1 69.1%) when the training population size is small (486 people). When the training population size increases (871 people), JLML is even more effective than all deep competitors in exploiting extra training classes by inducing more identity-discriminative joint person features in distinct context. For example, JLML gains 5.8%(87.0-81.2) more Rank-1 than the 2nd best method X-Corr in single-shot re-id, further improved the gain of 4.8%(69.8-65.0) under the 486/485 split. These results show consistent superiority and robustness of the proposed JLML model over the existing methods.</p><p>(IV) Evaluation on VIPeR. We evaluated the performance of JLML against 8 strong competitors on VIPeR, a more challenging test scenario with fewer training classes (316 people) and lower image resolution. On this dataset, the best performers are hand-crafted feature methods (SCS and NFST) rather than deep models. This is in contrast to the tests on CUHK01, CUHK03, and Market-1501. This is due to (i) the small training data insufficient for learning effectively discriminative deep models with millions of parameters; (ii) the greater disparity to CUHK03 in camera viewing conditions which makes knowledge transfer less effective (see Implementation). Nevertheless, the JLML model remains the best among all deep methods with or without deep verification metric learning. This validates the superiority and robustness of our deep joint global and local representation learning of multi-loss classification given sparse training data. We attribute this property to the JLML's capability of mining complementary features in different context for both handling local misalignment and optimising global matching. (V) Evaluation on GRID. We compared JLML against 4 competing methods on GRID 4 . In addition to poor image resolution, poor lighting and a small training size (125 people), GRID also has extra distractors in the testing population therefore presenting a very challenging but realistic re-id scenario. <ref type="table" target="#tab_11">Table 9</ref> shows a significant superiority of JLML over existing state-of-the-arts, with Rank-1 12.8%(37.5-24.7) better than the 2nd best method GOG, a 51.8% relative improvement. This demonstrates the unique and practically desirable advantage of JLML in handling more realistically challenging open-world re-id matching where large numbers of distractors are usually present. It is worth pointing out that this step-change advantage in re-id matching rate on GRID is achieved by deep learning from only a limited number of training identity classes with highly imbalanced images sampled from 8 distributed camera views, e.g. 25 images from the 6 th camera vs. 513 from the 5 th camera. This imbalanced sampling directly results in not only scarce pairwise training <ref type="bibr">4</ref> The GRID dataset has not been evaluated as extensively as others like VIPeR / CUHK01 / CUHK03, although GRID provides a more realistic test setting with a large number of distractors in testing.</p><p>One possible reason is the more challenging re-id setting imposed by GRID resulting in significantly poorer matching rates by all published methods (see http://personal.ie.cuhk.edu.hk/˜ccloy/ downloads_qmul_underground_reid.html), also as verified by our evaluation in <ref type="table" target="#tab_11">Table 9.</ref> data but also insufficient training samples for pairwise camera views, resulting in significant degradation in re-id performance from all pairwise supervised learning based models XQDA, GOG, SCS, and X-Corr. In contrast, JLML is designed to avoid the need for pairwise labelled information in model learning by instead learning from multi-loss classifications. Moreover, the joint learning of multi-loss classification benefits from concurrent local and global feature selections in different context, resulting in more robust and accurate re-id matching in a heterogeneous search space.  <ref type="bibr">, 2015]</ref>, <ref type="bibr">GoogLeNet [Szegedy et al., 2015]</ref>, and ResNet50 <ref type="bibr" target="#b9">[He et al., 2016]</ref>) in model size and complexity. <ref type="table" target="#tab_0">Table 10</ref> shows that the JLML has both the 2 nd smallest model size (7.2 million parameters) and the 2 nd smallest FLOPs (1.54×10 9 ), although containing more streams (5 vs. 1 in all other CNNs) and more layers (39, more than all except ResNet50).  (II) Importance of Branch Independence. We evaluated the importance of branch independence by comparing our Multi-Loss design with a UniLoss design that merges two branches into a single loss <ref type="bibr" target="#b4">[Cheng et al., 2016]</ref>. <ref type="table" target="#tab_0">Table 12</ref> shows that the proposed MultiLoss model significantly improves the discriminative power of global and local re-id features, e.g. with Rank-1 increase of 9.0%(85.1-76.1) (SQ) and 6.0%(89.7-83.7) (MQ); and mAP improvement of 13.3%(65.5-52.2) (SQ) and 11.7%(74.5-62.8) (MQ). This shows that branch independence plays a critical role in joint learning of multi-loss classification for effective feature optimisation. One plausible reason is due to the negative effect of a single loss imposed on the learning behaviour of both branches, caused by the potential divergence in discriminative features in different context (local and global). This is shown by the significant performance degradation of both global and local features when the UniLoss model is imposed. (III) Benefits from Shared Low-Level Features. We evaluated the effects of interaction between global and local branches introduced by the shared conv layer (common ground) by deliberately removing it and then comparing the re-id performance. <ref type="table" target="#tab_0">Table 13</ref> shows the benefits from jointly learning low-level features in the common conv layers, e.g. improving Rank-1 by 1.9%(85.1-83.2) / 1.4%(89.7-88.3) and mAP by 2.4%(65.5-63.1) / 2.4%(74.5-72.1) for single-/multiquery re-id. This confirms a similar finding as in multi-task learning study <ref type="bibr" target="#b1">[Argyriou et al., 2007]</ref>.  the full JLML feature. <ref type="table" target="#tab_0">Table 15</ref> shows that L1 and L2 generate very similar and competitive re-id matching accuracies. This suggests the flexibility of the JLML model in adopting generic matching metrics. (VI) Effects of Body Parts Number. We evaluated the sensitivity of local decomposition, i.e. body parts number m. <ref type="table" target="#tab_0">Table 16</ref> shows that the decomposition of 4 body-parts is the optimal choice, approximately corresponding to head+shoulder, upper-body, upper-leg and lower-leg ( <ref type="figure" target="#fig_4">Figure 4</ref>). We evaluated the complementary effects of the JLML deep features and conventional supervised metric learning (XQDA <ref type="bibr" target="#b10">[Liao et al., 2015]</ref>, KISSME <ref type="bibr" target="#b9">[Koestinger et al., 2012]</ref>, and CRAFT <ref type="bibr" target="#b4">[Chen et al., 2017b]</ref>). Results from <ref type="table" target="#tab_0">Table 17</ref> show that: (1) Given strong deep learning features such as JLML, additional distance metric learning does not benefit further from the same training data. (2) Moreover, it may even suffer from some adversary effect.</p><p>(VIII) Local Features vs. Global Features. A strength of the local features is the capability of mitigating misalignment and occlusion, as compared to the global features. This is inherently learned from data by the JLML local branch. <ref type="figure">Figure 5</ref> shows the single-query re-id results on six randomly selected probe persons with misalignment and/or occlusion. It is evident that the local features achieve better re-id matching ranks than the global counterparts in most cases. This clearly demonstrates the robustness of local features against the misalignment of and occlusion within a person bounding box.</p><p>(IX) Feature Extraction Time Cost. The average time for extracting JLML feature is 2.75 milliseconds per image (364 images per second) on a Nvidia Pascal P100 GPU card.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we presented a novel Joint Learning of Multi-Loss (JLML) CNN model (JLML-ResNet39) for person reidentification feature learning. In contrast to existing re-id approaches that often employ either global or local appearance features alone, the proposed model is capable of extracting and exploiting both and maximising their correlated complementary effects by learning discriminative feature representations in different context subject to multi-loss classification objectives in a unified framework. This is made possible by the proposed JLML-ResNet39 architecture design. Moreover, we introduce a structured sparsity based feature selective learning mechanism to reduce feature redundancy and further improve the joint feature selections. Extensive comparative evaluations on five re-id benchmark datasets were conducted to validate the advantages of the proposed JLML model over a wide range of the state-of-the-art methods on both manually labelled and more challenging auto-detected person images. We also provided component evaluations and analysis of model performance in order to give insights on the model design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Joint Learning Multi-Loss (JLML) CNN model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Group sparsity regularisations on fc layer parameter matrices (WG for the global branch and WL for the local branch) for selectively learning feature representations. Solid and dashed rectangles denote 2 norm and 1 norm respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>,GRID [Loy et al.,  2009], CUHK01<ref type="bibr" target="#b9">[Li et al., 2012]</ref>,CUHK03 [Li et al., 2014], and Market-1501<ref type="bibr" target="#b24">[Zheng et al., 2015]</ref>.Figure 3shows some examples of person bounding box images from these datasets. The datasets are collected by different data sampling protocols from different environments, where:(a) VIPeR has one image per person per view, with low-resolution under severe lighting change. (b) GRID provides one image per person per view, with additional images for 775 distracting persons under very poor lighting from underground stations. (c) CUHK01 contains two images person per view from a university campus. (d) CUHK03 consists of up to five images per person per view, obtained by both manually labelled and auto-detected person bounding boxes with the latter posing a more challenging re-id task due to detection bounding box misalignment and occlusion. (e) Market-1501 has variable numbers of images per person per view captured from a supermarket, with all bounding boxes automatically detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Example cross-view image pairs from five re-id datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visualisation of the optimal body part decomposition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>We adopt the Residual CNN unit<ref type="bibr" target="#b9">[He et al., 2016]</ref> as the JLML's building blocks due to its capacity for deeper model design whilst retaining a smaller model parameter size 3 . Specifically, we customise the ResNet50 architecture in both layer and filter numbers and design the JLML model as a 39 layers ResNet (JLML-ResNet39) tailored for re-id tasks. The configuration of JLML-ResNet39 is given inTable 1. Note that, the ReLU rectification non-linearity<ref type="bibr" target="#b9">[Krizhevsky et al., 2012]</ref> after each conv layer is omitted for brevity. Feature Selection. To optimise JLML model learning robustness against noise and diverse data source, we introduce a feature selection capability in JLML by a structure sparsity induced regularization<ref type="bibr" target="#b9">[Kong et al., 2014;</ref>  </figDesc><table><row><cell></cell><cell cols="7">JLML-ResNet39. MP: Max-Pooling; AP: Average-</cell></row><row><cell cols="8">Pooling; S: Stride; SL: Slice; CA: Concatenation; G: Global; L:</cell></row><row><cell>Local.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Layer # Layer Output Size Global Branch</cell><cell cols="3">Local Branch</cell></row><row><cell>1</cell><cell>conv1</cell><cell>112×112</cell><cell></cell><cell cols="4">3×3, 32, S-2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">3×3 MP, S-2 SL-4, 2×2 MP, S-1</cell></row><row><cell>9</cell><cell>conv2 x</cell><cell>G: 56×56 L: 28×56</cell><cell cols="2"> 1×1, 32    3×3, 32  ×3</cell><cell cols="2">  </cell><cell> 1×1, 16  3×3, 16 ×3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1×1, 64</cell><cell></cell><cell></cell><cell>1×1, 32</cell></row><row><cell>9</cell><cell>conv3 x</cell><cell>G: 28×28 L: 14×28</cell><cell>  </cell><cell>1×1, 64 3×3, 64 1×1, 128   ×3</cell><cell cols="2">  </cell><cell> 1×1, 32 3×3, 32 1×1, 64  ×3</cell></row><row><cell>9</cell><cell>conv4 x</cell><cell>G: 14×14 L: 7×14</cell><cell>  </cell><cell>1×1, 128  3×3, 128 1×1, 256  ×3</cell><cell>  </cell><cell cols="2"> 1×1, 128 1×1, 64 3×3, 64  ×3</cell></row><row><cell>9</cell><cell>conv5 x</cell><cell>G: 7×7 L: 4×7</cell><cell>  </cell><cell>1×1, 256  3×3, 256 1×1, 512  ×3</cell><cell>  </cell><cell cols="2"> 1×1, 128 3×3, 128 1×1, 256  ×3</cell></row><row><cell>1</cell><cell>fc</cell><cell>1×1</cell><cell></cell><cell>7×7 AP</cell><cell cols="3">4×7 AP, CA-4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1×1, 512</cell><cell></cell><cell></cell><cell>1×1, 512</cell></row><row><cell>1</cell><cell>fc</cell><cell>1×1</cell><cell></cell><cell>ID#</cell><cell></cell><cell></cell><cell>ID#</cell></row><row><cell cols="3">Network Construction.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, triplet ranking<ref type="bibr" target="#b4">[Cheng et al., 2016]</ref>, or both<ref type="bibr" target="#b20">[Wang et al., 2016a;</ref><ref type="bibr" target="#b3">Chen et al., 2017a</ref>]. Our JLML model training does not use any labelled negative pairs inherent to all person re-id training data, and we extract identity class labels from only positive pairs. The motivations for our JLML classification loss based learning are: (i) Significantly simplified training data batch construction, e.g. random sampling with no notorious tricks required, as shown by other deep classification methods<ref type="bibr" target="#b9">[Krizhevsky et al., 2012]</ref>. This makes our JLML model more scalable in real-world applications with very large training population sizes when available. This also eliminates the undesirable need for carefully forming pairs and/or triplets in preparing</figDesc><table /><note>re-id training splits, as in most existing methods, due to the inherent imbalanced negative and positive pair size distribu- tions. (ii) Visual psychophysical findings suggest that rep- resentations optimised for classification tasks generalise well to novel categories</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="7">: Settings of person re-id datasets. TS: Test Setting; SS:</cell></row><row><cell cols="7">Single-Shot; MS: Multi-Shot. SQ: Single-Query; MQ: Multi-Query.</cell></row><row><cell cols="7">Dataset Cams IDs Train IDs Test IDs Labelled Detected TS</cell></row><row><cell>VIPeR</cell><cell>2 632</cell><cell>316</cell><cell>316</cell><cell>1,264</cell><cell>0</cell><cell>SS</cell></row><row><cell>GRID</cell><cell>8 250</cell><cell>125</cell><cell>125</cell><cell>1,275</cell><cell>0</cell><cell>SS</cell></row><row><cell cols="5">CUHK01 2 971 871/485 100/486 1,942</cell><cell>0</cell><cell>SS/MS</cell></row><row><cell cols="3">CUHK03 6 1,467 1,367</cell><cell cols="3">100 14,097 14,097</cell><cell>SS</cell></row><row><cell>Market</cell><cell cols="2">6 1,501 751</cell><cell>750</cell><cell>0</cell><cell cols="2">32,668 SQ/MQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>].</cell></row><row><cell cols="7">Again, we reported the results averaged over 10 random trials</cell></row><row><cell cols="7">for either split. On GRID, the training/test split were 125/125</cell></row><row><cell cols="7">with 775 distractor people included in the test gallery. We</cell></row><row><cell cols="7">used the benchmarking 10 people splits [Loy et al., 2009]</cell></row><row><cell cols="7">and the averaged performance. On CUHK03, following [Li</cell></row><row><cell cols="7">et al., 2014] we repeated 20 times of random 1260/100 train-</cell></row><row><cell cols="7">ing/test splits and reported the averaged accuracies under the</cell></row><row><cell cols="7">single-shot evaluation setting. On Market-1501, we used the</cell></row><row><cell cols="4">standard training/test split (750/751)</cell><cell></cell><cell></cell><cell></cell></row></table><note>Evaluation Protocol. We adopted the standard supervised re-id setting to evaluate the proposed JLML model (Sec. 4.1). The training and test data splits and testing settings of each dataset is given in Table 2. Specifically, on VIPeR, we split randomly the whole population (632 people) into two halves: One for training (316) and another for testing (316). We re- peated 10 trials of random people splits and used the averaged results. On CUHK01, we considered two training/test splits: 485/486 [Liao et al., 2015] and 871/100 [Ahmed et al., 2015</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>. They range</cell></row></table><note>Implementation. We used the Caffe framework [Jia et al., 2014] for our JLML model implementation. We started by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Person re-id method categorisation by features and metrics. Cat: Category; DL: Deep Learning; CPSL: Camera-Pair Specific Learning; DVM: Deep Verification Metric; DVM,L2: Ensemble of DVM and L2; CHS: Fusion of Colour, HOG, SILPT features.</figDesc><table><row><cell>Cat</cell><cell>Method</cell><cell cols="2">Feature Hand-Crafted</cell><cell>DL</cell><cell cols="2">Metric CPSL Generic</cell></row><row><cell></cell><cell>XQDA [Liao et al., 2015]</cell><cell>LOMO</cell><cell></cell><cell>-</cell><cell>XQDA</cell><cell>-</cell></row><row><cell>A</cell><cell>GOG [Matsukawa et al., 2016b] NFST [Zhang et al., 2016]</cell><cell>GOG LOMO, KCCA</cell><cell></cell><cell>--</cell><cell>XQDA NSFT</cell><cell>--</cell></row><row><cell></cell><cell>SCS [Chen et al., 2016]</cell><cell>CHS</cell><cell></cell><cell>-</cell><cell>SCS</cell><cell>-</cell></row><row><cell></cell><cell>DCNN+ [Ahmed et al., 2015]</cell><cell>-</cell><cell></cell><cell cols="2">DCNN+ DVM</cell><cell>-</cell></row><row><cell>B</cell><cell>X-Corr [Subramaniam et al., 2016]</cell><cell>-</cell><cell></cell><cell cols="2">X-Corr DVM</cell><cell>-</cell></row><row><cell></cell><cell>MTDnet [Chen et al., 2017a]</cell><cell>-</cell><cell cols="3">MTDnet DVM, L2</cell><cell>-</cell></row><row><cell></cell><cell>S-CNN [Varior et al., 2016]</cell><cell>-</cell><cell></cell><cell>S-CNN</cell><cell>-</cell><cell>L2</cell></row><row><cell>C</cell><cell>DGD [Xiao et al., 2016] MCP [Cheng et al., 2016]</cell><cell>--</cell><cell></cell><cell>DGD MCP</cell><cell>--</cell><cell>L2 L2</cell></row><row><cell></cell><cell>JLML (Ours)</cell><cell>-</cell><cell></cell><cell>JLML</cell><cell>-</cell><cell>L2</cell></row><row><cell cols="7">pre-training the JLML model on ImageNet (ILSVRC2012).</cell></row><row><cell cols="7">Subsequently, for CUHK03 or Market, we used only their</cell></row><row><cell cols="7">own training data for model fine-tuning, i.e. ImageNet →</cell></row><row><cell cols="7">CUHK03/Market; For CUHK01 or VIPeR or GRID, we pre-</cell></row><row><cell cols="7">trained JLML on CUHK03+Market (whole datasets), and</cell></row><row><cell cols="7">then fine-tuned on their respective training images, i.e. Im-</cell></row><row><cell cols="7">ageNet → CUHK03+Market → CUHK01 / VIPeR / GRID.</cell></row><row><cell cols="7">All input person images were resized to 224 × 224 in pixel.</cell></row><row><cell cols="7">For local branch, according to a coarse body part layout we</cell></row><row><cell cols="7">evenly decomposed the whole shared convolutional feature</cell></row><row><cell cols="7">maps (i.e. the entire image) into four (m = 4) horizontal</cell></row><row><cell cols="7">strip-regions. We used the same parameter settings (sum-</cell></row><row><cell cols="2">marised in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>JLML training parameters. BLR: base learning rate; LRP:</figDesc><table><row><cell cols="6">learning rate policy; MOT: momentum; IT: iteration; BS: batch size.</cell></row><row><cell cols="2">Parameter BLR</cell><cell>LRP</cell><cell>MOT</cell><cell>IT #</cell><cell>BS</cell></row><row><cell>Pre-train</cell><cell cols="2">0.01 step (0.1, 100K)</cell><cell>0.9</cell><cell cols="2">300K 32</cell></row><row><cell>Train</cell><cell>0.01</cell><cell>step (0.1, 20K)</cell><cell>0.9</cell><cell>50K</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>CUHK03 evaluation. 1 st /2 nd best in red/blue.</figDesc><table><row><cell>Cat</cell><cell cols="9">Annotation Rank (%) R1 R5 R10 R20 R1 R5 R10 R20 Labelled Detected</cell></row><row><cell></cell><cell cols="9">XQDA 55.2 77.1 86.8 83.1 46.3 78.9 83.5 93.2</cell></row><row><cell>A</cell><cell>GOG</cell><cell cols="8">67.3 91.0 96.0 -65.5 88.4 93.7 -</cell></row><row><cell></cell><cell>NSFT</cell><cell cols="8">62.5 90.0 94.8 98.1 54.7 84.7 94.8 95.2</cell></row><row><cell></cell><cell cols="9">DCNN+ 54.7 86.5 93.9 98.1 44.9 76.0 83.5 93.2</cell></row><row><cell>B</cell><cell cols="9">X-Corr 72.4 95.5 -98.4 72.0 96.0 -98.2</cell></row><row><cell></cell><cell cols="5">MTDnet 74.7 96.0 97.5 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>S-CNN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">-68.1 88.1 94.6 -</cell></row><row><cell>C</cell><cell>DGD</cell><cell cols="2">75.3 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="9">JLML 83.2 98.0 99.4 99.8 80.6 96.9 98.7 99.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Market-1501 evaluation. 1 st /2 nd best in red/blue. All person bounding box images were auto-detected.</figDesc><table><row><cell>Cat</cell><cell>Query Type Measure (%)</cell><cell cols="2">Single-Query R1 mAP</cell><cell cols="2">Multi-Query R1 mAP</cell></row><row><cell></cell><cell>XQDA</cell><cell>43.8</cell><cell>22.2</cell><cell>54.1</cell><cell>28.4</cell></row><row><cell>A</cell><cell>SCS</cell><cell>51.9</cell><cell>26.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>NFST</cell><cell>61.0</cell><cell>35.6</cell><cell>71.5</cell><cell>46.0</cell></row><row><cell>C</cell><cell>S-CNN JLML</cell><cell>65.8 85.1</cell><cell>39.5 65.5</cell><cell>76.0 89.7</cell><cell>48.4 74.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>CUHK01 evaluation. 1 st /2 nd best in bold/typewriter.</figDesc><table><row><cell>Cat</cell><cell cols="7">Split Rank (%) R1 R5 R10 R20 R1 R5 R10 R20 871/100 split 486/485 split</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Single-Shot Testing Setting</cell></row><row><cell>A</cell><cell>GOG</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-57.8 79.1 86.2 92.1</cell></row><row><cell></cell><cell cols="3">DCNN+ 65.0 -</cell><cell>-</cell><cell cols="3">-47.5 71.6 80.3 87.5</cell></row><row><cell>B</cell><cell cols="7">X-Corr 81.2 97.3 -98.6 65.0 89.7 -94.4</cell></row><row><cell></cell><cell cols="5">MTDnet 78.5 96.5 97.5 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DGD</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-66.6 -</cell><cell>-</cell><cell>-</cell></row><row><cell>C</cell><cell>MCP</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-53.7 84.3 91.0 96.3</cell></row><row><cell></cell><cell cols="7">JLML 87.0 97.2 98.6 99.4 69.8 88.4 93.3 96.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Multi-Shot Testing Setting</cell></row><row><cell></cell><cell>XQDA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-63.2 83.9 90.0 94.2</cell></row><row><cell>A</cell><cell>GOG</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-67.3 86.9 91.8 95.9</cell></row><row><cell></cell><cell>NFST</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-69.1 86.9 91.8 95.4</cell></row><row><cell>C</cell><cell cols="7">JLML 91.2 98.4 99.2 99.8 76.7 92.6 95.6 98.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>VIPeR evaluation. 1 st /2 nd best in red/blue.</figDesc><table><row><cell>Cat</cell><cell>Rank (%)</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell><cell>R20</cell></row><row><cell></cell><cell>XQDA</cell><cell>40.0</cell><cell>68.1</cell><cell>80.5</cell><cell>91.1</cell></row><row><cell>A</cell><cell>GOG NFST</cell><cell>49.7 51.1</cell><cell>-82.1</cell><cell>88.7 90.5</cell><cell>94.5 95.9</cell></row><row><cell></cell><cell>SCS</cell><cell>53.5</cell><cell>82.6</cell><cell>91.5</cell><cell>96.7</cell></row><row><cell>B</cell><cell>DCNN+ MTDnet</cell><cell>34.8 47.5</cell><cell>63.6 73.1</cell><cell>75.6 82.6</cell><cell>84.5 -</cell></row><row><cell></cell><cell>MCP</cell><cell>47.8</cell><cell>74.7</cell><cell>84.8</cell><cell>91.1</cell></row><row><cell>C</cell><cell>DGD</cell><cell>38.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>JLML</cell><cell>50.2</cell><cell>74.2</cell><cell>84.3</cell><cell>91.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>GRID evaluation. 1 st /2 nd best in red/blue.</figDesc><table><row><cell>Cat</cell><cell>Rank (%)</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell><cell>R20</cell></row><row><cell></cell><cell>XQDA</cell><cell>16.6</cell><cell>33.8</cell><cell>41.8</cell><cell>52.4</cell></row><row><cell>A</cell><cell>GOG</cell><cell>24.7</cell><cell>47.0</cell><cell>58.4</cell><cell>69.0</cell></row><row><cell></cell><cell>SCS</cell><cell>24.2</cell><cell>44.6</cell><cell>54.1</cell><cell>65.2</cell></row><row><cell>B</cell><cell>X-Corr</cell><cell>19.2</cell><cell>38.4</cell><cell>53.6</cell><cell>66.4</cell></row><row><cell>C</cell><cell>JLML</cell><cell>37.5</cell><cell>61.4</cell><cell>69.4</cell><cell>77.4</cell></row><row><cell cols="4">4.2 CNN Architecture Comparisons</cell><cell></cell><cell></cell></row><row><cell cols="6">We compared the proposed JLML-ResNet39 model with</cell></row><row><cell cols="6">four seminal classification CNN architectures (Alexnet</cell></row><row><cell cols="6">[Krizhevsky et al., 2012], VGG16 [Simonyan and Zisserman</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell cols="5">: Comparisons of model size and complexity. FLOPs: the</cell></row><row><cell cols="5">number of FLoating-point OPerations; PN: Parameter Number.</cell></row><row><cell>Model</cell><cell>FLOPs</cell><cell cols="3">PN (million) Depth Stream #</cell></row><row><cell>AlexNet</cell><cell>7.25×10 8</cell><cell>58.3</cell><cell>7</cell><cell>1</cell></row><row><cell>VGG16</cell><cell>1.55×10 10</cell><cell>134.2</cell><cell>16</cell><cell>1</cell></row><row><cell>ResNet50</cell><cell>3.80×10 9</cell><cell>23.5</cell><cell>50</cell><cell>1</cell></row><row><cell>GoogLeNet</cell><cell>1.57×10 9</cell><cell>6.0</cell><cell>22</cell><cell>1</cell></row><row><cell cols="2">JLML-ResNet39 1.54×10 9</cell><cell>7.2</cell><cell>39</cell><cell>5</cell></row><row><cell cols="3">4.3 Further Analysis and Discussions</cell><cell></cell><cell></cell></row><row><cell cols="5">We further examined the component effects of our JLML</cell></row><row><cell cols="4">model on Market-1501 in the following aspects.</cell><cell></cell></row><row><cell cols="5">(I) Complementary Benefits of Global and Local Fea-</cell></row><row><cell cols="5">tures. We evaluated the complementary effects of our jointly</cell></row><row><cell cols="5">learned local and global features by comparing their individ-</cell></row><row><cell cols="5">ual re-id performance against that of the joint features. Table</cell></row><row><cell cols="5">11 shows: (i) Any of the two feature representations alone</cell></row><row><cell cols="5">is competitive for re-id, e.g. the local JLML feature sur-</cell></row><row><cell cols="5">passes S-CNN (Table 6) by Rank-1 13.1%(78.9-65.8) (SQ)</cell></row><row><cell cols="5">and 10.4%(86.4-76.0) (MQ); and by mAP 18.3%(57.8-39.5)</cell></row><row><cell cols="5">(SQ) and 20.0%(68.4-48.4) (MQ). (ii) A further performance</cell></row><row><cell cols="5">gain is obtained from the joint feature representation, yielding</cell></row><row><cell cols="5">further 6.2%(85.1-78.9) (SQ) and 3.3%(89.7-86.4) (MQ) in</cell></row><row><cell cols="5">Rank-1 increase, and 7.7%(65.5-57.8) (SQ) and 6.1%(74.5-</cell></row><row><cell cols="5">68.4) (MQ) in mAP boost. These results show the comple-</cell></row><row><cell cols="5">mentary advantages of jointly learning the local and global</cell></row><row><cell cols="4">features in different context using the JLML model.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Complementary benefits of global and local features.</figDesc><table><row><cell>Query Type</cell><cell cols="2">Single-Query</cell><cell cols="2">Multi-Query</cell></row><row><cell>Measure (%)</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>JLML (Global)</cell><cell>77.4</cell><cell>56.0</cell><cell>85.0</cell><cell>66.0</cell></row><row><cell>JLML (Local)</cell><cell>78.9</cell><cell>57.8</cell><cell>86.4</cell><cell>68.4</cell></row><row><cell>JLML (joint)</cell><cell>85.1</cell><cell>65.5</cell><cell>89.7</cell><cell>74.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Importance of branch independence.</figDesc><table><row><cell>Loss</cell><cell>Query Type Measure (%)</cell><cell cols="3">Single-Query Multi-Query R1 mAP R1 mAP</cell></row><row><cell></cell><cell cols="2">Global Feature 58.3</cell><cell>31.7</cell><cell>70.4 43.2</cell></row><row><cell>UniLoss</cell><cell>Local Feature</cell><cell>46.3</cell><cell>26.3</cell><cell>58.0 34.0</cell></row><row><cell></cell><cell>Full</cell><cell>76.1</cell><cell>52.2</cell><cell>83.7 62.8</cell></row><row><cell></cell><cell cols="2">Global Feature 77.4</cell><cell>56.0</cell><cell>85.0 66.0</cell></row><row><cell>MultiLoss</cell><cell>Local Feature</cell><cell>78.9</cell><cell>57.8</cell><cell>86.4 68.4</cell></row><row><cell></cell><cell>Full</cell><cell>85.1</cell><cell>65.5</cell><cell>89.7 74.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Benefits from shared low-level features.</figDesc><table><row><cell>Query Type</cell><cell cols="2">Single-Query</cell><cell cols="2">Multi-Query</cell></row><row><cell>Measure (%)</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>Without Shared Feature</cell><cell>83.2</cell><cell>63.1</cell><cell>88.3</cell><cell>72.1</cell></row><row><cell>With Shared Feature</cell><cell>85.1</cell><cell>65.5</cell><cell>89.7</cell><cell>74.5</cell></row><row><cell cols="5">(IV) Effects of Selective Feature Learning. We evaluated</cell></row><row><cell cols="5">the contribution of our structured sparsity based Selective</cell></row><row><cell cols="5">Feature Learning (SFL) (Eq. (6)). Table 14 shows that our</cell></row><row><cell cols="5">SFL mechanism can bring additional re-id matching bene-</cell></row><row><cell cols="5">fits, e.g. improving Rank-1 rate by 1.7%(85.1-83.4) (SQ) and</cell></row><row><cell cols="5">1.0%(89.7-88.7) (MQ); and mAP by 1.7%(65.5-63.8) (SQ)</cell></row><row><cell>and 1.6%(74.5-72.9) (MQ).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(V) Choice of Generic Matching Metrics. We evaluated the choice of generic matching distances on person re-id using</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Effects of selective feature learning (SFL).</figDesc><table><row><cell>Query Type</cell><cell cols="2">Single-Query</cell><cell cols="2">Multi-Query</cell></row><row><cell>Measure (%)</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>Without SFL</cell><cell>83.4</cell><cell>63.8</cell><cell>88.7</cell><cell>72.9</cell></row><row><cell>With SFL</cell><cell>85.1</cell><cell>65.5</cell><cell>89.7</cell><cell>74.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Effects of generic matching metrics.</figDesc><table><row><cell>Query-Type</cell><cell cols="2">Single-Query</cell><cell cols="2">Multi-Query</cell></row><row><cell>Measure (%)</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>L1</cell><cell>84.9</cell><cell>65.3</cell><cell>89.2</cell><cell>74.6</cell></row><row><cell>L2</cell><cell>85.1</cell><cell>65.5</cell><cell>89.7</cell><cell>74.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Effects of body parts number.</figDesc><table><row><cell>Query-Type</cell><cell cols="2">Single-Query</cell><cell cols="2">Multi-Query</cell></row><row><cell>Measure (%)</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>2</cell><cell>83.9</cell><cell>64.4</cell><cell>88.8</cell><cell>72.9</cell></row><row><cell>4</cell><cell>85.1</cell><cell>65.5</cell><cell>89.7</cell><cell>74.5</cell></row><row><cell>6</cell><cell>83.4</cell><cell>62.6</cell><cell>88.5</cell><cell>71.8</cell></row><row><cell>8</cell><cell>82.3</cell><cell>61.3</cell><cell>87.4</cell><cell>70.7</cell></row><row><cell>10</cell><cell>81.7</cell><cell>60.4</cell><cell>87.2</cell><cell>69.8</cell></row><row><cell cols="5">(VII) Complementary Effects between JLML Deep Fea-</cell></row><row><cell cols="4">tures and Supervised Metric Learning.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 :</head><label>17</label><figDesc>Complementary of JLML features and metric learning.</figDesc><table><row><cell>Query-Type</cell><cell cols="2">Single-Query</cell><cell cols="2">Multi-Query</cell></row><row><cell>Measure (%)</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>KISSME</cell><cell>82.1</cell><cell>61.4</cell><cell>87.5</cell><cell>70.2</cell></row><row><cell>XQDA</cell><cell>82.6</cell><cell>63.2</cell><cell>88.2</cell><cell>72.4</cell></row><row><cell>CRAFT</cell><cell>77.9</cell><cell>56.4</cell><cell>-</cell><cell>-</cell></row><row><cell>L2</cell><cell>85.1</cell><cell>65.5</cell><cell>89.7</cell><cell>74.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In person re-id context, the learning constraint refers to the image person identity label supervision.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The choice of base network is independent of our JLML model design. Other types, e.g. GoogLeNet[Szegedy et al., 2015]  orVGG-Net [Simonyan and Zisserman, 2015], can be readily applied in our model.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Argyriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multi-task deep network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Jinjun Wang, and Nanning Zheng. Person re-identification by multichannel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
	<note>Person re-identification by camera correlation aware feature augmentation. Local features: 1,2,3,8,9 Global features: 1,2,8,10,24 Local features: 1,6,8,11,15 Global features: 1,5,34,38,39 Local features: 1,5,34</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Global features: 1,7,76</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Global features: 1,11,15</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Each probe may have multiple truth matches in the gallery. Smaller numbers mean better ranking performances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Farenzena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Figure 5: Comparing the gallery true match ranks of each probe image</title>
		<editor>Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="449" to="467" />
		</imprint>
	</monogr>
	<note>CVPR. Gong et al., 2014. Person re-identification</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao ; Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Amit Adam, and Ehud Rivlin. Color invariants for person reidentification</title>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<editor>Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner</editor>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tao Xiang, and Shaogang Gong</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Chen Change Loy</orgName>
		</respStmt>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kin-Man Lam, and Yisheng Zhong. Person re-identification by unsupervised video matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Matsukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Forest before trees: The precedence of global features in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navon ; David Navon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="383" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep neural networks with inexact matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Arulkumar Subramaniam, Moitreya Chatterjee, and Anurag Mittal</title>
		<editor>Szegedy et al., 2015] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">766</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Varior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Rahul Rama Varior, Mrinal Haloi, and Gang Wang</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view clustering and feature learning via structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of generative topic saliency for person reidentification</title>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Highly efficient regression for scalable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shaogang Gong, Xiatian Zhu, and Tao Xiang</title>
		<imprint>
			<publisher>Hanxiao Wang</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fei Xiong, Mengran Gou, Octavia Camps, and Mario Sznaier. Person re-identification using kernel-based metric learning methods</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2501" to="2514" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wei-Shi Zheng, Shaogang Gong, and Tao Xiang. Reidentification by relative distance comparison</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013-03" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="653" to="668" />
		</imprint>
	</monogr>
	<note>Unsupervised salience learning for person re-identification</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
