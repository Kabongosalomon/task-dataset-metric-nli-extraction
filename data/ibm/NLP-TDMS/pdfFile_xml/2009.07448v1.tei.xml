<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Question Directed Graph Attention Network for Numerical Reasoning over Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunlong</forename><surname>Chen</surname></persName>
							<email>kunlong.ckl@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xu</surname></persName>
							<email>weidi.xwd@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zou</forename><surname>Xiaochuan</surname></persName>
							<email>xiaochuan.zxc@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
							<email>le.song@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
							<email>taifeng.wang@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qi</surname></persName>
							<email>yuan.qi@antgroup.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ant Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Computing Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Question Directed Graph Attention Network for Numerical Reasoning over Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine reading comprehension (MRC) aims to develop AI models that can answer questions for text documents. Recently, the performance of MRC in public datasets has been improved dramatically due to the advanced pre-trained models, such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, RoBERTa  and ALBERT <ref type="bibr" target="#b18">(Lan et al., 2019)</ref>.</p><p>However, pre-trained models are not explicitly aware of the concepts of numerical reasoning since numeracy supervision signals are rarely available during pretraining. The representations from these pre-trained models fall short in their ability to support downstream numerical reasoning. Yet such ability is critical for the comprehension of financial news and scientific articles, since basic numerical operations, such as addition, subtraction, sorting and counting, need to be conducted to extract the essential information <ref type="bibr" target="#b6">(Dua et al., 2019)</ref>.</p><p>Recently, <ref type="bibr" target="#b6">Dua et al. (2019)</ref> proposed a numericallyaware QANet (NAQANet), which treats the span extractions, counting, and numerical addition/subtraction separately. However, this work is preliminary in the sense that the model neglects the relative magnitude between numbers. To improve this method, <ref type="bibr" target="#b23">Ran et al. (2019)</ref> proposed NumNet, which constructs a number comparison graph that encodes the relative magnitude * Corresponding author information between numbers on directed edges. Although NumNet achieves superior performance than other numerically-aware models <ref type="bibr" target="#b11">(Hu et al., 2019a;</ref><ref type="bibr" target="#b0">Andor et al., 2019;</ref><ref type="bibr" target="#b8">Geva et al., 2020;</ref>, we argue that NumNet is insufficient for sophisticated numerical reasoning, since it lacks two critical ingredients for numerical reasoning:</p><p>1. Number Type and Entity Mention. The number comparison graph in NumNet is not able to identify different number types, and lacks the information of entities mentioned in the document that connect the number nodes.</p><p>2. Direct Interaction with Question. The graph reasoning module in NumNet leaves out the direct question representation, which may encounter difficulties in locating important numbers directed by the question as the pivot for numerical reasoning.</p><p>The number type and entity information play essential roles in numerical comprehension and reasoning. As per the study in the cognitive system -"this abstract, notation-independent appreciation of numbers develops gradually over the first several years of life ... human infants appreciate numerical quantities at a non-symbolic level: They know approximately how many objects they see before them even though they do not understand number words or Arabic numerals.", the concept of discrete number is gradually developed through the reallife experience <ref type="bibr" target="#b1">(Cantlon et al., 2009)</ref>. The association among the numbers and entities is a strong regularization for learning the numerical reasoning model: the comparison and addition/subtraction between numbers are typically applied to those with the same type or referring to the same entity. To illustrate it, we show two concrete examples of numerical reasoning over texts in <ref type="table" target="#tab_0">Table 1</ref>. In the first example, a question related to the "population" is being asked. There are 5 "people counting" numbers and 3 "date" numbers. When the type of number is given, the reasoning difficulty is largely reduced if the model learns to extract the "people counting" numbers conditioned on this "population" question. In addition, the entities in the graph provide explicit information on the correlation between the passage and the question. The entities in the question may occur in several sentences in the passage, indicating how each </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>34</head><p>In which quarter did Stephen Gostkowski kick his shortest field goal of the game?</p><p>The Cardinals' east coast struggles continued in the second quarter as quarterback Matt Cassel completed a 15yard touchdown pass to running back Kevin Faulk and an 11-yard touchdown pass to wide receiver Wes Welker, followed by kicker Stephen Gostkowski's 38-yard field goal. In the third quarter, Arizona's deficit continued to climb as Cassel completed a 76-yard touchdown pass to wide receiver Randy Moss, followed by Gostkowski's 35and 24-yard field goal. In the fourth quarter, New England concluded its domination with Gostkowski's 30-yard   <ref type="table" target="#tab_0">Table 1</ref> is illustrated on the left. The red (dark blue) nodes are the numbers (dates) and the others are entities. The edges encode the relations among the numbers and entities: (1) The numbers with the same number type, e.g., date, are wired together. (2) The graph connects the numbers and the entities that are in the same sentence to indicate their co-occurrence. In the first round, the model pays attention to a sub-graph that contains the Spanish and Portuguese entities since they are mentioned in the question. In the update, the model learns to distinguish between the numbers and the dates and extracts the numbers related to the question. In the second round, the representations of the numbers are updated by the messages from the entities as well as the question to conduct the reasoning.</p><p>number is related to each other through these bridging entities, which helps the QA model better collect and aggregate the information for numerical reasoning. We also observe that when the question entities co-occur in a single sentence (the last sentence in this example), this could be a hint that the answer can be derived from that sentence. The second example illustrates the case in span extraction. Similarly, the model is benefited when the correlations between the numbers and "Stephen Gostkowski" are explicitly provided.</p><p>To explicitly integrate the type and entity information into the model, we construct a heterogeneous directed graph where the nodes consist of entities and different types of numbers, and the edges can encode different types of relations. The corresponding graph of the example in <ref type="table" target="#tab_0">Table 1</ref> is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The graph nodes are composed of entities and numbers from both the question and the passage. The numbers of the same type are densely connected with each other. The cooccurred numbers and entities within a sentence are also connected with each other.</p><p>Based on this heterogeneous graph, we propose a question directed graph attention network (QDGAT) for the task of numerical MRC. As the answer-related numbers can be directed by the question, QDGAT incorporates the contextual encoding of the question in the graph reasoning process. More specifically, QDGAT employs a contextual encoder, such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and RoBERTa , to extract the representations of the numbers and entities in both the question and the passage, serving as the initial embeddings of each node in the graph. With the heterogeneous graph, QDGAT learns to collect information from the graph conditioned on the question for numerical reasoning. Each node is also described by a context-aware representation conditioned on the question, and the representations are updated through a message-passing iteration. After multiple iterations of message passing with graph neural networks, QDGAT gradually aggregates the node information to answer the question. In this sense, QDGAT abstracts the representation of passage and question in a way more consistent with human perception and reasoning, making the model produces a more interpretable reasoning pattern.</p><p>We evaluate QDGAT on two benchmark datasets: the DROP dataset <ref type="bibr" target="#b6">(Dua et al., 2019)</ref> which requires Discrete Reasoning Over the content of Paragraph, and a subset of the RACE dataset <ref type="bibr" target="#b17">(Lai et al., 2017)</ref> that contains the number-related questions. Experimental results indicate that QDGAT achieves remarkable performance on the DROP dataset, currently ranked as top 2 for all released models and top 1 for RoBERTa-based models on the public leaderboard. 1 2 Related Work Machine Reading Comprehension. Benefit from recent improvements of pre-trained deep language models like BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, XLNet <ref type="bibr" target="#b28">(Yang et al., 2019)</ref>, a considerable progress of MRC have been made on the annotated datasets such as SQuAD <ref type="bibr" target="#b22">(Rajpurkar et al., 2016)</ref>, RACE <ref type="bibr" target="#b17">(Lai et al., 2017)</ref>, TriviaQA <ref type="bibr" target="#b13">(Joshi et al., 2017)</ref> and so on. To answer complex questions of MRC, a number of neural architectures have been proposed such as Attentive Reader <ref type="bibr" target="#b10">(Hermann et al., 2015)</ref>, BiDAF <ref type="bibr" target="#b24">(Seo et al., 2017)</ref>, Gated Attention Reader (Dhingra et al., 2017), R-NET <ref type="bibr" target="#b26">(Wang et al., 2017)</ref>, QANet <ref type="bibr" target="#b29">(Yu et al., 2018)</ref>, which achieved excellent results on existing datasets. Some recent works (LCGN <ref type="bibr" target="#b12">(Hu et al., 2019b)</ref>, NMNs , NumNet (Ran et al., 2019)) attaching reasoning capabilities to models shows a promising direction. LCGN uses graph neural networks (GNN) conditioned on the input questions to support rational reasoning. NMNs parse the questions into one of several programs, each of which is responsible for specific reasoning ability.</p><p>Numerical Reasoning in MRC. Numerical reasoning has been studied when solving arithmetic word problems (AWP). However, existing AWP models only worked on small datasets, and the arithmetic expression must be clearly given. Numerical reasoning in MRC is more challenging since the numbers and reasoning rules are extracted from raw text, which requires a more sophisticated model. NAQANet improved the output layer of QANet to predict the answers from the arithmetic computation over numbers. In addition to NAQANet, GenBERT <ref type="bibr" target="#b8">(Geva et al., 2020)</ref> injects numerical skills into BERT by generating numerical data.  provides a semantic parser that points to locations in the text that can be used in further numerical operations. BERT-Calculator <ref type="bibr" target="#b0">(Andor et al., 2019)</ref> defines a set of executable programs and learns to choose one to derive numerical answers. NumNet (Ran et al., 2019) uses a numerically-aware graph neural network to encode numbers, which made further progress on the DROP dataset. However, the graph in NumNet contains only numbers and ignores their types and context information which play a key point in numerical reasoning. Our model differs from NumNet in two aspects: (1) We use a heterogeneous graph containing entities and different types of numbers to encode the relations among the entities and numbers, rather than the relations from numerical comparison; (2) We use the question embedding to modulate the attention over graph neighbors and 1 https://leaderboard.allenai.org/drop/submissions/public. We would update the entry to reveal our identities on the leaderboard after the blind review process. update the representation to achieve reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first introduce the machine reading comprehension task requiring numerical reasoning. Then the framework of our model is provided, followed by detailed descriptions about its components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>In the MRC task, each data sample consists of a passage P and a related question Q. The goal of an MRC model is to answer the question according to P . Besides predicting the text spans as in the standard MRC tasks, the answer A in the case of numerical reasoning can also be a number derived from arithmetic computations, such as sorting, counting, addition and subtraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Framework</head><p>The framework of the proposed model is briefly depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. The model is composed of three main components, i.e., a representation extractor module, a reasoning module, and a prediction module. The representation extractor is responsible for semantic comprehension. Upon the extractor, a heterogeneous graph with typed numbers and related entities is constructed. To aggregate the information between the numbers and entities, we propose a question directed graph attention network (QDGAT) to make sophisticated reasoning. This graph attention network directly employs the question Q to manage the message passing over the typed graph.</p><p>Word Representation Extractor. We employ RoBERTa  as the base architecture for the representation of textual inputs. The module takes the passage P and the question Q as input and outputs representation vectors for each token:</p><formula xml:id="formula_0">Q,P = RoBERTa(Q, P ) ,<label>(1)</label></formula><p>where RoBERTa denotes the transformer encoder initialized with RoBERTa parameters,P (Q) denotes the list of the token vectors of size d h in the passage (question). It takes the concatenation of [CLS], Q, [SEP], P and [SEP] as input, and outputs representations of Q and P asQ andP. Graph Construction. This module builds the heterogeneously typed graph from text data. The graph G = (V, E) contains numbers N and entities T as the nodes V = {N, T}, and its edges E encode the information of the number type and the relationship between the numbers and the entities. The details will be clarified in Section 3.3.</p><p>Numerical Reasoning Module. The numerical reasoning module, i.e., QDGAT, is built upon the representation and graph extractor. Based on the graph G = (V, E), the QDGAT network can be formulated as (1) the numbers of the same type are connected with each other by the type-specific edges, (2) the entities and the numbers are connected when they co-occur in a sentence. The reasoning is conditioned on the question explicitly to guide the message propagation over the graph. In each iteration, each node selectively receives the messages from the neighboring nodes with the question representation to update its representation. The derived representations of these nodes are then combined with the RoBERTa output for the final prediction module. The dashed circle means zero vector.</p><p>follows:</p><formula xml:id="formula_1">M Q = W MQ ,<label>(2)</label></formula><formula xml:id="formula_2">M P = W MP ,<label>(3)</label></formula><formula xml:id="formula_3">c = W c MEAN(Q) ,<label>(4)</label></formula><formula xml:id="formula_4">U = QDGAT(G; M P , M Q , c) ,<label>(5)</label></formula><p>where W M ∈ R d h ×d h is a shared projection matrix to obtain the input of QDGAT, MEAN denotes the mean pooling, W c ∈ R d h ×d h projects the averaged vector of the representations in the question to derive c. c is the question language embedding used to direct the reasoning in QDGAT. QDGAT then reasons over the representations (M P , M Q ) and the graph G conditioned on the question command c. Prediction Module The prediction module takes the output of graph reasoning network U for final prediction. At present, the types of answers are generally divided into three categories in NAQANet and NumNet+: (a) span extraction, (b) count, (c) arithmetic expression. We implemented separate modules for these answer types and all of them take the output of graph network U and question embedding c as input. They are specified as follows:</p><p>• Span extraction: There are three span extraction tasks, i.e., single passage span, multiple passage spans, single question span. The probability for single span extraction is derived by the product of the probabilities of the start and end positions in either question or passage. For multiple spans extraction, the probability is constructed referring to <ref type="bibr" target="#b7">(Efrat et al., 2019)</ref>.</p><p>• Count: This problem is regarded as a 10-class classification problem (0-9), which covers about 97% counting problems in the DROP dataset.</p><p>• Arithmetic expression: The answer is derived by an arithmetic computation. In the DROP dataset, only addition and subtraction operations are involved. We achieved this by classifying each number into one of (−1, 0, +1), which is then used as the coefficient of the number in the numerical expression to arrive at the final answer.</p><p>We used a unique classification network to classify the data sample into one of five fine-grained types (T ). And each type solver employs a unique output layer to calculate the conditional answer probability p(A|T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Construction with Typed Number and Entities</head><p>Here, we illustrate how to construct the heterogeneous graph G = (V, E) in our model. NumNet solely concerns the numerical comparisons between numbers by using the directed edges. The graph used in our model differs from NumNet significantly: Rather than modeling the numerical comparison, our graph instead exploits two sources of information, i.e., the type of numbers and the related entities. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the nodes of graph V consists of both entities T and numbers N, both of which are recognized by an external name entity recognition (NER) system 2 . Specifically, the NER software labels each token in the text into one of 21 pre-defined categories. The tokens labeled as NUMBER, PERCENT, MONEY, TIME, DATE, DURATION, ORDINAL are regarded as the numbers. Since DROP dataset contains a lot of samples related to American football games, we also used heuristic rules to extract the numbers of YARD type in the data samples. Besides, we leveraged a number extractor, i.e., word2num 3 , to extract the remaining numbers, which are labeled as NUMBER. All these tokens construct the number set N with 8 number types (V N = (NUMBER, PERCENT, MONEY, TIME, DATE, DURATION, ORDINAL, YARD)). As for other recognized tokens, we map them into the label ENTITY to build the entity set T whose type set V T is ENTITY. In the following, we use t(v) ∈ V N ∪ V T to indicate the type of the node. The type information can directly inform the model to find the numbers related to the question and thus reduces the reasoning difficulty.</p><p>The edges E encode the relationship among the numbers and the entities, which correspond to two situations.</p><p>• The edge between the numbers: An edge e i,j exists between two numbers v i and v j if and only if these two numbers are of the same type in V N . And its relation r i,j = r j,i corresponds to the number type.</p><p>• The edge between the entity and the number: An edge e i,j exists between an entity v i and a number v j if and only if v i and v j co-occur in the same sentence. In this situation, the relation r i,j = r j,i is ENT+DIGIT. The edges in the first situation cluster the same typed numbers together, which provides an evident clue to help to reason over the numbers. In the second situation, we assume that an entity is relevant to a number when they appear closely. This kind of edges roughly indicates the correlations between the numbers and the entities in most cases. On the other hand, the relative magnitude relations in Numnet+ are not considered in our graph since early experiments with these relations did not improve results. Overall, the graph has 9 relations R, i.e., 8 relations for number types and 1 relation for ENT+DIGIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Question Directed Graph Attention Network</head><p>Here, we present the details of the QDGAT function. Based on the heterogeneous graph G, our QDGAT makes context-aware numerical reasoning conditioned on the question, which collects the relational information through multiple iterations of message passing between the numbers and the entities. It dynamically determines which objects to interact with through the edges in the graph, and sends messages through the graph to propagate the relational information. To achieve this, we augment the reasoning module with the contextualized question representation. For instance in the example in <ref type="table" target="#tab_0">Table 1</ref>, the task is to find how many Spanish and Portuguese were injured or killed. The entities and the numbers are explicitly marked and are modeled in a heterogeneous graph, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Our model is able to extract the related entities, i.e., the Spanish and Portuguese, conditioned on c. Among the numbers related to these two entities, a number of them are of date type, while the others are about people. However, only the numbers related to people should be concerned as requested by the question. Then the model reasons over these numbers to derive the expression for the answer calculation.</p><p>Module Input. The graph neural network takes the representations from the extractor as the input. Each node is represented by the corresponding vector in M P and M Q . Formally, when v i is in the passage, the input of node v i is the v i = M P [I P (v i )], where I P returns the index of v i in M P 4 . The collected vectors from the question and the passage construct the input of reasoning module v 0 .</p><p>Question Directed Node Embedding Update. At each iteration t ∈ {1, ...T }, a question directed layer integrates the question information with the current node embedding representations. This step is to mimic the reasoning step of detecting relevant nodes. More specifically, the question, represented by c, is used to direct the information propagation between the nodes (i.e., the numbers and the entities). Each node collects the information from the neighbors with the question command. The role of numbers and entities is not only dependent on the input itself, but also the neighbors and the relations between them. Therefore, we adopt the self-attention layer <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref> to dynamically aggregate the information. The representation is first converted into three spaces denoting the query, key and value, conditioned on c:</p><formula xml:id="formula_5">m t = W t dc g(W f c c) ,<label>(6)</label></formula><formula xml:id="formula_6">x t q = W qv [v t : v 0 ] W qc m t ,<label>(7)</label></formula><p>x</p><formula xml:id="formula_7">t k = W kv [v t : v 0 ] W kc m t ,<label>(8)</label></formula><formula xml:id="formula_8">x t v = W vv [v t : v 0 ] W vc m t ,<label>(9)</label></formula><p>where m t denotes the command vector extracted dynamically from the c with W t dc and W f c ∈ R d h ×2d h , g denotes the ELU activation function <ref type="bibr" target="#b3">(Clevert et al., 2016)</ref>, W qv , W kv and W vv are of size d h × 2d h , W qc , W kc and W vc are of size d h × d h , [a : b] means the concatenation of a and b, and means the element-wise multiplication. These equation include the input v 0 to maintain the original information.</p><p>Directed Graph Attention. At each iteration, this graph attention layer for each node aggregates information from the neighbors of the node. This step is to mimic the reasoning step of selecting the relevant relations to operate on. More specifically, we compute the relatedness between the node i and j, which is measured by summarizing all relations:</p><formula xml:id="formula_9">a t i,j = f ( r∈Ri,j W r a [x t q ,i : x t k,j ]) ,<label>(10)</label></formula><p>where R i,j means the relations between the two nodes, a i,j denotes the attention score of the node i for the node j, W k a is the vector to map the representations into a scalar for the relation r and f denotes the leakyReLU activation function <ref type="bibr" target="#b27">(Xu et al., 2015)</ref>.</p><p>This attention score is used in the message propagation to collect the right amount of information from each neighboring node. In the propagation function, the calculation of the node interaction is as follows:</p><formula xml:id="formula_10">α t i,j = exp(a t i,j ) j ∈Ni exp(a t i,j ) ,<label>(11)</label></formula><formula xml:id="formula_11">x t i = j∈Ni α i,j x v,j ,<label>(12)</label></formula><formula xml:id="formula_12">v t+1 i = W u [v t i ;x t i ] ,<label>(13)</label></formula><p>where N i contains the adjacent nodes of the node i in the G and W u is in R d h ×2d h . With the weight α i,j obtained, the values of neighboring nodes are summarized to derive a new representationx. Finally, the new representation of v is computed by mapping the concatenation of v 0 andx. We denote the node embedding update and the graph attention layers as a function:</p><formula xml:id="formula_13">v t+1 = QDGAT-single(G, v t , c) .<label>(14)</label></formula><p>From the process of this reasoning step, we can see that the module receives the information from the question, which directly manages the message propagation among the numbers and the entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Module Output</head><p>We perform T iterations of the reasoning step of QDGAT-single to perform QDGAT in Equation 5. The output of the last layer v T is obtained for the numbers and entities in U. For other tokens, the representation vectors from the extractor are used. Formally, the calculation of the output U is implemented as follows:</p><formula xml:id="formula_14">U i = M i + v T J(i) , if i-th token ∈ V M i , otherwise<label>(15)</label></formula><p>where J(i) denotes the index of token i in the graph nodes, M denotes the combination of M P and M Q for simplicity. U is then used in the prediction module for the five answer types mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>We performed experiments on the DROP dataset <ref type="bibr" target="#b6">(Dua et al., 2019)</ref>, which was recently released for research on numerical machine reading comprehension (MRC). DROP is constructed by crowd-sourcing questionanswer pairs on passages from Wikipedia, which contains 77,409 / 9,536 / 9,622 samples in the original training / development / testing split. Following the previous work <ref type="bibr" target="#b6">(Dua et al., 2019)</ref>, we used Exact Match (EM) and F1 score as the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We choose publicly available methods (including nonpublished ones on the dataset leaderboard) as our baselines:</p><p>• Semantic parsing models: Syn Dep, OpenIE and SRL <ref type="bibr" target="#b6">(Dua et al., 2019)</ref>. All these models are enhanced versions of KDG <ref type="bibr" target="#b16">(Krishnamurthy et al., 2017)</ref> with different sentence representations.</p><p>• Traditional MRC models: (1) BiDAF, a model that uses a bi-directional attention flow network to obtain a query-aware context representation; (2) QANet, a model that combines convolution and self-attention models to answer the questions; (3) BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, a pre-trained deep Transformer <ref type="bibr" target="#b25">(Vaswani et al., 2017)</ref> model that has improved results on many NLP tasks.</p><p>• MRC models with numerical reasoning module:</p><p>(1) NAQANet <ref type="bibr" target="#b6">(Dua et al., 2019)</ref>, a model that adapts the output layer of QANet to numeric reasoning; (2) ALBERT-Calculator <ref type="bibr" target="#b0">(Andor et al., 2019)</ref>, a model based on ALBERT-xxlarge <ref type="bibr" target="#b19">(Lan et al., 2020</ref>) that picks one of executable programs from a predefined set to derive numerical answers.</p><p>(3) NumNet, a model that embeds numerical properties into the distributed representation by using a GNN on the number graph;</p><p>(4) NumNet+ 5 , an enhanced version of NumNet, which uses a pre-trained RoBERTa model and supports multi-span answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Settings</head><p>We use the large RoBERTa model as the contextual encoder, with 24 layers, 16 attention heads, and 1024 embedding dimensions. This indicates that the hidden size d h is 1024. The model was trained end-to-end for 5 epochs using Adam optimizer (Kingma and Ba, 2015) with a batch size of 16. For the hyperparameters of RoBERTa, the learning rate is 5e-5 and the L2 weight decay is 1e-6. For the other parts, the learning rate is 1e-4 and the L2 weight decay is 5e-5. We perform T = 4 iterations of the graph reasoning step, which performs best in our experiments. We adopt the standard data preprocessing following previous work <ref type="bibr" target="#b23">(Ran et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>The overall experimental results are reported in <ref type="table" target="#tab_2">Table 2</ref>, where the performance of baseline methods is obtained from previous work <ref type="bibr" target="#b6">(Dua et al., 2019;</ref><ref type="bibr" target="#b24">Seo et al., 2017;</ref><ref type="bibr" target="#b23">Ran et al., 2019;</ref><ref type="bibr" target="#b0">Andor et al., 2019)</ref> and the public leaderboard. <ref type="bibr">6</ref> The first three methods in <ref type="table" target="#tab_2">Table 2</ref> are based on either semantic parsing or information extraction, and perform poorly on the numerical MRC task. Traditional MRC methods BiDAF and QANet, which has no numerical reasoning modules, achieve slightly better performance but still far from satisfying. Methods that are customized for numerical reasoning, including NAQANet and NumNet, have achieved significantly better performance in terms of EM and F1 score. Compared to traditional MRC methods, these methods can handle different answer types, e.g., span extraction, counting, and addition/subtraction of numbers.</p><p>Our method QDGAT outperforms all the existing methods, achieving 86.38 F1 score and 83.23 EM on the test set, which narrows the human performance gap to less than 11 points. NumNet+ is the most relevant one to our method, which also leverages a graph neural network as well as the RoBERTa contextual encoder. Compared to NumNet+, QDGAT incorporates the number types and entity mentions into the graph attention network, and directs the graph reasoning process with the question. In this way, our method can better capture the relations between numbers and entities, and also reduce the learning difficulty due to the interaction with the question during the graph reasoning. Experimental results demonstrate the effectiveness of QDGAT, which outperforms NumNet+ by 1.23 in terms of EM and 1.37 in terms of F1 score. Ensembling three of our models with different random seeds and learning rates further improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Analysis</head><p>To examine the impact of different components of QDGAT, we conduct ablation studies and compare the performance in <ref type="table" target="#tab_3">Table 3</ref>. QDGAT NH removes the number type and entity from the graph, and QDGAT NQ removes question direction from QDGAT and instead uses a normal graph convolution message passing mechanism. NumNet+ serves as a baseline for reference, since it has no question attention, no entities and no number types in the graph. We observe that QDGAT NQ , which has no question directed attention, performs worse. This justifies that the reasoning with graph neural network is more effective when conditioned on the input question. We also observe that QDGAT NH performs significantly worse, which demonstrates the importance of incorporating the information of number types and entity mentions in the reasoning graph. This is consistent with our intu-  ition that numbers with the same type or connected to the same entity are more relevant to each other. <ref type="table" target="#tab_4">Table 4</ref> decomposes the QA performance on different answer types in the development set of DROP. As reported in the table, QDGAT works better on the questions relating to numbers and dates, which requires more specific numerical reasoning compared with the span extraction. The remarkable improvement indicates that the proposed method effectively benefits the reasoning module to comprehend the numerical problems. Notably, the performance in span extraction can still be improved by our method. The span extraction in DROP heavily relies on the ability to comprehend the relation between the number and the entity (c.f. the second example in <ref type="table" target="#tab_0">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Performance on RACENum</head><p>To investigate the generalization capability of QDGAT in numerical reasoning, we examine whether the pretrained model on DROP is transferable. We compare QDGAT with NumNet+ on RACE <ref type="bibr" target="#b17">(Lai et al., 2017)</ref>, a dataset collected from the English exams for middle and high school Chinese students. We extracted a special part of examples from RACE, where the questions start with "how many", referred to as RACENum. RACENum is then divided into middle school exam (RACENum-M) and high school exam (RACENum-H) categories. The RACENum-M and RACENum-H datasets contain 633 and 611 questions accordingly. Since the original RACE dataset is in the multiple-choice form, we converted them into the DROP data format. The accuracy of NumNet+, QDGAT and its ablation variants on RACENum are summarized in <ref type="table" target="#tab_6">Table 6</ref>, which is consistent with the performance comparison on the DROP dataset.</p><p>The overall low scores are attributed to the lack of training on the in-domain data. QDGAT achieves  43.7 points on RACENum on average, which is approximately 4.5 points higher than NumNet+. Both QDGAT NQ and QDGAT NH still outperform NumNet+ by a 2-3 points margin. We further confirmed that ablating either the entity information or question attention from the heterogeneous graph weakens the power of QDGAT to learn numeracy and the capability of understanding numbers in either digits or word form. Compared with QDGAT, ablating the question directed attention, i.e., QDGAT NQ , leads to about a 1 point drop.</p><p>For QDGAT NH that removes the number type and entity mentions from the graph, it performs consistently worse than QDGAT, demonstrating the impact of the heterogeneous graph for numerical reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Study</head><p>We show several examples to provide insights into how our model works. <ref type="table" target="#tab_5">Table 5</ref> compares the different model prediction results from NumNet+ and QDGAT:</p><p>• The first example shows the importance of number types. NumNet+ treats all numbers as the same type, which fails to capture that the question only cares about percentage and incorrectly predicts "19" (type age) as part of the result. In contrast, QDGAT extracts the relevant numbers and derives the correct answer.</p><p>• The second example highlights the importance of entity mentions. NumNet+ fails to extract "49yard", but QDGAT easily captures this number since "49-yard" and "45-yard" are connected to the same entity "Kassy" on the heterogeneous graph which is generated from the passage.</p><p>• The third example shows the importance of question conditioning. Solving this example requires to extract the two dates related to two events mentioned in the question. Without direct interaction between the question, the model tends to recognize this example as a counting problem since the question starts with "how many". However, when combined with question directed attention, correct numbers can be filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a novel method named QDGAT for numerical reasoning in the machine reading comprehension task. Our method not only builds a more compact graph containing different types of numbers, entities, and relations, which can be a general method for other sophisticated reasoning tasks but also conditions the reasoning directly on the question language embedding, which modulates the attention over graph neighbors and change messages being passed iteratively to achieve reasoning. The experimental results verify the effectiveness of our method. In the future, we plan to extend our model to learn the heterogeneous graph automatically, which assures more flexibility for numerical reasoning. We would also explore to learn the types of numbers and entities together the reasoning modules using variational autoencoder techniques (Kingma and Welling, 2014), which may help the NER system better adapt to the numerical reasoning task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The constructed heterogeneous typed graph of the example in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The framework of our model. It consists of a representation extractor (left), a reasoning module (middle) and a prediction module (right). The reasoning module reasons over a heterogeneous directed graph whose nodes are the numbers and the entities. Two kinds of relations are encoded:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Two MRC cases requiring numerical reasoning are illustrated. There are entities and numbers of different types. Both are emphasized by different colors: entity, number, percentage, date, ordinal. We explicitly encode the type information into our model and leverage the question representation to conduct the reasoning process. In 1754 Spanish and Portuguese military forces were dispatched to force the Guarani to leave the area ...Hostilities resumed in 1756 when an army of 3,000 Spanish, Portuguese, and native auxiliary soldiers under JosÃl' de Andonaegui and Freire de Andrade was sent to subdue the Guarani rebels. On February 7, 1756 the leader of the Guarani rebels, SepÃl' Tiaraju, was killed in a skirmish with Spanish and Portuguese troops. ... 1,511 Guarani were killed and 152 taken prisoner, while 4 Spanish and Portuguese were killed and about 30 were wounded...</figDesc><table><row><cell>Question</cell><cell>Passage</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overall results on the development and test set of DROP. For QDGAT p , we used more careful data preprocessing and a RoBERTa pre-trained on the SQuaD dataset. † denotes that the result is taken from the public leaderboard. Better results are in bold.</figDesc><table><row><cell>Method</cell><cell>Dev</cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>Syn Dep</cell><cell cols="2">9.38 11.64</cell><cell cols="2">8.51 10.84</cell></row><row><cell>OpenIE</cell><cell cols="2">8.80 11.31</cell><cell cols="2">8.53 10.77</cell></row><row><cell>SRL</cell><cell cols="2">9.28 11.72</cell><cell cols="2">8.98 11.45</cell></row><row><cell>BiDAF</cell><cell cols="4">26.06 28.85 24.75 27.49</cell></row><row><cell>QANet</cell><cell cols="4">27.50 30.44 25.50 28.36</cell></row><row><cell>BERT</cell><cell cols="4">30.10 33.36 29.45 32.70</cell></row><row><cell>NAQANet</cell><cell cols="4">46.20 49.24 44.07 47.01</cell></row><row><cell>ALBERT-Calculator</cell><cell cols="4">80.22 83.98 79.85 83.56</cell></row><row><cell>NumNet</cell><cell cols="4">64.92 68.31 64.56 67.97</cell></row><row><cell cols="5">NumNet+ (RoBERTa) 81.07  † 84.42  † 81.52  † 84.84  †</cell></row><row><cell>NumNet+ (ensemble)</cell><cell cols="4">82.63  † 85.59  † 83.14  † 86.16  †</cell></row><row><cell>QDGAT (RoBERTa)</cell><cell cols="4">82.74 85.85 83.23 86.38</cell></row><row><cell cols="5">QDGATp (RoBERTa) 84.07 87.05 84.53 87.57</cell></row><row><cell>QDGATp (ensemble)</cell><cell cols="4">85.31 88.10 85.46 88.38</cell></row><row><cell>Human</cell><cell></cell><cell></cell><cell cols="2">94.09 96.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results on the development set of DROP. QDGAT NH removes the number type and entity from the graph, and QDGAT NQ removes question direction from QDGAT. Better results are in bold.</figDesc><table><row><cell>Method</cell><cell>EM</cell><cell>F1</cell></row><row><cell>NumNet+</cell><cell>81.07</cell><cell>84.42</cell></row><row><cell>QDGATNH</cell><cell>81.98</cell><cell>84.94</cell></row><row><cell>QDGATNQ</cell><cell>82.04</cell><cell>85.01</cell></row><row><cell>QDGAT</cell><cell>82.74</cell><cell>85.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="7">: Decomposed performance on different answer</cell></row><row><cell cols="7">types in the development set of DROP. Better results are</cell></row><row><cell>in bold.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Number</cell><cell cols="2">Date</cell><cell cols="2">Span</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>NumNet+</cell><cell>82.89</cell><cell>83.13</cell><cell>56.67</cell><cell>63.91</cell><cell>82.00</cell><cell>86.84</cell></row><row><cell>QDGAT</cell><cell>86.00</cell><cell>86.23</cell><cell>60.27</cell><cell>67.48</cell><cell>84.05</cell><cell>88.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The cases from the DROP dataset. The predictions from the QDGAT and NumNet+ are illustrated. The differences between the output of these two models demonstrate the properties of the proposed model. The last two columns indicate the arithmetic expression, obtained by assigning a sign (plus, minus or zero) for each extracted numbers (we omitted the zero sign numbers). Then the answer was derived by summing up the signed numbers.The age distribution, in Aigle is; 933 children or 10.7% of the population are between 0 and 9 years old and 1,137 teenagers or 13.0% are between 10 and 19. Of the adult population, 1,255 people or 14.3% of the population are between 20 and 29 years old... Carolina scored first in the second quarter with kicker John Kasay hitting a 45-yard field goal . The Falcons took the lead with QB Joey Harrington completing a 69-yard TD pass to WR Roddy White . The Panthers followed up with QB Jake Delhomme completing a 13yard TD pass to RB DeShaun Foster ... In the fourth quarter , the Panthers scored again , with Kasay kicking a 49-yard field goal . The Falcons ' Andersen nailed a 25-yard field goal to end the scoring ... A sign that order had been restored among the Derg was the announcement of Mengistu Haile Mariam as head of state on 02/1977. However, the country remained in chaos as the military attempted to suppress its civilian opponents in a period known as the Red Terror ... Ethiopia closed the U.S. military mission and the communications centre in 04/1977. In 06/1977, Mengistu accused Somalia of infiltrating SNA soldiers into the Somali area to fight alongside the WSLF. Despite considerable evidence to the contrary...</figDesc><table><row><cell>Question &amp; Answer</cell><cell>Passage</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The accuracy on the unsupervised RACENum dataset.</figDesc><table><row><cell>Method</cell><cell>RACE-M</cell><cell>RACE-H</cell><cell>Avg.</cell></row><row><cell>NumNet+</cell><cell>46.98</cell><cell>31.59</cell><cell>39.29</cell></row><row><cell>QDGATNH</cell><cell>50.88</cell><cell>35.30</cell><cell>43.09</cell></row><row><cell>QDGATNQ</cell><cell>49.67</cell><cell>35.84</cell><cell>42.76</cell></row><row><cell>QDGAT</cell><cell>52.53</cell><cell>34.86</cell><cell>43.70</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We used Standford CoreNLP toolkit<ref type="bibr" target="#b21">(Manning et al., 2014)</ref>.3 https://pypi.org/project/word2number/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">When vi corresponds to several tokens, the average of these vectors is used.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/llamazing/numnet_plus 6 https://leaderboard.allenai.org/drop/submissions/public</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Giving BERT a calculator: Finding operations and arguments with reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="5946" to="5951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The neural development of an abstract concept of number</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><forename type="middle">E</forename><surname>Jessica F Cantlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Libertus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Pinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">M</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">A</forename><surname>Brannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pelphrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cognitive neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2217" to="2229" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gatedattention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1246</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Tagbased multi-span extraction in reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Shoham</surname></persName>
		</author>
		<idno>abs/1909.13375</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Injecting numerical reasoning skills into language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural module networks for reasoning over text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A multi-type multi-span network for reading comprehension that requires discrete reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1596" to="1606" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language-conditioned graph networks for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10294" to="10303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1516" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ALBERT: A lite BERT for selfsupervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>abs/1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Piyush Sharma, and Radu Soricut</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mc-Closky</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p14-5010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA, System Demonstrations</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-22" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Numnet: Machine reading comprehension with numerical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Qiu Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1251</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="2474" to="2484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hananneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast and accurate reading comprehension by combining self-attention and convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
