<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
						</author>
						<title level="a" type="main">MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called Men-torNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on We-bVision, a large benchmark containing 2.2 million images of real-world noisy labels. The code are at https://github.com/google/mentornet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head> <ref type="bibr" target="#b40">Zhang et al. (2017a)</ref> <p>found that deep convolutional neural networks (CNNs) are capable of memorizing the entire data even with corrupted labels, where some or all true labels are replaced with random labels. It is a consensus that deeper CNNs usually lead to better performance. However, the ability of deep CNNs to overfit or memorize the corrupted labels can lead to very poor generalization performance <ref type="bibr" target="#b40">(Zhang et al., 2017a)</ref>. Recently, <ref type="bibr" target="#b27">Neyshabur et al. (2017)</ref> and <ref type="bibr" target="#b0">Arpit et al. (2017)</ref>  deep CNNs, so as to improve generalization performance on the clean test data. Although learning models on weakly labeled data might not be novel, improving deep CNNs on corrupted labels is clearly an under-studied problem and worthy of exploration, as deep CNNs are more prone to overfitting and memorizing corrupted labels <ref type="bibr" target="#b40">(Zhang et al., 2017a)</ref>. To address this issue, we focus on training very deep CNNs from scratch, such as resnet-101  or inception-resnet <ref type="bibr" target="#b34">(Szegedy et al., 2017)</ref> which has a few hundred layers and orders-of-magnitude more parameters than the number of training samples. These networks can achieve the state-of-the-art result but perform poorly when trained on corrupted labels. Inspired by the recent success of Curriculum Learning (CL), this paper tackles this problem using CL ), a learning paradigm inspired by the cognitive process of human and animals, in which a model is learned gradually using samples ordered in a meaningful sequence. A curriculum specifies a scheme under which training samples will be gradually learned. CL has successfully improved the performance on a variety of problems. In our problem, our intuition is that a curriculum, similar to its role in education, may provide meaningful supervision to help a student overcome corrupted labels. A reasonable curriculum can help the student focus on the samples whose labels have a high chance of being correct.</p><p>However, for the deep CNNs, we need to address two limitations of the existing CL methodology. First, existing curriculums are usually predefined and remain fixed during training, ignoring the feedback from the student. The learning procedure of deep CNNs is quite complicated, and may not be accurately modeled by the predefined curriculum. Second, the alternating minimization, commonly used in CL and self-paced learning  requires alternative variable updates, which is difficult for training very deep CNNs via mini-batch stochastic gradient descent.</p><p>To this end, we propose a method to learn the curriculum from data by a network called MentorNet. MentorNet learns a data-driven curriculum to supervise the base deep CNN, namely StudentNet. MentorNet can be learned to approximate an existing predefined curriculum or discover new data-driven curriculums from data. The learned data-driven arXiv:1712.05055v2 [cs.CV] 13 Aug 2018 curriculum can be updated a few times taking into account of the StudentNet's feedback. Whenever MentorNet is learned or updated, we fix its parameter and use it together with StudentNet to minimize the learning objective, where Men-torNet controls the timing and attention to learn each sample. At the test time, StudentNet makes predictions alone without MentorNet.</p><p>The proposed method improves existing curriculum learning in two aspects. First, our curriculum is learned from data rather than predefined by human experts. It takes into account of the feedback from StudentNet and can be dynamically adjusted during training. Intuitively, this resembles a "collaborative" learning paradigm, where the curriculum is determined by the teacher and student together. Second, in our algorithm, the learning objective is jointly minimized using MentorNet and StudentNet via mini-batch stochastic gradient descent. Therefore, the algorithm can be conveniently parallelized to train deep CNNs on big data. We show the convergence and empirically verify it on largescale benchmarks.</p><p>We verify our method on four benchmarks. Results show that it can significantly improve the performance of deep CNNs trained on both controlled and real-world corrupted training data. Notably, to the best of our knowledge, it achieves the best-published result on WebVision <ref type="bibr" target="#b23">(Li et al., 2017a)</ref>, a large benchmark containing 2.2 million images of real-world noisy labels. To summarize, the contribution of this paper is threefold:</p><p>• We propose a novel method to learn data-driven curriculums for deep CNNs trained on corrupted labels.</p><p>• We discuss an algorithm to perform curriculum learning for deep networks via mini-batch stochastic gradient descent. • We verify our method on 4 benchmarks and achieve the best-published result on the WebVision benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary on Curriculum Learning</head><p>We formulate our problem based on the model in  and . Consider a classification problem with the training set D = {(x 1 , y 1 ), · · · , (x n , y n )}, where x i denotes the i th observed sample and y i ∈ {0, 1} m is the noisy label vector over m classes. Let g s (x i , w) denote the discriminative function of a neural network called StudentNet, parameterized by w ∈ R d . Further, let L(y i , g s (x i , w)), a mdimensional column vector, denote the loss over m classes. Introduce the latent weight variable, v ∈ R n×m , and optimize the objective:</p><formula xml:id="formula_0">min w∈R d ,v∈[0,1] n×m F(w,v) = 1 n n i=1 v T i L(y i ,g s (x i ,w)) + G(v; λ) + θ w 2 2<label>(1)</label></formula><p>where · 2 is the l 2 norm for weight decay, and data augmentation and dropout are subsumed inside g s . v i ∈ [0, 1] m×1 is a vector to represent the latent weight variable for the i-th sample. The function G defines a curriculum, parameterized by λ. This paper focuses on the one-hot label. For notation convenience, denote the loss L(y i ,g s (x i ,w)) = i , v i as a scalar v i , and y i as an integer y i ∈ [1, m].</p><p>In the existing literature, alternating minimization <ref type="bibr" target="#b7">(Csiszar, 1984)</ref>, or its related variants, is commonly employed to minimize the training objective, e.g. in <ref type="bibr" target="#b23">Ma et al., 2017a;</ref>. This is an algorithmic paradigm where w and v are alternatively minimized, one at a time while the other is held fixed. When v is fixed, the weighted loss is typically minimized by stochastic gradient descent. When w is fixed, we com-</p><formula xml:id="formula_1">pute v k = arg min v F(v k−1 , w k ) using the most recently updated w k at epoch k. For example, Kumar et al. (2010) employed G(v) = −λ v 1 . When w is fixed, the optimal v can be easily derived by: v * i = 1( i ≤ λ), ∀i ∈ [1, n],<label>(2)</label></formula><p>where 1 is the indicator function. Eq.</p><p>(2) intuitively explains the predefined curriculum in , known as self-paced learning. First, when updating v with a fixed w, a sample of smaller loss than the threshold λ is treated as an "easy" sample, and will be selected in training (v * i = 1). Otherwise, it will not be selected (v * i = 0). Second, when updating w with a fixed v, the classifier is trained only on the selected "easy" samples. The hyperparameter λ controls the learning pace and corresponds to the "age" of the model. When λ is small, only samples of small loss will be considered. As λ grows, more samples of larger loss will be gradually added to train a more "mature" model. As shown, the function G specifies a curriculum, i.e., a sequence of samples with their corresponding weights to be used in training. When w is fixed, its optimal solution, e.g. Eq.</p><p>(2), computes the time-varying weight that controls the timing and attention to learn every sample. Recent studies discovered multiple predefined curriculums and verified them in many real-world applications, e.g., in <ref type="bibr" target="#b11">(Fan et al., 2017;</ref><ref type="bibr" target="#b23">Ma et al., 2017a;</ref><ref type="bibr" target="#b29">Sangineto et al., 2016;</ref><ref type="bibr" target="#b11">Fan et al., 2017;</ref>. This paper studies learning curriculum from data. In the rest of this paper, Section 3 presents an approach to learn data-driven curriculum by MentorNet. Section 4 discusses an algorithm to optimize Eq. (1) using MentorNet and Stu-dentNet together via mini-batch training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Curriculum from Data</head><p>Existing curriculums are either predetermined as an analytic expression of G or a function to compute sample weights. Such predefined curriculums cannot be adjusted accordingly, taking into account of the feedback from the student. This section discusses a new way to learn data-driven curriculum by a neural network, called MentorNet. The MentorNet g m is learned to compute time-varying weights for each training sample. Let Θ denote the parameters in g m . Given a fixed w, our goal is to learn an Θ * to compute the weight:</p><formula xml:id="formula_2">g m (z i ; Θ * ) = arg min vi∈[0,1] F(w, v), ∀i ∈ [1, n] (3) where z i = φ(x i , y i , w)</formula><p>indicates the input feature to Men-torNet about the i-th sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Curriculum</head><p>MentorNet can be learned to 1) approximate existing curriculums or 2) discover new curriculums from data.</p><p>Learning to approximate predefined curriculums. Our first task is to learn a MentorNet to approximate a predefined curriculum. To do so, we minimize the objective in Eq. <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_3">arg min Θ (xi,yi)∈D g m (z i ; Θ) i + G(g m (z i ; Θ); λ) (4)</formula><p>Eq. (4) applies for both convex and non-convex G. This paper employs the following predefined curriculum. It is derived from  and works well in our experiments. As will be discussed later, it is also related to robust non-convex penalties.</p><formula xml:id="formula_4">G(v; λ) = n i=1 1 2 λ 2 v 2 i − (λ 1 + λ 2 )v i ,<label>(5)</label></formula><p>where λ 1 , λ 2 ≥ 0 are hyper-parameters. As G is convex, there exists a closed-form solution for the optimal value of Eq.</p><p>(3). Given a fixed w,</p><formula xml:id="formula_5">define F w (v) = n i=1 f (v i ): f (v i ) = v i i + 1 2 λ 2 v 2 i − (λ 1 + λ 2 )v i<label>(6)</label></formula><p>The minima are obtained at ∇ v F w (v) = 0, and can be decoupled by setting ∂f /∂v i = 0. We then have:</p><formula xml:id="formula_6">g m (z i ; Θ * ) = 1( i ≤ λ 1 ) λ 2 = 0 min(max(0, 1 − i−λ1 λ2 ), 1) λ 2 = 0 ,<label>(7)</label></formula><p>where Θ * is the optimal MentorNet parameter obtained by SGD. The closed-form solution in Eq. <ref type="formula" target="#formula_6">(7)</ref> gives some intuitions about the curriculum. When λ 2 = 0, it is similar to self-paced learning  i.e. only "easy" samples of i &lt; λ 1 will be selected in training (g m (z i ; Θ * ) = 1). When λ 2 = 0, samples of loss i ≥ λ 2 + λ 1 will not be selected in training. These samples represent the "hard" samples of greater loss. Otherwise, samples will be weighted linearly w.r.t. 1 − ( i − λ 1 )/λ 2 . As in , the hyper-parameters λ 1 and λ 2 control the learning pace.</p><p>Learning data-driven curriculums. Our next task is to learn a curriculum solely derived from labeled data. To this end, Θ is learned on another dataset D =</p><formula xml:id="formula_7">{(φ(x i , y i , w), v * i )},</formula><p>where (x i , y i ) is sampled from D and |D | |D|. v * i is a given annotation and we assume it approximates the optimal weight, i.e., v * i arg min vi∈[0,1] F(v, w). In this paper, we assign binary labels to v * i , where v * i = 1 iff y i is a correct label. As v * i is binary, Θ is learned by minimizing the cross-entropy loss between v * i and g(z i ; Θ). Intuitively, this process is similar to a mock test for the teacher (MentorNet) to learn to update her teaching strategy (curriculum). The student (StudentNet) provides features φ(·, ·, w) for the mock test using the latest model w. The teacher can learn an updated curriculum from the data to better supervise the latest student model. The learned curriculum is jointly determined by the teacher and student together.</p><p>The information on the correct label may not always be available on the target dataset D. In this case, we learn the curriculum on a different small dataset where the correct labels are available. Intuitively, it resembles first learning a teaching strategy with the student on one topic and transfer the strategy on a similar topic. Empirically, Section 5.1 substantiates that the learned curriculum on a small subset of CIFAR-10 can be applied to the target CIFAR-100 dataset.</p><p>A burn-in period is introduced before learning Θ. In the first 20% training epoch of the StudentNet, MentorNet is initialized and fixed as g m (z i ; Θ * ) = r i , where r i ∼ Bernoulli(p) is the Bernoulli random variable. This is equivalent to randomly dropping out p% training samples. We found that the burn-in process helps StudentNet stabilize the prediction and focus on learning simple and common patterns.</p><p>MentorNet architecture. We found that MentorNet can have a simple architecture. Appendix D shows that even MentorNet based on the two-layer perceptron can reasonably approximate the existing curriculum in the literature. Nevertheless, we use a MentorNet architecture shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, which works reasonably well compared to classical network architectures. It takes the input of a mini-batch of samples, and outputs their corresponding sample weights. The feature z i = φ(x i , y i , w) includes the loss, loss difference to the moving average, label and epoch percentage.</p><p>pt maintains an exponential moving average on the p-th percentile of the loss in each mini-batch. For a sample, its loss and loss difference − pt over the last few epochs can be encoded by a bidirectional LSTM network to capture the prediction variance . We verify the LSTM encoder in the experiments in Appendix D. For simplicity, we set the step size of the LSTM to 1 in Section 5.1 and only consider the loss and the loss difference of the current epoch. resents the first and 99 represents the last training epoch. The concatenated outputs from the LSTM and the embedding layers are fed into two fully-connected layers f c 1 , f c 2 , where f c 2 uses the sigmoid activation to ensure the output weights bounded between 0 and 1. The last layer in <ref type="figure" target="#fig_0">Fig. 1</ref> is a probabilistic sampling layer, and is used to implement the sample dropout in the burn-in process on the already learned MentorNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discussions</head><p>MentorNet is a general framework for both predefined and data-driven curriculum learning, where various curriculums can be learned by the same MentorNet structure with different parameters. This framework is conceptually general and practically flexible as we can switch curriculums by attaching different MentorNets without modifying the pipeline. Therefore, we also learn MentorNets for predefined curriculums. For predefined curriculums where G is unknown, we directly minimize the error between the MentorNet's outputs and desired weights. For example, the desired weight for focal loss <ref type="bibr" target="#b22">(Lin et al., 2017b)</ref> is computed by:</p><formula xml:id="formula_8">v * i = [1 − exp{− i }] γ ,<label>(8)</label></formula><p>where γ is a hyperparameter for smoothing the distribution.</p><p>This paper tackles the problem of overcoming corrupted labels. It is interesting to analyze why the learned curriculum can improve the generalization performance. It turns out that StudentNet, when jointly learned with MentorNet, may optimize an underlying robust objective and the objective is also related to the robust M-estimator <ref type="bibr" target="#b16">(Huber, 2011)</ref>.</p><p>To show this, let v * (λ, x) represent the optimal weight function for a loss variable x, and we define:</p><formula xml:id="formula_9">v * (λ, x) = argmin v∈[0,1] vx + G(v, λ).<label>(9)</label></formula><p>As g m is an approximator to Eq. (9), its property can then be analyzed by the function v * (λ, x).  investigated the insights of self-paced objective function, and proved that the optimization of SPL algorithm is intrinsically equivalent to minimizing a robust loss function. They showed that given a fixed λ and a decreasing v * (λ, x) with respect to x, the underlying objective of Eq. (1) can be obtained by:</p><formula xml:id="formula_10">F λ (w) = 1 n n i=1 i 0 v * (λ, x)dx,<label>(10)</label></formula><p>Based on it, the underlying learning objective of the curriculum in Eq. (5) can then be derived. Remark 1. When λ 1 , λ 2 are fixed and λ 2 = 0, the underlying objective function of the curriculum in Eq. (5) is calculated from:</p><formula xml:id="formula_11">F λ (w) = 1 n n i=1      i i ≤ λ 1 (λ 2 + 2λ 1 )/2 i ≥ λ 2 + λ 1 θ i − 2 i /(2λ 2 )− (θ−1) 2 λ2 2 otherwise (11) where θ = (λ 2 + λ 1 )/λ 2 .</formula><p>When θ = 1 it is equivalent to the minimax concave penalty .</p><p>As shown in Eq. (11), the underlying objective has a form of F λ (w) = i ρ( i )/n, where ρ is the penalty function in M-estimator . Particularly, when θ = 1, ρ( ) is equivalent to the minimax concave plus penalty , a popular non-convex robust loss. The result indicates the learned MentorNet that approximates our predefined curriculum in Eq. (5) leads to an underlying robust objective of the StudentNet.</p><p>For the data-driven curriculum, if the learned MentorNet satisfies certain conditions, we have: Proposition 1. Suppose (x, y) denotes a training sample and its corrupted label. For simplicity, let the MentorNet input φ(x, y, w) = be the loss computed by the StudentNet model parameter w. The MentorNet g m ( ; Θ) = v, where v is the sample weight. If g m decreases with respect to , then there exists an underlying robust objective F :</p><formula xml:id="formula_12">F (w) = 1 n n i=1 ρ( i ),</formula><p>where ρ( i ) = i 0 g m (x; Θ)dx. In the special cases, ρ( ) degenerates to the robust M-estimator: Huber  and the log-sum penalty . The proposition indicates that there exist some learned Men-torNets that are related to the robust M-estimator. On noisy data, the effect of the robust objective is evident, i.e., preventing StudentNet from being dominated by corrupted labels. <ref type="figure" target="#fig_1">Fig. 2</ref> visualizes curves of the sample loss = y i −g s (x i , w) and the learning objective for the Huber loss , log-sum penalty , self-paced , and our learned data-driven curriculum. We use the best learned curriculum Θ * on CIFAR-10 in our experiments and plot |g m (φ(x, y, w); Θ * ) × | since the G in the objective function is unknown. As shown, all curves are robust to great loss to different extents. The corrupted labels in our problem are harmful. As the sample loss grows bigger beyond some value, MentorNet starts to sharply decrease the sample's weight. The subtlety of learned curriculum is difficult to be predefined by the analytic expression. Proposition 1 does not guarantee there is an underlying robust objective for every learned MentorNet. Instead, it shows MentorNet's capability of learning such robust objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Algorithm</head><p>The alternating minimization algorithm <ref type="bibr" target="#b7">(Csiszar, 1984)</ref> used in related work is intractable for deep CNNs, especially on big datasets, for two important reasons. First, in the subroutine of minimizing w when fixing v, stochastic gradient descent often takes many steps before converging. This means that it can take a long time before moving past this single sub-step. However, such computation is often wasteful, particularly in the initial part of training, because, when v is far away from the optimal point, there is not much gain in finding the exact optimal w corresponding to this v. Second, the subroutine of minimizing v when fixing w is often difficult, because the fixed vector v may not only consume a considerable amount of the memory but also hinder the parallel training on multiple machines. Therefore, optimizing the objective with deep CNNs requires some thought on the algorithmic level.</p><p>To minimize Eq. (1), we propose an algorithm called SPADE (Scholastic gradient PArtial DEscent). The algorithm optimizes the StudentNet model parameter w jointly with a given MentorNet. It provides a simple and elegant way to minimize w and v stochastically over mini-batches. As a general approach, it can also take an input of G.</p><formula xml:id="formula_13">Let Ξ t = {(x j , y j )} b j=1 denotes a mini-batch of b samples, fetched uniformly at random and v t Ξ = [v t 1 , ..., v t b ] represent the sample weights in Ξ t . The MentorNet computes: v t Ξ = g m (φ(Ξ t , w t−1 )) = arg min vΞ F(w t−1 ,v t−1 ),<label>(12)</label></formula><p>where φ is the feature extraction function defined in Eq.</p><p>(3). Θ denotes the learned MentorNet discussed in Section 3.1.</p><p>As shown in Algorithm 1, for w, a stochastic gradient is computed (via a mini-batch) and applied (Step 12), where α t is the learning rate. For the latent weight variables v, gradient descent is only applied to a small subset thereof parameters corresponding only to the mini-batch (Step 9 or 11). The partial gradient update on weight parameters is performed when G is used (Step 9). Otherwise, we directly apply the weights computed by the learned MentorNet (Step 11). In both cases, the weights are computed on-the-fly within a mini-batch and thus do not need to be fixed. As a result, the algorithm can be conveniently parallelized across multiple machines.</p><p>Algorithm 1 SPADE for minimizing Eq. (1)</p><formula xml:id="formula_14">Input :Dataset D, a predefined G or a learned gm(·; Θ) Output :The model parameter w of StudentNet. 1 Initialize w 0 , v 0 , t = 0 2 while Not Converged do 3 Fetch a mini-batch Ξt uniformly at random 4 For every (xi, yi) in Ξt compute φ(xi, yi, w t ) 5 if update curriculum then 6 Θ ← Θ * , where Θ * is learned in Sec. 3.1 7 end 8 if G is used then 9 v t Ξ ← v t−1 Ξ − αt∇vF(w t−1 , v t−1 )|Ξ t 10 end 11 else v t Ξ ← gm(φ(Ξt, w t−1 ); Θ) ; 12 w t ← w t−1 − αt∇wF(w t−1 , v t )|Ξ t 13 t ← t + 1 14 end 15 return w t</formula><p>The curriculum can change during training. MentorNet is updated a few times in Algorithm 1. In Step 6, the Men-torNet parameter Θ is updated to adapt to the most recent model parameters of StudentNet. In experiments, we update Θ twice after the learning rate is changed. Each time, a datadriven curriculum is learned from the data generated by the most recent w using the method discussed in Section 3.1. The update is consistent with existing curriculum learning methodology <ref type="bibr" target="#b67">Kumar et al., 2010)</ref> and the difference here is that for each update, the curriculum is learned rather than specified by human experts.</p><p>Under standard assumptions, Theorem 1 shows that the algorithm stabilizes and converges to a stationary point (convergence to global/local minima cannot be guaranteed unless in specially structured non-convex objectives <ref type="bibr" target="#b6">(Chen et al., 2018;</ref><ref type="bibr" target="#b44">Zhou et al., 2017b;</ref><ref type="bibr">a)</ref>). The proof is in Appendix B. The theorem is a characterization of stability of the model parameters w. For the weight parameters v, as it is restricted in a compact set, convergence to a stationary point is not always guaranteed. As the model parameters are more important, we only provide a detailed characterization of the model parameter.</p><formula xml:id="formula_15">Theorem 1. Let the objective F(w, v) defined in Eq. (1) be differentiable, L(·) be Lipschitz continuous in w and ∇ v G(·) be Lipschitz continuous in v. Let w t , v t be iterates from Algorithm 1 and ∞ t=0 α t = ∞, ∞ t=0 α 2 t &lt; ∞ . Then, lim t→∞ E[ ∇ w F(w t , v t ) 2 2 ] = 0.</formula><p>For the manually designed curriculums, it may be unclear where or even whether such predefined curriculum would converge via mini-batch training. Theorem 1 shows that the learned curriculum can converge and produce a stable StudentNet model. The algorithm can be used to replace the alternating minimization method in related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section empirically verifies the proposed method on four benchmarks of controlled corrupted labels in Section 5.1 and real-world noisy labels in Section 5.2. The code can be found at https://github.com/google/ mentornet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on controlled corrupted labels</head><p>This section validates MentorNet on the controlled corrupted label. We follow a common setting in <ref type="bibr" target="#b40">(Zhang et al., 2017a)</ref> to train deep CNNs, where the label of each image is independently changed to a uniform random class with probability p, where p is noise fraction and is set to 0.2, 0.4 and 0.8. The labels of validation data remain clean for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and StudentNet:</head><p>We use the same benchmarks in <ref type="bibr" target="#b40">(Zhang et al., 2017a)</ref>: CIFAR-10, CIFAR-100 and Im-ageNet. CIFAR-10 and CIFAR-100 ) consist of 32 × 32 color images arranged in 10 and 100 classes. Both datasets contain 50,000 training and 10,000 validation images. ImageNet ILSVRC2012 ) contain about 1.2 million training and 50k validation images, split into 1,000 classes. Each image is resized to 299x299 with 3 color channels.</p><p>We employ 3 recent deep CNNs as our StudentNets: inception , resnet-101  with wide filters  and inceptionresnet v2 <ref type="bibr" target="#b34">(Szegedy et al., 2017)</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows their #model parameters, training, and validation accuracy when we train them on the clean training data (noise= 0). As shown, they achieve reasonable accuracy on each task. Baselines: MentorNet is compared against the following baselines: FullMode is the standard StudentNet trained using l 2 weight decay, dropout  and data augmentation <ref type="bibr" target="#b66">(Krizhevsky et al., 2012)</ref>. The hyperparameters are set to the best ones found on the clean training data. Unless specified otherwise, for a fair comparison, the StudentNet with the same hyperparameters is used in all baseline and our model. Forgetting was introduced in , in which the dropout parameter is searched in the range of (0.2-0.9). Self-paced  and Focal Loss <ref type="bibr" target="#b22">(Lin et al., 2017b)</ref> represent well-known predefined curriculums in the literature. We implemented  and Goldberger <ref type="bibr" target="#b13">(Goldberger &amp; Ben-Reuven, 2017)</ref> as the recent weakly-supervised learning methods. The above baseline methods are a mixture of the curriculum learning and the recent methods dealing with corrupted labels.</p><p>Our Model: MentorNet PD is the network learned using our predefined curriculum in Eq. (5) using no additional clean labels. MentorNet DD is the learned data-driven curriculum. It is trained on 5,000 images of true labels, randomly sampled from the CIFAR-10 training set. The same data are used to learn MentorNet DD on CIFAR-100. Note CIFAR-10 and CIFAR-100 are two different datasets that have not only different classes but also the different number of classes. Therefore, it is fair to compare MentorNet DD with other methods using no true labels on CIFAR-100. Algorithm 1 is used to optimize the StudentNet. The decay factor in computing the loss moving average is set to 0.95. The loss percentile in the moving average is set by the cross-validation. As mentioned, a burn-in process is used in the first 20% training epoch for both MentorNet DD and MentorNet PD. More details are discussed in Appendix E.</p><p>We first show the comparison to the baseline method on CIFAR-10 and CIFAR-100 in <ref type="table" target="#tab_3">Table 2</ref>. On both datasets, each method is verified with two StudentNets (resnet-101 and inception) under the noise fraction of 0.2, 0.4, and 0.8.</p><p>As we see on both datasets, MentorNet improves FullModel across different noise fractions, and the learned data-driven curriculum (MentorNet DD) achieves the best results. The improvement is more significant for the deeper CNN model resnet-101. For example, on the CIFAR-10 of 40% noise, MentorNet DD (with resnet-101) yields an absolute 20% gain over FullModel. After inspecting the result, we found that it may be because Mentor DD learns a more appropriate curriculum to give high weights to samples of correct labels. As a result, it helps the StudentNet focus on samples of correct labels. The results indicate that the learned Mentor-Net can improve the generalization performance of recent deep CNNs, and outperform the predefined curriculums (Self-paced and Focal Loss). <ref type="figure">Fig. 3</ref> plots the training and test error on the clean validation data, under a representative setting: resnet-101 on CIFAR-100 of 40% noise, where the x-axis denotes the training iteration. The y-axis is the validation error on the clean validation in <ref type="figure">Fig. 3(a)</ref> and the mini-batch training error on corrupted labels in <ref type="figure">Fig. 3(b</ref>   does not increase in <ref type="figure">Fig. 3(a)</ref>. It suggests that the learned curriculum is beneficial for StudentNet. The sharp change around the 20k iteration in <ref type="figure">Fig. 3</ref> is due to the learning rate change. Besides, our result is consistent with <ref type="bibr" target="#b40">(Zhang et al., 2017a)</ref> that deep CNNS is able to get 0 training error on the corrupted training data. Forgetting (the dashed curve) is the only one that does not converge within 30k steps. As indicated in , it is because forgetting reduces the speed at which DNNs memorize. As suggested in <ref type="bibr" target="#b42">(Zhang et al., 2017b)</ref>, a not converged model might yield a better result, e.g., stop the model at 20K in <ref type="figure">Fig. 3</ref>. However, as it is hard to predetermine the time for early stopping, our focus is comparing the converged model. <ref type="figure" target="#fig_4">Fig. 4</ref> illustrates the best learned data-driven curriculum in our experiments, where the z-axis denotes the weights computed by g m ; the y and x axes denote the sample loss and the loss difference to the moving average, where λ is the loss moving average. Two observations can be found in <ref type="figure" target="#fig_4">Fig. 4</ref>. First, the learned curriculum changes during the training of the StudentNet. <ref type="figure" target="#fig_4">Fig. 4 (a)</ref> and (b) are MentorNet learned at different epochs. As shown, (a) assigns greater weights to samples of big loss more aggressively. Second, the learned curriculums in <ref type="figure" target="#fig_4">Fig. 4</ref> generally satisfy the condition in Proposition 1, i.e., the weight generally decreases with the loss. It suggests that joint learning of StudentNet and MentorNet optimizes an underlying robust objective.    <ref type="bibr" target="#b1">(Azadi et al., 2016)</ref>, and implement other methods using the same resnet-101 StudentNet. The results show that our result is comparable and even better than the state-of-the-art. To verify MentorNet for large-scale training, we apply our method on the ImageNet ILSVRC12  benchmark to improve the inception-resnet v2 <ref type="bibr" target="#b34">(Szegedy et al., 2017)</ref> model. We train the model on the ImageNet of 40% noise. Inspired by <ref type="bibr" target="#b40">(Zhang et al., 2017a)</ref>, we start with an inception-resnet (NoReg) with no regularization (NoReg) and add weight decay, dropout, and data augmentation to the model. <ref type="table" target="#tab_6">Table 4</ref> shows the comparison. As shown, MentorNet improves the performance of both the inception-resnet without regularization (NoReg) and with full regularization (FullModel). It also outperforms the forgetting baseline (dropout keep probability = 0.2). The results suggest that MentorNet can improve deep CNNs on the large-scale training on corrupted labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments on real-world noisy labels</head><p>To verify MentorNet on real-world noisy labels, we conduct experiments on the large WebVision benchmark (Li et al., 2017a). It contains 2.4 million images of real-world noisy labels, crawled from the web using the 1,000 concepts in Im-ageNet ILSVRC12. We download the resized images from the official website 1 . The inception-resenet v2 <ref type="bibr" target="#b34">(Szegedy et al., 2017)</ref> is used as our StudentNet, trained using a distributed asynchronized momentum optimizer on 50 GPUs. Since the dataset is very big, for quick experiments, we compare baseline methods using the Google image subset on the first 50 classes. We use Mini to denote this subset and Entire for the entire WebVision. All the models are evaluated on the clean ILSVRC12 and WebVision validation set. <ref type="table" target="#tab_7">Table 5</ref> lists the comparison result. As we see, the proposed MentorNet significantly improves baseline methods on real-world noisy labels. The method marked by the start indicates it uses a pre-trained ImageNet model to obtain additional 30k labels for 118 classes. Following the same protocol, MentorNet* is trained using the additional labels. The results show that our method outperforms the baseline methods on real-world noisy labels. To the best of our knowledge, it achieves the best-published result on the WebVision (Li et al., 2017a) benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Curriculum learning (CL), proposed by <ref type="bibr" target="#b2">Bengio et al. (2009)</ref>, is a learning paradigm in which a model is learned by gradually including from easy to complex samples in training so as to increase the learning entropy . From the human behavioral perspective, <ref type="bibr" target="#b20">Khan et al. (2011)</ref> have shown that CL is consistent with the principle of hu-1 https://www.vision.ee.ethz.ch/webvision/download.html man teaching. CL has been empirically verified in a variety of problems, such as computer vision <ref type="bibr" target="#b32">(Supancic &amp; Ramanan, 2013;</ref><ref type="bibr" target="#b5">Chen &amp; Gupta, 2015)</ref>, natural language processing <ref type="bibr" target="#b35">(Turian et al., 2010)</ref>, multitask learning <ref type="bibr" target="#b14">(Graves et al., 2017)</ref>. A common CL approach is to predefine a curriculum. For example, <ref type="bibr" target="#b67">Kumar et al. (2010)</ref> proposed a curriculum called self-paced learning which favors training samples of smaller loss. After that, many predefined curriculums were proposed, e.g., in <ref type="bibr" target="#b32">(Supancic &amp; Ramanan, 2013;</ref><ref type="bibr" target="#b48">2015;</ref><ref type="bibr" target="#b29">Sangineto et al., 2016;</ref><ref type="bibr" target="#b23">Ma et al., 2017a;</ref>. For example,  introduced a curriculum of using easy and diverse samples. <ref type="bibr" target="#b11">Fan et al. (2017)</ref> proposed to use predefined sample weighting schemes as an implicit way to define a curriculum. Previous work has shown that predefined curriculums are useful in overcoming noisy labels <ref type="bibr" target="#b5">(Chen &amp; Gupta, 2015;</ref><ref type="bibr">Liang et al., 2016;</ref><ref type="bibr">Lin et al., 2017a)</ref>. In parallel to CL, the sample weighting schemes were also studied in <ref type="bibr">(Lin et al., 2017a;</ref><ref type="bibr" target="#b38">Wang et al., 2017;</ref><ref type="bibr" target="#b12">Fan et al., 2018;</ref><ref type="bibr" target="#b9">Dehghani et al., 2018)</ref>. Compared to the existing work, our paper presents a new way of learning data-driven curriculums for deep networks trained on corrupted labels.</p><p>Our work is related to the weakly-supervised learning methods. Among recent contributions,  developed a robust loss to model "prediction consistency". <ref type="bibr" target="#b26">Menon et al. (2015)</ref> used class-probability estimation to study the corruption process. <ref type="bibr" target="#b31">Sukhbaatar et al. (2014)</ref> proposed a noise transformation to estimate the noise distribution. The transformation matrix needs to be periodically updated and is non-trivial to learn. To address the issue, <ref type="bibr" target="#b13">Goldberger et al. (2017)</ref> proposed to add an additional softmax layer end-to-end with the base model. <ref type="bibr" target="#b1">Azadi et al. (2016)</ref> tackled this problem by a regularizer called AIR. This method was shown to be effective but it relied on additional clean labels to train the representation. More recently, methods utilized additional labels for label cleaning <ref type="bibr" target="#b37">(Veit et al., 2017)</ref>, knowledge distillation <ref type="bibr">(Li et al., 2017b)</ref> or semi-supervised learning <ref type="bibr" target="#b36">(Vahdat, 2017;</ref><ref type="bibr" target="#b8">Dehghani et al., 2017)</ref>. Different from previous work, we focus on learning curriculum to train very deep CNNs on corrupted labels from scratch. In addition, clean labels are not always needed for our method. In Section 5.1, the MentorNet is learned on a small subset of CIFAR-10 and applied to CIFAR-100</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we presented a novel method for training deep CNNs on corrupted labels. Our work was built on curriculum learning and advanced the methodology by proposing to learn data-driven curriculum via a neural network called MentorNet. We proposed an algorithm for jointly optimizing deep CNNs with MentorNet on large-scale data. We conducted comprehensive experiments on datasets of controlled and real-world noise. Our empirical results showed that generalization performance of deep CNNs trained on corrupted labels can be effectively improved by the learned data-driven curriculum.</p><p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks.</p><p>In NIPS, 2012.</p><p>Kumar, M. P., Packer, B., and Koller, D. Self-paced learning for latent variable models. In NIPS, 2010.</p><p>Lee, K.-H., He, X., Zhang, L., and Yang, L. Cleannet: Transfer learning for scalable image classifier training with label noise. arXiv preprint arXiv:1711.07131, 2017.</p><p>Li, W., Wang, L., Li, W., Agustsson, E., and Van Gool, L. Webvision database: Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017a.</p><p>Li, Y., Yang, J., Song, Y., Cao, L., Li, J., and Luo, J. Learning from noisy labels with distillation. In ICCV, 2017b.</p><p>Liang, J., Jiang, L., Meng, D., and Hauptmann, A. G. Learning to detect concepts from webly-labeled video data. In IJCAI, 2016.</p><p>Lin, L., Wang, K., Meng, D., Zuo, W., and Zhang, L. Active self-paced learning for cost-effective and progressive face identification. TPAMI, 2017a. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivation of Remark 1</head><p>Our objective function is:</p><formula xml:id="formula_16">min w∈R d ,v∈[0,1] n F(w,v) = 1 n n i=1 v i L(y i ,g s (x i ,w)) + G(v; λ) + θ w 2 2</formula><p>(1) <ref type="figure">w)</ref>) denote the loss of the i-th sample ( i ≥ 0). The predefined curriculum is defined as:</p><formula xml:id="formula_17">Let i = L(y i , g s (x i ,</formula><formula xml:id="formula_18">G(v; λ) = n i=1 1 2 λ 2 v 2 i − (λ 2 + λ 1 )v i .<label>(2)</label></formula><p>Denote F w as the objective function when the w is fixed. We have</p><formula xml:id="formula_19">F w (v) = 1 n n i=1 v i i + G(v; λ) + θ w 2 2 = 1 n n i=1 v i i + 1 2 λ 2 v 2 i − (λ 2 + λ 1 )v i + θ w 2 2 = 1 n n i=1 f (v i ) + θ w 2 2 (3) where f (v i ) = v i i + 1 2 λ 2 v 2 i − (λ 2 + λ 1 )v i . As f (v i ) is convex with respect to v i (λ 2 ≥ 0). Its minimum is obtained at arg min v∈[0,1] n ∇ v F w (v) = 0 ⇒ ∂f (v i ) ∂v i = i + λ 2 v i − λ 2 − λ 1 = 0, (∀i ∈ [1, n], v i ∈ [0, 1])<label>(4)</label></formula><p>Since λ 1 , λ 2 ≥ 0 and v i is bounded in [0, 1]. When λ 2 = 0, the optimal v * i is calculated from:</p><formula xml:id="formula_20">v * i =      1 ( i ≤ λ 1 ) ∧ (λ 2 = 0) 1 − i−λ1 λ2 (λ 1 &lt; i &lt; λ 2 + λ 1 ) ∧ (λ 2 = 0) 0 ( i ≥ λ 2 + λ 1 ) ∧ (λ 2 = 0) ,<label>(5)</label></formula><p>When λ 2 = 0, the optimal weight writes as: </p><formula xml:id="formula_21">v * i = 1 ( i &lt; λ 1 ) ∧ (λ 2 = 0) 0 ( i ≥ λ 1 ) ∧ (λ 2 = 0) .<label>(6</label></formula><formula xml:id="formula_22">v * i = 1( i ≤ λ 1 ) λ 2 = 0 min(max(0, 1 − i−λ1 λ2 ), 1) λ 2 = 0 (7)</formula><p>According to the definition of Θ, we have g m (φ(x i , y i , w); Θ * ) = v * i . Incorporating Eq. (7), we have</p><formula xml:id="formula_23">g m (φ(x i , y i , w); Θ * ) = 1( i ≤ λ 1 ) λ 2 = 0 min(max(0, 1 − i−λ1 λ2 ), 1) otherwise<label>(8)</label></formula><p>Now we derive its underlying objective. First, we define a function v * (λ, x) and incorporate the above optimal solution: v * (λ, x) = arg min</p><formula xml:id="formula_24">v∈[0,1] vx + G(v, λ) = 1(x ≤ λ 1 ) λ 2 = 0 min(max(0, 1 − x−λ1 λ2 ), 1) otherwise<label>(9)</label></formula><p>Let &gt; 0 denote a small positive constant. Using the condition λ 2 ≥ 0, we incorporate Eq. <ref type="formula" target="#formula_6">(7)</ref>:</p><formula xml:id="formula_25">v * (λ, x + ) − v * (λ, x) ≤ 0<label>(10)</label></formula><p>That indicates v * (λ, x) decreases with respect to x. According to , given the fixed hyperparameter λ (i.e. λ 1 , λ 2 ), its underlying objective function has the form of:</p><formula xml:id="formula_26">F λ (w) = 1 n n i=1 i 0 v * (λ; x)dx,<label>(11)</label></formula><p>After incorporating Eq. (5) and Eq. (6) into Eq. (11), we have when λ 2 = 0</p><formula xml:id="formula_27">F λ (w) = 1 n n i=1 min( i , λ 1 )<label>(12)</label></formula><p>When λ 2 = 0, we have:</p><formula xml:id="formula_28">F λ (w) = 1 n n i=1      i i ≤ λ 1 i + λ1 λ2 i − 1 2λ2 2 i − λ 2 1 2λ2 λ 1 &lt; i &lt; λ 2 + λ 1 λ2+2λ1 2 i ≥ λ 2 + λ 1 = 1 n n i=1      i i ≤ λ 1 θ i − 2 i /(2λ 2 ) − (θ−1) 2 λ2 2 λ 1 &lt; i &lt; λ 2 + λ 1 (λ 2 + 2λ 1 )/2 i ≥ λ 2 + λ 1 ,<label>(13)</label></formula><p>where θ = (λ 2 + λ 1 )/λ 2 and λ 1 , λ 2 ≥ 0. We λ 2 = 0, we have the Eq.(10) in the Remark 1 in the paper.</p><formula xml:id="formula_29">F λ (w) = 1 n n i=1      i i ≤ λ 1 (λ 2 + 2λ 1 )/2 i ≥ λ 2 + λ 1 θ i − 2 i /(2λ 2 ) − (θ−1) 2 λ2 2 otherwise ,<label>(14)</label></formula><p>When θ = 1 we have λ 1 = 0 and the above equation becomes:</p><formula xml:id="formula_30">F λ (w) = 1 n n i=1 i − 2 i /(2λ 2 ) i &lt; λ 2 λ 2 /2 i ≥ λ 2 .<label>(15)</label></formula><p>The above equation is equivalent to the minimax concave penalty (MCP) <ref type="bibr">(Gong et al., 2013;</ref>. Below is its formula presented in (Gong et al., 2013) (with the regularization hyperparameter setting to 1):</p><formula xml:id="formula_31">0 [1 − x t ]dx = − 2 /(2t) &lt; t t/2 ≥ t ,<label>(16)</label></formula><p>B. Proof of Theorem 1 Theorem 1 Let the objective F(w, v) defined in Eq.</p><p>(1) be differentiable, L(·) be Lipschitz continuous in w and ∇ v G(·) be Lipschitz continuous in v. Let w t , v t be iterates from Algorithm 1 and</p><formula xml:id="formula_32">∞ t=0 α t = ∞, ∞ t=0 α 2 t &lt; ∞ . Then, lim t→∞ E[ ∇ w F(w t , v t ) 2</formula><p>2 ] = 0. Proof 1 First, by the definition of the objective function F(w, v), it can be easily checked that L(·) being Lipschitz continuous in w implies that ∇ w F(w, v) is a Lipschitz function in w for every v. Similarly, ∇ v G(·) being Lipschitz continuous in v implies that ∇ v F(w, v) is a Lipschitz function in v for all w. Further, without loss of generality, we assume all the Lipschitz constants are (upper bounded by) L.</p><p>Throughout the proof, as in the main text, n is the size of the training dataset. Define the n-dimensional vector e k as e t i = 1 if (x i , y i ) ∈ Ξ t and 0 otherwise and denote by ⊗ the point-wise product operation between two vectors:</p><formula xml:id="formula_33">[a 1 , a 2 ] ⊗ [b 1 , b 2 ] = [a 1 b 1 , a 2 b 2 ].</formula><p>When G is used, the update in each iteration is two consecutive gradient steps as follows:</p><formula xml:id="formula_34">w t+1 = w t − α t ∇ w F(w t , v t )| Ξt , v t+1 = v t − α t ∇ v F(w t+1 , v t )| Ξt .</formula><p>Since the mini-batch Ξ t is draw uniformly at random, we can rewrite the update as:</p><formula xml:id="formula_35">w t+1 = w t − α t [∇ w F(w k , v t ) + ξ t ], v t+1 = v t − α t [e t ⊗ ∇ v F(w t+1 , v t )], where ξ t = ∇ w F(w t , v t )| Ξt − ∇ w F(w k , v t ).</formula><p>Note that both ξ t and e t are iid random variables with finite variance, since Ξ t are drawn iid with a finite number (b) of samples. Further,</p><formula xml:id="formula_36">E[∇ w F(w t , v t )| Ξt − ∇ w F(w k , v t )]</formula><p>= 0, since samples are drawn uniformly at random.</p><p>By Lipschitz continuity of ∇ w F (w, v) (and L being the Lipschitz constant), we obtain the following:</p><formula xml:id="formula_37">F(w t+1 , v t ) − F(w t , v t ) ≤ ∇ w F(w t , v t ), w t+1 − w t + L 2 w t+1 − w t 2 2 = ∇ w F(w t , v t ), −α t [∇ w F(w t , v t ) + ξ t ] + L 2 w t+1 − w t 2 2 = −α t { ∇ w F(w t , v t ) 2 2 + ∇ w F(w t , v t ), ξ t } + L 2 w t+1 − w t 2 2 = −α t { ∇ w F(w t , v t ) 2 2 + ∇ w F(w t , v t ), ξ t } + Lα 2 t 2 ∇ w F(w t , v t ) + ξ t 2 2 = −(α t − Lα 2 t 2 ) ∇ w F(w t , v t ) 2 2 + Lα 2 t 2 ξ t 2 2 − (α t − Lα 2 t ) ∇ w F(w t , v t ), ξ t .</formula><p>Similarly, by the Lipschitz continuity of ∇ v F (w, v), we have:</p><formula xml:id="formula_38">F(w t+1 , v t+1 ) − F(w t+1 , v t ) ≤ ∇ v F(w t+1 , v t ), v t+1 − v t + L 2 v t+1 − v t 2 2 = ∇ v F(w t+1 , v t ), −α t e t ⊗ ∇ v F(w t+1 , v t ) + L 2 v t+1 − v t 2 2 = −α t ∇ v F(w t+1 , v t ), e k ⊗ ∇ v F(w t+1 , v t ) + Lα 2 t 2 e t ⊗ ∇ v F(w t+1 , v t ) 2 2 .</formula><p>Note that when G is not used, the bound for F(w t+1 , v t ) − F(w t , v t ) is still the same, but the bound for</p><formula xml:id="formula_39">F(w t+1 , v t+1 ) − F(w t+1 , v t ) is now simply F(w t+1 , v t+1 ) − F(w t+1 , v t ) ≤ 0.</formula><p>Combining the above two equations, we then have: <ref type="bibr" target="#b69">(Mairal, 2013)</ref></p><formula xml:id="formula_40">Since ∞ k=0 α 2 k &lt; ∞, the above inequality immediately implies that ∞ k=0 α k E[ ∇ w F(w k , v k ) 2 2 ] &lt; ∞ and ∞ k=0 α k E[ ∇ v F(w k+1 , v k ) 2 2 ] &lt; ∞. If G is not used, then by a similar argument, we have ∞ k=0 α k E[ ∇ w F(w k , v k ) 2 2 ] &lt; ∞. By Lemma A.5 in</formula><formula xml:id="formula_41">, to show lim k→∞ E[ ∇ w F(w k , v k ) 2 2 ] = 0, since α k is not summable, it suffices to show E[ ∇ w F(w k+1 , v k+1 ) 2 2 ] − E[ ∇ w F(w k , v k ) 2 2 ] ≤ Cα k for some constant C.</formula><p>To do so, we first recall a useful fact: for any two vectors a, b and any finite-dimensional vector norm · ,</p><formula xml:id="formula_42">( a + b )( a − b ) ≤ a + b a − b .<label>(17)</label></formula><p>We have:</p><formula xml:id="formula_43">E[ ∇ w F(w t+1 , v t+1 ) 2 2 ] − E[ ∇ w F(w t , v t ) 2 2 ] = E ∇ w F(w t+1 , v t+1 ) 2 + ∇ w F(w t , v t ) 2 ∇ w F(w t+1 , v t+1 ) 2 − ∇ w F(w t , v t ) 2 ≤ E ∇ w F(w t+1 , v t+1 ) 2 + ∇ w F(w t , v t ) 2 · ∇ w F(w t+1 , v t+1 ) 2 − ∇ w F(w t , v t ) 2 ≤ E ∇ w F(w t+1 , v t+1 ) + ∇ w F(w t , v t ) 2 · ∇ w F(w t+1 , v t+1 ) 2 − ∇ w F(w t , v t ) 2 ≤ 2LBE (w t+1 , v t+1 ) − (w t , v t ) 2 ≤ 2LBα t E ∇ w F(w t , v t ) + ξ k , e t ⊗ ∇ w F(w t+1 , v t ) 2 = 2LBα t E ∇ w F(w t , v t ) + ξ k 2 2 + e t ⊗ ∇ w F(w t+1 , v t ) 2 2 ≤ 2LBα t E ∇ w F(w t , v t ) + ξ k 2 2 + E e t ⊗ ∇ w F(w t+1 , v t ) 2 2 ≤ 2LBα k 2E ∇ w F(w t , v t ) 2 2 + 2E ξ t 2 2 + E ∇ w F(w t+1 , v t ) 2 2 ≤ 2LBα t √ 5B 2 = 2 √ 5B 2 Lα t ,</formula><p>where the first inequality is an application of Jensen's inequality, the second inequality follows from Equation <ref type="formula" target="#formula_0">(17)</ref> the third inequality follows from Lipschitz continuity and the sixth inequality follows from another application of Jensen's inequality. Consequently, by Lemma and the above chain of inequalities, it follows that lim t→∞ E[ ∇ w F(w t , v t ) 2 2 ] = 0. Note that if G is used, then by a similar argument, it follows that</p><formula xml:id="formula_44">lim t→∞ E[ ∇ v F(w t , v t ) 2 2 ] = 0. Consequently, if G is used, then lim t→∞ E[ ∇F(w t , v t ) 2 2 ] = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Proposition 1</head><p>Proposition 1 Suppose (x, y) denotes a training sample and its corrupted label. For simplicity, let the MentorNet input φ(x, y, w) = be the loss computed by the StudentNet model parameter w. The MentorNet g m ( ; Θ) = v, where v is the sample weight. If g m decreases with respect to , then there exists an underlying robust objective F :</p><formula xml:id="formula_45">F (w) = 1 n n i=1 ρ( i ),</formula><p>where ρ( i ) = i 0 g m (x; Θ)dx. In the special cases, ρ( ) degenerates to the classical robust M-estimator: Huber  and the log-sum penalty . Proof 2 Given a MentorNet g m and its fixed parameter Θ. As φ(x, y, w) = , we have g m (φ(x, y, w); Θ) = g m ( ; Θ). g m is a neural network and hence is continuous with respect to . Define ρ( ) as</p><formula xml:id="formula_46">ρ( ) = 0 g m (x; Θ)dx (18)</formula><p>Given the condition in the proposition, g m is decreasing with respect to . The function ρ is then bounded by its 1st term of the Taylor series about a point w * . We have:</p><formula xml:id="formula_47">ρ(φ(x, y, w)) ≤ ρ(φ(x, y, w * )) + g m (φ(x, y, w * ); Θ)(φ(x, y, w) − φ(x, y, w * ))<label>(19)</label></formula><p>According to , the right-hand side in <ref type="figure" target="#fig_0">Eq. (19)</ref> is a tractable surrogate for ρ(φ(x, y, w)) and there exists an underlying robust objective. For the i-th sample, we have:</p><formula xml:id="formula_48">ρ(φ(x i , y i , w)) = ρ( i ) = i 0 g m (x; Θ)dx<label>(20)</label></formula><p>Finally, we have a robust objective derived from:</p><formula xml:id="formula_49">F (w) = 1 n n i=1 ρ(φ(x i , y i , w)) = 1 n n i=1 ρ( i )<label>(21)</label></formula><p>Now we show the connection between Eq. (20) to the robust M-estimator. For simplicity, we assume that the loss ≥ 0 is non-negative for every training sample. For the Huber loss, there exists an Θ * such that:</p><formula xml:id="formula_50">g m ( ; Θ * ) = 1 2 ≤ λ 2 λ 2 √ otherwise ,<label>(22)</label></formula><p>where λ &gt; 0. It is easy to verify g m is decreasing with respect to , and we have:</p><formula xml:id="formula_51">ρ( ) = 0 g m (x; Θ * )dx = 1 2 ≤ λ 2 λ( √ − 1 2 λ) otherwise<label>(23)</label></formula><p>Therefore we have:</p><formula xml:id="formula_52">ρ( 2 ) = 1 2 2 ≤ λ 2 λ( − 1 2 λ) otherwise<label>(24)</label></formula><p>Eq. (24) has a similar form of the Huber M-estimator .</p><p>Likewise, there exists an Θ * and a positive such that</p><formula xml:id="formula_53">g m ( ; Θ * ) = λ +<label>(25)</label></formula><p>Its underlying objective is identical to the log-sum penalty :</p><formula xml:id="formula_54">ρ( ) = 0 g m (x; Θ * )dx = λ log( + ) − λ log( ) = λ log(1 + )<label>(26)</label></formula><p>It leads to the following log-sum penalty:</p><formula xml:id="formula_55">F (w) = λ 1 n n i=1 log(1 + i )<label>(27)</label></formula><p>For the Lorentzian <ref type="bibr" target="#b54">(Black &amp; Anandan, 1996)</ref>. We have g m ( ; Θ * ) = 2 2δ 2 + 2 (28)</p><p>In special cases, we assume that δ is a small positive such that all sample loss ≥ √ 2δ, the underlying objective is</p><formula xml:id="formula_56">ρ( ) = 0 g m (x; Θ * )dx = log(2δ 2 + 2 ) − log 2δ 2 = log(1 + 1 2 ( δ ) 2 )<label>(29)</label></formula><p>The above objective becomes Lorentzian <ref type="bibr" target="#b54">(Black &amp; Anandan, 1996)</ref>, also known as Lorentzian/Cauchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison on MentorNet Architectures</head><p>This section examines MentorNet's architecture in terms of learning existing predefined curriculums (or weighting scheme).</p><p>To generate the data for training MentorNet, we enumerate the feasible input space of z i including the loss, difference to the loss moving average, label, and epoch percentage. For this experiment, the dataset contains a total of 300k samples. For each sample, we label a weight according to the weighting scheme in the predefined curriculum. For example, we compute the weight for focal loss by</p><formula xml:id="formula_57">v * i = [1 − exp{− i }] γ ,<label>(30)</label></formula><p>where γ is a hyperparameter for smoothing the distribution. We consider the following predefined curriculums: selfpaced , hard negative mining , linear weighting , focal loss , and prediction variance . Besides, we also consider a temporal mixture weighting which is a mix of the self-paced (when the epoch percentage &lt; 50) and the hard negative mining (when the epoch percentage ≥ 50). In some curriculums, the form G is unknown. We directly minimize the mean squared error between the MentorNet's output and the true weight.</p><p>We compare the MentorNet architecture in <ref type="figure" target="#fig_0">Fig. 1</ref> in the paper to a simple logistic regression and 3 classical networks: a 2-layer Multiple Layer Perceptron (MLP), a 2-layer CNN with mean pooling, and an LSTM network (RNN) to sequentially encode the features of every example in a mini-batch. The same features are used across all networks. The objective is minimized by the Adam optimizer <ref type="bibr" target="#b64">(Kingma &amp; Ba, 2015)</ref>. We use a very simple network for the proposed MentorNet. The bidirectional LSTM has a single layer of 10 base LSTM units (step size = 10). We use an embedding layer (size = 2) for the labels and an embedding layer (size = 5) for the integer epoch percentage between 0 and 99. The fully connected layer f c 1 uses the tangent activation function and has 20 hidden nodes.</p><p>The performance is evaluated by the Mean Squared Error (MSE) to the true weight produced by each curriculum. Each experiment is repeated 5 times using random starting values, and the average MSE (with the 90% confidence interval) is reported. <ref type="table" target="#tab_1">Table 1</ref> shows the comparison results. As we see, the simple network structure MLP performs well except for complex weighting schemes that are prediction variance  and Temporal mixture weighting. Nevertheless, the bi-LSTM structure in <ref type="figure" target="#fig_0">Fig.1</ref> of the paper performs better than other classical network architectures. <ref type="figure" target="#fig_0">Fig. 1</ref>   E. Implementation Details implementation 2 as our StudentNet. We train the model on the ImageNet of 40% noise. The</p><p>Step 12 in Algorithm 1 in the paper is implemented as a distributed asynchronized momentum SGD optimizer (momentum = 0.9) on 50 GPUs. We set the batch size to 32 and train the model until it converges. That is 500 thousand steps for the StudentNet without any regularization and 1 million steps for the StudentNet with full regularization (weight decay, dropout and data augmentation). The starting learning rate is 0.05 and is decreased by a factor of 0.1 every 10 training epochs. The default dropout-keepprobability hyperparameter set to 0.8. In the Forgetting baseline, we set it to 0.2, which is the best parameter found on CIFAR-100. The weight decay is 4e−5. The batch norm is used and its decay is set to 0.9997 and the epsilon is set to 0.001. The default data augmentation in the slim implementation is used. Training in this way on the clean training dataset, the validation accuracy is Hit@1=0.765.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Baselines</head><p>Regarding the baseline method. FullMode is the standard StudentNet trained using l 2 weight decay, dropout  and data augmentation <ref type="bibr" target="#b66">(Krizhevsky et al., 2012)</ref>. The same parameters discussed in Appendix E.1 are used. Forgetting was introduced in . It is same as the FullModel except that the dropout parameter is tuned in the range of (0.2-0.9). For the self-paced learning , we gradually increase λ in training by 20%.</p><p>Following  we tune the parameter in the range of the 50th, 60th and 75th percentile of average sample loss. For the focal loss , we tune its γ in the range of {1, 2, 3} in our experiments. It is easy to verify that Eq. (30) leads to the same classification objective in the focal loss , an award-winning method for object detection. For the , we implement two versions: the soft and hard version. Let q = [q 1 , . . . , q m ] be the prediction (logits after softmax) for m classes:</p><formula xml:id="formula_58">i = −(β m j=1 1(y i = j) log(q j ) + (1 − β) m j=1 q j log(q j )),<label>(31)</label></formula><p>where 1 is the indicator function and a hard version: i = −(β m j=1 1(y i = j) log(q j ) + (1 − β) max j log(q j )),</p><p>We tune the parameters β in the range of {0.7, 0.8, 0.9, 0.95}. Goldberger <ref type="bibr" target="#b13">(Goldberger &amp; Ben-Reuven, 2017</ref>) is a recent baseline weakly-supervised learning method. We implement the S-Model in the paper by appending an additional layer to the StudentNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Setups of Our Model</head><p>The details about the MentorNet architecture ( <ref type="figure" target="#fig_0">Fig. 1</ref> in the paper) are discussed in Appendix D. MentorNet PD is learned using the curriculum in Eq. (5) in the paper. The loss moving average pt is set to the 75th-percentile loss in a mini-batch. We tune the hyperparameter λ 1 and λ 2 . MentorNet DD is the learned data-driven curriculum. It is trained on 5,000 images of true labels, randomly sampled from CIFAR-10. We learn MentorNet DD on this dataset and apply it to CIFAR-100 on which no true labels are used. We use the CIFAR-10 subset of the same level of the noise fraction corresponding to the CIFAR-100. CIFAR-10 and CIFAR-100 are two different datasets that have not only different classes but also the different number of classes. As CIFAR-100 and CIFAR-10 have the different number of classes, to apply a MentorNet, we fix the class label to 0.</p><p>Algorithm 1 is used to train the MentorNet together with the StudentNet. The decay factor in computing the loss moving average is set to 0.95. As mentioned in the paper, a burn-in process is used in the first 20% training epoch for both MentorNet DD and MentorNet PD. We update and learn MentorNet twice after the learning rate is changed. That is on the 21% and 75% of the total epoch. More updates lead to insignificant performance difference. For each mini-batch, the weight decay parameter θ in Eq. (1) in the paper is normalized by the sum of the weight in a mini-batch. That is θ t = 1 b θ 0 b i=1 v Ξi , where θ 0 is the original weight decay parameter. The same learning rate scheduling strategy in the StudentNet is used in Algorithm 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The label and the training epoch percentage are encoded by two separate embedding layers. The epoch percentage is represented as an integer between 0 and 99. It is used to indicate the StudentNet's training progress, where 0 rep-The MentorNet architecture used in experiments. emb, fc and prob sampling stand for the embedding, fully-connected and probabilistic sampling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Visualization of sample loss and learning objective. The learned curriculum represents the best data-driven curriculum found in experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>error on corrupted labelsFigure 3. Error of resnet trained on CIFAR-100 of 40% noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The data-driven curriculums learned by MentorNet with the resnet-101 at epoch 21 in (a) and 76 in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Comparison of explicit and implicit MentorNet training. Convergence comparison of different MentorNet architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>proposed deep learning generalization theories to explain this interesting phenomenon. This paper studies how to overcome the corrupted label for 1 Google Inc., Mountain View, United States 2 Stanford University, Stanford, United States. Correspondence to: Lu Jiang &lt;lujiang@google.com&gt;. Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>StudentNet and their accuracies on the clean training data.</figDesc><table><row><cell>Dataset CIFAR10 CIFAR100 ImageNet</cell><cell>Model inception resnet101 inception resnet101 inception resnet 59M #para train acc val acc 1.7M 0.83 0.81 84M 1.00 0.96 1.7M 0.64 0.49 84M 1.00 0.79 0.88 0.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). For MentorNet, the training error is computed by i v i i . The figure shows two insights.</figDesc><table /><note>First, the training error of MentorNet approaches zero. This empirically verifies the convergence of the model. Second, MentorNet can overcome the overfitting to the corrupted label. While the training error is decreasing, the test error</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparison of validation accuracy on CIFAR-10 and CIFAR-100 under different noise fractions.</figDesc><table><row><cell>Method</cell><cell>Resnet-101 StudentNet CIFAR-100 CIFAR-10 0.2 0.4 0.8 0.2 0.4</cell><cell>0.8</cell><cell>Inception StudentNet CIFAR-100 CIFAR-10 0.2 0.4 0.8 0.2 0.4</cell><cell>0.8</cell></row><row><cell>FullModel Forgetting Self-paced</cell><cell cols="4">0.60 0.45 0.08 0.82 0.69 0.18 0.43 0.38 0.15 0.76 0.73 0.42 0.61 0.44 0.16 0.78 0.63 0.35 0.42 0.37 0.17 0.76 0.71 0.44 0.70 0.55 0.13 0.89 0.85 0.28 0.44 0.38 0.14 0.80 0.74 0.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table /><note>compares to recent published results under the set- ting: CIFAR of 40% noise fraction. We cite the number in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Validation</figDesc><table><row><cell cols="2">accuracy comparison to representative pub-</cell></row><row><cell cols="2">lished results on CIFAR of 40% noise.</cell></row><row><cell>ID Method 1 Reed Hard (Resnet) 2 Reed Soft (Resnet) 3 Goldberger (Resnet) 4 Azadi (AlexNet) (2016) 5 MentorNet (Resnet)</cell><cell>CIFAR-10 CIFAR-100 0.62 0.47 0.61 0.46 0.70 0.46 0.75 -0.89 0.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Accuracy on the clean ImageNet ImageNet validation set. Models are trained on the ImageNet training data of 40% noise.</figDesc><table><row><cell>Method NoReg NoReg+WeDecay NoReg+Dropout NoReg+DataAug NoReg+MentorNet FullModel Forgetting(FullModel) MentorNet(FullModel) 0.651 0.859 P@1 P@5 0.538 0.770 0.560 0.809 0.575 0.807 0.522 0.772 0.590 0.814 0.612 0.844 0.628 0.845</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Validation accuracy on the ImageNet ILSVRC12 and Web-</figDesc><table><row><cell>Vision validation set. The number outside (inside) the parentheses</cell></row><row><cell>denotes top-1 (top-5) classification accuracy (%). * marks the</cell></row><row><cell>method trained using additional verification labels. Dataset Method ILSVRC12 Entire Li et al. (2017a) 0.476 (0.704) 0.570 (0.779) WebVision Entire Forgetting 0.590 (0.808) 0.666 (0.856) Entire Lee et al. (2017)* 0.602 (0.811) 0.685 (0.865) Entire MentorNet 0.625 (0.830) 0.708 (0.880) Entire MentorNet* 0.642 (0.848) 0.726 (0.889) Mini FullModel 0.585 (0.818) -Mini Forgetting 0.562 (0.816) -Mini Reed Soft 0.565 (0.827) -Mini Self-paced 0.576 (0.822) -Mini MentorNet 0.638 (0.858) -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Supplementary Materials: MentorNet Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels Lu Jiang Zhengyuan Zhou Thomas Leung Li-Jia Li Li Fei-Fei</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>illustrates the error curve of different MentorNet architecture during training, where the x-axis is the training epoch and y-axis is the MSE.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorflow/models/tree/master/research/slim</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tensorflow/models/blob/master/research/slim/nets/inception resnet v2.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank anonymous reviewers for helpful comments and Deyu Meng, Sergey Ioffe, and Chong Wang for meaningful discussions and kind support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzkebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Auxiliary image regularization for deep cnns with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhancing sparsity by reweighted l1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier analysis and applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="877" to="905" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Active bias: Training a more accurate neural network by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07726</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Information geometry and alternating minimization procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Csiszar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and decisions</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="205" to="237" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Avoiding your teacher&apos;s mistakes: Training neural networks with controlled weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00313</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fidelity-weighted learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-paced learning: An implicit regularization perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to teach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training deep neuralnetworks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated curriculum learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Encyclopedia of Statistical Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1248" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How do humans teach: On curriculum learning and teaching dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-paced co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On convergence property of implicit self-paced objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09923</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">What objective does self-paced learning indeed optimize?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06049</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from corrupted binary labels via classprobability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Self paced deep learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07651</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<title level="m">Training convolutional networks with noisy labels</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-paced learning for long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Robust probabilistic modeling with bayesian data reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="894" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mirror descent in non-convex stochastic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glynn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05681</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stochastic mirror descent in variationally coherent optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Table 1. The MSE comparison of different MentorNet architecture on predefined curriculums</title>
	</analytic>
	<monogr>
		<title level="m">Weighting Scheme Logistic MLP CNN RNN MentorNet (bi-LSTM)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<title level="m">Hard negative mining</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>1±0.7E-3 1.6±0.6E-5 2.7±0.6E-3 2.2±0.4E-3 6.6±4.5E-7</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<title level="m">Linear weighting</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>2±0.1E-4 1.2±0.4E-4 1.1±0.2E-4 2.0±0.3E-2 4.4±1.3E-5 Prediction variance (Chang et al., 2017) 6.8±0.1E-3 4.0±0.1E-3 2.8±0.4E-2 6.2±0.2E-3 1.4±0.7E-3 Focal loss</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">We call it implicit training. When the form of G is known, we can learn a MentorNet by directly minimizing Eq.(4) in the paper, called explicit training. These two training approaches are theoretically identical and we empirically compare them on two curriculums of known G in Fig. 2. As shown</title>
	</analytic>
	<monogr>
		<title level="m">Table 1, we learn a MentorNet by minimizing the MSE between its output and the true weight</title>
		<imprint/>
	</monogr>
	<note>we found implicit training converges faster</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Both datasets contain 50,000 training and 10,000 validation images. The inception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno>wide-resnet- 101</idno>
		<editor>E.1. Dataset and StudentNet CIFAR-10 and CIFAR-100 (Krizhevsky &amp; Hinton</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>consist of 32 × 32 color images arranged in 10 and 100 classes. Zagoruyko &amp; Komodakis, 2016) are used as the StudentNet. Their implementations are based on the TensorFlow slim implementation 1 , where the inception is detailed in (Zhang et al., 2017) and the wider resnet is in (Zagoruyko &amp; Komodakis</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The Step 12 in Algorithm 1 in the paper is implemented by the momentum SGD optimizer (momentum = 0.9) on a single GPU. We use the common learning rate scheduling strategy and set the starting learning rate as 0.1 and multiply it by a factor of 0.1 at the 19.5k, 25k and 30k iteration for the resnet model, and 78k for the inception model</title>
	</analytic>
	<monogr>
		<title level="m">Training in this way on the clean training dataset, the validation accuracy reaches about 81.4% and 95.5% for inception and resnet-101 on CIFAR-10</title>
		<imprint/>
	</monogr>
	<note>training, the batch size is set to 128 and we train 39K iterations for resnet model and 120k for the inception model. and about 49.2% and 78.8% on CIFAR-100</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">2014) masks out network fully-connected layer outputs randomly. We use the best hyperparameter found on the clean training data. In the inception network, weight decay is set 4e−3 and the dropout keep probability is 0.5. In the resnet-101, weight decay is 2e−4 and the dropout keep probability is 1.0. In the Forgetting baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">By default, the StudentNet incorporates three types of regularization: 1) the weight decay which adds an l 2 norm of the model parameters into the learning objective; 2) data augmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>The data augmentation is the same for the inception and the resnet-101: we pad 4 pixels to each side and randomly sampling a 32 x 32 crop, randomly flip the image horizontally (left to right), and then linearly scale the image to have zero mean and unit norm. Unless specified otherwise, the StudentNet of the same hyperparameter, discussed above, is used in all baseline and the proposed model</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Each image is resized to 299 × 299 with 3 color channels. For the ImageNet, we use the inception-resnet v2 slim References Arpit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilsvrc2012 (</forename><surname>Imagenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>A closer look at memorization in deep networks</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="75" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Enhancing sparsity by reweighted l1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier analysis and applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="877" to="905" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Active bias: Training a more accurate neural network by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">; P</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2017. Gong</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Stochastic majorization-minimization algorithms for large-scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">What objective does self-paced learning indeed optimize?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06049</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="894" to="942" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
