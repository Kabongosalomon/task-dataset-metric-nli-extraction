<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Information Systems Technology and Design</orgName>
								<orgName type="institution">SUTD</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
							<email>hazarika@comp.nus.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Centro de Investigación en Computación</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Naik</surname></persName>
							<email>gautam@sentic.net</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@ntu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<email>mihalcea@umich.edu</email>
							<affiliation key="aff4">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion recognition in conversations (ERC) is a challenging task that has recently gained popularity due to its potential applications. Until now, however, there has been no largescale multimodal multi-party emotional conversational database containing more than two speakers per dialogue. To address this gap, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues from the TV-series Friends. Each utterance is annotated with emotion and sentiment labels, and encompasses audio, visual, and textual modalities. We propose several strong multimodal baselines and show the importance of contextual and multimodal information for emotion recognition in conversations. The full dataset is available for use at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid growth of Artificial Intelligence (AI), multimodal emotion recognition has become a major research topic, primarily due to its potential applications in many challenging tasks, such as dialogue generation, user behavior understanding, multimodal interaction, and others. A conversational emotion recognition system can be used to generate appropriate responses by analyzing user emotions <ref type="bibr" target="#b21">(Zhou et al., 2017;</ref><ref type="bibr" target="#b14">Rashkin et al., 2018)</ref>.</p><p>Although significant research work has been carried out on multimodal emotion recognition using audio, visual, and text modalities <ref type="bibr" target="#b18">(Zadeh et al., 2016a;</ref><ref type="bibr" target="#b17">Wollmer et al., 2013)</ref>, significantly less work has been devoted to emotion recognition in conversations (ERC). One main reason for this is the lack of a large multimodal conversational dataset.</p><p>According to , ERC presents several challenges such as conversational context modeling, emotion shift of the interlocutors, and others, which make the task more difficult to address. Recent work proposes solutions based on multimodal memory networks <ref type="bibr" target="#b5">(Hazarika et al., 2018)</ref>. However, they are mostly limited to dyadic conversations, and thus not scalable to ERC with multiple interlocutors. This calls for a multi-party conversational data resource that can encourage research in this direction.</p><p>In a conversation, the participants' utterances generally depend on their conversational context. This is also true for their associated emotions. In other words, the context acts as a set of parameters that may influence a person to speak an utterance while expressing a certain emotion. Modeling this context can be done in different ways, e.g., by using recurrent neural networks (RNNs) and memory networks <ref type="bibr" target="#b5">(Hazarika et al., 2018;</ref><ref type="bibr" target="#b12">Poria et al., 2017;</ref><ref type="bibr" target="#b16">Serban et al., 2017)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example where the speakers change their emotions (emotion shifts) as the dialogue develops. The emotional dynamics here depend on both the previous utterances and their associated emotions. For example, the emotion shift in utterance eight (in the <ref type="figure">figure)</ref> is hard to determine unless cues are taken from the facial expressions and the conversational history of both speakers. Modeling such complex inter-speaker dependencies is one of the major challenges in conversational modeling.</p><p>Conversation in its natural form is multimodal. In dialogues, we rely on others' facial expressions, vocal tonality, language, and gestures to anticipate their stance. For emotion recognition, multimodal-   ity is particularly important. For the utterances with language that is difficult to understand, we often resort to other modalities, such as prosodic and visual cues, to identify their emotions. <ref type="figure" target="#fig_1">Figure 2</ref> presents examples from the dataset where the presence of multimodal signals in addition to the text itself is necessary in order to make correct predictions of their emotions and sentiments.</p><p>Multimodal emotion recognition of sequential turns encounters several other challenges. One such example is the classification of short utterances. Utterances like "yeah", "okay", "no" can express varied emotions depending on the context and discourse of the dialogue. However, due to the difficulty of perceiving emotions from text alone, most models resort to assigning the majority class (e.g., non-neutral in EmotionLines). Approximately 42% of the utterances in MELD are shorter than five words. We thus provide access to the multimodal data sources for each dialogue and posit that this additional information would benefit the emotion recognition task by improving the context representation and supplementing the missing or misleading signals from other modalities. Surplus information from attributes such as the speaker's facial expressions or intonation in speech could guide models for better classification. We also provide evidence for these claims through our experiments.</p><p>The development of conversational AI thus depends on the use of both contextual and multimodal information. The publicly available datasets for multimodal emotion recognition in conversations -IEMOCAP and SEMAINE -have facilitated a significant number of research projects, but also have limitations due to their relatively small number of total utterances and the lack of multi-party conversations. There are also other multimodal emotion and sentiment analysis datasets, such as MOSEI , MOSI <ref type="bibr" target="#b20">(Zadeh et al., 2016b)</ref>, and MOUD <ref type="bibr" target="#b11">(Pérez-Rosas et al., 2013)</ref>, but they contain individual narratives instead of dialogues. On the other hand, EmotionLines <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> is a dataset that contains dialogues from the popular TV-series Friends with more than two speakers. However, EmotionLines can only be used for textual analysis as it does not provide data from other modalities.</p><p>In this work, we extend, improve, and further develop the EmotionLines dataset for the multimodal scenario. We propose the Multimodal Emotion-Lines Dataset (MELD), which includes not only textual dialogues, but also their corresponding visual and audio counterparts. This paper makes several contributions:</p><p>• MELD contains multi-party conversations that are more challenging to classify than dyadic variants available in previous datasets.</p><p>• There are more than 13,000 utterances in MELD, which makes our dataset nearly double the size of existing multimodal conversational datasets.</p><p>• MELD provides multimodal sources and can be used in a multimodal affective dialogue system for enhanced grounded learning.</p><p>• We establish a strong baseline, proposed by , which is capable of emotion recognition in multi-party dialogues by interparty dependency modeling.</p><p>The remainder of the paper is organized as follows: Section 2 illustrates the EmotionLines dataset; we then present MELD in Section 3; strong baselines and experiments are elaborated in Section 4; future directions and applications of MELD are covered in Section 5 and 6, respectively; finally, Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EmotionLines Dataset</head><p>The MELD dataset has evolved from the Emo-tionLines dataset developed by <ref type="bibr" target="#b2">Chen et al. (2018)</ref>. EmotionLines contains dialogues from the popular sitcom Friends, where each dialogue contains utterances from multiple speakers.</p><p>EmotionLines was created by crawling the dialogues from each episode and then grouping them based on the number of utterances in a dialogue into four groups of <ref type="bibr">[5,</ref><ref type="bibr">9]</ref>, <ref type="bibr">[10,</ref><ref type="bibr">14]</ref>, <ref type="bibr">[15,</ref><ref type="bibr">19]</ref>, and <ref type="bibr">[20,</ref><ref type="bibr">24]</ref> utterances respectively. Finally, 250 dialogues were sampled randomly from each of these groups, resulting in the final dataset of 1,000 dialogues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation</head><p>The utterances in each dialogue were annotated with the most appropriate emotion category. For this purpose, Ekman's six universal emotions (Joy, Sadness, Fear, Anger, Surprise, and Disgust) were considered as annotation labels. This annotation list was extended with two additional emotion labels: Neutral and Non-Neutral.</p><p>Each utterance was annotated by five workers from the Amazon Mechanical Turk (AMT) platform. A majority voting scheme was applied to select a final emotion label for each utterance. The overall Fleiss' kappa score of this annotation process was 0.34.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multimodal EmotionLines Dataset (MELD)</head><p>We start the construction of the MELD corpus by extracting the starting and ending timestamps of  all utterances from every dialogue in the Emo-tionLines dataset. To accomplish this, we crawl through the subtitles of all the episodes and heuristically extract the respective timestamps. In particular, we enforce the following constraints:</p><p>1. Timestamps of the utterances in a dialogue must be in an increasing order.</p><p>2. All the utterances in a dialogue have to belong to the same episode and scene.</p><p>These constraints revealed a few outliers in Emo-tionLines where some dialogues span across scenes or episodes. For example, the dialogue in <ref type="table">Table 2</ref> contains two natural dialogues from episode 4 and 20 of season 6 and 5, respectively. We decided to filter out these anomalies, thus resulting in a different number of total dialogues in MELD as compared to EmotionLines (see <ref type="table" target="#tab_2">Table 1</ref>).</p><p>Next, we employ three annotators to label each utterance, followed by a majority voting to decide the final label of the utterances. We drop a few utterances where all three annotations were different, and also remove their corresponding dialogues to maintain coherence. A total of 89 utterances spanning 11 dialogues fell under this category.</p><p>Finally, after obtaining the timestamp of each utterance, we extract their corresponding audiovisual clips from the source episode followed by the extraction of audio content from these clips. We format the audio files as 16-bit PCM WAV files for further processing. The final dataset includes visual, audio, and textual modalities for each utterance. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Re-annotation</head><p>The utterances in the original EmotionLines dataset were annotated by looking only at the transcripts. However, due to our focus on multimodality, we re-annotate all the utterances by asking the three annotators to also look at the available video clip of the utterances. We then use majority-voting to obtain the final label for each utterance.  <ref type="table">Table 2</ref>: A dialogue in EmotionLines where utterances from two different episodes are present. The first four utterances in this dialogue have been taken from episode 4 of season 6. The last three utterances in red font are from episode 20 of season 5.</p><p>The annotators were graduate students with high proficiency in English speaking and writing. Before starting the annotation, they were briefed about the annotation process with a few examples.</p><p>We achieve an overall Fleiss' kappa score of 0.43 which is higher than the original EmotionLines annotation whose kappa score was 0.34 (kappa of IEMOCAP annotation process was 0.4), thus suggesting the usefulness of the additional modalities during the annotation process.</p><p>2,772 utterances in the EmotionLines dataset were labeled as non-neutral where the annotators agreed that the emotion is not neutral but they could not reach agreement regarding the correct emotion label. This hampers classification, as the non-neutral utterance space and the other emotionlabel spaces get conflated. In our case, we remove the utterances where the annotators fail to reach an agreement on the definite emotion label.</p><p>The number of disagreements in our annotation process is 89, which is much lower than the 2,772 disagreements in EmotionLines, reflecting again the annotation improvement obtained through a multimodal dataset. <ref type="table" target="#tab_5">Table 3</ref> shows examples of utterances where the annotators failed to reach consensus. <ref type="table" target="#tab_6">Table 4</ref> shows the label-wise comparison between EmotionLines and MELD dataset. For most of the utterances in MELD, the annotations match the original annotations in EmotionLines. Yet, there exists a significant amount of samples whose utterances have been changed in the re-annotation process. For example, the utterance This guy fell asleep! (see <ref type="table" target="#tab_8">Table 5</ref>), was labeled as non-neutral   in EmotionLines but after viewing the associated video clip, it is correctly re-labeled as anger in MELD.</p><p>The video of this utterance reveals an angry and frustrated facial expression along with a high vocal pitch, thus helping to recognize its correct emotion. The annotators of EmotionLines had access to the context, but this was not sufficient, as the availability of additional modalities can sometime bring more information for the classification of such instances. These scenarios justify both context and multimodality to be important aspects for emotion recognition in conversation.</p><p>Timestamp alignment. There are many utterances in the subtitles that are grouped within identical timestamps in the subtitle files. In order to find the accurate timestamp for each utterance, we use a transcription alignment tool Gentle, 2 which automatically aligns a transcript with the audio by extracting word-level timestamps from the audio (see <ref type="table" target="#tab_10">Table 6</ref>). In <ref type="table" target="#tab_11">Table 7</ref>, we show the final format of the MELD dataset.</p><p>Dyadic MELD. We also provide another version of MELD where all the non-extendable contiguous dyadic sub-dialogues of MELD are extracted. For example, let a three-party dialogue in MELD with speaker ids 1, 2, 3 have their turns in the following order: <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">2,</ref><ref type="bibr">1,</ref><ref type="bibr">2]</ref>.</p><p>From this dialogue sequence, dyadic MELD will have the following sub-dialogues as samples: [1, 2, 1, 2], [2, 3, 2] and [2, 1, 2]. However, the reported results in this paper are obtained using only the multiparty variant of MELD.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Exploration</head><p>As mentioned before, we use seven emotions for the annotation, i.e., anger, disgust, fear, joy, neutral, sadness, and surprise, across the training, development, and testing splits (see <ref type="table" target="#tab_6">Table 4</ref>). It can be seen that the emotion distribution in the dataset is expectedly non-uniform with the majority emotion being neutral. We have also converted these fine-grained emotion labels into more coarse-grained sentiment classes by considering anger, disgust, fear, sadness as negative, joy as positive, and neutral as neutral sentiment-bearing class. Surprise is an example of a complex emotion which can be expressed with both positive and negative sentiment. The three annotators who performed the utterance annotation further annotated the surprise utterances into either positive or negative sentiment classes. The entire sentiment annotation task reaches a Fleiss' kappa score of 0.91. The distribution of positive, negative, neutral sentiment classes is given in <ref type="table" target="#tab_6">Table 4</ref>. <ref type="table" target="#tab_13">Table 8</ref> presents several key statistics of the dataset. The average utterance length -i.e. number of words in an utterance -is nearly the same across training, development, and testing splits. On average, three emotions are present in each dialogue of the dataset. The average duration of an utterance is 3.59 seconds. The emotion shift of a speaker in a dialogue makes emotion recognition task very challenging. We observe that the number of such emotion shifts in successive utterances of a speaker in a dialogue is very frequent: 4003, 427, and 1003 in train/dev/test splits, respectively. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example where speaker's emotion changes with time in the dialogue.</p><p>Character Distribution. In <ref type="figure">Figure 3</ref>, we present the distributional details of the primary characters in MELD. <ref type="figure">Figure a and b</ref> illustrate the distribution across the emotion and sentiment labels, respectively. <ref type="figure">Figure c</ref> shows the overall coverage of the speakers across the dataset. Multiple infrequent speakers (&lt; 1% utterances) are grouped as Others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Related Datasets</head><p>Most of the available datasets in multimodal sentiment analysis and emotion recognition are nonconversational. MOSI <ref type="bibr" target="#b20">(Zadeh et al., 2016b)</ref>, MO-SEI , and MOUD <ref type="bibr" target="#b11">(Pérez-Rosas et al., 2013)</ref> are such examples that have drawn significant interest from the research community. On the other hand, IEMOCAP and SEMAINE are two popular dyadic conversational datasets where each utterance in a dialogue is labeled by emotion.</p><p>The SEMAINE Database is an audiovisual database created for building agents that can engage a person in a sustained and emotional conversation <ref type="bibr" target="#b9">(McKeown et al., 2012)</ref>. It consists of interactions involving a human and an operator (either a machine or a person simulating a machine). The dataset contains 150 participants, 959 conversations, each lasting around 5 minutes. A subset of this dataset was used in AVEC 2012's fully continuous sub-challenge <ref type="bibr" target="#b15">(Schuller et al., 2012)</ref> that requires predictions of four continuous affective dimensions: arousal, expectancy, power, and valence. The gold annotations are available for every 0.2 second in each video for a total of 95 videos comprising 5, 816 utterances.</p><p>The Interactive Emotional Dyadic Motion Capture Database (IEMOCAP) consists of videos of dyadic conversations among pairs of 10 speakers spanning 10 hours of various dialogue scenarios <ref type="bibr" target="#b1">(Busso et al., 2008)</ref>. Videos are segmented into utterances with annotations of fine-grained emotion categories: anger, happiness, sadness, neutral, excitement, and frustration. IEMOCAP also provides continuous attributes: activation, valence, and dominance. These two types of discrete and continuous emotional descriptors facilitate the complementary insights about the emotional expressions of humans and emotional communications between people. The labels in IEMOCAP were annotated by at least three annotators per utterance and self-assessment manikins (SAMs) were also employed to evaluate the corpus <ref type="bibr" target="#b0">(Bradley and Lang, 1994)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with MELD</head><p>Both resources mentioned above are extensively used in this field of research and contain settings   that are aligned to the components of MELD. However, MELD is different in terms of both complexity and quantity. Both IEMOCAP and SE-MAINE contain dyadic conversations, wherein the dialogues in MELD are multi-party. Multi-party conversations are more challenging compared to dyadic. They provide a flexible setting where multiple speakers can engage. From a research perspective, such availability also demands proposed dialogue models to be scalable towards multiple speakers. MELD also includes more than 13000 emotion labeled utterances, which is nearly double the annotated utterances in IEMOCAP and SEMAINE. <ref type="table" target="#tab_15">Table 9</ref> provides information on the number of available dialogues and their constituent utterances for all three datasets, i.e., IEMOCAP, SEMAINE, and MELD. <ref type="table" target="#tab_2">Table 10</ref> shows the distribution for common emotions as well as highlights a few key statistics of IEMOCAP and MELD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Extraction</head><p>We follow <ref type="bibr" target="#b12">Poria et al. (2017)</ref> to extract features for each utterance in MELD. For textual features, we initialize each token with pre-trained 300-dimensional GloVe vectors <ref type="bibr" target="#b10">(Pennington et al., 2014)</ref> and feed them to a 1D-CNN to extract 100  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Models</head><p>To provide strong benchmarks for MELD, we perform experiments with multiple baselines. Hyperparameter details for each baseline can be found at http://github.com/senticnet/meld. text-CNN applies CNN to the input utterances without considering the context of the conversation <ref type="bibr" target="#b6">(Kim, 2014)</ref>. This model represents the simplest baseline which does not leverage context or multimodality in its approach.</p><p>bcLSTM is a strong baseline proposed by <ref type="bibr" target="#b12">Poria et al. (2017)</ref>, which represents context using a bi-directional RNN. It follows a two-step hierarchical process that models uni-modal context first and then bi-modal context features. For unimodal text, a CNN-LSTM model extracts contextual representations for each utterance taking the GloVe em-   DialogueRNN represents the current state of the art for conversational emotion detection . It is a strong baseline with effective mechanisms to model context by tracking individual speaker states throughout the conversation for emotion classification. DialogueRNN is capable of handling multi-party conversation so it can be directly applied on MELD. It employs three stages of gated recurrent units (GRU) <ref type="bibr" target="#b3">(Chung et al., 2014)</ref> to model emotional context in conversations. The spoken utterances are fed into two GRUs: global and party GRU to update the context and speaker state, respectively. In each turn, the party GRU updates its state based on 1) the utterance spoken, 2) the speaker's previous state, and 3) the conversational context summarized by the global GRU through an attention mechanism. Finally, the updated speaker state is fed into the emotion GRU which models the emotional information for classification. Attention mechanism is used on top of the emotion GRU to leverage contextual utterances by different speakers at various distances. To analyze the role of multimodal signals, we analyze DialogueRNN and bcLSTM on MELD for both uni and multimodal settings. Training involved usage of class weights to alleviate imbalance issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We provide results for the two tasks of sentiment and emotion classification on MELD. <ref type="table" target="#tab_2">Table 13</ref> shows the performance of sentiment classification by using DialogueRNN, whose multimodal variant achieves the best performance (67.56% F-score) surpassing multimodal bcLSTM (66.68% F-score). Multimodal DialogueRNN also outperforms its unimodal counterparts. However, the improvement due to fusion is about 1.4% higher than the textual modality which suggests the possibility of further improvement through better fusion mechanisms. The textual modality outperforms the audio modality by about 17%, which indicates the importance of spoken language in sentiment analysis. For positive sentiment, audio modality performs poorly. It would be interesting to analyze the clues specific to positive sentiment bearing utterances in MELD that the audio modality could not capture. Future work should aim for enhanced audio feature extraction schemes to improve the classification performance. <ref type="table" target="#tab_2">Table 11</ref> presents the results of the baseline models on MELD emotion classification. The performance on the emotion classes disgust, fear, and sadness are particularly poor. The primary reason for this is the inherent imbalance in the dataset which has fewer training instances for these mentioned emotion classes (see <ref type="table" target="#tab_6">Table 4</ref>). We partially tackle this by using class-weights as hyper-parameters.</p><p>Yet, the imbalance calls for further improvement for future work to address. We also observe high  mis-classification rate between the anger, disgust, and fear emotion categories as these emotions have subtle differences among them causing harder disambiguation. Similar to sentiment classification trends, the textual classifier outperforms (57.03% F-score) the audio classifier (41.79% F-score).</p><p>Multimodal fusion helps in improving the emotion recognition performance by 3%. However, multimodal classifier performs worse than the textual classifier in classifying sadness. To analyze further, we also run experiments on 5-class emotions by dropping the infrequent fear and disgust emotions (see <ref type="table" target="#tab_2">Table 12</ref>). Not surprisingly, the results improve over the 7-class setting with significantly better performance by the multimodal variant.</p><p>Overall, emotion classification performs poorer than sentiment classification. This observation is expected as emotion classification deals with classification with more fine-grained classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Additional Analysis</head><p>Role of Context. One of the main purposes of MELD is to train contextual modeling in a conversation for emotion recognition. <ref type="table" target="#tab_2">Table 11</ref> and 13 show that the improvement over the non-contextual model such as text-CNN -which only uses a CNN (see Section 4.1) -is 1.4% to 2.5%.</p><p>Inter-speaker influence. One of the important considerations while modeling conversational emo-  tion dynamics is the influence of fellow speakers in the multi-party setting. We analyze this factor by looking at the activation of the attention module on the global GRU in DialogueRNN. We observe that in 63% <ref type="formula">(882 1381</ref>  bcLSTM does not utilize speaker information while detecting emotion. <ref type="table" target="#tab_2">Table 11</ref> shows that in all the experiments, DialogueRNN outperforms bcLSTM by 1-2% margin. This result supports the claim by  that speaker-specific modeling of emotion recognition is beneficial as it helps in improving context representation and incorporates important clues such as inter-speaker relations.</p><p>Emotion shifts. The ability to anticipate the emotion shifts within speakers throughout the course of a dialogue has synergy with better emotion classification. In our results, DialogueRNN achieves a recall of 66% for detecting emotion shifts. However, in the ideal scenario, we would want to detect shift along with the correct emotion class. For this setting, DialogueRNN gets a recall of 36.7%.</p><p>The deterioration observed is expected as solving both tasks together has a higher complexity. Future methods would need to improve upon their capabilities of detecting shifts to improve the emotion classification.</p><p>Contextual distance. <ref type="figure" target="#fig_2">Figure 4</ref> presents the distribution of distances between the target utterance and its second highest attended utterance within the conversation by DialogueRNN in its emotion GRU. For the highest attention, the model largely focuses on utterances nearby to the target utterance. However, the dependency on distant utterances increases with the second highest attention. Moreover, it is interesting to see that the dependency exists both towards the historical and the future utterances, thus incentivizing utilization of bi-directional models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Directions</head><p>Future research using this dataset should focus on improving contextual modeling. Helping models reason about their decisions, exploring emotional influences, and identifying emotion shifts are promising aspects. Another direction is to use visual information available in the raw videos. Identifying face of the speaker in a video where multiple other persons are present is very challenging. This is the case for MELD too as it is a multi-party dataset. Enhancements can be made by extracting relevant visual features through processes utilizing audio-visual speaker diarization. Such procedures would enable utilizing a visual modality in the baselines. In our results, audio features do not help significantly. Thus, we believe that it is necessary to improve the feature extraction for these auxiliary modalities in order to improve the performance further. So far, we have only used concatenation as a feature fusion approach, and showed that it outperforms the unimodal baselines by about 1-3%.</p><p>We believe there is room for further improvement using other more advanced fusion methods such as MARN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Applications of MELD</head><p>MELD has multiple use-cases. It can be used to train emotion classifiers to be further used as emotional receptors in generative dialogue systems. These systems can be used to generate empathetic responses <ref type="bibr" target="#b21">(Zhou et al., 2017)</ref>. It can also be used for emotion and personality modeling of users in conversations <ref type="bibr" target="#b7">(Li et al., 2016)</ref>.</p><p>By being multimodal, MELD can also be used to train multimodal dialogue systems. Although by itself it is not large enough to train an end-to-end dialogue system <ref type="table" target="#tab_2">(Table 1)</ref>, the procedures used to create MELD can be adopted to generate a largescale corpus from any multimodal source such as popular sitcoms. We define multimodal dialogue system as a platform where the system has access to the speaker's voice and facial expressions which it exploits to generate responses. Multimodal dialogue systems can be very useful for real time personal assistants such as Siri, Google Assistant where the users can use both voice and text and facial expressions to communicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we introduced MELD, a multimodal multi-party conversational emotion recognition dataset. We described the process of building this dataset, and provided results obtained with strong baseline methods applied on this dataset. MELD contains raw videos, audio segments, and transcripts for multimodal processing. Additionally, we also provide the features used in our baseline experiments. We believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation. Building upon this dataset, future research can explore the design of efficient multimodal fusion algorithms, novel ERC frameworks, as well as the extraction of new features from the audio, visual, and textual modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Emotion shift of speakers in a dialogue in comparison with their previous emotions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Importance of multimodal cues. Green shows primary modalities responsible for sentiment and emotion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Histogram of ∆t = distance between the target and its context utterance based on emotion GRU attention scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison between the original EmotionLines dataset and MELD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>You-you didn't say it was going to be at nighttime.</figDesc><table><row><cell>Episode</cell><cell>Utterance</cell><cell>Speaker</cell><cell cols="2">Emotion Sentiment</cell></row><row><cell></cell><cell>What are you talkin about? I never left you! Youve always been my agent!</cell><cell>Joey</cell><cell>surprise</cell><cell>negative</cell></row><row><cell>S6.E4</cell><cell>Really?! Yeah! Oh well, no harm, no foul.</cell><cell>Estelle Joey Estelle</cell><cell>surprise joy neutral</cell><cell>positive positive neutral</cell></row><row><cell>S5.E20</cell><cell cols="3">Okay, you guys free tonight? Yeah!! Tonight? Chandler surprise Gary neutral Ross joy</cell><cell>neutral positive negative</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Some examples of the utterances for which annotators could not reach consensus.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">EmotionLines</cell><cell></cell><cell>MELD</cell></row><row><cell cols="2">Categories</cell><cell cols="2">Train Dev</cell><cell>Test</cell><cell cols="2">Train Dev</cell><cell>Test</cell></row><row><cell></cell><cell>anger</cell><cell>524</cell><cell>85</cell><cell>163</cell><cell cols="2">1109 153</cell><cell>345</cell></row><row><cell></cell><cell>disgust</cell><cell>244</cell><cell>26</cell><cell>68</cell><cell>271</cell><cell>22</cell><cell>68</cell></row><row><cell>Emotion</cell><cell>fear joy neutral</cell><cell cols="6">190 1283 123 29 4752 491 1287 4710 470 1256 36 268 40 50 304 1743 163 402</cell></row><row><cell></cell><cell>sadness</cell><cell>351</cell><cell>62</cell><cell>85</cell><cell>683</cell><cell>111</cell><cell>208</cell></row><row><cell></cell><cell>surprise</cell><cell cols="2">1221 151</cell><cell>286</cell><cell cols="2">1205 150</cell><cell>281</cell></row><row><cell>Sentiment</cell><cell>negative neutral positive</cell><cell>---</cell><cell>---</cell><cell>---</cell><cell cols="3">2945 406 4710 470 1256 833 2334 233 521</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Emotion and Sentiment distribution in MELD vs.</figDesc><table><row><cell>EmotionLines.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Difference in annotation between EmotionLines and MELD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Example of timestamp alignment using the Gentle alignment tool.</figDesc><table><row><cell>Utterance</cell><cell cols="6">Speaker Emotion D ID U ID Season Episode</cell><cell>StartTime</cell><cell>EndTime</cell></row><row><cell>But then who? The waitress I went out with last month?</cell><cell>Joey</cell><cell>surprise</cell><cell>1</cell><cell>0</cell><cell>9</cell><cell>23</cell><cell cols="2">00:36:40,364 00:36:42,824</cell></row><row><cell>You know? Forget it!</cell><cell>Rachel</cell><cell>sadness</cell><cell>1</cell><cell>1</cell><cell>9</cell><cell>23</cell><cell cols="2">00:36:44,368 00:36:46,578</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>MELD dataset format for a dialogue. Notations: D ID = dialogue ID, U ID = utterance ID. StartTime and EndTime are in hh:mm:ss,ms format.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Dataset Statistics. {a,v,t} = {audio, visual, text}</figDesc><table><row><cell>dimensional textual features. For audio, we use the</cell></row><row><cell>popular toolkit openSMILE (Eyben et al., 2010),</cell></row><row><cell>which extracts 6373 dimensional features constitut-</cell></row><row><cell>ing several low-level descriptors and various sta-</cell></row><row><cell>tistical functionals of varied vocal and prosodic</cell></row><row><cell>features. As the audio representation is high dimen-</cell></row><row><cell>sional, we employ L2-based feature selection with</cell></row><row><cell>sparse estimators, such as SVMs, to get a dense</cell></row><row><cell>representation of the overall audio segment. For the</cell></row><row><cell>baselines, we do not use visual features, as video-</cell></row><row><cell>based speaker identification and localization is an</cell></row><row><cell>open problem. Bimodal features are obtained by</cell></row><row><cell>concatenating audio and textual features.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Comparison among IEMOCAP, SEMAINE, and</figDesc><table><row><cell>proposed MELD datasets</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Comparison among IEMOCAP and proposed MELD datasets.</figDesc><table><row><cell>beddings as input. For unimodal audio, an LSTM</cell></row><row><cell>model gets audio representations for each audio ut-</cell></row><row><cell>terance feature vector. Finally, the contextual repre-</cell></row><row><cell>sentations from the unimodal variants are supplied</cell></row><row><cell>to the bimodal model for classification. bcLSTM</cell></row><row><cell>does not distinguish among different speakers and</cell></row><row><cell>models a conversation as a single sequence.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Test-set weighted F-score results of DialogueRNN for emotion classification in MELD. Note: w-avg denotes weighted-average. text-CNN and cMKL: contextual information were not used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc>Test-set weighted F-score results of DialogueRNN for 5-class emotion classification in MELD. Note: w-avg denotes weighted-average. surp: surprise emotion.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 13 :</head><label>13</label><figDesc>Test set weighted F-score results of DialogueRNN for sentiment classification in MELD.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We consulted a legal office to verify that the usage and distribution of very short length videos fall under the fair use category.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://github.com/lowerquality/gentle</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based in part upon work supported by the National Science Foundation (grant #1815291), by the John Templeton Foundation (grant #61156), and by DARPA (grant #HR001117S0026-AIDA-FP-045).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring emotion: the self-assessment manikin and the semantic differential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of behavior therapy and experimental psychiatry</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="59" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Emotionlines: An emotion corpus of multi-party conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yeh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Chun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08379</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Aglar Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DialogueRNN: An attentive RNN for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schroder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Affective Computing</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Utterance-level multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="973" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Emotion recognition in conversation: Research challenges, datasets, and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boureau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00207</idno>
		<title level="m">I know the feeling: Learning to converse with empathy</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Avec 2012: the continuous audio/visual emotion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Multimodal interaction</title>
		<meeting>the 14th ACM international conference on Multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Youtube movie reviews: Sentiment analysis in an audio-visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep constrained local models for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08657</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01074</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
