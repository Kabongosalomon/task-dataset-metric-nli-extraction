<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Knowledge Distillation for Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jathushan</forename><surname>Rajasegaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">CVL</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>mshah@ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Knowledge Distillation for Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world contains an overwhelmingly large number of object classes, learning all of which at once is infeasible. Few shot learning is a promising learning paradigm due to its ability to learn out of order distributions quickly with only a few samples. Recent works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref> show that simply learning a good feature embedding can outperform more sophisticated meta-learning and metric learning algorithms for few-shot learning. In this paper, we propose a simple approach to improve the representation capacity of deep neural networks for few-shot learning tasks. We follow a two-stage learning process: First, we train a neural network to maximize the entropy of the feature embedding, thus creating an optimal output manifold using a self-supervised auxiliary loss. In the second stage, we minimize the entropy on feature embedding by bringing self-supervised twins together, while constraining the manifold with student-teacher distillation. Our experiments show that, even in the first stage, self-supervision can outperform current state-of-the-art methods, with further gains achieved by our second stage distillation process. Our codes are available at: https://github.com/brjathu/SKD . Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern deep learning algorithms generally require a large amount of annotated data which is often laborious and expensive to acquire <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref>. Inspired by the fact that humans can learn from only a few-examples, few-shot learning (FSL) offers a promising machine learning paradigm. FSL aims to develop models that can generalize to new concepts using only a few annotated samples (typically ranging from <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Due to data scarcity and limited supervision, FSL remains a challenging problem.</p><p>Existing works mainly approach FSL using meta-learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref> to adapt the base learner for the new tasks, or by enforcing margin maximizing constraints through metric learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37]</ref>. In doing so, these FSL methods ignore the importance of intra-class diversity while seeking to achieve inter-class discriminability. In this work, instead of learning representations which are invariant to within class changes, we argue for an equivariant representation. Our main intuition is that major transformations in the input domain are desired to be reflected in their corresponding outputs to ensure output space diversity. By faithfully reflecting these changes in an equivariant manner, we seek to learn the true natural manifold of an object class.</p><p>We propose a two-stage self-supervised knowledge distillation (SKD) approach for FSL. Despite the availability of only few-shot labeled examples, we show that auxiliary self-supervised learning (SSL) signals can be mined from the limited data, and effectively leveraged to learn the true output-space Generation Zero Generation One <ref type="figure">Figure 1</ref>: Self-supervised Knowledge Distillation operates in two phases. In Gen-0, self-supervision is used to estimate the true prediction manifold, equivariant to input transformations. Specifically, we enforce the model to predict the amount of input rotation using only the output logits. In Gen-1, we force the original sample outputs to be the same as in Gen-0 (dotted lines), while reducing its distance with its augmented versions to enhance discriminability. manifold of each class. For this purpose, we take a direction in contrast to previous works which learn an invariant representation that maps augmented inputs to the same prediction. With the goal to enhance generalizability of the model, we first learn a Generation zero (Gen-0) model whose output predictions are equivariant to the input transformations, thereby avoiding overfitting and ensuring heterogeneity in the prediction space. For example, when learning to classify objects in the first stage of learning, the self-supervision based learning objective ensures that the output logits are rich enough to encode the amount of rotation applied to the input images.</p><p>Once the generation zero network is trained to estimate the optimal output manifold, we perform knowledge distillation by treating the learned model as a teacher network and training a student model with the teacher's outputs. Different to first stage, we now enforce that the augmented samples and original inputs result in similar predictions to enhance between-class discrimination. The knowledge distillation mechanism therefore guides the Generation one (Gen-1) model to develop two intuitive properties. First, the output class manifold is diverse enough to preserve major transformations in the input, thereby avoiding overfitting and improving generalization. Second, the learned relationships in the output space encode natural connections between classes e.g., two similar classes should have correlated predictions as opposed to totally independent categories considered in one-hot encoded ground-truths. Thus, by faithfully representing the output space via encoding inter-class relationships and preserving intra-class diversity, our approach learns improved representations for FSL.</p><p>The following are the main contributions of this work (see <ref type="figure">Fig. 1</ref> for an overview):</p><p>• Different to existing works that use SSL as an auxiliary task, we show the benefit of SSL towards enforcing diversity constraints in the prediction space with a simple architectural modification. • A dual-stage training regime which first estimates the optimal output manifold, and then minimizes the original-augmented pair distance while anchoring the original samples to the learned manifold using a distillation loss. • Extensive evaluations on four popular benchmark datasets with significant improvements on the FSL task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Self-supervised learning (SSL): This form of learning defines auxiliary learning tasks that can enhance model's learning capability without requiring any additional annotation effort. Generally, these surrogate tasks require a higher-level understanding, thereby forcing the learning agent to learn useful representations while solving the auxiliary tasks. The main difference in existing SSL techniques is regarding the way supervisory signal is obtained from the data. For example, <ref type="bibr" target="#b14">[15]</ref> learns useful representations by predicting the amount of rotation applied to an input image. Doersch et al. <ref type="bibr" target="#b7">[8]</ref> train a CNN to predict the relative position of a pair of randomly sampled image patches. This idea is further extended to predict permutations of multiple image patches in <ref type="bibr" target="#b25">[26]</ref>. Alternatively, image colorization and object counting were employed as pretext tasks to improve representation learning <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27]</ref>. Zhai et al. <ref type="bibr" target="#b43">[44]</ref> propose an SSL approach in a semi-supervised setting where some labelled and many unlabelled examples were available. Different from these works, our approach uses self-supervision to enforce additional constraints in the classification space. Close to our work is a set of approaches that seek to learn representations that are invariant to image transformations and augmentations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. In contrast, our approach does the exact opposite: we seek to learn an equivariant representation, so that the true natural manifold of an object class can be learned with only a few-examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-shot learning (FSL):</head><p>There has been several efforts on FSL ranging from metric learning to meta learning methods. Metric learning methods commonly learn a metric space, in which the support set can be easily matched with the query set. For example, Koch et al. <ref type="bibr" target="#b19">[20]</ref> use a Siamese network to learn a similarity metric to classify unknown classes, with the aid of a support set. Sung et al. <ref type="bibr" target="#b39">[40]</ref> use a relation module to learn the relationships between support set and the query image. Matching networks <ref type="bibr" target="#b41">[42]</ref> employ attention and memory to learn a network that matches support set to the query image. In addition, <ref type="bibr" target="#b36">[37]</ref> assigns the mean embedding as a prototype and minimizes the distance from it with rest of the samples in the query set. In contrast, we only use augmented pairs of an image to move their embeddings closer, while preserving their respective distances in the output space.</p><p>Another category of methods employ meta-learning to leverage from the knowledge acquired from the past tasks to learn new tasks. Finn et al. <ref type="bibr" target="#b10">[11]</ref> proposed a popular model-agnostic meta-learning (MAML) framework, which finds better initialization weights that can be quickly adopted to a given support set. Building on <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12]</ref> use meta-learned preconditioning to redirect the gradient-flow to achieve better convergence. In addition to these works, LEO (Latent Embedding Optimization) <ref type="bibr" target="#b34">[35]</ref> transforms network weights to a lower dimensional latent embedding space and applies MAML algorithm to scale to larger networks. MetaOptNet <ref type="bibr" target="#b20">[21]</ref> employs an SVM to model meta-learning as a convex optimization problem which is solved using quadratic programming.</p><p>Some recent works attribute the success of meta-learning to its strong feature representation capability rather than meta-learning itself <ref type="bibr" target="#b30">[31]</ref>. Others <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref> show the effectiveness of a simple baseline by learning a strong embedding. This work is an effort along the same direction, and proposes a novel self-supervised knowledge distillation approach which can learn effective feature representations for FSL. The closest to our work is Gidaris et al. <ref type="bibr" target="#b12">[13]</ref>, who use self-supervision to boost few-shot classification. However, <ref type="bibr" target="#b12">[13]</ref> simply uses self-supervision as an auxiliary loss for single training, while we use it to shape and constrain the learning manifold. Architecture wise, we use a sequential self-supervision layer, while <ref type="bibr" target="#b12">[13]</ref> has a parallel design. While <ref type="bibr" target="#b12">[13]</ref> does not have multiple generations, we further improve the representations in the second generation, by constraining the embedding space using distillation and bringing embeddings of rotated pairs closer to their original embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>The proposed SKD uses a two stage training pipeline; Generation zero (Gen-0) and Generation one (Gen-1). Gen-0 utilizes self-supervision to learn a wider classification manifold, in which the learned embeddings are equivariant to rotation (or another data transformation). Later, during Gen-1, we use Gen-0 model as a teacher and use original (non-rotated) images as anchors to preserve the learned manifold, while rotated version of the images are used to reduce intra-class distances in the embedding space to learn robust and discriminative feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setting</head><p>Lets assume a neural network F contains feature embedding parameters Φ, and classification weights Θ. Any input image x can be mapped to a feature vector v ∈ R d by a function</p><formula xml:id="formula_0">f Φ : x → v.</formula><p>Consequently, features v are mapped to logits p ∈ R c by another function f Θ : v → p, where c denotes the number of output classes. Hence, conventionally F is defined as a composition of these functions, F =f Φ •f Θ . In this work, we introduce another function f Ψ , parameterized by Ψ, such that, f Ψ : p → q, which maps logits p to a secondary set of logits q ∈ R s for self-supervised task (e.g., rotation classification). For each input x, we automatically obtain labels r ∈ {1, . . . , s} for the self-supervision task. Therefore, the complete network can be represented as</p><formula xml:id="formula_1">F Φ,Θ,Ψ =f Ψ •f Θ •f Φ .</formula><p>We consider a dataset D with n image-label pairs {x i , y i } n where y i ∈ {1, . . . , c}. During evaluation, we sample episodes as in classical few-shot learning literature. An episode D eval contains, D supp and D query . In an n-way k-shot setting, D supp has k number of samples for each of n classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generation Zero</head><p>During the first stage (aka Gen-0), a minibatch B={x,y} is randomly sampled from the dataset D, which has m number of image-label pairs such that x = {x i } m , y = {y i } m . We first take the images x and apply a transformation function T (·) to create augmented copies of x. For the sake of brevity, here we consider T (·) as a rotation transformation, however any other suitable transformation can Overall training process of SKD: Generation Zero uses multiple rotated versions of the images to train the neural network to predict the class as well as the rotated angle. Then during Generation One, we use original version of the images as anchor points to preserve the manifold while moving the logits for the rotated version closer, to increase the discriminative ability of the network.</p><p>also be considered as we show in our experiments (Sec. 4.2). Applying rotations of 90, 180 and 270 degrees to x, we create x 90 , x 180 and x 270 , respectively. Then we combine all augmented versions of images into a single tensor x = {x, x 90 , x 180 , x 270 } whose corresponding class labels are y ∈ R 4×m . Additionally, one-hot encoded labels r = {r i ∈ R s } 4×m for the rotation direction are also created, where s = 4 due to the four rotation directions in our self-supervised task.</p><p>First, we pass x through f Φ , resulting in the features v ∈ R d×(4×m) . Then, the features are passed through f Θ to get the corresponding logits p ∈ R c×(4×m) , and finally, the logits are passed through f Ψ , to get the rotation logits q ∈ R s×(4×m) ,</p><formula xml:id="formula_2">f Φ ( x) = v, f Θ ( v) = p, f Ψ ( p) = q.</formula><p>We employ, two loss functions to optimize the model in Gen-0: (a) categorical cross entropy loss L ce between the predicted logits p and the true labels y, and (b) a self-supervision loss L ss between the rotation logits q and rotation labels r. Note that, self-supervision loss is simply a binary cross entropy loss. These two loss terms are combined with a weighting coefficient α to get our final loss,</p><formula xml:id="formula_3">L Gen-0 = L ce + α · L ss , s.t., L ce (p, y) = − log e p y j e p j , L ss (q, r) = − log e q r j e q j .</formula><p>The whole process of training the Gen-0 model can be stated as following optimization problem,</p><formula xml:id="formula_4">min Φ, Θ, Ψ E x,y∼D L ce (f Φ,Θ ( x), y) + α · L ss (f Φ,Θ,Ψ ( x), r) .<label>(1)</label></formula><p>The above objective makes sure that the output logits are representative enough to encapsulate information about the input transformation, thereby successfully predicting the amount of rotation applied to the input. This behaviour allows us to maintain diversity in the output space and faithfully estimating the natural data manifold of each object category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generation One</head><p>Once the Gen-0 model is trained with cross entropy and self-supervision loss functions, we take two clones of the trained model: A teacher model F t and a student model F s . Weights of the teacher model are frozen and used only for inference. Again, we sample a minibatch B from D and generate a twinx ∈ x\x from x. In this case, a twinx is simply a rotated version of x (e.g., x 180 ). During Gen-1 training, x is used as an anchor point to constrain any changes to the classification manifold. This is enforced by a knowledge distillation <ref type="bibr" target="#b16">[17]</ref> loss between teacher and student networks. Concurrently, an auxiliary 2 loss is employed to bring the embeddings of x andx together to enhance feature discriminability while preserving the original output manifold.</p><p>Specifically, we first pass x through the teacher network F t = f t Φ,Θ • f t Ψ and its logits p t are obtained. Then, x,x are passed through the F s to get their corresponding logits p s , andp s respectively.</p><formula xml:id="formula_5">f t Φ,Θ (x) = p t , f s Φ,Θ ({x,x}) = {p s ,p s } s.t., f Φ,Θ =f Θ •f Φ .</formula><p>We use Kullback-Leibler (KL) divergence measure between p t = {p t i } and p s = {p s i } for knowledge distillation, and apply an 2 loss between p s andp s to achieve better discriminability,</p><formula xml:id="formula_6">L KD (p s , p t , T ) = KL σ(p s /T ), σ(p t /T ) , L 2 = p s −p s 2 ,</formula><p>where, σ is a softmax function and T is a temperature parameter used to soften the output distribution. Finally, we combine these two loss terms by a coefficient β as follows,</p><formula xml:id="formula_7">L Gen-1 = L KD + β · L 2 .</formula><p>The overall Gen-1 training process can be stated as the following optimization problem,</p><formula xml:id="formula_8">min Φ, Θ E x,y∼D L KD (f s Φ,Θ (x),f t Φ,Θ (x)) + β · L 2 (f s Φ,Θ (x),f s Φ,Θ (x)) .<label>(2)</label></formula><p>Note that, in our setting, it is necessary to have the rotation classification head sequentially added to the classification layer, unlike the previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">39]</ref> which connect rotation classification head directly after the feature embedding layer. This is because, during the Gen-0, we encourage the penultimate layer to encode information about both the image class and its rotation (thus preserving output space diversity), and later in Gen-1, we bring the logits of the rotated pairs closer (to improve discrimination). These benefits are not possible if the rotation head is connected directly to the feature embedding layer or if distillation is performed on the feature embeddings.</p><p>Algorithm 1 Training procedure of SKD</p><formula xml:id="formula_9">1: Require:f Φ , f Θ , f Ψ , D 2:</formula><p>for e iterations do Generation Zero training 3:</p><p>while B ∼ D do 4:</p><p>x 90 , x 180 , x 270 ← rotate(x) 5:</p><p>x ← {x, x 90 , x 180 , x 270 }, and y ← {y, y, y, y} 6:</p><formula xml:id="formula_10">r ← {0, 1, 2, 3}</formula><p>where 0 is an all zero vector with length m 7</p><p>:</p><formula xml:id="formula_11">v ← f Φ ( x), p ← f Θ ( v), q ← f Ψ ( p) 8: L 0 ← L ce ( p, y) + α · L ss ( q, r) 9: { Φ, Θ, Ψ} ← { Φ, Θ, Ψ} -∇ { Φ, Θ, Ψ} L 0 10: F t , F s ← F 11:</formula><p>for e iterations do Generation One training 12:</p><p>while B ∼ D do 13:</p><p>x 180 ← rotate(x) 14:</p><p>p</p><formula xml:id="formula_12">t ← f t Φ,Θ (x), {p s ,p s } ← f s Φ,Θ ({x, x 180 }) 15: L 1 ← L KD (p s , p t ) + β · L 2 (p s ,p s ) 16: { Φ, Θ} ← { Φ, Θ} − ∇ { Φ, Θ} L 1 return f s Φ,Θ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation</head><p>During evaluation, a held out part of the dataset is used to sample tasks. This comprises of a support set and a query set {D supp , D query }. D supp has image-label pairs {x supp , y supp }, while D query comprise of an image tensor x query . Both x supp and x query are fed to the final trained f s Φ model to get the feature embeddings v supp and v query , respectively. We use a simple logistic regression classifier <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b2">3]</ref> to map the labels from support set to query set. The embeddings are normalized onto a unit sphere <ref type="bibr" target="#b40">[41]</ref>. We randomly sample 600 tasks, and report mean classification accuracy with 95% confidence interval. Unlike popular meta-learning algorithms (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>), we do not need to train multiple models for different values of n and k in n-way, k-shot classification. Since, the classification is disentangled from feature learning in our case, the same model can be used to evaluate for any value of n and k in FSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We comprehensively compare our method on four benchmark few-shot learning datasets i.e., mini-ImageNet <ref type="bibr" target="#b41">[42]</ref>, tieredImageNet <ref type="bibr" target="#b33">[34]</ref>, CIFAR-FS <ref type="bibr" target="#b2">[3]</ref> and FC100 <ref type="bibr" target="#b27">[28]</ref>. Additionally, we provide an extensive ablation study to investigate the individual contributions of different components (Sec. 4.2).</p><p>Implementation Details: To be consistent with previous methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b20">21]</ref>, we use ResNet-12 as the backbone in our experiments. The backbone architecture contains 4 residual blocks of 64, 160, 320, 640 filters as in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21]</ref>, each with 3 × 3 convolutions. A 2 × 2 max pooling operation is applied after the first 3 blocks and a global average pooling is applied after the last block. Additionally, a 4 neuron fully-connected layer is added after the final classification layer.</p><p>We use SGD with an initial learning rate of 0.05, momentum of 0.9, and a weight decay of 5e−4. The learning rate is reduced after epoch 60 by a factor of 0.1. Gen-0 and Gen-1 models on CIFAR-FS are trained for 65 epochs, while rest of the models are trained for 8 epochs only. consistent with previous approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35]</ref>, random crop, color jittering and random horizontal flip are applied for data augmentation during training. Further, the hyper-parameters α, β are tuned on a validation set, and we used the same value of 4.0 as in <ref type="bibr" target="#b40">[41]</ref> for temperature coefficient T during distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We evaluate our method on four widely used FSL benchmarks. These include two datasets which are subsets of the ImageNet i.e., miniImageNet <ref type="bibr" target="#b41">[42]</ref> and tieredImageNet <ref type="bibr" target="#b33">[34]</ref>, and the other two which are splits of CIFAR100 i.e., CIFAR-FS <ref type="bibr" target="#b2">[3]</ref> and FC100 <ref type="bibr" target="#b27">[28]</ref>. For miniImageNet <ref type="bibr" target="#b41">[42]</ref>, we use the split proposed in <ref type="bibr" target="#b31">[32]</ref>, with 64, 16 and 20 classes for training, validation and testing. The tieredImageNet <ref type="bibr" target="#b33">[34]</ref> contains 608 classes which are semantically grouped into 34 high-level classes, that are further divided into 20, 6 and 8 for training, validation, and test splits, thus making the splits more diverse. CIFAR-FS <ref type="bibr" target="#b2">[3]</ref> contains a random split of 100 classes into 64, 16 and 20 for training, validation, and testing, while FC100 <ref type="bibr" target="#b27">[28]</ref> uses a split similar to tieredImageNet, making the splits more diverse. FC100 has 60, 20, 20 classes for training, validation, and testing respectively.   <ref type="bibr" target="#b39">[40]</ref> 64-96-128-256 55.0 ± 1.0 69.3 ± 0.8 --R2D2 <ref type="bibr" target="#b2">[3]</ref> 96-192-384-512 65.3 ± 0.2 79.4 ± 0.1 --TADAM <ref type="bibr" target="#b27">[28]</ref> ResNet-12 --40.1 ± 0.4 56.1 ± 0.4 Shot-Free <ref type="bibr" target="#b32">[33]</ref> ResNet-12 69.2 ± n/a 84.7 ± n/a --TEWAM <ref type="bibr" target="#b29">[30]</ref> ResNet-12 70.4 ± n/a 81.3 ± n/a --Prototypical Networks † <ref type="bibr" target="#b36">[37]</ref> ResNet-12 72.2 ± 0.7 83.5 ± 0.5 37.5 ± 0.6 52.5 ± 0.6 Boosting <ref type="bibr" target="#b12">[13]</ref> WRN-28-10 73.6 ± 0.3 86.0 ± 0.2 --MetaOptNet <ref type="bibr" target="#b20">[21]</ref> ResNet-12 72.6 ± 0.7 84.3 ± 0.5 41.1 ± 0.6 55.5 ± 0.6 RFS-simple <ref type="bibr" target="#b40">[41]</ref> ResNet-12 71.5 ± 0.8 86.0 ± 0.5 42.6 ± 0.7 59.1 ± 0.6 RFS-distill <ref type="bibr" target="#b40">[41]</ref> ResNet-12 73.9 ± 0.8 86.9 ± 0.5 44.6 ± 0.7 60.9 ± 0.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SKD-GEN0</head><p>ResNet-12 74.5 ± 0.9 88.0 ± 0.6 45.3 ± 0.8 62.2 ± 0.7 SKD-GEN1</p><p>ResNet-12 76.9 ± 0.9 88.9 ± 0.6 46.5 ± 0.8 63.1 ± 0.7 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Few-shot learning results</head><p>Our results shown in <ref type="table" target="#tab_1">Table 1</ref> and 2 suggest that the proposed method consistently outperforms the current methods on all four datasets. Even, our Gen-0 alone performs better than the current state-ofthe-art (SOTA) methods by a considerable margin. For example, SKD Gen-0 model surpasses SOTA performance on miniImageNet by ∼1% on both 5-way 1-shot and 5-way 5-shot tasks. The same can be observed on other datasets. Compared to RFS-simple <ref type="bibr" target="#b40">[41]</ref> (similar to our Gen-0), SKD shows an improvement of 3.91% on 5-way 1-shot and 3.51% on 5-way 5-shot learning. The same trend can be observed across other evaluated datasets with consistent 2-3% gains over RFS-simple. This is due to the novel self-supervision which enables SKD to learn diverse and generalizable embedding space.</p><p>Gen-1 incorporates knowledge distillation and proves even more effective compared with Gen-0. On miniImageNet, we achieve 67.04% and 83.54% on 5-way 1-shot and 5-way 5-shot learning tasks, respectively. These are gains of 2.22% and 1.4% on 5-way 1-shot and 5-way 5-shot tasks. Similar consistent gains of 2-3% over SOTA results can be observed across other evaluated datasets. Note that, RFS-distill <ref type="bibr" target="#b40">[41]</ref> uses multiple iterations (up to 3-4 generations) for model distillation, while SKD only uses a single generation for the distillation. We attribute our gain to the way we use knowledge distillation to constrain changes in the embedding space, while minimizing the embedding distance between images and their rotated pairs, thus enhancing representation capabilities of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies and Analysis</head><p>Choices of loss function: We study the impact of different contributions by progressively integrating them into our proposed method. To this end, we first evaluate our method with and without the self-supervision loss. If we train the Gen-0 with only cross entropy loss, which is same as RFSsimple <ref type="bibr" target="#b40">[41]</ref>, the model achieve 71.5 ± 0.8% and 62.02 ± 0.63% on 5-way 1-shot task on CIFAR-FS and miniImageNet, respectively. Then, if we train the Gen-0 with additional self supervision, the model performance improves to 74.5 ± 0.9% and 65.93 ± 0.81%. This shows an absolute gain of 3.0% and 3.91%, by incorporating our proposed self-supervision. Additionally, if we only keep knowledge distillation for Gen-1, we can see that self-supervision for Gen-0 has a clear impact on next generation. As shown in <ref type="table">Table 3</ref>, self-supervision at Gen-0 is responsible for 2% performance improvement on Gen-1. Further, during Gen-1, the advantage of using the L 2 loss to bring logits of rotated augmentations closer, is demonstrated in <ref type="table">Table 3</ref>. We can see that, even for both Gen-0 models trained on L ce and L ce + αL ss , addition of L 2 loss during Gen-1 gives about ∼ 1% gain compared with using knowledge distillation only. These emprical evaluations clearly establish individual importance of different contributions (self-supervision, knowledge distillation and ensuring proximity of augmented versions of the image in output space) in our proposed two stage approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choices of self supervision:</head><p>We further investigate different choices of self-supervision. Instead of rotations based self-supervision, we use a 2 × 2 crop of an image, and train the final classifier to predict the correct crop quadrant <ref type="bibr" target="#b38">[39]</ref>. The results in <ref type="table" target="#tab_4">Table 4</ref> show that the crop-based self-supervision method can also surpass the SOTA FSL methods, though it performs slightly lower than the rotations  <ref type="table">Table 3</ref>: FSL results on CIFAR-FS <ref type="bibr" target="#b2">[3]</ref> and FC100 <ref type="bibr" target="#b27">[28]</ref>, with different combinations of loss functions for Gen-0 and Gen-1. For Gen-1, the loss functions on the left side of the arrow were used to train the Gen-0 model.  based self-supervision. We further experiment with simCLR loss <ref type="bibr" target="#b4">[5]</ref>, which also aims to bring augmented pairs closer together, along-with knowledge distillation during Gen-1. Our experiments show that simCLR only achieves 75.0 ± 0.8 and 88.2 ± 0.6% on 1 and 5 shot tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-FS: 5-way 1-shot ( Variations)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variations of α:</head><p>During Gen-0 of SKD, α controls the contribution of self-supervision over classification. <ref type="figure" target="#fig_1">Fig. 3 (left)</ref> shows the Gen-0 performance by changing α. We observe that the performance increases from 0 till 2, and then decreases. The results indicate that the performance is not sensitive to the values of α. It is important to note that Gen-0 without self-supervision i.e. α = 0 performs the lowest compared with other values of α, thus establishing the importance of self-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variations of β:</head><p>At Gen-1, we again use a coefficient β to control the contribution of loss L 2 over knowledge distillation. From results in <ref type="figure" target="#fig_1">Fig. 3 (right)</ref>, we observe a similar trend as for the case of α, that the performance first improves for 0 ≤ β ≤ 0.1, and then decreases with larger values of β. However, even if we change β from 0.1 to 0.5, the performance drops only by ∼ 0.6%. Note that, the performance on CIFAR-FS, on 5-way 1-shot without L 2 loss is only 75.6 ± 0.9%, which is the lowest compared with other values of β, showing the importance of L 2 . Time Complexity Lets assume T is the time required for training one generation. RFS <ref type="bibr" target="#b40">[41]</ref> has the time complexity of O(n × T ), where n is the number of generations, which is usually about 3-4. However, our complexity is O(2 × T ). Note that, additional rotation augmentations do not affect the training time T much, with parallel computing on GPUs. Also, we generally train Gen-1 model for less number of epochs than Gen-0. Using a single Tesla V100 GPU on CIFAR-FS, for the first generation, both RFS and SKD take approximately the same time, i.e., T = 88 minutes. The complete training time on CIFAR-FS of RFS is ∼ 4 hours, while SKD only takes ∼ 2 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Deep learning models can easily overfit on the scarce data available in FSL settings. To enhance generalizability, existing approaches regularize the model to preserve margins or encode high-level learning behaviour via meta-learning. In this work, we take a different approach and propose to learn the true output classification manifold via self-supervised learning. Our approach operates in two phases: first, the model learns to classify inputs such that the diversity in the outputs is not lost, thereby avoiding overfitting and modeling the natural output manifold structure. Once this structure is learned, our approach trains a student model that preserves the original output manifold structure while jointly maximizing the discriminability of learned representations. Our results on four popular benchmarks show the benefit of our approach where it establishes a new state-of-the-art for FSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This research aims to equip machines with capabilities to learn new concepts using only a few examples. Developing machine learning models which can generalize to a large number of object classes using only a few examples has numerous potential applications with a positive impact on society. Examples include enabling visually impaired individuals to understand the environment around them and enhancing the capabilities of robots being used in healthcare and elderly care facilities. It has the potential to reduce expensive and laborious data acquisition and annotation effort required to learn models in domains including image classification, retrieval, language modelling and object detection. However, we must be cautious that the few shot learning techniques can be misused by authoritarian government agencies which can compromise an individual's privacy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall training process of SKD: Generation Zero uses multiple rotated versions of the images to train the neural network to predict the class as well as the rotated angle. Then during Generation One, we use original version of the images as anchor points to preserve the manifold while moving the logits for the rotated version closer, to increase the discriminative ability of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study on the sensitivity of the loss coefficient hyper-parameters α and β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">CIFAR-FS, 5-way 1-shot 5-shot</cell><cell cols="2">FC100, 5-way 1-shot 5-shot</cell></row><row><cell>MAML [11]</cell><cell>32-32-32-32</cell><cell>58.9 ± 1.9</cell><cell>71.5 ± 1.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Prototypical Networks  † [37]</cell><cell>64-64-64-64</cell><cell>55.5 ± 0.7</cell><cell>72.0 ± 0.6</cell><cell>35.3 ± 0.6</cell><cell>48.6 ± 0.6</cell></row><row><cell>Relation Networks</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>FSL results on miniImageNet [42] and tieredImageNet [34] datasets, with mean accurcy and 95% confidence interval.† results obtained by training on train+val sets. Table is an extended version from [41].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FSL on CIFAR-FS<ref type="bibr" target="#b2">[3]</ref> and FC100<ref type="bibr" target="#b27">[28]</ref> datasets, with mean accurcy and 95% confidence interval.</figDesc><table /><note>† results obtained by training on train+val sets. Table is an extended version from [41].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>L CE → L KD 73.9 ± 0.8 86.9 ± 0.5 64.82 ± 0.60 82.14 ± 0.43 L CE → L KD + βL 2 74.9 ± 1.0 87.6 ± 0.6 64.76 ± 0.84 81.84 ± 0.54 L CE + αL SS → L KD 75.6 ± 0.9 88.7 ± 0.6 66.48 ± 0.84 83.64 ± 0.53 L CE + αL SS → L KD + βL</figDesc><table><row><cell>Generation</cell><cell>Loss Function</cell><cell cols="2">CIFAR-FS, 5-way 1-shot 5-shot</cell><cell cols="2">miniImageNet, 5-way 1-shot 5-shot</cell></row><row><cell>GEN-0</cell><cell>L CE L CE + αL SS</cell><cell>71.5 ± 0.8 74.5 ± 0.9</cell><cell>86.0 ± 0.5 88.0 ± 0.6</cell><cell>62.02 ± 0.63 65.93 ± 0.81</cell><cell>79.64 ± 0.44 83.15 ± 0.54</cell></row><row><cell>GEN-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>76.9 ± 0.9</cell><cell>88.9 ± 0.6</cell><cell>67.04 ± 0.85</cell><cell>83.54 ± 0.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Few</figDesc><table><row><cell cols="7">-shot learning results on CIFAR-FS [3] and FC100 [28]</cell><cell cols="8">Generation 0, 5-way Self-supervision Type 1-shot 5-shot</cell><cell cols="2">Generation 1, 5-way 1-shot 5-shot</cell></row><row><cell cols="7">datasets, with a comparison to no self-supervision vs finding the rotation and finding the location of a patch as self-supervision method.</cell><cell></cell><cell cols="2">None Rotation Location</cell><cell></cell><cell cols="6">71.5 ± 0.8 86.0 ± 0.5 73.9 ± 0.8 86.9 ± 0.5 74.5 ± 0.9 88.0 ± 0.6 76.9 ± 0.9 88.9 ± 0.6 74.1 ± 0.9 88.0 ± 0.6 76.2 ± 0.9 87.8 ± 0.6</cell></row><row><cell></cell><cell>76</cell><cell cols="8">CIFAR-FS: 5-way 1-shot ( Variations)</cell><cell>77.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>77.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy %</cell><cell>73 74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy %</cell><cell>76.6 76.7 76.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>71</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>76.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.0</cell><cell>0.5</cell><cell>1.0</cell><cell>1.5</cell><cell>2.0 values 2.5</cell><cell>3.0</cell><cell>3.5</cell><cell>4.0</cell><cell>76.3</cell><cell>0.05</cell><cell>0.07</cell><cell>0.1</cell><cell>0.13 values 0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Infinite mixture prototypes for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diversity with cooperation: Ensemble methods for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-learning with warped gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Flennerhag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collect and select: Semantic alignment metric learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A guide to convolutional neural networks for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="207" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<title level="m">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Few-shot image recognition with knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rapid learning or feature reuse? towards understanding the effectiveness of maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09157</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized zero-and few-shot learning via aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11539</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Parn: Position-aware relation networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on computer vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
