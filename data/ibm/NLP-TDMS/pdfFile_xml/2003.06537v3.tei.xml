<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OccuSeg: Occupancy-aware 3D Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OccuSeg: Occupancy-aware 3D Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>This work was supported in part by Natural Science Foundation of China (NSFC) under contract No. 61722209 and 6181001011, and was carried out at Tsinghua University. tion. The proposed approach achieves state-of-the-art per-formance on 3 real-world datasets, i.e. ScanNetV2, S3DIS and SceneNN, while maintaining high efficiency.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Input Geometry (e) Feature Term (d) Spatial Term (f) Occupancy (b) Result (c) Ground Truth Instance Figure 1</p><p>. Given the input colored point cloud, occupancy size is regressed for each voxel, which predicts the number of voxels occupied by its belonging instance. An adaptive clustering scheme jointly considers both the occupancy information and embedding distance is further applied for 3D instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>3D instance segmentation, with a variety of applications in robotics and augmented reality, is in large demands these days. Unlike 2D images that are projective observations of the environment, 3D models provide metric reconstruction of the scenes without occlusion or scale ambiguity. In this paper, we define "3D occupancy size", as the number of voxels occupied by each instance. It owns advantages of robustness in prediction, on which basis, OccuSeg, an occupancy-aware 3D instance segmentation scheme is proposed. Our multi-task learning produces both occupancy signal and embedding representations, where the training of spatial and feature embedding varies with their difference in scale-aware. Our clustering scheme benefits from the reliable comparison between the predicted occupancy size and the clustered occupancy size, which encourages hard samples being correctly clustered and avoids over segmenta-Corresponding</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. Given the input colored point cloud, occupancy size is regressed for each voxel, which predicts the number of voxels occupied by its belonging instance. An adaptive clustering scheme jointly considers both the occupancy information and embedding distance is further applied for 3D instance segmentation.</p><p>Abstract 3D instance segmentation, with a variety of applications in robotics and augmented reality, is in large demands these days. Unlike 2D images that are projective observations of the environment, 3D models provide metric reconstruction of the scenes without occlusion or scale ambiguity. In this paper, we define "3D occupancy size", as the number of voxels occupied by each instance. It owns advantages of robustness in prediction, on which basis, OccuSeg, an occupancy-aware 3D instance segmentation scheme is proposed. Our multi-task learning produces both occupancy signal and embedding representations, where the training of spatial and feature embedding varies with their difference in scale-aware. Our clustering scheme benefits from the reliable comparison between the predicted occupancy size and the clustered occupancy size, which encourages hard samples being correctly clustered and avoids over segmenta-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The past ten years have witnessed a rapid development of real-time 3D reconstruction technologies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b14">15]</ref> with the popularity of commercial RGB-D depth sensors like Kinect, Xtion, etc. Given the reconstructed scene, there is an increasing attention for instance-level semantic understanding of the 3D environment. More specifically, 3D instance segmentation aims to recognize points belonging to the same object and simultaneously infer their semantic class, which serves as the fundamental technique for mobile robots as well as augmented/virtual reality applications.</p><p>Although scene understanding on 2D images has achieved significant progress recently with the development of deep learning techniques, the irregularity of 3D data introduces new challenges beyond the capability of 2D solutions. As demonstrated in previous works <ref type="bibr" target="#b17">[18]</ref>, directly projecting the state-of-the-art 2D instance segmentation MaskRCNN <ref type="bibr" target="#b16">[17]</ref> predictions into 3D space leads to poor performance, which inspires better solutions by incorporating 3D geometry information into the network design. A popular solution for 3D instance segmentation <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b21">22]</ref> is to marry the powerful 3D feature extractors (spatially sparse convolutional networks <ref type="bibr" target="#b13">[14]</ref> or PointNet++ <ref type="bibr" target="#b38">[39]</ref>) with conventional 2D image instance segmentation techniques <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">29]</ref>. Such existing 3D solutions pay less attention to utilizing the inherent property of the 3D model itself, which provides metric reconstruction of the environment without occlusion or scale ambiguity.</p><p>In this paper, we propose an occupancy-aware 3D instance segmentation approach, OccuSeg. It takes the 3D geometry model as input, and produces point-wise predictions of instance level semantic information, as illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>. Given the insight that the 3D metric space provides more reliable perception than the 2D image-based projective observations for the 3D scene, we particularly introduce "3D occupancy signal", representing the number of voxels occupied by each instance. Such an occupancy signal represents the inherent and fundamental property of each 3D instance, showing a strong potential to handle the ambiguity of scale, location, texture, lighting and occlusion under the 3D setting. Thus, we encode the novel occupancy signal into the conventional 3D instance segmentation pipeline, i.e., learning stage followed by clustering stage. In our occupancy-aware approach, both the learning and clustering stages fully utilize the characteristic of the occupancy signal, leading to competitive performance on public datasets. The considerable gain on mAP (around 12.3 in mAP) further demonstrates that our occupancy-aware approach owns the superiority of preserving the inherent and fundamental nature of the instances in the 3D environment.</p><p>More specifically, the learning stage takes a colored 3D scene as input, and utilizes the spatially sparse convolution approaches <ref type="bibr" target="#b12">[13]</ref> to extract a hybrid vector for each voxel <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref>. It not only learns the classic embedding such as spatial ( <ref type="figure" target="#fig_1">Fig. 1(d)</ref>) and feature embedding ( <ref type="figure" target="#fig_1">Fig.   1</ref>(e)), but also produces an occupancy signal ( <ref type="figure" target="#fig_1">Fig. 1(f)</ref>) that implies the object-level volume. To make full use of both the semantic and geometric information, our feature and spatial embedding are explicitly supervised with different objectives, and are further combined through covariance estimation for both feature and spatial embedding distance. For the clustering stage, the 3D input point cloud is grouped into super-voxels based on the geometric and appearance constraints using a graph-based segmentation algorithm <ref type="bibr" target="#b9">[10]</ref>. Then, to merge the super-voxels with similar feature embedding into the same instance, we utilize an adaptive threshold to evaluate the similarity between the embedding distance and the occupancy size. Aided by the reliable comparison between the predicted occupancy size and the clustered occupancy size, our clustering encourages hard samples to be correctly clustered and eliminates the false positives where partial instances are recognized as an independent instance. The technical contributions are summarized as follows.</p><p>• We present an occupancy-aware 3D instance segmentation scheme OccuSeg. It achieves state-of-the-art performance on three public datasets: ScanNetV2 <ref type="bibr" target="#b3">[4]</ref>, S3DIS <ref type="bibr" target="#b0">[1]</ref> and SceneNN <ref type="bibr" target="#b18">[19]</ref>, ranking first in all metrics with a significant margin while remaining high efficiency, e.g., 12.3 gain in mAP on the ScanNetV2 benchmark.</p><p>• In particular, a novel occupancy signal is proposed in this paper, which predicts the number of occupied voxels for each instance. The occupancy signal is learnt jointly with a combination of feature and spatial embedding and employed to guide the clustering stage of 3D instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>2D Instance Segmentation. 2D Instance segmentation methods are generally divided into two categories: proposal-based and proposal-free approaches. Proposal-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b45">46]</ref> firstly generate region proposals (predefined rectangles) that contain objects and further classify pixels inside each proposal as objects or background. By arguing that convolutional operators are translational invariant and thus cannot distinguish similar objects at different places well, Novotny et al. <ref type="bibr" target="#b34">[35]</ref> propose semi-convolutional operators based on the coordinates of each pixel for better instance segmentation.</p><p>On the other hand, proposal-free methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> learn an embedding vector for each pixel and apply a clustering step in the embedding space as post processing for instance segmentation. Brabandere et al. <ref type="bibr" target="#b6">[7]</ref> propose to train a per-pixel embedding vector and adopt a discriminative cost to encourage pixels belonging to the same instance to be as close as possible, while the embedding center of different instances to be far away from each other. Liang et al. <ref type="bibr" target="#b25">[26]</ref> regress an offset vector pointing to the object center for each pixel and further use the predicted centers for instance segmentation from a "voting" perspective <ref type="bibr" target="#b23">[24]</ref>. Recently, Neven et al. <ref type="bibr" target="#b31">[32]</ref> introduce a learnable clustering bandwidth instead of learning embedding using hand-crafted cost functions, achieving accurate instance segmentation in real-time.</p><p>While all these approaches have achieved promising results in the 2D domain, the extension to the 3D domain is non-trivial. How to utilize the fundamental property of 3D instance remains a challenging problem. 3D Instance Segmentation. Unlike 2D images with regular pixel grids, the irregular distribution of 3D point clouds in physical space raises new challenges for 3D instance segmentation. Pioneer works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b17">18]</ref> have tried to directly extend 2D convolutional neural networks to 3D space by voxelizing the input points into uniform voxels, and applying 3D convolutions instead. Yet most computations are wasted on the inactive empty voxels. Thus, recent methods utilize more feasible 3D feature extractors to tackle this problem. Point-based instance segmentation approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref> directly consume unordered point clouds as input and use a permutation invariant neural network PointNet <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> for feature extraction. Landrieu and Simonovsky <ref type="bibr" target="#b22">[23]</ref> generate superpoint graphs from point clouds, followed by ConvGNNs to learn contextual information. While volumetric approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref> take advantage of the sparsity of 3D data and employ sparse convolution to omit computations on inactive voxels. Specifically, SGPN <ref type="bibr" target="#b42">[43]</ref> proposes to learn a similarity matrix for all point pairs, based on which, similar points are merged for instance segmentation. 3D BoNet <ref type="bibr" target="#b47">[48]</ref> directly predicts the bounding boxes of objects for efficient instance segmentation. GSPN <ref type="bibr" target="#b49">[50]</ref> introduces a generative shape proposal network and relies on object proposals to identify instances in 3D point clouds. VoteNet <ref type="bibr" target="#b36">[37]</ref> predicts offset vectors to the corresponding object centers for seed 2D Occupancy: 2D Occupancy: <ref type="figure">Figure 3</ref>. Toy example of 2D observations at different view angles of the same 3D scene. The number of occupied pixels/voxels of each instance (denoted as occupancy) is uncertain on 2D image, yet can be predicted robustly for the reconstructed 3D model. points, followed by a clustering module to generate object proposals. Additionally, 3DSIS <ref type="bibr" target="#b17">[18]</ref> jointly learns 2D and 3D features by back projecting features extracted from 2D convolution on images to 3D space. It further applies 3D convolution for volumetric feature learning for proposalbased 3D instance segmentation. For proposal-free 3D instance segmentation, MASC <ref type="bibr" target="#b27">[28]</ref> combines the SSCN architecture with instance affinity prediction across multiple scales. Liang et al. <ref type="bibr" target="#b26">[27]</ref> apply embedding learning <ref type="bibr" target="#b6">[7]</ref> on top of the superior performance of SSCN. Lahoud et al. <ref type="bibr" target="#b21">[22]</ref> further combine directional information of each object with semantic feature embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>Recall our goal is that, we take a voxelized 3D colored scene as input , and produce a 3D object instance label for each voxel, where the voxels belonging to the same object share an unique instance label.</p><p>Examining the aforementioned approaches, few of them explicitly utilize the inherent nature of 3D models that differs from 2D image observations: reconstruction of the environment in metric space without occlusion or scale ambiguity. As shown in <ref type="figure">Fig. 3</ref>, for the same instance in 3D space, its observations on 2D images can vary greatly. The number of occupied pixels/voxels of each instance (denoted as occupancy) is unpredictable on 2D image, yet can be predicted robustly from the reconstructed 3D model.</p><p>On the basis of occupancy signal, we propose an occupancy-aware 3D instance segmentation scheme. The pipeline is illustrated in <ref type="figure">Fig. 2</ref>. While it follows the classic learning followed by clustering procedure, both the learning stage and clustering stage differ from existing approaches. First, the input 3D scene is voxelized at a resolution of 2cm and is then fed into a 3D convolutional neural network (U-Net <ref type="bibr" target="#b39">[40]</ref>) for feature extraction. Then, the learned feature is forwarded to task-specific heads to learn different representations for each input voxel, including semantic segmentation, which aims to assign a class label, feature and spatial embedding, as well as occupancy regression (Sec. 3.1). Finally, a graph-based occupancy-aware clustering scheme is performed, which utilizes both the predicted occupancy information and the feature embedding from the previous stage (Sec. <ref type="bibr">3.2)</ref>. Note that all the 3D convolutions are real-ized using a submanifold sparse convolutional network <ref type="bibr" target="#b13">[14]</ref> to employ the sparsity nature of input 3D scene. The details of the network are provided in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-task Learning</head><p>In order to jointly leverage the inherent occupancy and semantic and spatial information from the 3D scene, we propose a multi-task learning framework to learn taskspecific representations for the i-th input voxel, including (1) c i for the semantic segmentation, which aims to assign a class label; (2) s i and d i for the joint feature and spatial embedding, as well as the corresponding b i for covariance prediction to fuse feature and spatial information; and (3) o i for the occupancy regression. The network is trained to minimize a joint cost function L joint :</p><formula xml:id="formula_0">L joint = L c + L e + L o .<label>(1)</label></formula><p>Here L c is a conventional cross-entropy loss <ref type="bibr" target="#b11">[12]</ref> for semantic segmentation. L e aims to learn an embedding vector that considers jointly feature and spatial embedding for instance segmentation (Sec. 3.1.1). L o serves for the regression of the occupancy size of each voxel's belonging instance (Sec. 3.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Embedding Learning</head><p>Unlike previous methods <ref type="bibr" target="#b34">[35]</ref> that concatenate the feature and spatial embedding directly, we propose to separate them explicitly and supervise their learning process with different objectives. Our key observation is that while spatial embedding is scale-aware and has an explicit physical explanation, such as an offset vector from current voxel to the spatial center of its belonging instance, feature embedding suffers from inherently ambiguous scale, and thus has to be regularized using additional cost functions. Both embeddings are further regularized using the covariance estimation. Our learning function for embedding L e consists of three terms, i.e., spatial term L sp , feature term L se , and covariance term L cov ,</p><formula xml:id="formula_1">L e = L sp + L se + L cov .<label>(2)</label></formula><p>Spatial Term. Spatial embedding d i for the i-th voxel is a 3-dimensional vector that regresses to the object center, which is supervised using the following spatial term:</p><formula xml:id="formula_2">L sp = 1 C C c=1 1 N c Nc i=1 ||d i + µ i − 1 N c Nc i=1 µ i ||,<label>(3)</label></formula><p>where C is the number of instances in the input 3D scene, N c is the number of voxels in the c-th instance, and µ i represents the 3D position of the i-th voxel of the c-th instance.</p><p>Feature Term. Feature embedding s i is learned using a discriminative loss function <ref type="bibr" target="#b6">[7]</ref> that consists of three terms:</p><formula xml:id="formula_3">L se = L var + L dist + L reg ,<label>(4)</label></formula><p>where the variance term L var draws current embedding towards the mean embedding of each instance, the distance term L dist pushes instances away from each other, and the regularization term L reg draws all instances towards the origin to keep the activation bounded. The detailed formulations are as follows.</p><formula xml:id="formula_4">L var = 1 C C c=1 1 N c N C i=1 [||u c − s i || − δ v ] 2 + ,<label>(5)</label></formula><formula xml:id="formula_5">L dist = 1 C(C − 1) C c A =1 C c B =c A +1 [2δ d − ||u c A − u c B ||] 2 + ,<label>(6)</label></formula><formula xml:id="formula_6">L reg = 1 C C c=1 ||u c ||.<label>(7)</label></formula><p>Here, u c = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nc</head><p>Nc i=1 s i represents the mean feature embedding of the c-th instance. The predefined thresholds δ v and δ d are set to be 0.1 and 1.5, ensuring that the intra-instance embedding distance is smaller than the inter-instance distance. Covariance Term. The covariance term aims to learn an optimal clustering region for each instance. Let b i = (σ i s , σ i d ) denote the predicted feature/spatial covariance for the i-th voxel in the c-th instance. By averaging b i , we obtain (σ c s , σ c d ), the embedding covariance of the c-th instance. Then, the probability of the i-th voxel belonging to the c-th instance, denoted as p i , is formulated as:</p><formula xml:id="formula_7">p i = exp(−( ||s i − u c || σ c s ) 2 − ( ||µ i + d i − e c || σ c d ) 2 ),<label>(8)</label></formula><p>where e c = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nc</head><p>Nc k=0 (µ k + d k ) represents the predicted spatial center of the c-th instance. Since p i is expected to be larger than 0.5 for voxels that belong to the c-th instance, the covariance term is then formulated by a binary crossentropy loss,</p><formula xml:id="formula_8">L cov = − 1 C C c=1 1 N N i=1 [y i log(p i ) + (1 − y i )log(1 − p i )],<label>(9)</label></formula><p>where y i = 1 indicates i belongs to c and y i = 0 otherwise, N indicates the number of points in the input point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Occupancy Regression</head><p>To utilize the occupancy information under the 3D setting, for the i-th voxel in the c-th instance, we predict a positive value o i to indicate the number of voxels occupied by the current instance. Then, the average of o i will serve as the predicted occupancy size of the current instance. For more robust prediction, we regress the logarithm instead of the original value and formulate the following occupancy term,</p><formula xml:id="formula_9">L o = 1 C C c=1 1 N c Nc i=1 ||o i − log(N c )||,<label>(10)</label></formula><p>where N c is the number of voxels in the c-th instance.</p><p>To evaluate the feasibility of our occupancy prediction strategy, we use the relative prediction error R c to measure the occupancy prediction performance of the c-th instance,</p><formula xml:id="formula_10">R c = |N c − exp( 1 Nc Nc i=1 o i )| N c .<label>(11)</label></formula><p>We particularly plot the cumulative distribution function of R c in <ref type="figure" target="#fig_2">Fig. 4</ref>. For over 4000 instances in the validation set of ScanNetV2 dataset <ref type="bibr" target="#b3">[4]</ref>, more than 68% instances are predicted, with a relative error smaller than 0.3, which illustrates the effectiveness of our occupancy regression for the following clustering stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Instance Clustering</head><p>In this subsection, based on the multi-representation learning from the previous stage, a graph-based occupancyaware clustering scheme is introduced to tackle the 3D instance segmentation problem during inference. Specifically, we adopt a bottom-up strategy and group the input voxels into super-voxels using an efficient graph-based segmentation scheme <ref type="bibr" target="#b9">[10]</ref>. Compared with super-pixel representations in 2D space <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b48">49]</ref>, super-voxel representation works better to separate different instances where the instance boundaries in 3D space is easier to identify thanks to the geometry continuity or local convexity constraints <ref type="bibr" target="#b2">[3]</ref>.</p><p>Let Ω i denote the collection of all the voxels belonging to the super-voxel v i , we define the spatial embedding D i of v i as,  <ref type="figure">Figure 5</ref>. Qualitative comparisons between OccuSeg and a previous approach <ref type="bibr" target="#b21">[22]</ref> on the validation set of the ScanNetV2 <ref type="bibr" target="#b3">[4]</ref>. OccuSeg generates more consistent instance labels and successfully distinguishes nearby small instances thanks to the proposed occupancy aware clustering scheme.</p><formula xml:id="formula_11">D i = 1 |Ω i | k∈Ωi (d i + µ i ),<label>(12)</label></formula><p>v i are computed based on a similar averaging operation for all the voxels belonging to v i . We further define the following occupancy ratio r i to guide the clustering step,</p><formula xml:id="formula_12">r i = O i |Ω i | .<label>(13)</label></formula><p>Note that r i &gt; 1 indicates that there are too many voxels in v i for instance segmentation, otherwise v i should attract more voxels. Given the super-voxel representation, an undirected graph G = (V, E, W ) is established, where the vertices v i ∈ V represent the generated super-voxels, e i,j = (v i , v j ) ∈ E indicates the pairs of vertices with a weight w i,j ∈ W . The weight w i,j represents the similarity between v i and v j . Here w i,j is formulated as</p><formula xml:id="formula_13">w i,j = exp(−( ||S i −S j || σs ) 2 − ( ||D i −D j || σ d ) 2 ) max(r, 0.5) ,<label>(14)</label></formula><p>where σ s , σ d and r represent the feature covariance, spatial covariance and occupancy ratio of the virtual super-voxel that merges both v i and v j . Note that a larger weight indicates a higher possibility that v i and v j belong to the same instance. And during the calculation of the merging weight, our occupancy ratio helps to punish over-segmented instances and encourages partial instances to be merged together as shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Geometry</head><p>Predicted Semantic GT Instance Predicted Instance GT Semantic <ref type="figure">Figure 6</ref>. Representative 3D instance segmentation results on the validation set of public datasets, including ScanNetV2 <ref type="bibr" target="#b3">[4]</ref> and S3DIS <ref type="bibr" target="#b0">[1]</ref>.</p><p>For all the edges in E, we select edge e i,j with the highest weight w i,j and merge v i , v j as a new vertex if w i,j &gt; T 0 , where the merge threshold T 0 is set to be 0.5. The graph G is then updated after every merge operation. This process is iterated until none of the weight is larger than T 0 . Finally, the remaining vertices in G are labeled as instances if their occupancy ratio r satisfies the constraint of 0.3 &lt; r &lt; 2 to reject false positives in instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Training</head><p>We employ a simple UNet-like structure <ref type="bibr" target="#b39">[40]</ref> for feature extraction from the input point cloud with color information. Network details are presented in the Appendix. For the sake of efficiency, the chunk-based sparse convolution strategy in <ref type="bibr" target="#b19">[20]</ref> is adopted, which is 4× faster than the original implementation of the SCN <ref type="bibr" target="#b13">[14]</ref>. The network is trained using Adam optimizer with an initial learning rate of 1e-3. For all the datasets including ScanNetV2 <ref type="bibr" target="#b3">[4]</ref>, Stanford3D <ref type="bibr" target="#b0">[1]</ref> and SceneNN <ref type="bibr" target="#b18">[19]</ref> as shown in the experiments of Sec. 4, we use the same hyper-parameters and train the network from scratch for 320 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our method on a variety of challenging scenarios. For experiments on public datasets, we run our method on a PC with a NVIDIA TITAN Xp GPU and an Intel(R) Xeon(R) E5-2650 CPU. For real-world experiments, our method is conducted on the laptop Microsoft Surface Book 2 with a NVIDIA GTX 1060 (Mobile) GPU and an Intel Core i7-8650U CPU. Using the real-time 3D reconstruction method FlashFusion <ref type="bibr" target="#b14">[15]</ref> for the 3D geometric input, we present the demo of online 3D instance segmentation on the portable device. More details are provided in the supplementary video.</p><p>We employ the popular 3D instance segmentation benchmark ScanNetV2 <ref type="bibr" target="#b3">[4]</ref>, as well as the widely used S3DIS <ref type="bibr" target="#b0">[1]</ref> and SceneNN <ref type="bibr" target="#b18">[19]</ref> datasets. ScanNetV2 benchmark <ref type="bibr" target="#b3">[4]</ref> contains 1513 indoor RGBD scans with 3D instance annotations, while Stanford Large-Scale 3D Indoor Space Dataset (S3DIS) <ref type="bibr" target="#b0">[1]</ref> contains 6 large-scale indoor areas covering over 6000m 2 with 13 object classes. SceneNN <ref type="bibr" target="#b18">[19]</ref> is a smaller indoor 3D dataset with 50 scans as the training set and 26 scans for evaluation, which is used to evaluate our performance under less training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Qualitative Evaluation</head><p>The representative 3D instance segmentation results on the validation set of public datasets are presented in <ref type="figure">Fig. 6</ref>, which demonstrate that the proposed approach achieves robust instance segmentation results for complex environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometry</head><p>Instance <ref type="bibr" target="#b0">(1)</ref> (2) (3) (4) (5) <ref type="figure">Figure 7</ref>. 3D instance segmentation results on real-world scenes. Here the 3D geometric models are reconstructed using FlashFusion <ref type="bibr" target="#b14">[15]</ref> system, with the input being the depth and color sequences from consumer-level RGB-D camera. Our scheme generates robust instance segmentation results in real-world environments using the network trained on a public dataset, ScanNetV2 <ref type="bibr" target="#b3">[4]</ref>.</p><p>Method mAP bath bed bkshf cab chair cntr curt desk door ofurn pic fridg showr sink sofa tabl toil wind 3D-SIS <ref type="bibr" target="#b17">[18]</ref> 16. To further verify the robustness of our method on realworld scenes, we implement our method on the basis of real-time 3D reconstruction method FlashFusion <ref type="bibr" target="#b14">[15]</ref> for online 3D instance segmentation. As shown in <ref type="figure">Fig. 7</ref>, our network pre-trained on ScanNetV2 can generate 3D instance segmentation results robustly in real-world scenarios. More live results are provided in the supplementary video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Evaluation</head><p>Based on the public datasets, our methods are quantitatively compared with a number of representative exist-mPrec mRec PartNet <ref type="bibr" target="#b29">[30]</ref> 56.4 43.4 ASIS <ref type="bibr" target="#b43">[44]</ref> 63.6 47.5 3D-BoNet <ref type="bibr" target="#b47">[48]</ref> 65.6 47.6 OccuSeg 72.8 60.3 <ref type="table">Table 3</ref>. Comparison on the S3DIS <ref type="bibr" target="#b0">[1]</ref> dataset. Our method outperforms previous methods in terms of mean Precision (mPrec) and mean recall (mRec) with an IoU threshold of 0.5.</p><p>ing methods, including SGPN <ref type="bibr" target="#b42">[43]</ref>, 3D-SIS <ref type="bibr" target="#b17">[18]</ref>, Panop-ticFusion <ref type="bibr" target="#b30">[31]</ref>, 3D-BoNet <ref type="bibr" target="#b47">[48]</ref>, MTML <ref type="bibr" target="#b21">[22]</ref>, ASIS <ref type="bibr" target="#b43">[44]</ref> and JSIS3D <ref type="bibr" target="#b35">[36]</ref>.</p><p>ScanNetV2. We follow the benchmark <ref type="bibr" target="#b3">[4]</ref> to use the mean average precision at overlap 0.25 (mAP@0.25), overlap 0.5 (mAP@0.5) and overlaps in the range [0.5 : 0.95 : 0.05] (mAP) as evaluation metrics. Tab. 1 and Tab. 2 summarize the per-class mAP and the overall performance, respectively. Overall, our method achieves a significant margin on all the three metrics, especially the hardest mAP metric, indicating the effectiveness of our method for 3D instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3DIS.</head><p>Following the previous methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b43">44]</ref>, we employ the 6-fold cross validation and use the mean precision  <ref type="table">Table 5</ref>. The processing time (seconds) on the validation set of ScanNetV2 <ref type="bibr" target="#b3">[4]</ref>. Note that all the other methods are evaluated based on their released codes according to <ref type="bibr" target="#b47">[48]</ref> (mPrec) / mean recall (mRec) with an IoU threshold 0.5 to evaluate our method in the S3DIS dataset. As shown in Tab. 3, our scheme outperforms all the previous methods by a significant margin in terms of both mPrec and mRec, indicating its ability to segment more instances precisely. SceneNN. Similar to previous work <ref type="bibr" target="#b35">[36]</ref>, mAP@0.5 metric is adopted to evaluate our approach in the SceneNN dataset. As presented in Tab. 4, even using only 50 scans for training, our approach outperforms the previous method <ref type="bibr" target="#b35">[36]</ref> by a significant margin (35 for mAP@0.5), which illustrates the effectiveness of our approach under small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Complexity Analysis</head><p>Maintaining high efficiency plays a vital role when applying 3D instance segmentation to mixed reality or robotics applications. Similar to the evaluation in <ref type="bibr" target="#b47">[48]</ref>, we made a comparison of the processing times on the 312 scans of indoor environments from the validation split of ScanNetV2. Both the proposal-based methods (3DSIS <ref type="bibr" target="#b17">[18]</ref>, GSPN <ref type="bibr" target="#b49">[50]</ref> and 3D-BoNet <ref type="bibr" target="#b47">[48]</ref>) and the proposal-free methods (SGPN <ref type="bibr" target="#b42">[43]</ref> ASIS <ref type="bibr" target="#b43">[44]</ref>) are concerned. The processing time of the full pipeline and the main stages are respectively reported in Tab. 5. Remarkably, our method is more than 4× faster than the existed most efficient approach 3D-BoNet <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Here, we evaluate the individual components of our method on the ScanNetV2 validation split. Let w/o feature and w/o spatial denote the variations of our method without the feature embedding or spatial feature embedding, respectively. To evaluate the influence of the novel occupancy signal, we disable the occupancy prediction in the learning stage and set the occupancy ratio r = 1 for all vertices in Eqn. 14 during the clustering stage, denoted as w/o occupancy.</p><p>The quantitative comparison results of all the variations of our method are provided in Tab. 6, which demonstrate that the proposed occupancy aware scheme helps to improve the overall quality of 3D instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>We presented OccuSeg, an occupancy-aware instance segmentation method for 3D scenes. Our learning stage leverages feature embedding and spatial embedding, as well as a novel 3D occupancy signal to imply the inherent property of 3D objects. The occupancy signal further guides our graph-based clustering stage to correctly merge hard samples and prohibit over-segmented clusters. Extensive experimental results demonstrate the effectiveness of our method, which outperforms previous methods by a significant margin and retains high efficiency. In the future work, we will improve our method by incorporating tailored designs for partially reconstructed objects. Also, we intend to investigate the sub-object level 3D instance segmentation and further improve the efficiency, enabling the practical usage of high quality 3D instance segmentation for tremendous applications in AR/VR, gaming and mobile robots.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1. Given the input colored point cloud, occupancy size is regressed for each voxel, which predicts the number of voxels occupied by its belonging instance. An adaptive clustering scheme jointly considers both the occupancy information and embedding distance is further applied for 3D instance segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Cumulative distribution function of the relative prediction error on the validation set of the ScanNetV2<ref type="bibr" target="#b3">[4]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2003.06537v3 [cs.CV] 28 Apr 2020 Overview of the proposed instance segmentation scheme. For the input point cloud, our method takes RGB feature as input and employ 3D UNet for point-wise feature learning. The learned feature is decoded to various representations though a fully connected layer for 3D instance segmentation.</figDesc><table><row><cell></cell><cell></cell><cell>Graph cut clustering</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Feature term</cell><cell>Covariance</cell><cell>Super-voxel</cell></row><row><cell>Input geometry</cell><cell>3D UNet</cell><cell>Spatial term</cell><cell></cell><cell>Final graph</cell><cell>Final result</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Initial graph</cell></row><row><cell></cell><cell></cell><cell cols="2">Weighting</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Occupancy</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>w i,j</cell><cell></cell></row><row><cell>Figure 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>34.5 18.6 29.8 33.9 23.1 41.3 80.7 34.5 50.6 42.4 97.2 29.1 Quantitative comparison on the ScanNetV2<ref type="bibr" target="#b3">[4]</ref> benchmark in terms of mAP score on 18 classes. Our approach achieves the best performance in 17 out of 18 classes. Note that the ScanNetV2 benchmark data is accessed on 11/14/2019.</figDesc><table><row><cell></cell><cell cols="3">1 40.7 15.5 6.8 4.3 34.6 0.1 13.4 0.5 8.8 10.6 3.7 13.5 32.1 2.8 33.9 11.6 46.6 9.3</cell></row><row><cell cols="4">PanopticFusion [31] 21.4 25.0 33.0 27.5 10.3 22.8 0.0 34.5 2.4 8.8 20.3 18.6 16.7 36.7 12.5 22.1 11.2 66.6 16.2</cell></row><row><cell>3D-BoNet [48]</cell><cell cols="3">25.3 51.9 32.4 25.1 13.7 34.5 3.1 41.9 6.9 16.2 13.1 5.2 20.2 33.8 14.7 30.1 30.3 65.1 17.8</cell></row><row><cell>MTML [22]</cell><cell cols="3">28.2 57.7 38.0 18.2 10.7 43.0 0.1 42.2 5.7 17.9 16.2 7.0 22.9 51.1 16.1 49.1 31.3 65.0 16.2</cell></row><row><cell>Occipital-SCS</cell><cell cols="3">32.0 67.9 35.2 33.4 22.9 43.6 2.5 41.2 5.8 16.1 24.0 8.5 26.2 49.6 18.7 46.7 32.8 77.5 23.1</cell></row><row><cell>OccuSeg</cell><cell cols="3">44.3 85.2 56.0 38.0 24.9 67.9 9.7 mAP mAP@0.5 mAP@0.25</cell></row><row><cell>3D-SIS [18]</cell><cell>16.1</cell><cell>38.2</cell><cell>55.8</cell></row><row><cell cols="2">3D-BoNet [48] 25.3</cell><cell>48.8</cell><cell>68.7</cell></row><row><cell>MASC [28]</cell><cell>25.4</cell><cell>44.7</cell><cell>61.5</cell></row><row><cell>MTML [22]</cell><cell>28.2</cell><cell>54.9</cell><cell>73.1</cell></row><row><cell>Occipital-SCS</cell><cell>32.0</cell><cell>51.2</cell><cell>68.8</cell></row><row><cell>OccuSeg</cell><cell>44.3</cell><cell>63.4</cell><cell>73.9</cell></row><row><cell cols="4">Table 2. Quantitative results on the ScanNetV2 [4] benchmark</cell></row><row><cell cols="4">in terms of mAP, mAP@0.5 and mAP@0.25, respectively. Our</cell></row><row><cell cols="4">approach outperforms previous methods by a significant margin.</cell></row><row><cell cols="4">ScanNetV2 benchmark data is accessed on 11/14/2019.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Quantitative results on the SceneNN<ref type="bibr" target="#b18">[19]</ref> dataset in terms of mAP@0.5 score of each class. Our approach achieves the best performance for all the 10 classes.</figDesc><table><row><cell cols="2">Method</cell><cell cols="8">mAP@0.5 wall floor cabinet bed chair sofa table desk</cell><cell>tv</cell><cell>prop</cell></row><row><cell cols="2">MT-PNet [36]</cell><cell>8.5</cell><cell cols="2">13.1 27.3</cell><cell>0.0</cell><cell>15.0 21.2</cell><cell>0.0</cell><cell>0.7</cell><cell>0.0</cell><cell>6.0</cell><cell>2.0</cell></row><row><cell cols="2">MLS-CRF [36]</cell><cell>12.1</cell><cell cols="2">13.9 44.5</cell><cell>0.0</cell><cell>32.9 12.9</cell><cell>0.0</cell><cell>5.7</cell><cell>10.8</cell><cell>0.0</cell><cell>0.8</cell></row><row><cell cols="2">OccuSeg</cell><cell>47.1</cell><cell cols="2">39.0 93.8</cell><cell>5.7</cell><cell>66.7 91.3</cell><cell>8.7</cell><cell cols="3">50.0 31.6 76.9 7.14</cell></row><row><cell></cell><cell></cell><cell>Details</cell><cell></cell><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">network(GPU): 650</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SGPN [43]</cell><cell cols="3">group merging(CPU): 46562</cell><cell>49433</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">block merging(CPU): 2221</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">network(GPU): 650</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASIS [44]</cell><cell cols="3">mean shift(CPU): 53886</cell><cell>56757</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">block merging(CPU): 2221</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">network(GPU): 500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GSPN [50]</cell><cell cols="3">point sampling(GPU): 2995</cell><cell>3963</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">neighbour search(CPU): 468</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D-SIS [18]</cell><cell cols="4">network (GPU+CPU): 38841 38841</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">network(GPU): 650</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D-BoNet [48]</cell><cell cols="3">SCN (GPU parallel): 208</cell><cell>2871</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">block merging(CPU): 2221</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">network(GPU): 59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OccuSeg</cell><cell cols="3">supervoxel(CPU): 375</cell><cell>594</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">clustering(GPU+CPU): 160</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture Details</head><p>The network architecture adopted in OccuSeg is presented in this section. We employ the widely used UNetstyle network <ref type="bibr" target="#b39">[40]</ref> for feature learning as shown in <ref type="figure">Fig. 8</ref>. The network is mainly built upon submanifold sparse convolution(SSC) layers and sparse convolution layers, both of which are originally introduced by Graham <ref type="bibr" target="#b12">[13]</ref>. In <ref type="figure">Fig. 8</ref>, K represents the dimension of the semantic term of the learned embedding, which is set to 32 in our experiment. Additionally, C stands for the semantic class numbers, which conforms to the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Qualitative Comparisons</head><p>In this section, we show some more qualitative comparisons on the ScanNetV2 <ref type="bibr" target="#b3">[4]</ref>. As shown in <ref type="figure">Fig. 9</ref>, our results are compared with sparse convolutional networks <ref type="bibr" target="#b13">[14]</ref>, SGPN <ref type="bibr" target="#b42">[43]</ref> and multi-task metric learning method from Lahoud et al. <ref type="bibr" target="#b21">[22]</ref>. Our results generally show a better capacity of dealing with small objects, as well as produce less noise.</p><p>Input SCN <ref type="bibr" target="#b13">[14]</ref> SGPN <ref type="bibr" target="#b42">[43]</ref> Instance GT Lahoud et al. <ref type="bibr" target="#b21">[22]</ref> Ours <ref type="figure">Figure 9</ref>. More qualitative comparisons on ScanNetV2 <ref type="bibr" target="#b3">[4]</ref>. All the results of the previous methods are taken from Lahoud et al. <ref type="bibr" target="#b21">[22]</ref>. Note that the instance segmentation result of sparse convolution networks(SCN) <ref type="bibr" target="#b13">[14]</ref> is obtained by showing connected components of its semantic segmentation results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08755</idno>
		<title level="m">Minkowski convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>4d spatio-temporal convnets</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object partitioning using local convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Simon Christoph Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Schoeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentin</forename><surname>Papon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Worgotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02551</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6070</idno>
		<title level="m">Spatially-sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Flashfusion: Real-time globally consistent dense 3d reconstruction using cpu computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boundary-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4421" to="4430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scenenn: A scene meshes dataset with annotations</title>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<editor>Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Yu, and Sai-Kit Yeung</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Realtime semantic 3d perception for immersive augmented reality</title>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>Anonymous (Attached in the supplementary material)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent pixel embedding for instance grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08650</idno>
		<title level="m">Marc Pollefeys, and Martin R Oswald. 3d instance segmentation via multi-task metric learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Bastian Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2978" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">3d graph embedding learning with a structure-aware loss function for point cloud semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05247</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Masc: Multi-scale affinity with sparse convolution for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04478</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Affinity derivation and graph merge for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Partnet: A largescale benchmark for fine-grained and hierarchical part-level 3d object understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Panopticfusion: Online volumetric semantic mapping at the level of stuff and things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaku</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohsuke</forename><surname>Kaji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01177</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8837" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time 3d reconstruction at scale using voxel hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamminger</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-convolutional operators for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Jsis3d: Joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8827" to="8836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09664</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient semantic image segmentation with superpixel pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathijs</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02705</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8818" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unstructuredfusion: Realtime 4d geometry and texture reconstruction using commercialrgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01140</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic 3d occupancy mapping through efficient high order crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3947" to="3956" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
