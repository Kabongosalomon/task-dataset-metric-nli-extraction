<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SOM-VAE: INTERPRETABLE DISCRETE REPRESENTATION LEARNING ON TIME SERIES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Fortuin</surname></persName>
							<email>fortuin@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich Universitätsstrasse 6</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hüser</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich Universitätsstrasse 6</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
							<email>locatelf@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich Universitätsstrasse 6</orgName>
								<address>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Strathmann</surname></persName>
							<email>heiko.strathmann@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Gatsby Unit</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>25 Howland Street</addrLine>
									<postCode>W1T 4JG</postCode>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Rätsch</surname></persName>
							<email>raetsch@inf.ethz.ch</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<addrLine>ETH Zürich Universitätsstrasse 6</addrLine>
									<postCode>8092</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SOM-VAE: INTERPRETABLE DISCRETE REPRESENTATION LEARNING ON TIME SERIES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time.</p><p>To address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty. We evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Interpretable representation learning on time series is a seminal problem for uncovering the latent structure in complex systems, such as chaotic dynamical systems or medical time series. In areas where humans have to make decisions based on large amounts of data, interpretability is fundamental to ease the human task. Especially when decisions have to be made in a timely manner and rely on observing some chaotic external process over time, such as in finance or medicine, the need for intuitive interpretations is even stronger. However, many unsupervised methods, such as clustering, make misleading i.i.d. assumptions about the data, neglecting their rich temporal structure and smooth behaviour over time. This poses the need for a method of clustering, where the clusters assume a topological structure in a lower dimensional space, such that the representations of the time series retain their smoothness in that space. In this work, we present a method with these properties.</p><p>We choose to employ deep neural networks, because they have a very successful tradition in representation learning <ref type="bibr" target="#b5">(Bengio et al., 2013)</ref>. In recent years, they have increasingly been combined with generative modeling through the advent of generative adversarial networks (GANs) <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref> and variational autoencoders (VAEs) <ref type="bibr" target="#b18">(Kingma and Welling, 2013)</ref>. However, the representations learned by these models are often considered cryptic and do not offer the necessary interpretability . A lot of work has been done to improve them in this regard, in GANs  as well as VAEs <ref type="bibr" target="#b16">(Higgins et al., 2017;</ref><ref type="bibr" target="#b9">Esmaeili et al., 2018)</ref>. Alas, these works have focused entirely on continuous representations, while discrete ones are still underexplored.</p><p>In order to define temporal smoothness in a discrete representation space, the space has to be equipped with a topological neighborhood relationship. One type of representation space with such a structure is induced by the self-organizing map (SOM) <ref type="bibr" target="#b21">(Kohonen, 1990)</ref>. The SOM allows to map states from an uninterpretable continuous space to a lower-dimensional space with a predefined topologically interpretable structure, such as an easily visualizable two-dimensional grid. However, while yielding promising results in visualizing static state spaces, such as static patient states <ref type="bibr" target="#b27">(Tirunagari et al., 2015)</ref>, the classical SOM formulation does not offer a notion of time. The time component can be incorporated using a probabilistic transition model, e.g. a Markov model, such that the representations of a single time point are enriched with information from the adjacent time points in the series. It is therefore potentially fruitful to apply the approaches of probabilistic modeling alongside representation learning and discrete dimensionality reduction in an end-to-end model.</p><p>In this work, we propose a novel deep architecture that learns topologically interpretable discrete representations in a probabilistic fashion. Moreover, we introduce a new method to overcome the non-differentiability in discrete representation learning architectures and develop a gradient-based version of the classical selforganizing map algorithm with improved performance. We present extensive empirical evidence for the model's performance on synthetic and real world time series from benchmark data sets, a synthetic dynamical system with chaotic behavior and real world medical data.</p><p>Our main contributions are to • Devise a novel framework for interpretable discrete representation learning on time series.</p><p>• Show that the latent probabilistic model in the representation learning architecture improves clustering and interpretability of the representations on time series.</p><p>• Show superior clustering performance of the model on benchmark data and a real world medical data set, on which it also facilitates downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBABILISTIC SOM-VAE</head><p>Our proposed model combines ideas from self-organizing maps <ref type="bibr" target="#b21">(Kohonen, 1990)</ref>, variational autoencoders <ref type="bibr" target="#b18">(Kingma and Welling, 2013)</ref> and probabilistic models. In the following, we will lay out the different components of the model and their interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">INTRODUCING TOPOLOGICAL STRUCTURE IN THE LATENT SPACE</head><p>A schematic overview of our proposed model is depicted in <ref type="figure">Figure 1</ref>. An input x ∈ R d is mapped to a latent encoding z e ∈ R m (usually m &lt; d) by computing z e = f θ (x), where f θ (·) is parameterized by the encoder neural network. The encoding is then assigned to an embedding z q ∈ R m in the dictionary of embeddings E = {e 1 , . . . , e k | e i ∈ R m } by sampling z q ∼ p(z q |z e ). The form of this distribution is flexible and can be a design choice. In order for the model to behave similarly to the original SOM algorithm (see below), in our experiments we choose the distribution to be categorical with probability mass 1 on the closest embedding to z e , i.e. p(z q |z e ) = 1[z q = arg min e∈E z e − e 2 ], where 1[·] is the indicator function. A reconstructionx of the input can then be computed asx = g φ (z), where g φ (·) is parameterized by the decoder neural network.</p><p>Since the encodings and embeddings live in the same space, one can compute two different reconstructions, namelyx e = g φ (z e ) andx q = g φ (z q ).</p><p>To achieve a topologically interpretable neighborhood structure, the embeddings are connected to form a self-organizing map. A self-organizing map consists of k nodes V = {v 1 , . . . , v k }, where every node corresponds to an embedding in the data space e v ∈ R d and a representation in a lower-dimensional discrete space  <ref type="bibr">[red]</ref>. In order to achieve a discrete representation, every latent data point (z e ) is mapped to its closest node in the SOM (z q ). A Markov transition model [blue] is learned to predict the next discrete representation (z t+1 q ) given the current one (z t q ). The discrete representations can then be decoded by another neural network back into the original data space.</p><formula xml:id="formula_0">m v ∈ M ,</formula><p>node u ∈ V is then updated according to e u ← e u + N (m u , mṽ)η(x i − e u ), where η is the learning rate and N (m u , mṽ) is a neighborhood function between the nodes defined on the representation space M . There can be different design choices for N (m u , mṽ). A more thorough review of the self-organizing map algorithm is deferred to the appendix (Sec. A).</p><p>We choose to use a two-dimensional SOM because it facilitates visualization similar to <ref type="bibr" target="#b27">Tirunagari et al. (2015)</ref>. Since we want the architecture to be trainable end-to-end, we cannot use the standard SOM training algorithm described above. Instead, we devise a loss function term whose gradient corresponds to a weighted version of the original SOM update rule (see below). We implement it in such a way that any time an embedding e i,j at position (i, j) in the map gets updated, it also updates all the embeddings in its immediate neighborhood N (e i,j ). The neighborhood is defined as N (e i,j ) = {e i−1,j , e i+1,j , e i,j−1 , e i,j+1 } for a two-dimensional map.</p><p>The loss function for a single input x looks like</p><formula xml:id="formula_1">L SOM-VAE (x,x q ,x e ) = L reconstruction (x,x q ,x e ) + α L commitment (x) + β L SOM (x)<label>(1)</label></formula><p>where x, z e , z q ,x e andx q are defined as above and α and β are weighting hyperparameters.</p><p>Every term in this function is specifically designed to optimize a different model component. The first term is the reconstruction loss L reconstruction (x,x q ,x e ) = x−x q 2 + x−x e 2 . The first subterm of this is the discrete reconstruction loss, which encourages the assigned SOM node z q (x) to be an informative representation of the input. The second subterm encourages the encoding z e (x) to also be an informative representation. This ensures that all parts of the model have a fully differentiable credit assignment path to the loss function, which facilitates training. Note that the reconstruction loss corresponds to the evidence lower bound (ELBO) of the VAE part of our model <ref type="bibr" target="#b18">(Kingma and Welling, 2013)</ref>. Since we assume a uniform prior over z q , the KL-term in the ELBO is constant w.r.t. the parameters and can be ignored during optimization.</p><p>The term L commitment encourages the encodings and assigned SOM nodes to be close to each other and is defined as L commitment (x) = z e (x) − z q (x) 2 . Closeness of encodings and embeddings should be expected to already follow from the L reconstruction term in a fully differentiable architecture. However, due to the nondifferentiability of the embedding assignment in our model, the L commitment term has to be explicitly added to the objective in order for the encoder to get gradient information about z q .</p><formula xml:id="formula_2">The SOM loss L SOM is defined as L SOM (x) = ẽ∈N (zq(x)) ẽ − sg[z e (x)] 2 , where N (·)</formula><p>is the set of neighbors in the discrete space as defined above and sg[·] is the gradient stopping operator that does not change the outputs during the forward pass, but sets the gradients to 0 during the backward pass. It encourages the neighbors of the assigned SOM node z q to also be close to z e , thus enabling the embeddings to exhibit a self-organizing map property, while stopping the gradients on z e such that the encoding is not pulled in the direction of the neighbors. This term enforces a neighborhood relation between the discrete codes and encourages all SOM nodes to ultimately receive gradient information from the data. The gradient stopping in this term is motivated by the observation that the data points themselves do not get moved in the direction of their assigned SOM node's neighbors in the original SOM algorithm either (see above). We want to optimize the embeddings based on their neighbors, but not the respective encodings, since any single encoding should be as close as possible to its assigned embedding and not receive gradient information from any other embeddings that it is not assigned to. Note that the gradient update of a specific SOM node in this formulation depends on its distance to the encoding, while the step size in the original SOM algorithm is constant. It will be seen that this offers some benefits in terms of optimization and convergence (see Sec. 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">OVERCOMING THE NON-DIFFERENTIABILITY</head><p>The main challenge in optimizing our architecture is the non-differentiability of the discrete cluster assignment step. Due to this, the gradients from the reconstruction loss cannot flow back into the encoder. A model with a similar problem is the recently proposed vector-quantized VAE (VQ-VAE) (van den <ref type="bibr" target="#b29">Oord et al., 2017)</ref>. It can be seen as being similar to a special case of our SOM-VAE model, where one sets β = 0, i.e. disables the SOM structure.</p><p>In order to mitigate the non-differentiability, the authors of the VQ-VAE propose to copy the gradients from z q to z e . They acknowledge that this is an ad hoc approximation, but observed that it works well in their experiments. Due to our smaller number of embeddings compared to the VQ-VAE setup, the average distance between an encoding and its closest embedding is much larger in our case. The gradient copying (see above) thus ceases to be a feasible approximation, because the true gradients at points in the latent space which are farther apart will likely be very different.</p><p>In order to still overcome the non-differentiability issue, we propose to add the second reconstruction subterm to L reconstruction , where the reconstructionx e is decoded directly from the encoding z e . This adds a fully differentiable credit assignment path from the loss to the encoder and encourages z e to also be an informative representation of the input, which is a desirable model feature. Most importantly, it works well in practice (see Sec. 4.1).</p><p>Note that since z e is continuous and therefore much less constrained than z q , this term is optimized easily and becomes small early in training. After that, mostly the z q -term contributes to L reconstruction . One could therefore view the z e -term as an initial encouragement to place the data encodings at sensible positions in the latent space, after which the actual clustering task dominates the training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ENCOURAGING SMOOTHNESS OVER TIME</head><p>Our ultimate goal is to predict the development of time series in an interpretable way. This means that not only the state representations should be interpretable, but so should be the prediction as well. To this end, we use a temporal probabilistic model. Learning a probabilistic model in a high-dimensional continuous space can be challenging. Thus, we exploit the low-dimensional discrete space induced by our SOM to learn a temporal model. For that, we define a system state as the assigned node in the SOM and then learn a Markov model for the transitions between those states. The model is learned jointly with the SOM-VAE, where the loss function becomes</p><formula xml:id="formula_3">L(x t−1 , x t ,x t q ,x t e ) = L SOM-VAE (x t ,x t q ,x t e ) + γ L transitions (x t−1 , x t ) + τ L smoothness (x t−1 , x t )<label>(2)</label></formula><p>with weighting hyperparameters γ and τ .</p><p>The term L transitions encourages the probabilities of actually observed transitions to be high. It is defined as</p><formula xml:id="formula_4">L transitions (x t−1 , x t ) = − log P M (z q (x t−1 ) → z q (x t )), with P M (z q (x t−1 ) → z q (x t )) being the probability of a transition from state z q (x t−1 ) to state z q (x t ) in the Markov model.</formula><p>The term L smoothness encourages the probabilities for transitions to nodes that are far away from the current data point to be low or respectively the nodes with high transition probabilities to be proximal. It achieves this by taking large values only for transitions to far away nodes that have a high probability under the model. It is</p><formula xml:id="formula_5">defined as L smoothness (x t−1 , x t ) = E P M (zq(x t−1 )→ẽ) ẽ − z e (x t ) 2 .</formula><p>The probabilistic model can inform the evolution of the SOM through this term which encodes our prior belief that transitions in natural data happen smoothly and that future time points will therefore mostly be found in the neighborhood of previous ones. In a setting where the data measurements are noisy, this improves the clustering by acting as a temporal smoother.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>From the early inception of the k-means algorithm for clustering <ref type="bibr" target="#b24">(Lloyd, 1982)</ref>, there has been much methodological improvement on this unsupervised task. This includes methods that perform clustering in the latent space of (variational) autoencoders <ref type="bibr" target="#b1">(Aljalbout et al., 2018)</ref> or use a mixture of autoencoders for the clustering <ref type="bibr" target="#b32">(Zhang et al., 2017;</ref><ref type="bibr">Locatello et al., 2018)</ref>. The method most related to our work is the VQ-VAE (van den <ref type="bibr" target="#b29">Oord et al., 2017)</ref>, which can be seen as a special case of our framework (see above). Its authors have put a stronger focus on the discrete representation as a form of compression instead of clustering. Hence, our model and theirs differ in certain implementation considerations (see Sec. 2.2). All these methods have in common that they only yield a single number as a cluster assignment and provide no interpretable structure of relationships between clusters.</p><p>The self-organizing map (SOM) <ref type="bibr" target="#b21">(Kohonen, 1990)</ref>, however, is an algorithm that provides such an interpretable structure. It maps the data manifold to a lower-dimensional discrete space, which can be easily visualized in the 2D case. It has been extended to model dynamical systems <ref type="bibr" target="#b4">(Barreto and Araujo, 2004)</ref> and combined with probabilistic models for time series <ref type="bibr" target="#b25">(Sang et al., 2008)</ref>, although without using learned representations. There are approaches to turn the SOM into a "deeper" model <ref type="bibr" target="#b8">(Dittenbach et al., 2000)</ref>, combine it with multi-layer perceptrons <ref type="bibr" target="#b11">(Furukawa et al., 2005)</ref> or with metric learning (Płoński and Zaremba, 2012). However, it has (to the best of our knowledge) not been proposed to use SOMs in the latent space of (variational) autoencoders or any other form of unsupervised deep learning model.</p><p>Interpretable models for clustering and temporal predictions are especially crucial in fields where humans have to take responsibility for the model's predictions, such as in health care. The prediction of a patient's future state is an important problem, particularly on the intensive care unit (ICU) <ref type="bibr" target="#b15">(Harutyunyan et al., 2017;</ref><ref type="bibr" target="#b3">Badawi et al., 2018)</ref>. Probabilistic models, such as Gaussian processes, have been successfully applied in this domain <ref type="bibr" target="#b7">(Colopy et al., 2016;</ref><ref type="bibr" target="#b26">Schulam and Arora, 2016)</ref>. Recently, deep generative models have been proposed <ref type="bibr" target="#b10">(Esteban et al., 2017)</ref>, sometimes even in combination with probabilistic modeling <ref type="bibr" target="#b23">(Lim and Schaar, 2018)</ref>. To the best of our knowledge, SOMs have only been used to learn interpretable static representations of patients <ref type="bibr" target="#b27">(Tirunagari et al., 2015)</ref>, but not dynamic ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We performed experiments on MNIST handwritten digits <ref type="bibr" target="#b22">(LeCun et al., 1998)</ref>, Fashion-MNIST images of clothing <ref type="bibr" target="#b31">(Xiao et al., 2017)</ref>, synthetic time series of linear interpolations of those images, time series from a chaotic dynamical system and real world medical data from the eICU Collaborative Research Database <ref type="bibr" target="#b12">(Goldberger et al., 2000)</ref>. If not otherwise noted, we use the same architecture for all experiments, sometimes including the latent probabilistic model (SOM-VAE_prob) and sometimes excluding it (SOM-VAE). For model implementation details, we refer to the appendix (Sec. B) 1 .</p><p>We found that our method achieves a superior clustering performance compared to other methods. We also show that we can learn a temporal probabilistic model concurrently with the clustering, which is on par with the maximum likelihood solution, while improving the clustering performance. Moreover, we can learn interpretable state representations of a chaotic dynamical system and discover patterns in real medical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CLUSTERING ON MNIST AND FASHION-MNIST</head><p>In order to test the clustering component of the SOM-VAE, we performed experiments on MNIST and Fashion-MNIST. We compare our model (including different adjustments to the loss function) against k-means <ref type="bibr" target="#b24">(Lloyd, 1982)</ref> (sklearn-package (Pedregosa et al., 2011)), the VQ-VAE (van den Oord et al., 2017), a standard implementation of a SOM (minisom-package <ref type="bibr" target="#b30">(Vettigli, 2017)</ref>) and our version of a GB-SOM (gradient-based SOM), which is a SOM-VAE where the encoder and decoder are set to be identity functions. The k-means algorithm was initialized using k-means++ <ref type="bibr" target="#b2">(Arthur and Vassilvitskii, 2007)</ref>. To ensure comparability of the performance measures, we used the same number of clusters (i.e. the same k) for all the methods.</p><p>The results of the experiment in terms of purity and normalized mutual information (NMI) are shown in <ref type="table" target="#tab_1">Table 1</ref>. The SOM-VAE outperforms the other methods w.r.t. the clustering performance measures. It should be noted here that while k-means is a strong baseline, it is not density matching, i.e. the density of cluster centers is not proportional to the density of data points. Hence, the representation of data in a space induced by the k-means clusters can be misleading.</p><p>As argued in the appendix (Sec. C), NMI is a more balanced measure for clustering performance than purity. If one uses 512 embeddings in the SOM, one gets a lower NMI due to the penalty term for the number of  0.114 ± 0.000 0.001 ± 0.000 0.110 ± 0.009 0.018 ± 0.016 gradcopy* 0.583 ± 0.004 0.436 ± 0.004 0.556 ± 0.008 0.444 ± 0.005 SOM-VAE* 0.731 ± 0.004 0.594 ± 0.004 0.678 ± 0.005 0.590 ± 0.003 clusters, but it yields an interpretable two-dimensional representation of the manifolds of MNIST <ref type="figure" target="#fig_0">(Fig. 2,  Supp. Fig. S4</ref>) and Fashion-MNIST (Supp. <ref type="figure">Fig. S5</ref>).</p><p>The experiment shows that the SOM in our architecture improves the clustering (SOM-VAE vs. VQ-VAE) and that the VAE does so as well (SOM-VAE vs. GB-SOM). Both parts of the model therefore seem to be beneficial for our task. It also becomes apparent that our reconstruction loss term on z e works better in practice than the gradient copying trick from the VQ-VAE (SOM-VAE vs. gradcopy), due to the reasons described in Section 2.2. If one removes the z e reconstruction loss and does not copy the gradients, the encoder network does not receive any gradient information any more and the learning fails completely (no_grads). Another interesting observation is that stochastically optimizing our SOM loss using Adam (Kingma and Ba, 2014) seems to discover a more performant solution than the classical SOM algorithm (GB-SOM vs. minisom). This could be due to the dependency of the step size on the distance between embeddings and encodings, as described in Section 2.1. Since k-means seems to be the strongest competitor, we are including it as a reference baseline in the following experiments as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MARKOV TRANSITION MODEL ON THE DISCRETE REPRESENTATIONS</head><p>In order to test the probabilistic model in our architecture and its effect on the clustering, we generated synthetic time series data sets of (Fashion-)MNIST images being linearly interpolated into each other. Each time series consists of 64 frames, starting with one image from (Fashion-)MNIST and smoothly changing sequentially into four other images over the length of the time course.</p><p>After training the model on these data, we constructed the maximum likelihood estimate (MLE) for the Markov model's transition matrix by fixing all the weights in the SOM-VAE and making another pass over the training set, counting all the observed transitions. This MLE transition matrix reaches a negative log likelihood of 0.24, while our transition matrix, which is learned concurrently with the architecture, yields 0.25. Our model is therefore on par with the MLE solution.</p><p>Comparing these results with the clustering performance on the standard MNIST and Fashion-MNIST test sets, we observe that the performance in terms of NMI is not impaired by the inclusion of the probabilistic model into the architecture (Tab. 2). On the contrary, the probabilistic model even slightly increases the performance on Fashion-MNIST. Note that we are using 64 embeddings in this experiment instead of 16, leading to a higher clustering performance in terms of purity, but a slightly lower performance in terms of NMI compared to Table 1. This shows again that the measure of purity has to be interpreted with care when comparing <ref type="table">Table 2</ref>: Performance comparison of the SOM-VAE with and without latent Markov model (SOM-VAEprob) against k-means in terms of purity and normalized mutual information on different benchmark data sets. The values are the means of 10 runs and the respective standard errors. Each method is used to fit 64 embeddings/clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST</head><p>Fashion-MNIST</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Purity NMI Purity NMI k-means 0.791 ± 0.005 0.537 ± 0.001 0.703 ± 0.002 0.492 ± 0.001 SOM-VAE 0.868 ± 0.003 0.595 ± 0.002 0.739 ± 0.002 0.520 ± 0.002 SOM-VAE-prob 0.858 ± 0.004 0.596 ± 0.001 0.724 ± 0.003 0.525 ± 0.002 different experimental setups and that therefore the normalized mutual information should be preferred to make quantitative arguments.</p><p>This experiment shows that we can indeed fit a valid probabilistic transition model concurrently with the SOM-VAE training, while at the same time not hurting the clustering performance. It also shows that for certain types of data the clustering performance can even be improved by the probabilistic model (see Sec. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">INTERPRETABLE REPRESENTATIONS OF CHAOTIC TIME SERIES</head><p>In order to assess whether our model can learn an interpretable representation of more realistic chaotic time series, we train it on synthetic trajectories simulated from the famous Lorenz system (Lorenz, 1963). The Lorenz system is a good example for this assessment, since it offers two well defined macro-states (given by the attractor basins) which are occluded by some chaotic noise in the form of periodic fluctuations around the attractors. A good interpretable representation should therefore learn to largely ignore the noise and model the changes between attractor basins. For a review of the Lorenz system and details about the simulations and the performance measure, we refer to the appendix (Sec. D.2).</p><p>In order to compare the interpretability of the learned representations, we computed entropy distributions over simulated subtrajectories in the real system space, the attractor assignment space and the representation spaces for k-means and our model. The computed entropy distributions over all subtrajectories in the test set are depicted in <ref type="figure" target="#fig_1">Figure 3</ref>. The experiment shows that the SOM-VAE representations <ref type="figure" target="#fig_1">(Fig. 3d</ref>) are much closer in entropy to the groundtruth attractor basin assignments <ref type="figure" target="#fig_1">(Fig. 3c</ref>) than the k-means representations <ref type="figure" target="#fig_1">(Fig. 3e</ref>). For most of the subtrajectories without attractor basin change they assign a very low entropy, effectively ignoring the noise, while the k-means representations partially assign very high entropies to those trajectories. In total, the k-means representations' entropy distribution is similar to the entropy distribution in the noisy system space <ref type="figure" target="#fig_1">(Fig. 3b</ref>). The representations learned by the SOM-VAE are therefore more interpretable than the k-means representations with regard to this interpretability measure. As could be expected from these figures, the SOM-VAE representation is also superior to the k-means one in terms of purity with respect to the attractor assignment (0.979 vs. 0.956) as well as NMI (0.577 vs. 0.249).</p><p>Finally, we use the learned probabilistic model on our SOM-VAE representations to sample new latent system trajectories and compute their entropies. The distribution looks qualitatively similar to the one over real <ref type="table">Table 3</ref>: Performance comparison of our method with and without probabilistic model (SOM-VAE-prob and SOM-VAE) against k-means in terms of normalized mutual information on a challenging unsupervised prediction task on real eICU data. The dynamic endpoints are the maximum of the physiology score within the next 6, 12 or 24 hours (physiology_6_hours, physiology_12_hours, physiology_24_hours). The values are the means of 10 runs and the respective standard errors. Each method is used to fit 64 embeddings/clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method physiology_6_hours physiology_12_hours physiology_24_hours</head><p>k-means 0.0411 ± 0.0007 0.0384 ± 0.0006 0.0366 ± 0.0005 SOM-VAE 0.0407 ± 0.0005 0.0376 ± 0.0004 0.0354 ± 0.0004 SOM-VAE-prob 0.0474 ± 0.0006 0.0444 ± 0.0006 0.0421 ± 0.0005 ). It can be seen that our model is the only one that learns a topologically interpretable structure. trajectories <ref type="figure" target="#fig_1">(Fig. 3</ref>), but our model slightly overestimates the attractor basin change probabilities, leading to a heavier tail of the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">LEARNING REPRESENTATIONS OF REAL MEDICAL TIME SERIES</head><p>In order to demonstrate interpretable representation learning on a complex real world task, we trained our model on vital sign time series measurements of intensive care unit (ICU) patients. We analyze the performance of the resulting clustering w.r.t. the patients' future physiology states in <ref type="table">Table 3</ref>. This can be seen as a way to assess the representations' informativeness for a downstream prediction task. For details regarding the data selection and processing, we refer to the appendix (Sec. D.3).</p><p>Our full model (including the latent Markov model) performs best on the given tasks, i.e. better than k-means and also better than the SOM-VAE without probabilistic model. This could be due to the noisiness of the medical data and the probabilistic model's smoothing tendency (see Sec. 2.3).</p><p>In order to qualitatively assess the interpretability of the probabilistic SOM-VAE, we analyzed the average future physiology score per cluster <ref type="figure" target="#fig_2">(Fig. 4)</ref>. Our model exhibits clusters where higher scores are enriched compared to the background level. Moreover, these clusters form compact structures, facilitating interpretability. We do not observe such interpretable structures in the other methods. For full results on acute physiology scores, an analogue experiment showing the future mortality risk associated with different regions of the map, and an analysis of enrichment for particular physiological abnormalities, we refer to the appendix (Sec. D.4).</p><p>As an illustrative example for data visualization using our method, we show the trajectories of two patients that start in the same state <ref type="figure" target="#fig_2">(Fig. 4d)</ref>. The trajectories are plotted in the representation space of the probabilistic SOM-VAE and should thus be compared to the visualization in <ref type="figure" target="#fig_2">Figure 4c</ref>. One patient (green) stays in the regions of the map with low average physiology score and eventually gets discharged from the hospital healthily. The other one (red) moves into map regions with high average physiology score and ultimately dies. Such knowledge could be helpful for doctors, who could determine the risk of a patient for certain deterioration scenarios from a glance at their trajectory in the SOM-VAE representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>The SOM-VAE can recover topologically interpretable state representations on time series and static data. It provides an improvement to standard methods in terms of clustering performance and offers a way to learn discrete two-dimensional representations of the data manifold in concurrence with the reconstruction task. It introduces a new way of overcoming the non-differentiability of the discrete representation assignment and contains a gradient-based variant of the traditional self-organizing map that is more performant than the original one. On a challenging real world medical data set, our model learns more informative representations with respect to medically relevant prediction targets than competitor methods. The learned representations can be visualized in an interpretable way and could be helpful for clinicians to understand patients' health states and trajectories more intuitively.</p><p>It will be interesting to see in future work whether the probabilistic component can be extended to not just improve the clustering and interpretability of the whole model, but also enable us to make predictions. Promising avenues in that direction could be to increase the complexity by applying a higher order Markov Model, a Hidden Markov Model or a Gaussian Process. Another fruitful avenue of research could be to find more theoretically principled ways to overcome the non-differentiability and compare them with the empirically motivated ones. Lastly, one could explore deviating from the original SOM idea of fixing a latent space structure, such as a 2D grid, and learn the neighborhood structure as a graph directly from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A SELF-ORGANIZING MAPS</head><p>The general idea of a self-organizing map (SOM) is to approximate a data manifold in a high-dimensional continuous space with a lower dimensional discrete one <ref type="bibr" target="#b21">(Kohonen, 1990)</ref>. It can therefore be seen as a nonlinear discrete dimensionality reduction. The mapping is achieved by a procedure in which this discrete representation (the map) is randomly embedded into the data space and then iteratively optimized to approach the data manifold more closely.</p><p>The map consists of k nodes V = {v 1 , . . . , v k }, where every node corresponds to an embedding in the data space e v ∈ R d and a representation in the lower-dimensional discrete space m v ∈ M , where usually M ⊂ N 2 . There are two different geometrical measures that have to be considered during training: the neighborhood function N (m u , mṽ) that is defined on the low-dimensional map space and the Euclidean distance D(e u , eṽ) = e u − eṽ 2 in the high-dimensional data space. The SOM optimization tries to induce a coupling between these two properties, such that the topological structure of the representation reflects the geometrical structure of the data.</p><p>Algorithm 1 Self-organizing map training</p><formula xml:id="formula_6">Require: data set D = {x 1 , . . . , x n | x i ∈ R d }, number of nodes k, neighborhood function N (·), step size η initialize set of k nodes V = {v 1 , . . . , v k } initialize embeddings e v ∈ R d ∀ v ∈ V while not converged do for all x i ∈ D do find the closest SOM nodeṽ := arg min v∈V x i − e v 2 update node embedding eṽ ← eṽ + η (x i − eṽ)</formula><p>for all u ∈ V \ṽ do update neighbor embedding e u ← e u + η N (mṽ, m u )(x i − e u ) end for end for end while</p><p>The SOM training procedure is described in Algorithm 1. During training on a data set D, a winner nodeṽ is chosen for every point x i according to the Euclidean distance of the point and the node's embedding in the data space. The embedding vector for the winner node is then updated by pulling it into the direction of the data point with some step size η. The embedding vectors of the other nodes are also updated -potentially with a smaller step size -depending on whether they are neighbors of the winner node in the map space M .</p><p>The neighborhood is defined by the neighborhood function N (m u , mṽ). There can be different design choices for the neighborhood function, e.g. rectangular grids, hexagonal grids or Gaussian neighborhoods. For simplicity and ease of visualization, we usually choose a two-dimensional rectangular grid neighborhood in this paper.</p><p>In this original formulation of the SOM training, the nodes are updated one by one with a fixed step size. In our model, however, we use a gradient-based optimization of their distances to the data points and update them in minibatches. This leads to larger step sizes when they are farther away from the data and smaller step sizes when they are close. Overall, our gradient-based SOM training seems to perform better than the original formulation (see Tab. 1).</p><p>It also becomes evident from this procedure that it will be very hard for the map to fit disjoint manifolds in the data space. Since the nodes of the SOM form a fully connected graph, they do not possess the ability to model spatial gaps in the data. We overcome this problem in our work by mapping the data manifold with a variational autoencoder into a lower-dimensional latent space. The VAE can then learn to close the aforementioned gaps and map the data onto a compact latent manifold, which can be more easily modeled with the SOM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION DETAILS</head><p>The hyperparameters of our model were optimized using Robust Bayesian Optimization with the packages sacred and labwatch <ref type="bibr" target="#b14">(Greff et al., 2017)</ref> for the parameter handling and RoBo  for the optimization, using the mean squared reconstruction error as the optimization criterion. Especially the weighting hyperparameters α, β, γ and τ (see Eq.</p><p>(1) and Eq. (2)) have to be tuned carefully, such that the different parts of the model converge at roughly the same rate. We found that 2000 steps of Bayesian optimization sufficed to yield a performant hyperparameter assignment.</p><p>Since our model defines a general framework, some competitor models can be seen as special cases of our model, where certain parts of the loss function are set to zero or parts of the architecture are omitted. We used the same hyperparameters for those models. For external competitor methods, we used the hyperparameters from the respective publications where applicable and otherwise the default parameters from their packages. The models were implemented in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> and optimized using Adam (Kingma and Ba, 2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C CLUSTERING PERFORMANCE MEASURES</head><p>Given that one of our most interesting tasks at hand is the clustering of data, we need some performance measures to objectively compare the quality of this clustering with other methods. The measures that we decided to use and that have been used extensively in the literature are purity and normalized mutual information (NMI) <ref type="bibr">(Manning et al., 2008)</ref>. We briefly review them in the following.</p><p>Let the set of ground truth classes in the data be C = {c 1 , c 2 , . . . , c J } and the set of clusters that result from the algorithm Ω = {ω 1 , ω 2 , . . . , ω K }. The purity π is then defined as π(C, Ω) = 1 N K k=1 max j |ω k ∩ c j | where N is the total number of data points. Intuitively, the purity is the accuracy of the classifier that assigns the most prominent class label in each cluster to all of its respective data points.</p><p>While the purity has a very simple interpretation, it also has some shortcomings. One can for instance easily observe that a clustering with K = N , i.e. one cluster for every single data point, will yield a purity of 1.0 but still probably not be very informative for most tasks. It would therefore be more sensible to have another measure that penalizes the number of clusters. The normalized mutual information is one such measure.</p><p>The NMI is defined as NMI(C, Ω) = 2 I(C,Ω) H(C)+H(Ω) where I(C, Ω) is the mutual information between C and Ω and H(·) is the Shannon information entropy. While the entropy of the classes is a data-dependent constant, the entropy of the clustering increases with the number of clusters. It can therefore be seen as a penalty term to regularize the trade-off between low intra-cluster variance and a small number of clusters. Both NMI and purity are normalized, i.e. take values in [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTAL DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 CLUSTERING ON MNIST AND FASHION-MNIST</head><p>Additionally to the results in <ref type="table" target="#tab_1">Table 1</ref>, we performed experiments to assess the influence of the number of clusters k on the clustering performance of our method. We chose different values for k between 4 and 64 and tested the clustering performance on MNIST and Fashion-MNIST (Tab. S1).</p><p>It can be seen that the purity increases monotonically with k, since it does not penalize larger numbers of clusters (see Sec. C). The NMI, however, includes an automatic penalty for misspecifying the model with too many clusters. It therefore increases first, but then decreases again for too large values of k. The optimal k according to the NMI seems to lie between 16 and 36.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 INTERPRETABLE REPRESENTATIONS OF CHAOTIC TIME SERIES</head><p>The Lorenz system is the system of coupled ordinary differential equations defined by 0.626 ± 0.006 0.554 ± 0.004 0.558 ± 0.007 0.560 ± 0.006 k = 16 0.721 ± 0.006 0.587 ± 0.003 0.684 ± 0.003 0.589 ± 0.003 k = 25 0.803 ± 0.003 0.613 ± 0.002 0.710 ± 0.003 0.572 ± 0.002 k = 36 0.850 ± 0.002 0.612 ± 0.001 0.732 ± 0.002 0.556 ± 0.002 k = 49 0.875 ± 0.002 0.608 ± 0.001 0.750 ± 0.002 0.545 ± 0.001 k = 64 0.894 ± 0.002 0.599 ± 0.001 0.758 ± 0.002 0.532 ± 0.001 with tuning parameters a, b and c. For parameter choices a = 10, b = 28 and c = 8 3 , the system shows chaotic behavior by forming a strange attractor <ref type="bibr" target="#b28">(Tucker, 1999)</ref> with the two attractor points being given by</p><formula xml:id="formula_7">dX dt = a(Y − X) dY dt = X(b − Z) − Y dZ dt = XY − cZ</formula><formula xml:id="formula_8">p 1,2 = [± c(b − 1), ± c(b − 1), b − 1] T .</formula><p>We simulated 100 trajectories of 10,000 time steps each from the chaotic system and trained the SOM-VAE as well as k-means on it with 64 clusters/embeddings respectively. The system chaotically switches back and forth between the two attractor basins. By computing the Euclidian distance between the current system state and each of the attractor points p 1,2 , we can identify the current attractor basin at each time point.</p><p>In order to assess the interpretability of the learned representations, we have to define an objective measure of interpretability. We define interpretability as the similarity between the representation and the system's ground truth macro-state. Since representations at single time points are meaningless with respect to this measure, we compare the evolution of representations and system state over time in terms of their entropy.</p><p>We divided the simulated trajectories from our test set into spans of 100 time steps each. For every subtrajectory, we computed the entropies of those subtrajectories in the real system space (macro-state and noise), the assigned attractor basin space (noise-free ground-truth macro-state), the SOM-VAE representation and the k-means representation. We also observed for every subtrajectory whether or not a change between attractor basins has taken place. Note that the attractor assignments and representations are discrete, while the real system space is continuous. In order to make the entropies comparable, we discretize the system space into unit hypercubes for the entropy computation. For a representation R with assignments R t at time t and starting time t start of the subtrajectory, the entropies are defined as</p><formula xml:id="formula_9">H (R, t start ) = H ({R t | t start ≤ t &lt; t start + 100})<label>(3)</label></formula><p>with H(·) being the Shannon information entropy of a discrete set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 LEARNING REPRESENTATIONS OF ACUTE PHYSIOLOGICAL STATES IN THE ICU</head><p>All experiments were performed on dynamic data extracted from the eICU Collaborative Research Database <ref type="bibr" target="#b12">(Goldberger et al., 2000)</ref>. Irregularly sampled time series data were extracted from the raw tables and then resampled to a regular time grid using a combination of forward filling and missing value imputation using global population statistics. We chose a grid interval of one hour to capture the rapid dynamics of patients in the ICU.</p><p>Each sample in the time-grid was then labeled using a dynamic variant of the APACHE score <ref type="bibr" target="#b20">(Knaus et al., 1985)</ref>, which is a proxy for the instantaneous physiological state of a patient in the ICU. Specifically, the variables MAP, Temperature, Respiratory rate, HCO3, Sodium, Potassium, and Creatinine were selected from the score definition, because they could be easily defined for each sample in the eICU time series. The value range of each variable was binned into ranges of normal and abnormal values, in line with the definition of the APACHE score, where a higher score for a variable is obtained for abnormally high or low values. The scores were then summed up, and we define the predictive score as the worst (highest) score in the next t hours, for t ∈ {6, 12, 24}. Patients are thus stratified by their expected pathology in the near future, which corresponds closely to how a physician would perceive the state of a patient. The training set consisted of 7000 unique patient stays, while the test set contained 3600 unique stays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 DETAILED ANALYSIS OF SOMVAEP R O B PATIENT STATES</head><p>As mentioned in the main text (see <ref type="figure" target="#fig_2">Fig 4c)</ref> the SOMVAEProb is able to uncover compact and interpretable structures in the latent space with respect to future physiology scores. In this section we show results for acute physiology scores in greater detail, analyze enrichment for future mortality risk, arguably the most important severity indicator in the ICU, and explore phenotypes for particular physiological abnormalities.  <ref type="figure">Figure S1</ref>: (a) shows the difference in distribution of the acute physiology score in the next 24 hours, between time-points assigned to the most abnormal cell in the SOMVAEprob map with coordinates [2,0] vs. a normal cell chosen from the middle of the map with coordinates <ref type="bibr">[4,</ref><ref type="bibr">3]</ref>. It is apparent that the distributions are largely disjoint, which means that the representation induced by SOMVAEprob clearly distinguishes these risk profiles. Statistical tests for difference in distribution and location parameter are highly significant at p-values of p ≤ 10 −3 , as we have validated using a 2-sample t-test and Kolmogorov-Smirnov test. In (b-c) the enrichment of the map for the mean acute physiology score in the next 6 and 12 hours is shown, for completeness. The enrichment patterns on the 3 maps, for the future horizons {6, 12, 24}, are almost identical, which provides empirical evidence for the temporal stability of the SOMVAEProb embedding.  <ref type="figure" target="#fig_0">Figure S2</ref>: (a) Dynamic mortality risk in the next 24 hours. (b) Short-term dynamic mortality risk in the next 6 hours. We observe that the left-edge and right-edge regions of the SOMVAEprob map which are enriched for higher acute physiology scores (see <ref type="figure" target="#fig_2">Fig 4c)</ref> also exhibit elevated mortality rates over the baseline. Interestingly, according to future mortality risk, which is an important severity indicator, patients on the left-edge are significantly more sick on average than those on the right edge, which is less visible from the enrichment for acute physiology scores.  <ref type="figure" target="#fig_1">Figure S3</ref>: (a) Prevalence of abnormally low sodium lab value in the next 24 hours, (b-d) Prevalence of abnormally high potassium/creatinine/HCO3 lab values in the next 24 hours. Each sub-figure illustrates the enrichment of a distinct phenotype on the SOMVAEprob map. Low sodium and high potassium states are enriched near the left edge, and near the right edge, respectively, which could represent sub-types of the high-risk phenotype found in these regions (compare <ref type="figure" target="#fig_2">Fig 4c for</ref> the distribution of the acute physiology score). Elevated creatinine is a trait that occurs in both these regions. A compact structure associated with elevated HCO3 can be found in the center of the map, which could represent a distinct phenotype with lower mortality risk in our cohort. In all phenotypes, the tendency of SOMVAEprob to recover compact structures is exemplified. <ref type="figure" target="#fig_2">Figure S4</ref>: Images generated from the SOM-VAE's latent space with 512 embeddings trained on MNIST. It yields an interpretable discrete two-dimensional representation of the data manifold in the higher-dimensional latent space. <ref type="figure">Figure S5</ref>: Images generated from the SOM-VAE's latent space with 512 embeddings trained on Fashion-MNIST. It yields an interpretable discrete two-dimensional representation of the data manifold in the higherdimensional latent space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Images generated from a section of the SOM-VAE's latent space with 512 embeddings trained on MNIST. It yields a discrete two-dimensional representation of the data manifold in the higher-dimensional latent space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Histograms of entropy distributions (entropy on the x-axes) over all Lorenz attractor subtrajectories [a] of 100 time steps length in our test set. Subtrajectories without a change in attractor basin are colored in blue, the ones where a change has taken place in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of the patient state representations learned by different models. The clusters are colored by degree of patient abnormality as measured by a variant of the APACHE physiology score (more yellow means "less healthy"). White squares correspond to unused clusters, i.e. clusters that contain less than 0.1 percent of the data points. Subfigure (d) shows two patient trajectories in the SOM-VAE-prob representation over their respective whole stays in the ICU. The dots mark the ICU admission, the stars the discharge from the ICU (cured [green] or dead[red]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FULL</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>DYNAMIC</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of our method and some baselines in terms of purity and normalized mutual information on different benchmark data sets. The methods marked with an asterisk are variants of our proposed method. The values are the means of 10 runs and the respective standard errors. Each method was used to fit 16 embeddings/clusters.</figDesc><table><row><cell></cell><cell>MNIST</cell><cell></cell><cell cols="2">Fashion-MNIST</cell></row><row><cell>Method</cell><cell>Purity</cell><cell>NMI</cell><cell>Purity</cell><cell>NMI</cell></row><row><cell>k-means</cell><cell cols="4">0.690 ± 0.000 0.541 ± 0.001 0.654 ± 0.001 0.545 ± 0.000</cell></row><row><cell>minisom</cell><cell cols="4">0.406 ± 0.006 0.342 ± 0.012 0.413 ± 0.006 0.475 ± 0.002</cell></row><row><cell>GB-SOM</cell><cell cols="4">0.653 ± 0.007 0.519 ± 0.005 0.606 ± 0.006 0.514 ± 0.004</cell></row><row><cell>VQ-VAE</cell><cell cols="4">0.538 ± 0.067 0.409 ± 0.065 0.611 ± 0.006 0.517 ± 0.002</cell></row><row><cell>no_grads*</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table S1 :</head><label>S1</label><figDesc>Performance comparison of our method with different numbers of clusters in terms of purity and normalized mutual information on different benchmark data sets. The values are the means of 10 runs and the respective standard errors.</figDesc><table><row><cell></cell><cell>MNIST</cell><cell></cell><cell cols="2">Fashion-MNIST</cell></row><row><cell>Number of clusters</cell><cell>Purity</cell><cell>NMI</cell><cell>Purity</cell><cell>NMI</cell></row><row><cell>k = 4</cell><cell cols="4">0.364 ± 0.009 0.378 ± 0.018 0.359 ± 0.005 0.431 ± 0.008</cell></row><row><cell>k = 9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/ratschlab/SOM-VAE.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>FL is supported by the Max Planck/ETH Center for Learning Systems. MH is supported by the Grant No. 205321_176005 "Novel Machine Learning Approaches for Data from the Intensive Care Unit" of the Swiss National Science Foundation (to GR). VF, FL, MH and HS are partially supported by ETH core funding (to GR). We thank Natalia Marciniak for her administrative efforts; Marc Zimmermann for technical support; Gideon Dresdner, Stephanie Hyland, Viktor Gal, Maja Rudolph and Claire Vernade for helpful discussions; and Ron Swanson for his inspirational attitude.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawar</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07648</idno>
		<title level="m">Clustering with deep learning: Taxonomy and new methods</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluation of icu risk models adapted for use as continuous markers of severity of illness throughout the icu stay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Badawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkan</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><forename type="middle">J</forename><surname>Amelung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical care medicine</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="361" to="367" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identification and control of dynamical systems using the self-organizing map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guilherme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aluizio Fr</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Araujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1244" to="1259" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bayesian gaussian processes for identifying the deteriorating patient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><forename type="middle">Wright</forename><surname>Colopy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clifton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5311" to="5314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The growing hierarchical self-organizing map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dittenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Merkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE-INNS-ENNS International Joint Conference on</title>
		<meeting>the IEEE-INNS-ENNS International Joint Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="15" to="19" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Esmaeili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Willem</forename><surname>Van De Meent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02086</idno>
		<title level="m">Hierarchical disentangled representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Real-valued (medical) time series generation with recurrent conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristóbal</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rätsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02633</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Modular network som (mnsom): From vector space to function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuo</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiro</forename><surname>Tokunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syozo</forename><surname>Yasui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1581" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plamen</forename><forename type="middle">Ch</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">B</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Kang</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Eugene</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="215" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The sacred infrastructure for computational research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chovanec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python in Science Conferences-SciPy Conferences</title>
		<meeting>the Python in Science Conferences-SciPy Conferences</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multitask learning and benchmarking with clinical time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Galstyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07771</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robo: A flexible and robust bayesian optimization framework in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Numair</forename><surname>Mansur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Bayesian Optimization Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Apache ii: a severity of disease classification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">A</forename><surname>William A Knaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">E</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical care medicine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="818" to="829" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The self-organizing map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teuvo</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1464" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Disease-atlas: Navigating disease trajectories using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Schaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="137" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Interpreting selforganizing maps through space-time data models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyan</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">E</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lennard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Hegerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Hewitson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1194" to="1216" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Disease trajectory maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schulam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4709" to="4717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Identifying similar patients using self-organising maps: a case study on type-1 diabetes self-care survey responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Tirunagari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Windridge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.06316</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The lorenz attractor exists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warwick</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus de l&apos;Académie des Sciences-Series I-Mathematics</title>
		<imprint>
			<biblScope unit="volume">328</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1197" to="1202" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">MiniSom: a minimalistic implementation of the Self Organizing Maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Vettigli</surname></persName>
		</author>
		<ptr target="https://github.com/JustGlowing/minisom" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep unsupervised clustering using mixture of autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Balzano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07788</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
