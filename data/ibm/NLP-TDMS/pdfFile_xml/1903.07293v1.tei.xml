<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heterogeneous Graph Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
							<email>xiaowang@bupt.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houye</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Wang</surname></persName>
							<email>wangbai@bupt.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
							<email>cuip@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
							<email>psyu@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
							<email>yanfang.ye@mail.wvu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houye</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfang</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">West Virginia University WV</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Heterogeneous Graph Attention Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>ACM Reference Format: Ye. 2019. Heterogeneous Graph Attention Network. In Proceedings of WWW 2019, Jennifer B. Sartor, Theo D&apos;Hondt, and Wolfgang De Meuter (Eds.). ACM, New York, NY, USA, Article 4, 11 pages. https://doi.org/10.475/123_4</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Social Network</term>
					<term>Neural Network</term>
					<term>Graph Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its metapath based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>web. Graph neural network (GNN), as a powerful deep representation learning method for such graph data, has shown superior performance on network analysis and aroused considerable research interest. For example, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref> leverage deep neural network to learn node representations based on node features and the graph structure. Some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref> propose the graph convolutional networks by generalizing the convolutional operation to graph. A recent research trend in deep learning is the attention mechanism, which deals with variable sized data and encourages the model to focus on the most salient parts of data. It has demonstrated the effectiveness in deep neural network framework and is widely applied to various applications, such as text analysis <ref type="bibr" target="#b0">[1]</ref>, knowledge graph <ref type="bibr" target="#b24">[25]</ref> and image processing <ref type="bibr" target="#b37">[38]</ref>. Graph Attention Network (GAT) <ref type="bibr" target="#b34">[35]</ref>, a novel convolution-style graph neural network, leverages attention mechanism for the homogeneous graph which includes only one type of nodes or links.</p><p>Despite the success of attention mechanism in deep learning, it has not been considered in the graph neural network framework for heterogeneous graph. As a matter of fact, the real-world graph usually comes with multi-types of nodes and edges, also widely known as heterogeneous information network (HIN) <ref type="bibr" target="#b27">[28]</ref>. For convenience, we uniformly call it heterogeneous graph in this paper. Because the heterogeneous graph contains more comprehensive information and rich semantics, it has been widely used in many data mining tasks. Meta-path <ref type="bibr" target="#b31">[32]</ref>, a composite relation connecting two objects, is a widely used structure to capture the semantics. Taking the movie data IMDB 1 shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a) as an example, it contains three types of nodes include movie, actor and director. A relation between two movies can be revealed by meta-path Movie-Actor-Movie (MAM) which describes the co-actor relation, while Movie-Director-Movie (MDM) means that they are directed by the same director. As can be seen, depending on the meta-paths, the relation between nodes in the heterogeneous graph can have different semantics. Due to the complexity of heterogeneous graph, traditional graph neural network cannot be directly applied to heterogeneous graph.</p><p>Based on the above analysis, when designing graph neural network architecture with attention mechanism for heterogeneous graph, we need to address the following new requirements.</p><p>Heterogeneity of graph. The heterogeneity is an intrinsic property of heterogeneous graph, i.e., various types of nodes and edges. For example, different types of nodes have different traits and their features may fall in different feature space. Still taking IMDB as an example, the feature of an actor may involve in sex, age and nationality. On the other hand, the feature of movie may involve in plot and actors. How to handle such complex structural information and preserve the diverse feature information simultaneously is an urgent problem that needs to be solved.</p><p>Semantic-level attention. Different meaningful and complex semantic information are involved in heterogeneous graph, which are usually reflected by meta-paths <ref type="bibr" target="#b31">[32]</ref>. Different meta-paths in heterogeneous graph may extract diverse semantic information. How to select the most meaningful meta-paths and fuse the semantic information for the specific task is an open problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>. Semantic-level attention aims to learn the importance of each metapath and assign proper weights to them. Still taking IMDB as an example, The Terminator can either connect to The Terminator 2 via Movie-Actor-Movie (both starred by Schwarzenegger) or connect to Birdy via Movie-Year-Movie (both shot in 1984). However, when identifying the genre of the movie The Terminator, MAM usually plays more important role, rather than MYM. Therefore, treating different meta-paths equally is unpractical and will weaken the semantic information provided by some useful meta-paths.</p><p>Node-level attention. In a heterogeneous graph, nodes can be connected via various types of relation, e.g., meta-path. Given a meta-path, each node has lots of meta-path based neighbors. How to distinguish the subtle difference of there neighbors and select some informative neighors is required. For each node, node-level attention aims to learn the importance of meta-path based neighbors and assign different attention values to them. Still taking IMDB as an example, when using the meta-path Movie-Director-Moive (the movies are with the same director), The Terminator will connect to Titanic and The Terminator 2 via director James Cameron. To better identify the genre of The Terminator as sci-fi movie, the model should pay more attention to The Terminator 2, rather than Titanic. Therefore, how to design a model which can discover the subtle differences of neighbors and learn their weights properly will be desired.</p><p>In this paper, we propose a novel Heterogeneous graph Attention Network, named HAN, which considers both of node-level and semantic-level attentions. In particular, given the node features as input, we use the type-specific transformation matrix to project different types of node features into the same space. Then the node-level attention is able to learn the attention values between the nodes and their meta-path based neighbors, while the semantic-level attention aims to learn the attention values of different meta-paths for the specific task in the heterogeneous graph. Based on the learned attention values in terms of the two levels, our model can get the optimal combination of neighbors and multiple meta-paths in a hierarchical manner, which enables the learned node embeddings to better capture the complex structure and rich semantic information in a heterogeneous graph. After that, the overall model can be optimized via backpropagation in an end-to-end manner.</p><p>The contributions of our work are summarized as follows:</p><p>• To our best knowledge, this is the first attempt to study the heterogeneous graph neural network based on attention mechanism. Our work enables the graph neural network to be directly applied to the heterogeneous graph, and further facilitates the heterogeneous graph based applications.</p><p>• We propose a novel heterogeneous graph attention network (HAN) which includes both of the node-level and semantic-level attentions. Benefitting from such hierarchical attentions, the proposed HAN can take the importance of nodes and meta-paths into consideration simultaneously. Moreover, our model is high efficiency, with the linear complexity with respect to the number of meta-path based node pairs, which can be applied to large-scale heterogeneous graph.</p><p>• We conduct extensive experiments to evaluate the performance of the proposed model. The results show the superiority of the proposed model by comparing with the state-of-the-art models. More importantly, by analysing the hierarchical attention mechanism, the proposed HAN demonstrates its potentially good interpretability for heterogeneous graph analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Graph Neural Network</head><p>Graph neural networks (GNNs) which aim to extend the deep neural network to deal with arbitrary graph-structured data are introduced in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. Yujia Li et al. <ref type="bibr" target="#b19">[20]</ref> proposes a propagation model which can incorporate gated recurrent units to propagate information across all nodes. Recently, there is a surge of generalizing convolutional operation on the graph-structured data. The graph convolutional neural work generally falls into two categories, namely spectral domain and non-spectral domain. On one hand, spectral approaches work with a spectral representation of the graphs. Joan Bruna et al. <ref type="bibr" target="#b1">[2]</ref> extends convolution to general graphs by finding the corresponding Fourier basis. Michaël et al. <ref type="bibr" target="#b5">[6]</ref> utilizes K-order Chebyshev polynomials to approximate smooth filters in the spectral domain. Kipf et al. <ref type="bibr" target="#b17">[18]</ref> proposes a spectral approach, named Graph Convolutional Network, which designs a graph convolutional network via a localized first-order approximation of spectral graph convolutions. On the other hand, we also have non-spectral approaches, which define convolutions directly on the graph, operating on groups of spatially close neighbors. Hamilton et al. <ref type="bibr" target="#b13">[14]</ref> introduces GraphSAGE which performs a neural network based aggregator over a fixed size node neighbor. It can learn a function that generates embeddings by aggregating features from a nodeâȂŹs local neighborhood. Attention mechanisms, e.g., self-attention <ref type="bibr" target="#b33">[34]</ref> and soft-attention <ref type="bibr" target="#b0">[1]</ref>, have become one of the most influential mechanisms in deep learning. Some previous works introduce the attention mechanism for graph based applications, e.g., the recommendation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Inspired by attention mechanism, Graph Attention Network <ref type="bibr" target="#b34">[35]</ref> is proposed to learn the importance between nodes and its neighbors and fuse the neighbors to perform node classification. However, the above graph neural network cannot deal with various types of nodes and edges and can only be applied to the homogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Embedding</head><p>Network embedding, i.e., network representation learning (NRL), is proposed to embed network into a low dimensional space while preserving the network structure and property so that the learned embeddings can be applied to the downstream network tasks. For example, the random walk based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>, the deep neural network based methods <ref type="bibr" target="#b35">[36]</ref>, the matrix factorization based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>, and others, e.g., LINE <ref type="bibr" target="#b32">[33]</ref>. However, all these algorithms are proposed for the homogeneous graphs. Some elaborate reviews can be found in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Heterogeneous graph embedding mainly focuses on preserving the meta-path based structural information. ESim <ref type="bibr" target="#b25">[26]</ref> accepts userdefined meta-paths as guidance to learn vertex vectors in a userpreferred embedding space for similarity search. Even through ESim can utilize multiple meta-paths, it cannot learn the importance of meta-paths. To achieve the best performance, ESim needs to conduct grid search to find the optimal weights of hmeta-paths. It is pretty hard to find the optimal combination for specific task. Meta-path2vec <ref type="bibr" target="#b6">[7]</ref> designs a meta-path based random walk and utilizes skip-gram to perform heterogeneous graph embedding. However, metapath2vec can only utilize one meta-path and may ignore some useful information. Similar to metapath2vec, HERec <ref type="bibr" target="#b26">[27]</ref> proposes a type constraint strategy to filter the node sequence and capture the complex semantics reflected in heterogeneous graph. HIN2Vec <ref type="bibr" target="#b8">[9]</ref> carries out multiple prediction training tasks which learn the latent vectors of nodes and meta-paths simultaneously. Chen et al. <ref type="bibr" target="#b2">[3]</ref> proposes a projected metric embedding model, named PME, which can preserve node proximities via Euclidian Distance. PME projects different types of node into the same relation space and conducts heterogeneous link prediction. To study the problem of comprehensive describe heterogeneous graph, Chen et al. <ref type="bibr" target="#b28">[29]</ref> proposes HEER which can embed heterogeneous graph via edge representations. Fan et al. <ref type="bibr" target="#b7">[8]</ref> proposes a embedding model metagraph2vec, where both the structures and semantics are maximally preserved for malware detection. Sun et al. <ref type="bibr" target="#b29">[30]</ref> proposes meta-graph-based network embedding models, which simultaneously considers the hidden relations of all meta information of a meta-graph. In summary, all these aforementioned algorithms do not consider the attention mechanism in heterogeneous graph representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>A heterogeneous graph is a special kind of information network, which contains either multiple types of objects or multiple types of links.</p><p>Definition 3.1. Heterogeneous Graph <ref type="bibr" target="#b30">[31]</ref>. A heterogeneous graph, denoted as G = (V, E), consists of an object set V and a link set E. A heterogeneous graph is also associated with a node type mapping function ϕ : V → A and a link type mapping function ψ : E → R. A and R denote the sets of predefined object types and link types, where |A| + |R| &gt; 2.</p><p>Example. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), we construct a heterogeneous graph to model the IMDB. It consists of multiple types of objects ( Actor (A), Movie (M), Director (D)) and relations (shoot relation between movies and directors, role-play relation between actors and movies).</p><p>In heterogeneous graph, two objects can be connected via different semantic paths, which are called meta-paths.</p><formula xml:id="formula_0">Definition 3.2. Meta-path [32]. A meta-path Φ is defined as a path in the form of A 1 R 1 − − → A 2 R 2 − − → · · · R l − − → A l +1 (abbreviated as A 1 A 2 · · · A l +1 ), which describes a composite relation R = R 1 • R 2 • · · · • R l between objects A 1 and A l +1 , where • denotes the composition operator on relations.</formula><p>Example. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), two movies can be connected via multiple meta-paths, e.g., Movie-Actor-Movie (MAM) and Movie-Director-Movie (MDM). Different meta-paths always reveal different semantics. For example, the MAM means the coactor relation, while Movie-Director-Movie (MDM) means they are directed by the same director.</p><p>Given a meta-path Φ, there exists a set of meta-path based neighbors of each node which can reveal diverse structure information and rich semantics in a heterogeneous graph. Definition 3.3. Meta-path based Neighbors. Givien a node i and a meta-path Φ in a heterogeneous graph, the meta-path based neighbors N Φ i of node i are defined as the set of nodes which connect with node i via meta-path Φ. Note that the node's neighbors includes itself.</p><p>Example. Taking <ref type="figure" target="#fig_0">Figure 1</ref>(d) as an example, given the meta-path Movie-Actor-Movie, the meta-path based neighbors of m 1 includes m1 (itself), m 2 and m 3 . Similarly, the neighbors of m 1 based on meta-path Movie-Director-Movie includes m 1 and m 2 . Obviously, meta-path based neighbors can exploit different aspects of structure information in heterogeneous graph. We can get meta-path based neighbors by the multiplication of a sequences of adjacency matrices.</p><p>Graph neural network has been proposed to deal with arbitrary graph-structured data. However, all of them are designed for homogeneous network <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref>. Since meta-path and meta-path based neighbors are two fundamental structures in a heterogeneous graph, next, we will present a novel graph neural network for heterogeneous graph data, which is able to exploit the subtle difference of nodes and meta-paths. The notations we will use throughout the article are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PROPOSED MODEL</head><p>In this section, we propose a novel semi-supervised graph neural network for heterogeneous graph. Our model follows a hierarchical attention structure: node-level attention → semantic-level attention. <ref type="figure" target="#fig_1">Figure 2</ref> presents the whole framework of HAN. First, we propose a node-level attention to learn the weight of meta-path based neighbors and aggregate them to get the semantic-specific node embedding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>Explanation</p><formula xml:id="formula_1">Φ Meta-path h Initial node feature M ϕ Type-specific transformation matrix h ′ Projected node feature e Φ i j Importance of meta-path based node pair (i,j) a Φ Node-level attention vector for meta-path Φ α Φ i j Weight of meta-path based node pair (i,j) N Φ Meta-path based neighbors Z Φ Semantic-specific node embedding q Semantic-level attention vector w Φ Importance of meta-path Φ β Φ Weight of meta-path Φ Z</formula><p>The final embedding After that, HAN can tell the difference of meta-paths via semanticlevel attention and get the optimal weighted combination of the semantic-specific node embedding for the specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Node-level Attention</head><p>Before aggregating the information from meta-path neighbors for each node, we should notice that the meta-path based neighbors of each node play a different role and show different importance in learning node embedding for the specific task. Here we introduce node-level attention can learn the importance of meta-path based neighbors for each node in a heterogeneous graph and aggregate the representation of these meaningful neighbors to form a node embedding.</p><p>Due to the heterogeneity of nodes, different types of nodes have different feature spaces. Therefore, for each type of nodes (e.g.,node with type ϕ i ), we design the type-specific transformation matrix M ϕ i to project the features of different types of nodes into the same feature space. Unlike <ref type="bibr" target="#b12">[13]</ref>, the type-specific transformation matrix is based on node-type rather than edge-type. The projection process can be shown as follows:</p><formula xml:id="formula_2">h ′ i = M ϕ i · h i ,<label>(1)</label></formula><p>where h i and h ′ i are the original and projected feature of node i, respectively. By type-specific projection operation, the node-level attention can handle arbitrary types of nodes.</p><p>After that, we leverage self-attention <ref type="bibr" target="#b33">[34]</ref> to learn the weight among various kinds of nodes. Given a node pair (i, j) which are connected via meta-path Φ, the node-level attention e Φ i j can learn the importance e Φ i j which means how important node j will be for node i. The importance of meta-path based node pair (i, j) can be formulated as follows:</p><formula xml:id="formula_3">e Φ i j = att node (h ′ i , h ′ j ; Φ).<label>(2)</label></formula><p>Here att node denotes the deep neural network which performs the node-level attention. Given meta-path Φ, att node is shared for all meta-path based node pairs. It is because there are some similar connection patterns under one meta-path. The above Eq. <ref type="formula" target="#formula_3">(2)</ref> shows that given meta-path Φ, the weight of meta-path based node pair (i, j) depends on their features. Please note that, e Φ i j is asymmetric, i.e., the importance of node i to node j and the importance of node j to node i can be quite difference. It shows node-level attention can preserve the asymmetry which is a critical property of heterogenous graph.</p><p>Then we inject the structural information into the model via masked attention which means we only calculate the e Φ i j for nodes j ∈ N Φ i , where N Φ i denotes the meta-path based neighbors of node i (include itself). After obtaining the importance between meta-path based node pairs, we normalize them to get the weight coefficient α Φ i j via softmax function:</p><formula xml:id="formula_4">α Φ i j = so f tmax j (e Φ i j ) = exp σ (a T Φ · [h ′ i ∥h ′ j ]) k ∈N Φ i exp σ (a T Φ · [h ′ i ∥h ′ k ]) ,<label>(3)</label></formula><p>where σ denotes the activation function, ∥ denotes the concatenate operation and a Φ is the node-level attention vector for meta-path Φ.</p><p>As we can see from Eq. (3), the weight coefficient of (i, j) depends on their features. Also please note that the weight coefficient α Φ i j is asymmetric which means they make different contribution to each other. Not only because the concatenate order in the numerator, but also because they have different neighbors so the normalize term (denominator) will be quite difference.</p><p>Then, the meta-path based embedding of node i can be aggregated by the neighbor's projected features with the corresponding coefficients as follows:</p><formula xml:id="formula_5">z Φ i = σ j ∈N Φ i α Φ i j · h ′ j .<label>(4)</label></formula><p>where z Φ i is the learned embedding of node i for the meta-path Φ. To better understand the aggregating process of node-level, we also give a brief explanation in <ref type="figure" target="#fig_3">Figure 3</ref>   aggregated by its neighors. Since the attention weight α Φ i j is generated for single meta-path, it is semantic-specific and able to caputre one kind of semantic information.</p><p>Since heterogeneous graph present the property of scale free, the variance of graph data is quite high. To tackle the above challenge, we extend node-level attention to multihead attention so that the training process is more stable. Specifically, we repeat the nodelevel attention for K times and concatenate the learned embeddings as the semantic-specific embedding:</p><formula xml:id="formula_6">z Φ i = K ∥ k =1 σ j ∈N Φ i α Φ i j · h ′ j .<label>(5)</label></formula><p>Given the meta-path set {Φ 0 , Φ 1 , . . . , Φ P }, after feeding node features into node-level attention, we can obtain P groups of semanticspecific node embeddings, denoted as Z Φ 0 , Z Φ 1 , . . . , Z Φ P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic-level Attention</head><p>Generally, every node in a heterogeneous graph contains multiple types of semantic information and semantic-specific node embedding can only reflect node from one aspect. To learn a more comprehensive node embedding, we need to fuse multiple semantics which can be revealed by meta-paths. To address the challenge of meta-path selection and semantic fusion in a heterogeneous graph, we propose a novel semantic-level attention to automatically learn the importance of different meta-paths and fuse them for the specific task. Taking P groups of semantic-specific node embeddings learned from node-level attention as input, the learned weights of each meta-path (β Φ 0 , β Φ 1 , . . . , β Φ P ) can be shown as follows:</p><formula xml:id="formula_7">(β Φ 0 , β Φ 1 , . . . , β Φ P ) = att sem (Z Φ 0 , Z Φ 1 , . . . , Z Φ P ).<label>(6)</label></formula><p>Here att sem denotes the deep neural network which performs the semantic-level attention. It shows that the semantic-level attention can capture various types of semantic information behind a heterogeneous graph.</p><p>To learn the importance of each meta-path, we first transform semantic-specific embedding through a nonlinear transformation (e.g., one-layer MLP). Then we measure the importance of the semantic-specific embedding as the similarity of transformed embedding with a semantic-level attention vector q. Furthermore, we average the importance of all the semantic-specific node embedding which can be explained as the importance of each meta-path. The importance of each meta-path, denoted as w Φ i , is shown as follows:</p><formula xml:id="formula_8">w Φ i = 1 |V | i ∈V q T · tanh(W · z Φ i + b),<label>(7)</label></formula><p>where W is the weight matrix, b is the bias vector, q is the semanticlevel attention vector. Note that for the meaningful comparation, all above parameters are shared for all meta-paths and semantic-specific embedding. After obtaining the importance of each meta-path, we normalize them via softmax function. The weight of meta-path Φ i , denoted as β Φ i , can be obtained by normalizing the above importance of all meta-paths using softmax function,</p><formula xml:id="formula_9">β Φ i = exp(w Φ i ) P i=1 exp(w Φ i ) ,<label>(8)</label></formula><p>which can be interpreted as the contribution of the meta-path Φ i for specific task. Obviously, the higher β Φ i , the more important meta-path Φ i is. Note that for different tasks, meta-path Φ i may has different weights. With the learned weights as coefficients, we can fuse these semantic-specific embeddings to obtain the final embedding Z as follows:</p><formula xml:id="formula_10">Z = P i=1 β Φ i · Z Φ i .<label>(9)</label></formula><p>To better understand the aggregating process of semantic-level, we also give a brief explanation in <ref type="figure" target="#fig_3">Figure 3</ref> (b). The final embedding is aggregated by all semantic-specific embedding. Then we can apply the final embedding to specific tasks and design different loss fuction. For semi-supervised node classification, we can minimize the Cross-Entropy over all labeled node between the ground-truth and the prediction:</p><formula xml:id="formula_11">L = − l ∈Y L Y l ln(C · Z l ),<label>(10)</label></formula><p>where C is the parameter of the classifier, Y L is the set of node indices that have labels, Y l and Z l are the labels and embeddings of labeled nodes. With the guide of labeled data, we can optimize the proposed model via back propagation and learn the embeddings of nodes. The overall process of HAN in shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of the Proposed Model</head><p>Here we give the analysis of the proposed HAN as follows: • The proposed model can deal with various types of nodes and relations and fuse rich semantics in a heterogeneous graph. The information can transfer from one kind of nodes to another kind of nodes via diverse relation. Benefitted from such a heterogeneous graph attention network, different types of node embedding can enhance the mutual integration, mutual promotion and mutual upgrade.</p><p>• The proposed HAN is highly efficient and can be easily parallelized. The computation of attention can compute individually across all nodes and meta-paths. Given a meta-path Φ, the time complexity of node-level attention is</p><formula xml:id="formula_12">O(V Φ F 1 F 2 K + E Φ F 1 K),</formula><p>where K is the number of attention head, V Φ is the number of nodes, E Φ is the number of meta-path based node pairs, F 1 and F 2 are the numbers </p><formula xml:id="formula_13">1 for Φ i ∈ {Φ 0 , Φ 1 , . . . , Φ P } do 2 for k = 1...K do 3 Type-specific transformation h ′ i ← M ϕ i · h i ; 4 for i ∈ V do 5</formula><p>Find the meta-path based neighbors N Φ i ;</p><formula xml:id="formula_14">6 for j ∈ N Φ i do 7 Calculate the weight coefficient α Φ i j ; 8 end 9</formula><p>Calculate the semantic-specific node embedding</p><formula xml:id="formula_15">z Φ i ← σ j ∈N Φ i α Φ i j · h ′ j ; 10 end 11</formula><p>Concatenate the learned embeddings from all attention</p><formula xml:id="formula_16">head z Φ i ← K ∥ k =1 σ j ∈N Φ i α Φ i j · h ′ j ; 12 end 13</formula><p>Calculate the weight of meta-path β Φ i ; <ref type="bibr" target="#b13">14</ref> Fuse the semantic-specific embedding <ref type="bibr" target="#b16">17</ref> Back propagation and update parameters in HAN; 18 return Z , α, β.</p><formula xml:id="formula_17">Z ← P i=1 β Φ i · Z Φ i ; 15 end 16 Calculate Cross-Entropy L = − l ∈Y L Y l ln(C · Z l ) ;</formula><p>of rows and columns of the transformation matrix, respectively. The overall complexity is linear to the number of nodes and meta-path based node pairs. The proposed model can be easily parallelized, because the node-level and semantic-level attention can be parallelized across node paris and meta-paths, respectively. The overall complexity is linear to the number of nodes and meta-path based node pairs.</p><p>• The hierarchical attention is shared for the whole heterogeneous graph which means the number of parameters is not dependent on the scale of a heterogeneous graph and can be used for the inductive problems <ref type="bibr" target="#b13">[14]</ref>. Here inductive means the model can generate node embeddings for previous unseen nodes or even unseen graph.</p><p>• The proposed model has potentionally good interpretability for the learned node embedding which is a big advantage for heterogeneous graph analysis. With the learned importance of nodes and meta-paths, the proposed model can pay more attention to some meaningful nodes or meta-paths for the specific task and give a more comprensive description of a heterogeneous graph. Based on the attention values, we can check which nodes or meta-paths make the higher (or lower) contributions for our task, which is beneficial to analyze and explain our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Datasets</head><p>The detailed descriptions of the heterogeneous graph used here are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>• DBLP 2 . We extract a subset of DBLP which contains 14328 papers (P), 4057 authors (A), 20 conferences (C), 8789 terms (T). The authors are divided into four areas: database, data mining, machine learning, information retrieval. Also, we label each author's research area according to the conferences they submitted. Author features are the elements of a bag-of-words represented of keywords. Here we employ the meta-path set {APA, APCPA, APTPA} to perform experiments.</p><p>• ACM 3 . We extract papers published in KDD, SIGMOD, SIG-COMM, MobiCOMM, and VLDB and divide the papers into three classes (Database, Wireless Communication, Data Mining). Then we construct a heterogeneous graph that comprises 3025 papers (P), 5835 authors (A) and 56 subjects (S). Paper features correspond to elements of a bag-of-words represented of keywords. We employ the meta-path set {PAP, PSP} to perform experiments. Here we label the papers according to the conference they published.</p><p>• IMDB. Here we extract a subset of IMDB which contains 4780 movies (M), 5841 actors (A) and 2269 directors (D). The movies are divided into three classes (Action, Comedy, Drama) according to their genre. Movie features correspond to elements of a bag-ofwords represented of plots. We employ the meta-path set {MAM, MDM} to perform experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare with some state-of-art baselines, include the (heterogeneous) network embedding methods and graph neural network based methods, to verfify the effectiveness of the proposed HAN. To verify the effectiveness of our node-level attention and semantic-level attention, respectively, we also test two variants of HAN.</p><p>• DeepWalk <ref type="bibr" target="#b22">[23]</ref>: A random walk based network embedding method which designs for the homogeneous graphs. Here we ignore the heterogeneity of nodes and perform DeepWalk on the whole heterogeneous graph.</p><p>• ESim <ref type="bibr" target="#b25">[26]</ref>: A heterogeneous graph embedding method which can capture semantic information from multiple meta-paths. Because it is difficult to search the weights of a set of meta-paths, we assign the weights learned from HAN to ESim.</p><p>• metapath2vec <ref type="bibr" target="#b6">[7]</ref>: A heterogeneous graph embedding method which performs meta-path based random walk and utilizes skip-gram to embed the heterogeneous graphs. Here we test all the meta-paths for metapath2vec and report the best performance.</p><p>• HERec <ref type="bibr" target="#b26">[27]</ref>: A heterogeneous graph embedding method which designs a type constraint strategy to filter the node sequence and utilizes skip-gram to embed the heterogeneous graphs. Here we test all the meta-paths for HERec and report the best performance.</p><p>• GCN <ref type="bibr" target="#b17">[18]</ref>: It is a semi-supervised graph convolutional network that designed for the homogeneous graphs. Here we test all the meta-paths for GCN and report the best performance.</p><p>• GAT <ref type="bibr" target="#b34">[35]</ref>: It is a semi-supervised neural network which considers the attention mechanism on the homogeneous graphs. Here we test all the meta-paths for GAT and report the best performance.</p><p>• HAN nd : It is a variant of HAN, which removes node-level attention and assigns the same importance to each neighbor.</p><p>• HAN sem : It is a variant of HAN, which removes the semanticlevel attention and assigns the same importance to each meta-path.</p><p>• HAN: The proposed semi-supervised graph neural network which employs node-level attention and semantic-level attention simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>For the proposed HAN, we randomly initialize parameters and optimize the model with Adam <ref type="bibr" target="#b16">[17]</ref>. For the proposed HAN, we set the learning rate to 0.005, the regularization parameter to 0.001, the dimension of the semantic-level attention vector q to 128, the number of attention head K to 8, the dropout of attention to 0.6. And we use early stopping with a patience of 100, i.e. we stop training if the validation loss does not decrease for 100 consecutive epochs. To make our experiments repeatable, we make our dataset and codes publicly available at website 4 .For GCN and GAT, we optimize their parameters using the validation set. For semi-supervised graph neural network, including GCN, GAT and HAN, we split exactly the same training set, validation set and test set to ensure fairness. For random walk based methods include DeepWalk, ESim, metapath2vec, and HERec, we set window size to 5, walk length to 100, walks per node to 40, the number of negative samples to 5. For a fair comparison, we set the embedding dimension to 64 for all the above algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Classification</head><p>Here we employ KNN classifier with k = 5 to perform node classification. Since the variance of graph-structured data can be quite high, we repeat the process for 10 times and report the averaged Macro-F1 and Micro-F1 in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Based on <ref type="table" target="#tab_2">Table 3</ref>, we can see that HAN achieves the best performance. For traditional heterogeneous graph embedding method, ESim which can leverage multiple meta-paths performs better than metapath2vec. Generally, graph neural network based methods which combine the structure and feature information, e.g., GCN and GAT, usually perform better. To go deep into these methods, compared to simply average over node neighbors, e.g., GCN and HAN nd , GAT and HAN can weigh the information properly and improve the performance of the learned embedding. Compared to GAT, the proposed HAN, which designs for heterogeneous graph, captures the rich semantics successfully and shows its superiority. Also, without nodelevel attention (HAN nd ) or semantic-level attention (HAN sem ), the performance becomes worse than HAN, which indicates the importance of modeling the attention mechanism on both of the nodes and semantics. Note that in ACM and IMDB, HAN improves classification results more significantly than in DBLP. Mainly because APCPA is the much more important than the rest meta-paths. We will explain this phenomenon in Section 5.7 by analyzing the semanticlevel attention.</p><p>Through the above analysis, we can find that the proposed HAN achieves the best performance on all datasets. The results demonstrate that it is quite important to capture the importance of nodes and meta-paths in heterogeneous graph analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Clustering</head><p>We also conduct the clustering task to evaluate the embeddings learned from the above algorithms. Once the proposed HAN trained, we can get all the node embedding via feed forward. Here we utilize the KMeans to perform node clustering and the number of clusters K is set to the number of classes. We use the same ground-truth as in node classification. And we adopt NMI and ARI to assess the quality of the clustering results. Since the performance of KMeans is affected by initial centroids, we repeat the process for 10 times and report the average results in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>As can be seen in <ref type="table" target="#tab_3">Table 4</ref>, we can find that HAN performs consistently much better than all baselines. Also, graph neural network based algorithms usually achieve better performance. Besides, without distinguishing the importance of nodes or meta-paths, metap-ath2vec and GCN cannot perform well. With the guide of multiple meta-paths, HAN performs significantly better than GCN and GAT. On the other hand, without node-level attention (HAN nd ) or semantic-level attention (HAN sem ), the performance of HAN has shown various degrees of degeneration. It demonstrates that via assigning the different importance to nodes and meta-paths, the proposed HAN can learn a more meaningful node embedding.</p><p>Based on the above analysis, we can find that the propsed HAN can give a comprehensive description of heterogeneous graph and achieve a significant improvements.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Analysis of Hierarchical Attention Mechanism</head><p>A salient property of HAN is the incorporation of the hierarchical mechanism, which takes the importance of node neighbors and meta-paths into consideration in learning representative embedding. Recall that we have learned the node-level attention weight α Φ i j and the semantic-level attention weight β Φ i . To better understand the importance of the neighbors and meta-paths, we provide a detailed analysis on the hierarchical attention mechanism.</p><p>Analysis of node-level attention. As mentioned before, given a specific task, our model can learn the attention values between nodes and its neighbors in a meta-path. Some important neighbors which are useful for the specific task tend to have larger attention values. Here we take the paper P831 5 in ACM dataset as an illustrative example. Given a meta-path Paper-Author-Paper which describes the co-author of different papers, we enumerate the meta-path based neighbors of paper P831 and their attention values are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. From <ref type="figure" target="#fig_5">Figure 4</ref>(a), we can see that P831 connects to P699 6 and P133 7 , which all belong to Data Mining; conects to P2384 8 and  <ref type="figure" target="#fig_5">Figure 4</ref>(b), we can see that paper P831 gets the highest attention value from node-level attention which means the node itself plays the most important role in learning its representation. It is reasonable because all information supported by neighbors are usually viewed as a kind of supplementary information. Beyond itself, P699 and P133 get the second and third largest attention values. This is because P699 and P133 also belong to Data Mining and they can make significant contribution to identify the class of P831. The rest neighbors get minor attention values that because they do not belong to Data Mining and cannot make important contribution to identify the P831's class. Based on the above analysis, we can see that the node-level attention can tell the difference among neighbors and assign higher weights to some meaningful neighbors. Analysis of semantic-level attention. As mentioned before, the proposed HAN can learn the importance of meta-paths for the specific task. To verify the ability of semantic-level attention, taking DBLP and ACM as examples, we report the clustering results (NMI) of single meta-path and corresponding attention values in <ref type="figure" target="#fig_6">Figure 5</ref>. Obviously, there is a positive correlation between the performance of a single meta-path and its attention value. For DBLP, HAN gives APCPA the largest weight, which means that HAN considers the APCPA as the most critical meta-path in identifying the author's research area. It makes sense because the author's research area and the conferences they submitted are highly correlated. For example, some natural language processing researchers mainly submit their papers to ACL or EMNLP, whereas some data mining researchers may submit their papers to KDD or WWW. Meanwhile, it is difficult for APA to identify the author's research area well. If we treat these meta-paths equally, e.g., HAN sem , the performance will drop significantly. Based on the attention values of each meta-path, we can find that the meta-path APCPA is much more useful than APA and APTPA. So even the proposed HAN can fuse them, APCPA still plays a leading role in identifying the author's research area while APA and APTPA do not. It also explains why the performance of HAN in DBLP may not be as significant as in ACM and IMDB. We get the similar conclusions on ACM. For ACM, the results show that HAN gives the most considerable weight to PAP. Since the performance of PAP is slightly better than PSP, so HAN sem can achieve good performance by simple average operation. We can see that semantic-level attention can reveal the difference between these meta-paths and weights them adequately. <ref type="bibr" target="#b8">9</ref> Daniel Barbara, Tomasz Imielinski. Sleepers and Workaholics: Caching Strategies in Mobile Environments, VLDB'95 10 Hector Garcia-Holina, Daniel Barbara. The cost of data replication, SIGCOMM'81</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Visualization</head><p>For a more intuitively comparation, we conduct the task of visualization, which aims to layout a heterogeneous graph on a low dimensional space. Specifically, we learn the node embedding based on the proposed model and project the learned embedding into a 2-dimensional space. Here we utilize t-SNE <ref type="bibr" target="#b20">[21]</ref> to visualize the author embedding in DBLP and coloured the nodes based on their research areas.</p><p>From <ref type="figure" target="#fig_7">Figure 6</ref>, we can find that GCN and GAT which design for the homogeneous graphs do not perform well. The authors belong to different research areas are mixed with each other. Metapath2vec performs much better than the above homogeneous graph neural networks. It demonstrates that the proper meta-path(e.g., APCPA) can make a significant contribution to heterogeneous graph analysis. However, since metapath2vec can only take only one meta-path into consideration, the boundary is still blurry. From <ref type="figure" target="#fig_7">Figure 6</ref>, we can see that the visualization of HAN peform best. With the guide of multiple meta-paths, the embedding learned by HAN has high intra-class similarity and separates the authors in different research area with distinct boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Parameters Experiments</head><p>In this section, we investigate the sensitivity of parameters and report the results of clustering (NMI) on ACM dataset with various parameters in <ref type="figure" target="#fig_8">Figure 7</ref>.</p><p>• Dimension of the final embedding Z. We first test the effect of the dimension of the final embedding Z. The result is shown in <ref type="figure" target="#fig_8">Figure 7</ref>(a). We can see that with the growth of the embedding dimension, the performance raises first and then starts to drop slowly. The reason is that HAN needs a suitable dimension to encode the semantics information and larger dimension may introduce additional redundancies.</p><p>• Dimension of semantic-level attention vector q. Since the ability of semantic-level attention is affected by the dimension of the semantic-level attention vector q, we explore the experimental results with various dimension. The result is shown in <ref type="figure" target="#fig_8">Figure 7</ref>(b). We can find that the performance of HAN grows with the dimension of semantic-level attention vector and achieves the best performance when the dimension of q is set to 128. After that, the performance of HAN starts to degenerate which may because of overfitting.</p><p>• Number of attention head K. In order to check the impact of multihead attention, we explore the performance of HAN with various number of attention head. The result is shown in <ref type="figure" target="#fig_8">Figure  7</ref>(c). Note that the multihead attention is removed when the number of attention head is set to 1. Based on the results, we can find  that the more number of attention head will generally improve the performance of HAN. However, with the change of attention head, the performance of HAN improve only slightly. Meanwhile, we also find that multihead attention can make the training process more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we tackle several fundamental problems in heterogeneous graph analysis and propose a semi-supervised heterogeneous graph neural network based solely on attention mechanism. The proposed HAN can capture complex structures and rich semantics behind heterogeneous graph. The proposed model leverages nodelevel attention and semantic-level attention to learn the importance of nodes and meta-paths, respectively. Meanwhile, the proposed model utilizes the structural information and the feature information in a uniform way. Experimental results include classification and clustering demonstrate the effectiveness of HAN. By analyzing the learned attention weights include both node-level and semantic-level, the proposed HAN has proven its potentially good interpretability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustrative example of a heterogenous graph (IMDB). (a) Three types of nodes (i.e., actor, movie, director). (b) A heterogenous graph IMDB consists three types of nodes and two types of connections. (c) Two meta-paths involved in IMDB (i.e., Moive-Actor-Moive and Movie-Director-Movie). (d) Moive m 1 and its meta-path based neighbors (i.e., m 1 , m 2 and m 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overall framework of the proposed HAN. (a) All types of nodes are projected into a unified feature space and the weight of meta-path based node pair can be learned via nodelevel attention. (b) Joint learning the weight of each meta-path and fuse the semantic-specific node embedding via semanticlevel attention. (c) Calculate the loss and end-to-end optimization for the proposed HAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a). Every node embedding is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Explanation of aggregating process in both node-level and semantic-level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Meta-path based neighbors of P831 (b) Attention values of P831's neighbors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Meta-path based neighbors of node P831 and corresponding attention values (Different colors mean different classes, e.g., green means Data Mining, blue means Database, orange means Wireless Communication).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>(a) NMI values on DBLP (b) NMI values on ACM Performance of single meta-path and corresponding attention value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Visualization embedding on DBLP. Each point indicates one author and its color indicates the research area.(a) Dimension of the final embedding Z (b) Dimension of the semantic-level attention vector q (c) Number of attention head K .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Parameter sensitivity of HAN w.r.t. Dimension of the final embedding Z , Dimension of the semantic-level attention vector q and Number of attention head K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notations and Explanations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell cols="10">Dataset Relations(A-B) Number of A Number of B Number of A-B Feature Training Validation Test Meta-paths</cell></row><row><cell></cell><cell>Paper-Author</cell><cell>14328</cell><cell>4057</cell><cell>19645</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>APA</cell></row><row><cell>DBLP</cell><cell>Paper-Conf</cell><cell>14328</cell><cell>20</cell><cell>14328</cell><cell>334</cell><cell>800</cell><cell>400</cell><cell>2857</cell><cell>APCPA</cell></row><row><cell></cell><cell>Paper-Term</cell><cell>14327</cell><cell>8789</cell><cell>88420</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>APTPA</cell></row><row><cell>IMDB</cell><cell>Movie-Actor Movie-Director</cell><cell>4780 4780</cell><cell>5841 2269</cell><cell>14340 4780</cell><cell>1232</cell><cell>300</cell><cell>300</cell><cell>2687</cell><cell>MAM MDM</cell></row><row><cell>ACM</cell><cell>Paper-Author Paper-Subject</cell><cell>3025 3025</cell><cell>5835 56</cell><cell>9744 3025</cell><cell>1830</cell><cell>600</cell><cell>300</cell><cell>2125</cell><cell>PAP PSP</cell></row><row><cell cols="3">Algorithm 1: The overall process of HAN.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Input :The heterogeneous graph G = (V, E),</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>The node feature {h</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>i , ∀i ∈ V}, The meta-path set {Φ 0 , Φ 1 , . . . , Φ P }. The number of attention head K, Output :The final embedding Z , The node-level attention weight α , The semantic-level attention weight β .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Qantitative results (%) on the node classification task. Metrics Training DeepWalk ESim metapath2vec HERec GCN GAT HAN nd HAN sem HAN</figDesc><table><row><cell>Datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell>77.25</cell><cell>77.32</cell><cell>65.09</cell><cell>66.17 86.81 86.23</cell><cell>88.15</cell><cell>89.04</cell><cell>89.40</cell></row><row><cell>Macro-F1</cell><cell>40% 60%</cell><cell>80.47 82.55</cell><cell>80.12 82.44</cell><cell>69.93 71.47</cell><cell>70.89 87.68 87.04 72.38 88.10 87.56</cell><cell>88.41 87.91</cell><cell>89.41 90.00</cell><cell>89.79 89.51</cell></row><row><cell>ACM</cell><cell>80% 20%</cell><cell>84.17 76.92</cell><cell>83.00 76.89</cell><cell>73.81 65.00</cell><cell>73.92 88.29 87.33 66.03 86.77 86.01</cell><cell>88.48 87.99</cell><cell>90.17 88.85</cell><cell>90.63 89.22</cell></row><row><cell>Micro-F1</cell><cell>40% 60%</cell><cell>79.99 82.11</cell><cell>79.70 82.02</cell><cell>69.75 71.29</cell><cell>70.73 87.64 86.79 72.24 88.12 87.40</cell><cell>88.31 87.68</cell><cell>89.27 89.85</cell><cell>89.64 89.33</cell></row><row><cell></cell><cell>80%</cell><cell>83.88</cell><cell>82.89</cell><cell>73.69</cell><cell>73.84 88.35 87.11</cell><cell>88.26</cell><cell>89.95</cell><cell>90.54</cell></row><row><cell></cell><cell>20%</cell><cell>77.43</cell><cell>91.64</cell><cell>90.16</cell><cell>91.68 90.79 90.97</cell><cell>91.17</cell><cell>92.03</cell><cell>92.24</cell></row><row><cell>Macro-F1</cell><cell>40% 60%</cell><cell>81.02 83.67</cell><cell>92.04 92.44</cell><cell>90.82 91.32</cell><cell>92.16 91.48 91.20 92.80 91.89 90.80</cell><cell>91.46 91.78</cell><cell>92.08 92.38</cell><cell>92.40 92.80</cell></row><row><cell>DBLP</cell><cell>80% 20%</cell><cell>84.81 79.37</cell><cell>92.53 92.73</cell><cell>91.89 91.53</cell><cell>92.34 92.38 91.73 92.69 91.71 91.96</cell><cell>91.80 92.05</cell><cell>92.53 92.99</cell><cell>93.08 93.11</cell></row><row><cell>Micro-F1</cell><cell>40% 60%</cell><cell>82.73 85.27</cell><cell>93.07 93.39</cell><cell>92.03 92.48</cell><cell>93.18 92.31 92.16 93.70 92.62 91.84</cell><cell>92.38 92.69</cell><cell>93.00 93.31</cell><cell>93.30 93.70</cell></row><row><cell></cell><cell>80%</cell><cell>86.26</cell><cell>93.44</cell><cell>92.80</cell><cell>93.27 93.09 92.55</cell><cell>92.69</cell><cell>93.29</cell><cell>93.99</cell></row><row><cell></cell><cell>20%</cell><cell>40.72</cell><cell>32.10</cell><cell>41.16</cell><cell>41.65 45.73 49.44</cell><cell>49.78</cell><cell>50.87</cell><cell>50.00</cell></row><row><cell>Macro-F1</cell><cell>40% 60%</cell><cell>45.19 48.13</cell><cell>31.94 31.68</cell><cell>44.22 45.11</cell><cell>43.86 48.01 50.64 46.27 49.15 51.90</cell><cell>52.11 51.73</cell><cell>50.85 52.09</cell><cell>52.71 54.24</cell></row><row><cell>IMDB</cell><cell>80% 20%</cell><cell>50.35 46.38</cell><cell>32.06 35.28</cell><cell>45.15 45.65</cell><cell>47.64 51.81 52.99 45.81 49.78 55.28</cell><cell>52.66 54.17</cell><cell>51.60 55.01</cell><cell>54.38 55.73</cell></row><row><cell>Micro-F1</cell><cell>40% 60%</cell><cell>49.99 52.21</cell><cell>35.47 35.64</cell><cell>48.24 49.09</cell><cell>47.59 51.71 55.91 49.88 52.29 56.44</cell><cell>56.39 56.09</cell><cell>55.15 56.66</cell><cell>57.97 58.32</cell></row><row><cell></cell><cell>80%</cell><cell>54.33</cell><cell>35.59</cell><cell>48.81</cell><cell>50.99 54.61 56.97</cell><cell>56.38</cell><cell>56.49</cell><cell>58.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Qantitative results (%) on the node clustering task.Datasets Metrics DeepWalk ESim metapath2vec HERec GCN GAT HAN nd HAN sem HAN</figDesc><table><row><cell>ACM</cell><cell>NMI ARI</cell><cell>41.61 35.10</cell><cell>39.14 34.32</cell><cell>21.22 21.00</cell><cell cols="3">40.70 51.40 57.29 37.13 53.01 60.43</cell><cell>60.99 61.48</cell><cell>61.05 59.45</cell><cell>61.56 64.39</cell></row><row><cell>DBLP</cell><cell>NMI ARI</cell><cell>76.53 81.35</cell><cell>66.32 68.31</cell><cell>74.30 78.50</cell><cell cols="3">76.73 75.01 71.50 80.98 80.49 77.26</cell><cell>75.30 81.46</cell><cell>77.31 83.46</cell><cell>79.12 84.76</cell></row><row><cell>IMDB</cell><cell>NMI ARI</cell><cell>1.45 2.15</cell><cell>0.55 0.10</cell><cell>1.20 1.70</cell><cell>1.20 1.65</cell><cell>5.45 4.40</cell><cell>8.45 7.46</cell><cell>9.16 7.98</cell><cell>10.31 9.51</cell><cell>10.87 10.01</cell></row><row><cell cols="5">P2328 9 while P2384 and P2328 both belong to Database; connects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">to P1973 10 while P1973 belongs to Wireless Communication. From</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.imdb.com arXiv:1903.07293v1 [cs.SI] 18 Mar 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://dblp.uni-trier.de 3 http://dl.acm.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/Jhy1993/HAN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Xintao Wu, Daniel Barbara, Yong Ye. Screening and Interpreting Multi-item Associations Based on Log-linear Modeling, KDD'03 6 Xintao Wu, Jianpin Fan, Kalpathi Subramanian. B-EM: a classifier incorporating bootstrap with EM approach for data mining, KDD'02 7 Daniel Barbara, Carlotta Domeniconi, James P. Rogers. Detecting outliers using transduction and statistical testing, KDD'06 8 Walid G. Aref, Daniel Barbara, Padmavathi Vallabhaneni. The Handwritten Trie: Indexing Electronic Ink, SIGMOD'95</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENTS</head><p>This work is supported in part by the National Natural Science Foundation of China (No. 61702296, 61772082, 61532006), the Beijing Municipal Natural Science Foundation (4182043), and the CCF-Tencent Open Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PME: Projected Metric Embedding on Heterogeneous Networks for Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1177" to="1186" />
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, and Xue Li</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Task-guided and path-augmented heterogeneous network embedding for author identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">2017. metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gotcha-sly malware!: Scorpion a metagraph2vec based malware detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melih</forename><surname>Abdulhayoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HIN2Vec: Explore Metapaths in Heterogeneous Information Networks for Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCNN</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Ferrara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02801</idno>
		<title level="m">Graph embedding techniques, applications, and performance: A survey</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Embedding logical queries on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2030" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aspect-Level Deep Collaborative Filtering via Heterogeneous Information Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senzhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3393" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Leveraging Meta-path based Context for Top-N Recommendation with A Neural Co-Attention Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1531" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised clustering in attributed heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudian</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1621" to="1629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<idno>abs/1610.09769</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heterogeneous Information Network Embedding for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Survey of Heterogeneous Information Network Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="17" to="37" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Easing Embedding Learning by Comprehensive Transcription of Heterogeneous Information Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2190" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint embedding of meta-path and meta-graph for heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Knowledge (ICBK)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mining heterogeneous information networks: a structural analysis approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigkdd Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention is All you Need. In NIPS</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph Attention Networks. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Community Preserving Network Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="203" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
