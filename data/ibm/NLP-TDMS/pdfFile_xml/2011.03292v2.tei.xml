<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Machine Reading Comprehension with Single-choice Decision and Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud TI-ONE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud TI-ONE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Gong</surname></persName>
							<email>jennygong@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud TI-ONE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahui</forename><surname>Cheng</surname></persName>
							<email>huecheng@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud TI-ONE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Meng</surname></persName>
							<email>pengmeng@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud TI-ONE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiliang</forename><surname>Lin</surname></persName>
							<email>weilianglin@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud TI-ONE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
							<email>ruibobchen@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud TI-ONE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud TI-ONE</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tencent</forename><surname>Cloud Xiaowei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent Cloud TI-ONE</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Machine Reading Comprehension with Single-choice Decision and Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-choice Machine Reading Comprehension (MMRC) aims to select the correct answer from a set of options based on a given passage and question. Due to task specific of MMRC, it is non-trivial to transfer knowledge from other MRC tasks such as SQuAD, Dream. In this paper, we simply reconstruct multi-choice to single-choice by training a binary classification to distinguish whether a certain answer is correct. Then select the option with the highest confidence score. We construct our model upon ALBERT-xxlarge model and estimate it on the RACE dataset. During training, We adopt AutoML strategy to tune better parameters. Experimental results show that the single-choice is better than multi-choice. In addition, by transferring knowledge from other kinds of MRC tasks, our model achieves a new state-of-the-art results in both single and ensemble settings. *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equal contribution.</head><p>Correspondence to {frostwu, garyyfjiang, jennygong, huecheng}@tencent.com.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The last several years have seen a land rush in research on machine reading (MRC) comprehension and various dataset have been proposed such as SQuAD1.1, SQuAD2.0, NewsQA and CoQA <ref type="bibr" target="#b7">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b12">Trischler et al., 2016;</ref><ref type="bibr" target="#b9">Reddy et al., 2019)</ref>. Different from the above which are extractive MRC, RACE is a multi-choice MRC dataset (MMRC) proposed by <ref type="bibr" target="#b4">(Lai et al., 2017)</ref>. RACE was extracted from middle and high school English examinations in China. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets is that the answers in RACE often cannot be directly extracted from the passages, as illustrated by the two example questions (Q1 &amp; Q2) Passage: For the past two years, 8-year-old Harli Jordean from Stoke Newington, London, has been selling marbles . His successful marble company, Marble King, sells all things marble-related -from affordable tubs of the glass playthings to significantly expensive items like Duke of York solitaire tables -sourced, purchased and processed by the mini-CEO himself. "I like having my own company. I like being the boss," Harli told the Mirror....Tina told The Daily Mail. "At the moment he is annoying me by creating his own Marble King marbles -so that could well be the next step for him." Q1: Harli's Marble Company became popular as soon as he launched it because . A: it was run by "the world's youngest CEO" B: it filled the gap of online marble trade C: Harli was fascinated with marble collection D: Harli met the growing demand of the customers Q2: How many mass media are mentioned in the passage? A: One B: Two C: Three D: Four <ref type="table">Table 1</ref>: An example passage and two related multichoice questions. The ground-truth answers are in bold.</p><p>in <ref type="table">Table 1</ref>. Thus, answering these questions needs inferences.</p><p>Recently, pretrained language models (LM) such as BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref>, RoBERTa <ref type="bibr" target="#b6">(Liu et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b5">(Lan et al., 2019)</ref> have achieved great success on MMRC tasks. Notably, Megatron-LM <ref type="bibr" target="#b10">(Shoeybi et al., 2019)</ref> which is a 48 layer BERT with 3.9 billion parameters yields the highest score on the RACE leaderboard in both single and ensemble settings. The key point to model MMRC is: first encode the context, question, options with BERT like LM, then add a matching network on top of BERT to score the options. Generally, the matching network can be various <ref type="bibr" target="#b8">(Ran et al., 2019;</ref><ref type="bibr" target="#b13">Zhang et al., 2020;</ref><ref type="bibr" target="#b14">Zhu et al., 2020)</ref>. <ref type="bibr" target="#b8">Ran et al. (2019)</ref> proposes an option comparison network (OCN) to compare options at word-level to better identify their correlations to help reasoning. <ref type="bibr" target="#b13">Zhang et al. (2020)</ref> proposes a dual co-matching network (DCMN) which models the relationship among passage, question and answer options bidirectionally. All these matching networks show promising improvements compared with pretrained language models. One point they have in common is that the answer together with the distractors are jointly considered which we name multi-choice models. We argue that the options can be concerned separately for two reasons, 1) when human works on MMRC problem, they always consider the options one by one and select the one with the highest confidence. 2) MMRC suffers from the data scarcity problem. Multi-choice models are inconvenient to take advantage of other MRC dataset.</p><p>In this paper, we propose a single-choice model for MMRC. Our model considers the options separately. The key component of our method is a binary classification network on top of pretrained language models. For each option of a given context and question, we calculate a confidence score. Then we select the one with the highest score as the final answer. In both training and decoding, the right answer and the distractors are modeled independently. Our proposed method gets rid of the multi-choice framework, and can leverage amount of other resources. Taking SQuAD as an example, we can take a context, one of its question and the corresponding answer as a positive instance for our classification with golden label 1. In this way many QA dataset can be used to enhance RACE. Experimental results show that single-choice model performs better than multi-choice models, in addition by transferring knowledge from other QA dataset, our single model achieves 90.7% and ensemble model achieves 91.4%, both are the best score on the leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>Multi-choice MRC (MMRC) can be represented as a triple &lt; P, Q, A &gt;, where P = s 1 , s 2 , ..., s m is an article consist of multiple sentences s, Q is a question asked upon the article and A = {A 1 , ...A n } is a set of candidate answers. Only one answer in A is correct and others are distractors. The purpose of the MMRC is to select the right one. RACE is one kind of MMRC task,</p><formula xml:id="formula_0">P P P P Q Q Q Q A 4 A 3 A 2 A 1 Enc Dec 0 0 1 0 Loss (a) Multi-choice Model : Back Propagation P P P P 3 Q Q Q Q 2 A 4 A 3 A 2 A 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enc</head><p>Dec Loss which is created by domain experts to test students' reading comprehension skills, consequently requiring non-trivial reasoning techniques. Each article in RACE has several questions and the questions always have 4 candidate answers, one answer and three distractors.</p><formula xml:id="formula_1">0 1 0 1 1 0 0 1 (b) Single-choice Model</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Previous works have verified the effectiveness of Pretrained language models such as BERT, XLNet,Roberta and Albert in Multi-choice MRC tasks. Pretrained language models are used as encoder to get the global context representation. After that, a decoder is employed to find the correct answer given all the information contained in the global representation. Let P , Q, and {A 1 , ...A n } denote the passage, question and option set separately. The input of Pretrained encoder is defined as (P ⊕ Q ⊕ A i ), the concatenation of P , Q and A i one of the option in candidate set. Moreover, for the same question, the inputs with different options are concatenated together as a complete training sample, which is more intuitive and similar to human that select the correct answer compared with other options. After encoding all the inputs for a single question, the contextual representations T = {T CLS1 , ..., T CLSn } is used to classify which is the correct answer given passage and question(see <ref type="bibr">Figure2(a)</ref>). An single full connection layer is added to compute the probability p({A 1 , ...A n }|P, Q) for all the answers and the ground truth y is the index of correct answer in candidates. T ∈ R n×h , n denotes the number of the options in candidate set. We define the score to be:</p><formula xml:id="formula_2">p({A 1 , ...A n }|P, Q) = σ(W T + b)<label>(1)</label></formula><p>where W ∈ R h×1 is the weight and b is the bias. Parameter matrices are finetuned based on pretrained language model with the cross entropy loss function which is formulated as:</p><formula xml:id="formula_3">loss = − y log(p)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single-choice Model</head><p>As all input sequences with the same passage and question are tied together, each training sample contain much duplicate content. For example, the passage with multiple sentences repeat n times in a single training sample which may degrade the diversity in each training step. Moreover, this method need to fix the data format that each question must have the same number of options which is also inconvenient to take advantage of other MRC datasets. Alternatively, we reconstruct the multi-choice to single-choice. We just need to distinguish whether the answer is correct without considering other options in the candidate set. By this way, we keep the diversity in training batches and relax the constraints on multi-choice framework.</p><p>Instead of concatenate all inputs with the same question together, we just encode a single input and use its contextual representations T CLSi to classify whether the answer is correct (see <ref type="figure" target="#fig_1">Fig-ure2(b)</ref>). The ground truth is y ∈ {0, 1}, Thus we re-define the score g(P, Q, A i ) as:</p><formula xml:id="formula_4">g(P, Q, A i ) = σ(W T CLSi + b)<label>(3)</label></formula><p>where W ∈ R h×label . Correspondingly, the cross entropy loss function can be re-formulated as:</p><formula xml:id="formula_5">loss = − y log(g(P, Q, A i )) + (1 − y) log(1 − g(P, Q, A i )) (4)</formula><p>In the end, to get the correct answers, we select the top-n answers with respect to score. Here n denotes the number of correct answers. E.g., n=1 in RACE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transfer Learning</head><p>In this section, We propose a simple yet effective strategy to transfer knowledge from other QA dataset. As the single-choice model relax the constraints on multi-choice framework, more QA datasets such as SQuAD2.0, ARC, CoQA and DREAM can be used to enhance RACE. It consists of three steps:</p><p>(1) we preprocess data with different formats to the same input type as mentioned in section 3. For multiple-choice MRC datasets, like DREAM and ARC, we concatenate each option with corresponding context and question. And for extractive MRC datasets like SQuAD2.0 and CoQA, we take the context (passage or dialog), one of its question and corresponding answer as a positive instance for the binary classification.</p><p>(2) We collect and corrupt the preprocessed data from different QA datasets and then train the binary classification on this mixed data. We find that the model benefits a lot from the large amount of MRC datasets.</p><p>(3) Finally, we further finetune the model from step 2 on the raw RACE data to adapt the model parameters to the task.  vice platforms. We adopt the AutoML algorithm to select better hyper parameters and accelerate the process by distributed training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AutoML</head><p>Finetuning pretrained language models on downstream tasks is sensitive to the selection of hyper parameters. A good set of hyper parameter affects the final performance to a great extent. However, it is impossible to manually search an optimize set from the huge amount of hyper parameter combinations. To alleviate this problem, we take advantage of automated machine learning (AutoML) <ref type="bibr" target="#b3">(He et al., 2019;</ref><ref type="bibr" target="#b15">Zöller and Huber, 2019;</ref><ref type="bibr" target="#b2">Elshawi et al., 2019)</ref> to automatically adapt hyper parameters.</p><p>Our AutoML system named TianFeng is a lightweight, extensible, and easy-to-use framework. TianFeng incorporates most current stateof-the-art algorithms and can make good use of resources. It consists of three parts, internal layers, algorithm layer, controller layer and resources layer, as shown in <ref type="figure" target="#fig_1">Figure2.</ref> (1) Algorithm layer. The algorithm layer has 4 sub modules, which are used to search model parameters from different perspectives.</p><p>(2) Controller layer. Responsible for docking Client, issuing algorithm logic and exception handling.</p><p>(3) Resources layer. Effective management of resource pools.</p><p>In general, the controller layer receives request from client and select a proper algorithm from algorithm layer. Then uses the idle GPU computing resources in the resource pool to perform multiple tasks in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distributed Training</head><p>Due to the huge amount parameters of pretrained language models, it is very time-consuming to conduct AutoML training. We take advantage of the distributed training techniques on Tencent Cloud TI-ONE platform to make the train-ing more efficient. The mixed precision training is used, which greatly accelerate training on single-machine. When training on multiple machines, TI-ONE's fast communication framework can fully leverage more GPUs. Two main advantages on multi-machine communication are: 1) Use optimized all-reduce algorithm and multistream to make full use of the bandwidth of VPC network 2) Support gradient fusion of multiple strategies to improve communication efficiency. Our best model is trained on 4 machines with 32 V100 GPUs. The training time can be shortened to 33 percent of the original single machine with 8 cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>RACE RACE <ref type="bibr" target="#b4">(Lai et al., 2017)</ref> is dataset collected from middle and high school Englis exams in China. RACE has a wide variety of question type such as summarization, inference, deduction and context matching. It contains articles from multiple domains (i.e. news, ads, story) and most of the questions need reasoning.</p><p>In the transfer learning stage, we also consider other MRC tasks. Specifically, we consider SQuAD2.0 <ref type="bibr" target="#b7">(Rajpurkar et al., 2016)</ref>, ARC <ref type="bibr" target="#b0">(Clark et al., 2018)</ref>, CoQA <ref type="bibr" target="#b9">(Reddy et al., 2019)</ref> and DREAM <ref type="bibr" target="#b11">(Sun et al., 2019)</ref>. We give a brief description of these datasets.</p><p>SQuAD2.0 and CoQA SQuAD2.0 and CoQA are extractive MRC tasks, the articles of which are wiki passages and dialogs. Their questions do not have candidate answers, instead participants are asked to extract the answer from the passage.</p><p>ARC ARC is the largest public-domain multiple-choice dataset that consist of natural and grade-school science question. It is partitioned into a Challenge Set and an Easy set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models Test</head><p>Roberta <ref type="bibr" target="#b6">(Liu et al., 2019)</ref> 83.2 ALBERT (single) <ref type="bibr" target="#b5">(Lan et al., 2019)</ref> 86.5 ALBERT (ensemble) <ref type="bibr" target="#b5">(Lan et al., 2019)</ref> 89.4 ALBERT + DUMA (single) <ref type="bibr" target="#b14">(Zhu et al., 2020)</ref> 88.0 ALBERT + DUMA (ensemble) <ref type="bibr" target="#b14">(Zhu et al., 2020)</ref> 89.8 Megatron-BERT (single) <ref type="bibr" target="#b10">(Shoeybi et al., 2019)</ref> 89.5 Megatron-BERT (ensemble) <ref type="bibr" target="#b10">(Shoeybi et al., 2019)</ref>   DREAM DREAM is multiple-choice dialoguebased Reading comprehension examination dataset. It article is a dialog and each question has only three options. Although we have transferred as much data as we can, the MMRC task still suffers data insufficiency problem. Thus we crawl different kind of MRC data from website. <ref type="table" target="#tab_1">Table 2</ref> lists all the resources we use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>Our implementation was based on Transformers 2 . We use the ALBERT-xxlarge as encoder. For hyper parameters, we follow <ref type="table">Table 15</ref> in <ref type="bibr" target="#b5">(Lan et al., 2019)</ref>, except that we set the learning rate to 1e-5 and the warmup steps to 2000. Because we find this is better for the huggingface ALBERT-xxlarge model. After adding the other resources, we do not use a fixed "Training Steps", the training steps after two epochs and the warm up step is 10% of the total training steps. All the models are trained on 8 nVidia V100 GPUs. The training takes about 2 days.</p><p>Baseline Our baseline is the original huggingface ALBERT-xxlarge model with the default multi-choice strategy. The hyper parameters follow the description above. In addition, we compare our model with many other public results from both papers or the leaderboard. <ref type="table" target="#tab_3">Table 3</ref> shows the results of our models and the baselines. The top part of the table lists the results from the current leaderboard 3 and papers. Megatron-BERT <ref type="bibr" target="#b10">(Shoeybi et al., 2019</ref>) achieves 2 https://github.com/huggingface/transformers 3 http://www.qizhexie.com/data/RACE leaderboard.html the best single and ensemble results. It is a variant of BERT <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref> with 3.9 billion parameters which is almost 40 times bigger than ALBERT-xxlarge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The results of our models are listed below the table. Our ALBERT baseline yields better result than original ALBERT due to the different choice of hyper parameters illustrated in 5.2 showing that the task is sensitive to hyper parameters. Compared with the baseline, our single-choice model achieves 0.8 more score, which shows that single-choice is better than multi-choice under the ALBERT-xxlarge model. After transferring knowledge from other MRC dataset, we get another 0.4 more score. With the help of autoML, our single model achieves 90% which surpasses Megatron-BERT <ref type="bibr" target="#b10">(Shoeybi et al., 2019)</ref> and becomes the new state-of-the-art single model results. When adding the web crawl corpus into transfer learning, our single model get the final score as high as 90.7%. This illustrates that single-choice model is easy to incorporate other resources and we achieve this by a simple transfer learning strategy. Our ensemble model gets the best score of 91.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a single-choice model for MMRC that consider the options separately. Experiments results demonstrate that our method achieves significantly improvements and by taking advantage of other MRC datasets, we achieve a new state-of-the-art performance. We plan to consider the difference between two methods and if we can combine them together in future study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An overview of Standard Model and Singlechoice Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of our AutoML architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Details of different MRC resources. "#Instance" refers to the number of true training samples built by different resources.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on RACE dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://cloud.tencent.com/product/tione</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno>abs/1803.05457</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automated machine learning: State-ofthe-art and open challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radwa</forename><surname>Elshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherif</forename><surname>Sakr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02287</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automl: A survey of the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00709</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<title level="m">Race: Large-scale reading comprehension dataset from examinations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Option comparison network for multiple-choice reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DREAM: A challenge dataset and models for dialogue-based reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<title level="m">Newsqa: A machine comprehension dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dcmn+: Dual co-matching network for multi-choice reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuailiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Duma: Reading comprehension with transposition thinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-André</forename><surname>Zöller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco F</forename><surname>Huber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12054</idno>
		<title level="m">Survey on automated machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
