<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CityPersons: A Diverse Dataset for Pedestrian Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CityPersons: A Diverse Dataset for Pedestrian Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convnets have enabled significant progress in pedestrian detection recently, but there are still open questions regarding suitable architectures and training data. We revisit CNN design and point out key adaptations, enabling plain Fas-terRCNN to obtain state-of-the-art results on the Caltech dataset.</p><p>To achieve further improvement from more and better data, we introduce CityPersons, a new set of person annotations on top of the Cityscapes dataset. The diversity of CityPersons allows us for the first time to train one single CNN model that generalizes well over multiple benchmarks. Moreover, with additional training with CityPersons, we obtain top results using FasterRCNN on Caltech, improving especially for more difficult cases (heavy occlusion and small scale) and providing higher localization quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian detection is a popular topic in computer vision community, with wide applications in surveillance, driving assistance, mobile robotics, etc. During the last decade, several benchmarks have been created for this task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref>. These benchmarks have enabled great progress in this area <ref type="bibr" target="#b1">[2]</ref>.</p><p>While existing benchmarks have enabled progress, it is unclear how well this progress translate in open world performance. We think it is time to give emphasis not only to intra-dataset performance, but also across-datasets.</p><p>Lately, a wave of convolutional neural network (convnet) variants have taken the Caltech benchmark top ranks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27]</ref>. Many of these are custom architectures derived from the FasterRCNN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref> general object detector. We show here that a properly adapted Faster-RCNN can match the detection quality of such custom architectures. However since convnets are high capacity models, it is unclear if such model will benefit from more data.</p><p>To move forward the field of pedestrian detection, we introduce "CityPersons", a new set of annotations on top of Cityscapes <ref type="bibr" target="#b4">[5]</ref>. These are high quality annotations, that provide a rich diverse dataset, and enable new experiments both for training better models, and as new test benchmark.</p><p>In summary, our main contributions are:</p><p>1. We introduce CityPersons, a new set of high quality bounding box annotations for pedestrian detection on the Cityscapes dataset (train, validation, and test sets). The train/val. annotations will be public, and an online benchmark will be setup.</p><p>2. We report new state-of-art results for FasterRCNN on Caltech and KITTI dataset, thanks to properly adapting the model for pedestrian detection and using CityPersons pre-training. We show in particular improved results for more difficult detection cases (small and occluded), and overall higher localization precision.</p><p>3. Using CityPersons, we obtain the best reported acrossdataset generalization results for pedestrian detection. <ref type="bibr" target="#b3">4</ref>. We show preliminary results exploiting the additional Cityscapes annotations. Using semantic labelling as additional supervision, we obtain promising improvements for detecting small persons. Section 1.1 covers the related work, section 2 discusses how to adapt FasterRCNN for best detection quality, section 3 describes our annotation process, some statistics of the new data and baseline experiments. Finally, section 4 explores different ways to use CityPersons to improve person detection quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>In this paper, we investigate convnets, datasets and semantic labels for pedestrian detection, so we discuss related works for these three aspects.</p><p>Convnets for pedestrian detection. Convolutional neural networks (convnets) have achieved great success in classification and detection on the ImageNet <ref type="bibr" target="#b19">[20]</ref>, Pascal, and MS COCO datasets <ref type="bibr" target="#b14">[15]</ref>. FasterRCNN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref> has become the de-facto standard detector architecture. Many variants work try to extend it <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref>, but few improve results with a simpler architecture. A notable exception is SSD <ref type="bibr" target="#b22">[23]</ref>, which obtains comparable results with a simpler architecture.</p><p>Initial attempts to apply convnets for pedestrian detection, used existing detectors (mainly decision forests over hand-crafted features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>) outputs and re-scored them with a convnet classifier (plus bounding box regression) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27]</ref>. Better results are shown when using the reverse configuration: detections resulted from a convnet are re-scored with decision forests classifier (trained over convnet features) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>. Recently good results are presented by customized pure convnet architectures such as MS-CNN <ref type="bibr" target="#b2">[3]</ref> and SA-FastRCNN <ref type="bibr" target="#b20">[21]</ref>. In this paper we show that a properly adapted plain Fas-terRCNN matches state-of-the-art detection quality without needing add-ons.</p><p>Pedestrian datasets. In the last decade several datasets have been created for pedestrian detection training and evaluation. INRIA <ref type="bibr" target="#b6">[7]</ref>, ETH <ref type="bibr" target="#b10">[11]</ref>, TudBrussels <ref type="bibr" target="#b28">[29]</ref>, and Daimler <ref type="bibr" target="#b9">[10]</ref> represent early efforts to collect pedestrian datasets. These datasets have been superseded by larger and richer datasets such as the popular Caltech-USA <ref type="bibr" target="#b8">[9]</ref> and KITTI <ref type="bibr" target="#b11">[12]</ref>. Both datasets were recorded by driving through large cities and provide annotated frames on video sequences. Despite the large number of frames, both datasets suffer from low-density. With an average of ∼ 1 person per image, occlusions cases are severely under-represented. Another weakness of both dataset, is that each was recorded in a single city. Thus the diversity in pedestrian and background appearances is limited. Building upon the strengths of the Cityscapes data <ref type="bibr" target="#b4">[5]</ref>, our new annotations provide high quality bounding boxes, with larger portions of occluded persons, and the diversity of 27 different cities. Such diversity enables models trained on CityPersons to better generalize to other test sets.</p><p>Semantic labels for pedestrian detection. In section 4.3 we will explore using the semantic labels from Cityscapes to train a pedestrian detector with better context modelling. The idea of using semantic labels to improve detections is at least a decade old <ref type="bibr" target="#b29">[30]</ref>, and two recent incarnations are <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b5">6]</ref>. We will use the semantic probability maps computed from a semantic labeller network as additional input channels (next to RGB channels) for the pedestrian detection convnet (see section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A convnet for pedestrian detection</head><p>Before delving into our new annotations (in section 3), we first build a strong reference detector, as a tool for our experiments in sections 3.4 and 4. We aim at finding a straightforward architecture that provides good performance on the Caltech-USA dataset <ref type="bibr" target="#b8">[9]</ref>.</p><p>Training, testing (MR O , MR N ). We train our Caltech models using the improved 10× annotations from <ref type="bibr" target="#b31">[32]</ref>, which are of higher quality than the original annotations (less false positives, higher recall, improved ignore regions, and better aligned bounding boxes). For evaluation we follow the standard Caltech evaluation <ref type="bibr" target="#b8">[9]</ref>; log miss-rate (MR) is averaged over the FPPI (false positives per image) range of [10 −2 , 10 0 ] FPPI. Following <ref type="bibr" target="#b31">[32]</ref>, we evaluate both on the "original annotations" (MR O ) and new annotations (MR N ); and indicate specifically which test set is being used each time. Unless otherwise specified, the evaluation is done on the "reasonable" setup <ref type="bibr" target="#b8">[9]</ref>.</p><p>FasterRCNN. The FasterRCNN detector obtains competitive performance on general object detection. After retraining with default parameters it will under-perform on the pedestrian detection task (as reported in <ref type="bibr" target="#b30">[31]</ref>). The reason why vanilla FasterRCNN underperforms on the Caltech dataset is that it fails to handle small scale objects (50 ∼ 70 pixels), which are dominant on this dataset. To better handle small persons, we propose five modifications (M i ) that bring the MR O (miss-rate) from 20.98 down to 10.27 (lower is better, see table 1). As of writing, the best reported results on Caltech is 9.6 MR o , and our plain Fas-terRCNN ranks third with less than a point difference. We train FasterRCNN with VGG16 convolutional layers, initialized via ImageNet classification pre-training <ref type="bibr" target="#b25">[26]</ref>. ) and assume a uniform distribution of object scales. However, when we look at the training data on Caltech, we find much more small scale people than large ones. Other architectures. We also explored other architectures such as SSD <ref type="bibr" target="#b22">[23]</ref> or MS-CNN <ref type="bibr" target="#b2">[3]</ref> but, even after ad-aptations, we did not manage to obtain improved results. Amongst all the variants reaching ∼ 10% MR our Faster-RCNN is the simplest.</p><p>Conclusion. Once properly adapted, FasterRCNN obtains competitive performance for pedestrian detection on the Caltech dataset. This is the model we will use in all following experiments. In section 3 we introduce a new dataset that will enable further improvements of detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CityPersons dataset</head><p>The Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref> was created for the task of semantic segmentation in urban street scenes. It consists of a large and diverse set of stereo video sequences recorded in streets from different cities in Germany and neighbouring countries. Fine pixel-level annotations of 30 visual classes are provided for 5 000 images from 27 cities. The fine annotations include instance labels for persons and vehicles. Additionally 20 000 images from 23 other cities are annotated with coarse semantic labels, without instance labels.</p><p>In this paper, we present the CityPersons dataset, built upon the Cityscapes data to provide a new dataset of interest for the pedestrian detection community. For each frame in the 5 000 fine-annotations subset, we have created high quality bounding box annotations for pedestrians (section 3.1). In section 3.2 we contrast CityPersons with previous datasets regarding: volume, diversity and occlusion. In section 4 we show how to use this new data to improve results on other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bounding box annotations</head><p>The Cityscapes dataset already provides instance level segments for each human. These segments indicate the visible parts of humans. Simply using bounding boxes of these segments would raise three issues. I1) The box aspect ratio would be irregular, persons walking have varying width. It has been proposed to thus normalize aspect ratio for pedestrian annotations. I2) Even after normalizing aspect ratio, the boxes would not align amongst each other. They will be off in the horizontal axis due to being normalized based on the segment centre rather the object centre. They will be off in the vertical axis due to variable level of occlusion for each person. It has been shown that pedestrian detectors benefit from well aligned training samples <ref type="bibr" target="#b31">[32]</ref>, and conversely, training with misaligned samples will hamper results. I3) Existing datasets (INRIA, Caltech, KITTI) have defined bounding boxes covering the full object extent, not just the visible area. In order to train compatible, high quality models, we need to have annotations that align well the full extent of the persons bodies ("amodal bounding box" <ref type="bibr" target="#b21">[22]</ref>).</p><p>Fine-grained categories. In the Cityscapes dataset, humans are labelled as either person or rider. In this paper, we provide further fine-grained labels for persons. Based on the postures, we group all humans into four categories: pedestrian (walking, running or standing up), rider (riding bicycles or motorbikes), sitting person, and other person (with unusual postures, e.g. stretching).</p><p>Annotation protocol. For pedestrians and riders (cyclists, motorists), we follow the same protocol as used in <ref type="bibr" target="#b31">[32]</ref>, where the full body is annotated by drawing a line from the top of the head to the middle of two feet, and the bounding box is generated using a fixed aspect ratio (0.41). This protocol has been shown to provide accurate alignments. The visible bounding box for each instance is the tightest one fully covering the segment mask, and can be generated automatically from the segment. See an illustration in <ref type="figure" target="#fig_3">figure 2</ref>. The occlusion ratio can then be computed as area(BB−vis)</p><p>area(BB−f ull) .</p><p>As of other categories of persons, i.e. sitting and other persons, there is no uniform alignment to apply, so we only provide the segment bounding box for each of them without full body annotations.</p><p>Apart from real persons, we also ask the annotators to search over the whole image for areas containing fake humans, for instance, people on posters, statue, mannequin, people's reflection in mirror or window, etc., and mark them as ignore regions.</p><p>Annotation tool. Since we already have the segment mask for each instance, we can do the annotations in a more efficient way than from scratch. To this end, we develop a new annotation tool to avoid searching for persons over the images by exploiting the available instance segments. This tool pops out one person segment at a time and asks the annotator to recognize the fine-grained category first and then do the full body annotation for pedestrians and riders. Thanks to the high-quality of segmentation annotations, using such a tool also reduces the risk of missing persons, especially at crowded scenes. But the ignore region annotations have to be done by searching over the whole images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Statistics</head><p>Volume. We show the number of bounding box annotations provided by us in table 2. In a total of 5 000 images, we have~35k person and~13k ignore region annotations. And we notice the density of persons are consistent across train/validation/test subsets. Please note we use the same split as Cityscapes.</p><p>Diversity. We compare the diversity of Caltech, KITTI and CityPersons in table 3. Since KITTI test set annota-    tions are not publicly available, we only consider the training subset for a fair comparison.</p><p>The CityPersons training subset was recorded across 18 different cities, three different seasons, and various weather conditions. While the Caltech and KITTI datasets are only recorded in one city at one season each.</p><p>In terms of density, we have on average~7 persons per image. This number is much higher than that on the Caltech and KITTI datasets, where each image only contains 1 person on average. Also, the number of identical persons is another important evidence of diversity. On our CityPersons dataset, the number of identical persons amounts up to ∼ 20 000. In contrast, the Caltech and KITTI dataset only contains~1 300 and~6 000 unique pedestrians respectively. Note KITTI and CityPersons frames are sampled very sparsely, so each person is considered as unique.</p><p>CityPersons also provides fine-grained labels for persons. As shown in <ref type="figure" target="#fig_4">figure 3</ref>, pedestrians are the majority (83%). Although riders and sitting persons only occupy 10% and 5% respectively, the absolute numbers are still considerable, as we have a large pool of~35k persons.</p><p>Occlusion. The Cityscapes data was collected by driving through the centre of some highly populated cities, e.g. Frankfurt and Hamburg. We notice that on some images, there are~100 people walking on the street, highly occluded by each other. Such a high occlusion is rarely    seen in previous datasets. In <ref type="figure">figure 4</ref>, we compare the distribution of pedestrians at different occlusion levels for Caltech and CityPersons. We notice that on Caltech there are more than 60% fully visible pedestrians, while on CityPersons there are less than 30%. This indicates we have two times more occlusions than Caltech, which makes CityPersons a more interesting ground for occlusion handling. Moreover, on the reasonable subset (&lt;=0.35 occlusion) the community typically use, Caltech is dominated by fully visible pedestrians, while CityPersons has more occlusion cases. In order to understand which kinds of occlusions we have on CityPersons, we quantize all persons into 11 patterns and show the top 9 of them in <ref type="figure">figure 5</ref> (the last two patterns are not shown as they are of less than 1% and thus noisy). For visualization, we resize each full body bounding box to a fixed size, and then overlay the segmentation mask. For each pattern, the bright area shows the visible part and the two numbers on top indicate the percentage and average occlusion ratio of corresponding pattern. The first two patterns (55.9%) roughly cover the "reasonable" subset; the third and fourth patterns correspond to occlusions from either left or right side. Apart from that, we still have about 30% pedestrians distributed in various patterns, some of which have a very high occlusion ratio (&gt;0.9). Such distributed occlusion patterns increase the diversity of the data and hence makes the dataset a more challenging test base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Benchmarking</head><p>With the publication of this paper, we will create a website for CityPersons dataset, where train/validation annotations can be downloaded, and an online evaluation server is available to compute numbers over the held-out test annotations. <ref type="bibr" target="#b0">1</ref> We follow the same evaluation protocol as used for Caltech <ref type="bibr" target="#b8">[9]</ref>, by allowing evaluation on different subsets. In this paper, MR stands for log-average miss rate on the "reasonable" setup (scale [50, ∞], occlusion ratio [0, 0.35]) unless otherwise specified. While evaluating pedestrian detection performance, cyclists/sitting persons/other persons/ignore regions are not considered, which means detections matching with those areas are not counted as mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Baseline experiments</head><p>To understand the difficulties of pedestrian detection on the CityPersons dataset, we train and evaluate three different detectors. ACF <ref type="bibr" target="#b7">[8]</ref> and Checkerboards <ref type="bibr" target="#b32">[33]</ref> are representatives from the Integral Channel Features detector (ICF) family, while FasterRCNN <ref type="bibr" target="#b25">[26]</ref> acts as the state-ofthe-art detector. We set up the FasterRCNN detector by following the practices we learned from Caltech experiments (section 2). Since CityPersons images are~7 times larger than Caltech, we are only able to use an upsampling factor of 1.3 to fit in 12GB of GPU memory.</p><p>We re-train each detector using the CityPersons train- In <ref type="figure">figure 6</ref>, we show the comparison of the above three detectors on CityPersons and Caltech. FasterRCNN outperforms ICF detectors by a large margin, which indicates the adaptation of FasterRCNN on Caltech is also transferable to CityPersons. Moreover, we find the ranking of three detectors on CityPersons is consistent with that on Caltech, but the performance on CityPersons dataset is lower for all three detectors. This comparison shows that CityPersons is a more challenging dataset, thus more interesting for future research in this area.</p><p>To understand the impact of having a larger amount of training data, we show how performance grows as training data increases in <ref type="figure" target="#fig_6">figure 7</ref>. We can see performance keeps improving with more data. Therefore, it is of great importance to provide CNNs with a large amount of data.</p><p>Considering the trade off between speed and quality, we use an alternative model of our FasterRCNN by switching off input image upsampling for the analysis experiments shown in <ref type="figure" target="#fig_6">figure 7 and</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Improve quality using CityPersons</head><p>Having the CityPersons dataset at hand, we now proceed to illustrate three different ways it enables to improve pedestrian detection results ( §4.1, §4.2, §4.3). As we will see, CityPersons is particularly effective at improving results for small scale pedestrians, occluded ones, and providing higher localization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generalization across datasets</head><p>Commonly, a detector is trained on the training set of the target benchmark. As such, one needs to train multiple detectors for different benchmarks. Ideally, one would wish to train one detector that is able to perform well on multiple benchmarks. Since the CityPersons dataset is large and diverse, we wonder if it can allow us to train a detector with good generalization capabilities.</p><p>To see how well CityPersons data generalizes across different datasets, we train models on Caltech, KITTI and CityPersons datasets, and then apply each of them on six different test sets: Caltech, KITTI, CityPersons, IN-RIA, ETH and Tud-Brussels. For KITTI, we split the public training data into training and validation subsets (2:1) by random sampling. <ref type="table" target="#tab_8">Table 4</ref> shows comparisons of two detectors: ACF <ref type="bibr" target="#b7">[8]</ref> and FasterRCNN <ref type="bibr" target="#b25">[26]</ref>.</p><p>We observe:</p><p>(1) Overall, when trained with the same data Faster-RCNN generalizes better across datasets than ACF. (Note that FasterRCNN benefits from ImageNet pre-training, while ACF does not.)</p><p>(2) For both detectors, the mean MR across test sets is significantly better for models trained with CityPersons training data. CityPersons generalizes better than Caltech and KITTI.</p><p>These experiments confirm the generalization ability of CityPersons dataset, that we attribute to the size and diversity of the Cityscapes data, and to the quality of the bounding boxes annotations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Better pre-training improves quality</head><p>In table 4, we find the CityPersons data acts as very good source of training data for different datasets, assuming we are blind to the target domain. Furthermore, when we have some training data from the target domain, we show CityPersons data can be also used as effective external training data, which helps to further boost performance.</p><p>First, we consider Caltech as the target domain, and compare the quality of two models. One is trained on Caltech data only; and the other is first trained on CityPersons, and then finetuned on Caltech (CityPersons→Caltech). From table 5, we can see the additional training with CityPersons data improves the performance in the following three aspects.</p><p>(1) CityPersons data improves overall performance. When evaluated on the reasonable setup, the CityPersons→Caltech model obtains~1 pp gain.</p><p>(2) CityPersons data improves more for harder cases, e.g. smaller scale, heavy occlusion. We notice the gap for heavy occlusion is large (~9 pp), due to more occluded training samples on the CityPersons dataset. Similar trend is also found for smaller scale persons ( <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">80]</ref>   When we increase the IoU threshold for matching from 0.50 to 0.75, the gain from CityPersons data also grows from 1 pp to 5 pp. This gap indicates the high quality of CityPersons annotations are beneficial to produce betteraligned detections. Compared with other state-of-the-art detectors, our best model using CityPersons for pre-training obtains 5.1% MR N at IoU 0.50 evaluation, outperforming previous best reported results <ref type="bibr">(7.</ref>3% MR N ) by 2.2 pp (figure 8a); this gap becomes even larger (~20 pp) when we use a stricter IoU of 0.75 ( <ref type="figure">figure 8b)</ref>. From the comparison, our FasterRCNN detector obtains state-of-the-art results on Caltech, and improves the localization quality significantly.</p><p>When we consider KITTI as the target domain, we also see improvements brought by additional training with CityPersons data. As shown in table 6, the gain on reasonable evaluation setup is 2.5 pp, while for smaller scale, the gap becomes more impressive (10.7 pp). The 4.1 pp gap at IoU 0.75 again verifies CityPersons data helps to produce better aligned detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Exploiting Cityscapes semantic labels</head><p>In this subsection, we explore how much improvement can be obtained for pedestrian detection by leveraging the semantic labels available on the Cityscapes dataset. We use an FCN-8s <ref type="bibr" target="#b23">[24]</ref> model trained on Cityscapes coarse annotations to predict semantic labels. Note we cannot involve fine-annotation images in this semantic labelling training, otherwise our following detection training will suffer from overfitting. Although this model is only trained on coarse annotations, we can see the semantic segmentation mask provides a reasonable structure for the whole scene ( <ref type="figure">figure 9</ref>). Then we concatenate semantic channels  with RGB channels and feed them altogether into convnets, letting convnets to figure out the hidden complementarity. For the reasonable evaluation setup, we get an overall improvement of~0.6 pp from semantic channels. When we look at the fine-grained improvements for different scale ranges, we find that semantic channels help more for small persons, which is a hard case for our task <ref type="table" target="#tab_13">(table 7)</ref>.</p><p>As a preliminary trial, we already get some improvements from semantic labels, which encourage us to explore more effective ways of using semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>In this paper, we first show that a properly adapted FasterRCNN can achieve state-of-the-art performance on Caltech. Aiming for further improvement from more and better data, we propose a new diverse dataset namely CityPersons by providing bounding box annotations for persons on top of Cityscapes dataset. CityPersons shows high contrast to previous datasets as it consists of images recorded across 27 cities, 3 seasons, various weather conditions and more common crowds.</p><p>Serving as training data, CityPersons shows strong generalization ability from across dataset experiments. Our FasterRCNN model trained on CityPersons obtains reasonable performance over six different benchmarks. Moreover, it further improves the detection performance with additional finetuning on the target data, especially for harder cases (small scale and heavy occlusion), and also enhance the localization quality.</p><p>On the other hand, CityPersons can also be used as a new test benchmark as there are more challenges, e.g. more occlusions and diverse environments. We will create a website for this benchmark and only allows for online evaluations by holding out the test set annotations.</p><p>Other than bounding box annotations for persons, there are additional information to leverage on CityPersons, for instance, fine semantic segmentations, other modalities of data (stereo, GPS), and un-annotated neighbouring frames. Our preliminary results of using semantic labels show promising complementarity. These rich data will motivate more efforts to solve the problem of pedestrian detec-tion.</p><p>In this supplementary material, we will show some more illustrations, discussions and experiments for the CityPersons dataset.</p><p>• Section B shows some examples of our annotations.</p><p>• Section C provides some analysis regarding the annotations, including height statistics (section C.1), analysis experiments regarding annotation quality (section C.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CityPersons annotation examples</head><p>In <ref type="figure" target="#fig_0">figure 10</ref>, we show some examples of our bounding box annotations and Cityscapes segmentation annotations from different cities. We can see the diversity in terms of people's appearance, clothing, and background objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of CityPersons annotations</head><p>In this section, we provide some analysis regarding the height statistics and quality of CityPersons annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Height statistics</head><p>In <ref type="figure" target="#fig_0">figure 11</ref>, we compare the height distribution of Ci-tyPersons and Caltech. The CityPersons is more diverse than Caltech in terms of scale:</p><p>(1) CityPersons covers a larger range of height, as it consists of larger images.</p><p>(2) More than 70% of Caltech pedestrians fall in one single bin [50,100], while CityPersons are more evenly distributed in different scale ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Quality</head><p>The segment for each person only reflects the visible part, while losing information of the aligned full body. In <ref type="bibr" target="#b31">[32]</ref>, it is shown that better alignment of training annotations improve the detection quality a lot. Therefore, in this paper we aim to provide high quality well aligned annotations for each pedestrian. On the other hand, as shown in the second section of the main paper, properly handling ignore regions also affects the results, so we also make efforts to label ignore regions over all images.</p><p>In table 8, we show that our high quality annotations improve the performance by~7 pp, among which~6 pp is gained from better alignment, and another~1pp from ignore regions handling.</p><p>Another argument for our aligned bounding box annotations is the comparison of performance on an external benchmark (Caltech) using two types of training annotations. From table 9, we can see the model trained with seg-   ment bounding boxes fails not only on CityPersons, but also on Caltech. The reason is other benchmarks, e.g. Caltech, also provide aligned bounding box annotations. Therefore, using our annotations helps to train a better generalizable model over multiple benchmarks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The diversity of the newly introduced CityPersons annotations allows to train one convnet model that generalizes well over multiple benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>M 1</head><label>1</label><figDesc>Quantized RPN scales. The default scales of the RPN (region proposal network in FasterRCNN) are sparse ([0.5, 1, 2]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Image (b) Segmentation mask (c) Bounding box anno.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of bounding box annotations for pedestrians. For each person, the top of the head and middle of the feet is drawn by the annotator. An aligned bounding box is automatically generated using the fixed aspect ratio (0.41). The bounding box covering the segmentation mask is used to estimate the visible part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Fine-grained person categories on CityPersons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Comparison of occlusion distributions onCityPersons and Caltech datasets. CityPersons contains more occlusions in the reasonable subset than Caltech.28.5%(0.0) 27.4%(0.2) 8.3%(0.4) 7.8%(0.4) 6.9%(0.6) 5.8%(0.8) 5.2%(0.5) 4.2%(0.9) 3.9%(0.4) Top 9 of quantized 11 occlusion patterns of pedestrians on CityPersons dataset. Two numbers on top indicate percentage and average occlusion ratio of samples clustered into each pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Quality as function of training volume. Faster-RCNN model trained/evaluated on CityPersons train/val. set (MR: lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Comparison of state-of-the-art results on the Caltech test set (reasonable subset), MR N . (a) Original image (b) Semantic map Example of semantic map generated by an FCN-8s model trained on Cityscapes coarse annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Examples of annotations from different cities. Left: our bounding box annotations; right: Cityscapes segmentation annotations. For visualization, we use different masks for pedestrians/riders, sitting persons, other persons, group of people, and ignore regions. Height distributions of CityPersons and Caltech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of bounding box annotations on</figDesc><table /><note>CityPersons dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of diversity on different datasets (training subset only).</figDesc><table><row><cell>rider</cell><cell></cell></row><row><cell>10%</cell><cell></cell></row><row><cell></cell><cell>sitting</cell></row><row><cell>pedestrian</cell><cell>5%</cell></row><row><cell>83%</cell><cell></cell></row><row><cell></cell><cell>other</cell></row><row><cell></cell><cell>2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>section 4.3. This model is about 2x faster at both training and test time, but only drops the performance by~2 pp (from 13% MR to 15% MR). Conclusion. The CityPersons dataset can serve as a large and diverse database for training a powerful model, as well as a more challenging test base for future research on pedestrian detection.</figDesc><table><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25</cell><cell>24.57</cell><cell></cell><cell></cell></row><row><cell>MR</cell><cell>20</cell><cell></cell><cell>19.76</cell><cell>17.83</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>15.67</cell><cell>15.14</cell></row><row><cell></cell><cell>15</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>12.50%</cell><cell>25%</cell><cell>50%</cell><cell>75%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Proportion of training data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Generalization ability of two different methods, trained and tested over different datasets. All numbers are MR on reasonable subset. Bold indicates the best results obtained via generalization across datasets (different train and test).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Gains from additional CityPersons training at different evaluation setups on Caltech test set. MR O and MR N indicate numbers evaluated on original and new annotations<ref type="bibr" target="#b31">[32]</ref>. CityPersons pre-training helps more for more difficult cases. See also table 6.</figDesc><table><row><cell>Setup</cell><cell>Scale range</cell><cell cols="2">IoU KITTI</cell><cell>CityPersons ∆MR →KITTI</cell></row><row><cell cols="3">Reasonable [50, ∞] 0.5</cell><cell>8.4</cell><cell>5.9</cell><cell>+ 2.5</cell></row><row><cell cols="4">Reasonable [50, ∞] 0.75 43.3</cell><cell>39.2</cell><cell>+ 4.1</cell></row><row><cell>Smaller</cell><cell cols="3">[30, 80] 0.5 37.8</cell><cell>27.1</cell><cell>+ 10.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Gains from additional CityPersons training at different evaluation setups on KITTI validation set. All numbers are MR (see §2). Here also, CityPersons pretraining helps more for more difficult cases. See also table 5. detections. The Caltech new annotations are well aligned, thus a good test base for alignment quality of detections.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Improvements from semantic channels in different scale ranges. Numbers are MR on the CityPersons val. set. Albeit there is small overall gain, adding semantic channels helps for the difficult case of small persons.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>The effects on performance of using high quality training annotations. CityPersons validation set evaluation. Training with our aligned bounding box annotations and ignore region annotations gives better performance than training with segment bounding box annotations.</figDesc><table><row><cell>Train anno.</cell><cell>Seg.</cell><cell>Aligned</cell></row><row><cell>Test set</cell><cell cols="2">bounding box bounding box</cell></row><row><cell>Caltech</cell><cell>37.5</cell><cell>26.9</cell></row><row><cell>CityPersons</cell><cell>22.5</cell><cell>15.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Comparison of performance using two types of training annotations. Numbers are MR on CityPersons validation set; and MR O on Caltech test set. Using our aligned bounding box for training obtains better quality on both Caltech and CityPersons.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As a subset of the Cityscapes dataset, CityPersons annotations and benchmark will be available on the Cityscapes website. The evaluation server is being setup and the metrics will change.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>A. Content</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time pedestrian detection with deep network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ten years of pedestrian detection, what have we learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, CVRSUAD workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic channels for fast pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monocular pedestrian detection: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A mobile vision system for robust multi-person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Taking a deeper look at pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pushing the limits of deep cnns for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaleaware fast r-cnn for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Amodal instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pedestrian detection aided by deep learning semantic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-cue onboard pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A critical view of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bileschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
