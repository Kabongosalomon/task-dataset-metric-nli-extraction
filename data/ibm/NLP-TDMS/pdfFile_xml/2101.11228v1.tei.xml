<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GAITGRAPH: GRAPH CONVOLUTIONAL NETWORK FOR SKELETON-BASED GAIT RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-27">27 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torben</forename><surname>Teepe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gilg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Herzog</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hörmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GAITGRAPH: GRAPH CONVOLUTIONAL NETWORK FOR SKELETON-BASED GAIT RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-27">27 Jan 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Gait Recognition, Graph Neural Net- works</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gait recognition is a promising video-based biometric for identifying individual walking patterns from a long distance. At present, most gait recognition methods use silhouette images to represent a person in each frame. However, silhouette images can lose fine-grained spatial information, and most papers do not regard how to obtain these silhouettes in complex scenes. Furthermore, silhouette images contain not only gait features but also other visual clues that can be recognized. Hence these approaches can not be considered as strict gait recognition.</p><p>We leverage recent advances in human pose estimation to estimate robust skeleton poses directly from RGB images to bring back model-based gait recognition with a cleaner representation of gait. Thus, we propose GaitGraph that combines skeleton poses with Graph Convolutional Network (GCN) to obtain a modern model-based approach for gait recognition. The main advantages are a cleaner, more elegant extraction of the gait features and the ability to incorporate powerful spatiotemporal modeling using GCN. Experiments on the popular CASIA-B gait dataset show that our method archives stateof-the-art performance in model-based gait recognition.</p><p>The code and models are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Compared to other unique biometrics like face, fingerprint, and iris, gait is remarkable in the recognition from a great distance and without the cooperation or intrusion to the subject. Hence, it opens up enormous potential for applications such as social security, access control, and forensic identification.</p><p>However, gait can be sensitive to surface type, clothing, carried items, and clutter or occlusions in the scene. These represent the challenges of tackling the gait identification task to learn unique and invariant features from the human gait. ©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. <ref type="bibr" target="#b0">1</ref> github.com/tteepe/GaitGraph Most current approaches <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> use silhouettes extracted from a video sequence to represent the gait. These approaches apply the following steps: silhouette extraction, feature learning, and similarity comparison. The silhouette extraction is mostly done using background subtraction <ref type="bibr" target="#b3">[4]</ref>. While background subtraction is easy to apply in a lab setting, it becomes cumbersome in a cluttered and rapidly-changing real-world scenario. Most applications do not consider the complexity of the background subtraction task. Other approaches go up to the extent of training a separate Convolutional Neural Network (CNN) for this task <ref type="bibr" target="#b0">[1]</ref>.</p><p>With robust human pose estimators emerging, other approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> started new model-based methods for gait. Current pose estimation algorithms are very robust against occlusion, cluttered and changing backgrounds, carried items, and clothing. Compared to silhouette images, multiple poses can be extracted from an image simultaneously, even if they overlap <ref type="bibr" target="#b6">[7]</ref>. Pose estimation in 2D and 3D is an active area of research, and our approach will profit from further improve-ments. Furthermore, a skeleton sequence is a cleaner representation of the gait since silhouette images also capture visual information of the person like physique or hairstyle. Hence, silhouette-based approaches recognize gait features and other appearance clues, which make these approaches more comparable to person re-identification methods.</p><p>This paper proposes GaitGraph, a novel approach where we apply a GCN on a graph of human skeleton poses. Inspired by the success of GCNs in skeleton-based action recognition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, we adapted the methods to the gait recognition task. The pose estimation replaces the silhouette extraction from previous approaches. The skeleton-based representation brings back real gait recognition while also using less sensitive personal data.</p><p>Our contributions can be summarized as follows:</p><p>(1) We use a modern interpretation of model-based gait recognition, exploiting robust human pose estimation and powerful temporal and spatial modeling of GCNs.</p><p>(2) Our empirical experiments show state-of-the-art (SOTA) results compared to the current model-based approaches and even competitive results compared to appearancebased methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Current works in gait recognition can be grouped by their spatial feature extraction and their temporal modeling.</p><p>For the spatial feature extraction, there are two categories: appearance-based and model-based approaches. Appearancebased methods relied on a binary human silhouette image extracted from the original image <ref type="bibr" target="#b3">[4]</ref>. The extraction is usually obtained by background subtraction for static scenes but becomes more complicated for dynamic and changing settings <ref type="bibr" target="#b0">[1]</ref>. While most approaches [1, 2, 10] use the whole shape as input, recent methods <ref type="bibr" target="#b2">[3]</ref> focus on specific body parts. Model-based approaches consider the underlying physical structure of the body <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref>. The features extracted from the model data are mostly handcrafted and contain velocity, angles, etc. While model-based approaches used to be computationally expensive, the advances in pose estimation have now made them an interesting possibility.</p><p>The temporal modeling can be divided into single-image, sequence-based, and set-based approaches. Early approaches proposed to encode a gait cycle into a single image, i.e., Gait Energy Image (GEI) <ref type="bibr" target="#b11">[12]</ref>. These representations are easy to compute but lose most of the temporal information. Sequence-based approaches focus on each input separately. For modeling the temporal information 3D-CNNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> or LSTMs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> are used. These approaches can comprehend more spatial information and gather more temporal information but require higher computational costs. The set-based approach <ref type="bibr" target="#b1">[2]</ref> models no temporal information, thus has less computational complexity. GaitPart <ref type="bibr" target="#b2">[3]</ref> introduces a novel temporal module, that focuses on capturing short-range temporal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SKELETON-BASED GAIT RECOGNITION</head><p>In this section, we describe our method for learning discriminative information from a sequence of human poses. The overall pipeline is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation. A human skeleton graph is denoted as</head><formula xml:id="formula_0">G = (V, E), where V = {v 1 , . . . , v N } is the set of N</formula><formula xml:id="formula_1">A ∈ R N ×N with A i,j = 1 if an edge connects from v i to v j and A i,j = 0 otherwise. A is symmetric since G is undirected.</formula><p>Gait as a sequence of graphs has a node feature set</p><formula xml:id="formula_2">X = {x t,n ∈ R C | t, n ∈ Z, 1 ≤ t ≤ T, 1 ≤ n ≤ N } represented as a feature tensor X ∈ R T ×N ×C , where x t,n = X t,n,: is the C dimensional feature vector for node v n at time t over a total of T frames.</formula><p>Thus, the input gait can be described by A structurally and by X feature-wise, with X t ∈ R N ×C being the node features at time t. The feature X in the C dimension consists of the 2D coordinate, and it's confidence. A learnable weight matrix at layer l of a network is denoted as</p><formula xml:id="formula_3">Θ (l) ∈ R C l ×C l+1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolutional Networks (GCNs).</head><p>On skeleton inputs, defined by features X and graph structure A, the layer-wise update rule of GCNs can be applied to features at time t as:</p><formula xml:id="formula_4">X (l+1) t = σ D − 1 2ÃD − 1 2 X (l) t Θ (l) ,<label>(1)</label></formula><p>whereÃ = A + I is the skeleton graph with added selfloops to keep identity features,D is the diagonal degree matrix ofÃ, and σ(·) is an activation function. The term</p><formula xml:id="formula_5">D − 1 2ÃD − 1 2 X (l)</formula><p>t can be intuitively interpreted as an approximate spatial mean feature aggregation from the messages passed by the direct neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Human Pose Extraction</head><p>For the feature extraction from the raw input images we estimate the human pose in each frame. The pose estimation or simply a keypoint detection aims to detect the locations of N keypoints (e.g., shoulder, hip, knee, etc.) from an image I ∈ R W ×H×3 . The SOTA method <ref type="bibr" target="#b6">[7]</ref> solve this problem by estimating N heatmaps</p><formula xml:id="formula_6">{H 1 , H 2 , . . . , H N } of size W ′ × H ′ ,</formula><p>where the heatmap H n indicates the location of the n-th keypoint. The location of the maximum of these heatmaps H n yields the location of the keypoint v n that define the edges V.</p><p>In our approach we use HRNet [15] 2 as a 2D human pose estimator. We use the provided network, which is pre-trained on the COCO dataset <ref type="bibr" target="#b15">[16]</ref>. The COCO dataset pose annotations consist of 17 keypoints. There is no provided set of bones or edges E but we use a commonly used configuration as shown in the last row of <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network and Implementation Details</head><p>The network's main architecture follows the design proposed as the ResGCN in <ref type="bibr" target="#b8">[9]</ref> with adaptions to our use case. The network is composed of ResGCN blocks. The block consists of a Graph Convolution followed by a classic 2D Convolution in the temporal domain and a residual connection with an optional bottleneck structure. The network is then composed of multiple ResGCN blocks in sequence (see Tab. 1 for detailed configuration), followed by an average pooling and a fully connected layer that is yielding the feature vector. As the loss function, we use supervised contrastive (SupCon) loss <ref type="bibr" target="#b16">[17]</ref>. Augmentation. For augmentation on the skeleton graph, we use multiple unique augmentation techniques. First, we flip the order of the sequence, which can be interpreted as the person walking backward. Secondly, we mirror the skeleton graph along a vertical axis through the graph's center of gravity. This augmentation causes the person to walk in the opposite direction. Furthermore, we add small Gaussian noise to each joint and the same joint in the sequence to make our network more robust to the pose estimation's inaccuracies.</p><p>Testing. At testing, the distance between gallery and probe is defined as the Euclidean distance of the corresponding feature vectors. Besides, we feed the original and a flipped order sequence to the network and take the average of two feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this part, we compare GaitGraph to other SOTA methods in public gait dataset CASIA-B <ref type="bibr" target="#b17">[18]</ref>. We compare the performance on multiple views and multiple walking conditions with model-based and appearance-based methods and conduct ablation studies to evaluate our temporal and spatial modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Training Details</head><p>Most available gait datasets do not provide RGB images since they are tailored to gait methods that rely on silhouettes or GEIs. Therefore we cannot evaluate on the largest public gait dataset OU-MVLP <ref type="bibr" target="#b18">[19]</ref>, the evaluation on the commonly used dataset CASIA-B provides a comparison with other methods. CASIA-B <ref type="bibr" target="#b17">[18]</ref> is a widely used gait dataset and composed of 124 subjects. For each of the 124 subjects the dataset contains 11 views (0°, 18°, . . . , 180°) and 3 waking conditions. The walking conditions are normal (NM) (6 sequences per subject), walking with a bag (BG) (2 sequences per subject), and wearing a coat or a jacket (CL) (2 sequences per subject). Summed up, each subject contains 11 × (6 + 2 + 2) = 110 sequences.</p><p>Since there is no official partition of training and test set, there are various experiment protocols <ref type="bibr" target="#b19">[20]</ref>. For a fair comparison, this paper follows the popular protocol by <ref type="bibr" target="#b9">[10]</ref>. Furthermore, we use the commonly called large-sample training (LT) partition. In LT, the first 74 subjects comprise the training set, whereas the remaining 50 subjects form the test  set. In the test sets of all three settings, the first four sequences of the NM condition (NM #1-4) are kept in the gallery, and the remaining six sequences are divided into three probe subsets, i.e., NM subsets containing NM #5-6, BG subsets containing BG #1-2 and CL subsets containing CL #1-2.</p><p>Training Details. The pose sequence is partitioned as a graph using the spatial configuration as mentioned in <ref type="bibr" target="#b7">[8]</ref> with a sequence length T = 60 frames. Adam optimizer is used with a 1-cycle learning rate <ref type="bibr" target="#b20">[21]</ref> and a weight decay penalty of 1e-5. For the first cycle, the maximum learning rate is set to 0.01 for 300 epochs, and for the second cycle, the maximum learning rate is 1e-5 for 100 epochs. The loss function's temperature is set to 0.01, and the batch size is 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Art Methods</head><p>Tab 2 shows the comparison of GaitGraph to PoseGait <ref type="bibr" target="#b4">[5]</ref>, which represents the sole pose-based approach to gait recognition utilizing handcrafted pose features. Our approach indicates significant improvements throughout all cross-views and walking conditions. With both approaches using a similar performing pose extractor, this proves the superiority of our GCN architecture as a feature extractor. The currently best performing models use appearancebased features. In Tab 3, we compare the appearance-based and model-based methods with our approach. The first three methods all use explicitly <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> or implicitly <ref type="bibr" target="#b1">[2]</ref> silhouette images as their feature representation. Notably, with our lower dimension feature representation, we can still archive competitive results against these appearance-based methods. Furthermore, our approach shows a high ability to model temporal features as shown in Tab 4. When trained with sorted sequences and tested with shuffled sequences (row c), the performance drops profoundly. As a comparison, the same ablation study was conducted by GaitPart <ref type="bibr" target="#b2">[3]</ref>, with only a slight drop in performance from row b to c. These results further support our claim of bringing back real temporal features to gait recognition. Tab 4 also illustrates the spatial modeling abilities in row a. Despite the missing temporal and appearance information, the network is still able to learn appearance-invariant features of the person's underlying physic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we present a novel approach to interpret gait as a sequence of skeleton graphs. Thus, GaitGraph is proposed, which uses a human pose estimator to extract the 2D skeleton pose, and extract the gait information considering the inherent graph structure of the skeleton. Furthermore, experiments conducted on the well-known database CASIA-B <ref type="bibr" target="#b17">[18]</ref> show SOTA results in model-based gait recognition and competitive results against appearance-based methods in gait recognition. Our spatial-temporal ablations proves our claim to bring back true temporal gait features instead of mostly relying on the appearance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Comparison of different gait representations of a subject in the CASIA-B gait dataset at different timesteps. Each row depicts the same frames as RGB image, silhouette image, and 2D skeleton pose, respectively, from top-to-bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the Pipeline. Starting with a sequence of images, for each image a pose is estimated. The sequence of poses is then feed through the ResGCN yielding the feature embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>nodes representing joints, and E is the set of edges representing bones captured by an adjacency matrix</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overview of the ResGCN-N39-R8 network architecture for a pose with 17 joints and sequence length of 60.</figDesc><table><row><cell>Block</cell><cell>Module</cell><cell>Output Dimensions</cell></row><row><cell cols="2">Block 0 BatchNorm</cell><cell>60 × 17 × 3</cell></row><row><cell></cell><cell>Basic</cell><cell>60 × 17 × 64</cell></row><row><cell>Block 1</cell><cell>Bottleneck</cell><cell>60 × 17 × 64</cell></row><row><cell></cell><cell>Bottleneck</cell><cell>60 × 17 × 32</cell></row><row><cell></cell><cell>Bottleneck</cell><cell>30 × 17 × 128</cell></row><row><cell>Block 2</cell><cell>Bottleneck Bottleneck</cell><cell>30 × 17 × 128 15 × 17 × 256</cell></row><row><cell></cell><cell>Bottleneck</cell><cell>15 × 17 × 256</cell></row><row><cell>Block 3</cell><cell>AvgPool2D FCN</cell><cell>1 × 256 1 × 128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Averaged Rank-1 accuracies in percent on CASIA-B per probe angle excluding identical-view cases compared with other model-based methods.</figDesc><table><row><cell cols="2">Gallery NM#1-4 Probe</cell><cell>0°18°36°54°72°90°108°126°144°162°180°N</cell><cell>0°-180°m ean</cell></row><row><cell>M#5-6</cell><cell cols="3">PoseGait [6] 49.7 61.6 67.0 66.7 60.8 59.0 62.5 61.4 67.3 62.0 47.5 GaitGraph 85.3 88.5 91.0 92.5 87.2 86.5 88.4 89.2 87.9 85.9 81.9</cell><cell>60.5 87.7</cell></row><row><cell>BG#1-2</cell><cell cols="3">PoseGait [6] 32.6 42.2 45.3 44.6 41.9 41.5 39.7 41.0 42.5 37.3 27.6 GaitGraph 75.8 76.7 75.9 76.1 71.4 73.9 78.0 74.7 75.4 75.4 69.2</cell><cell>39.6 74.8</cell></row><row><cell>CL#1-2</cell><cell cols="3">PoseGait [6] 20.6 24.0 33.5 33.5 32.7 30.5 36.0 36.1 33.8 27.6 19.0 GaitGraph 69.6 66.1 68.8 67.2 64.5 62.0 69.5 65.6 65.7 66.1 64.3</cell><cell>29.8 66.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Averaged</figDesc><table><row><cell></cell><cell cols="4">Rank-1 accuracies in percent on CASIA-</cell></row><row><cell cols="5">B comparison with both appearance-based and model-based</cell></row><row><cell>methods.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Probe</cell></row><row><cell cols="2">Type Method</cell><cell>NM</cell><cell>BG</cell><cell>CL</cell></row><row><cell>appearance -based</cell><cell>GaitNet [1] GaitSet [2] GaitPart [3]</cell><cell cols="3">91.6 85.7 58.9 95.0 87.2 70.4 96.2 91.5 78.7</cell></row><row><cell>model</cell><cell cols="4">PoseGait [6] 60.5 39.6 29.8</cell></row><row><cell>-based</cell><cell>GaitGraph</cell><cell cols="3">87.7 74.8 66.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Spatio-temporal Study. Control Condition: shuffle/sort the input sequence at train/test phase. Results are rank-1 accuracies on CASIA-B averaged in percent.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">GaitGraph</cell><cell>GaitPart [3]</cell></row><row><cell></cell><cell>Train</cell><cell>Test</cell><cell>NM BG</cell><cell cols="2">CL NM BG</cell><cell>CL</cell></row><row><cell cols="2">a Shuffle</cell><cell>Sort</cell><cell cols="3">47.3 36.9 26.9 95.6 89.9 71.5</cell></row><row><cell>b</cell><cell>Sort</cell><cell>Sort</cell><cell cols="3">87.7 74.8 66.3 96.2 91.5 78.7</cell></row><row><cell>c</cell><cell>Sort</cell><cell cols="4">Shuffle 26.4 22.0 16.7 92.5 85.8 65.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">github.com/HRNet/HRNet-Human-Pose-Estimation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GaitNet: An end-to-end network for gait based human identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106988</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GaitSet: Regarding gait as a set for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8126" to="8133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">GaitPart: Temporal part-based model for gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjie</forename><surname>Chao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunshui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Silhouette analysis-based gait recognition for human identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1505" to="1518" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pose-based temporal-spatial network (ptsn) for gait recognition with carrying and clothing variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rijun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunshui</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Edel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A model-based gait recognition method with body pose and human prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rijun</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">107069</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Graph Convolutional Baseline for Skeleton-Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stronger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">More</forename><surname>Faster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Explainable</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1625" to="1633" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comprehensive study on cross-view gait based human identification with deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-based feature extraction for gait analysis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Bouchrika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark S Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision / Computer Graphics Collaboration Techniques and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="150" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Individual recognition using gait energy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bir</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="316" to="322" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-view gait recognition using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Babaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="4165" to="4169" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pose-based deep gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="134" to="143" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Supervised contrastive learning,&quot; 2020</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoliang</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR. IEEE</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="441" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-view large population gait dataset and its performance evaluation for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daigo</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomio</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasushi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ Transactions on Computer Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gait recognition via disentangled representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousef</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4710" to="4719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Superconvergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
