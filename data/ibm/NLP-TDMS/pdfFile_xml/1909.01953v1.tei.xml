<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixture Content Selection for Diverse Sequence Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Clova AI</orgName>
								<orgName type="institution" key="instit2">NAVER</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
							<email>minjoon@cs.washington.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Clova AI</orgName>
								<orgName type="institution" key="instit2">NAVER</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mixture Content Selection for Diverse Sequence Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one-to-many relationships between source and the target sequences. We present a method to explicitly separate diversification from generation using a general plug-and-play module (called SELECTOR) that wraps around and guides an existing encoder-decoder model. The diversification stage uses a mixture of experts to sample different binary masks on the source sequence for diverse content selection. The generation stage uses a standard encoder-decoder model given each selected content from the source sequence. Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask, we leverage a proxy for ground-truth mask and adopt stochastic hard-EM for training. In question generation (SQuAD) and abstractive summarization (CNN-DM), our method demonstrates significant improvements in accuracy, diversity and training efficiency, including state-of-the-art top-1 accuracy in both datasets, 6% gain in top-5 accuracy, and 3.7 times faster training over a state-of-the-art model. Our code is publicly available at https://github.com/ clovaai/FocusSeq2Seq.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating target sequences given a source sequence has applications in a wide range of problems in NLP with different types of relationships between the source and target sequences. For instance, paraphrasing or machine translation exhibit a one-to-one relationship because the source and the target should carry the same meaning. On the other hand, summarization or question generation exhibit one-to-many relationships because * Most work done during internship at Clova AI.</p><p>Source Passage: in december 1878 , tesla left graz and severed all relations with his family to hide the fact that he dropped out of school . Target: what did tesla do in december 1878?</p><p>Focus 1: in december 1878 , tesla left graz and severed all relations with his family to hide the fact that he dropped out of school. (Ours) ⇒ what did tesla do?</p><p>Focus 2: in december 1878 , tesla left graz and severed all relations with his family to hide the fact that he dropped out of school. (Ours) ⇒ what did tesla do in december 1878?</p><p>Focus 3: in december 1878 , tesla left graz and severed all relations with his family to hide the fact that he dropped out of school . (Ours) ⇒ what did tesla do to hide he dropped out of school? <ref type="figure">Figure 1</ref>: Sample questions produced by our method from given passage-answer pair (answer is underlined). Our method generates diverse questions, by selecting different tokens to focus (colored) in contrast to 3mixture decoder <ref type="bibr" target="#b46">(Shen et al., 2019)</ref> that generates 3 identical questions: "what did tesla do to hide the fact that he dropped out of school?". a single source often results in diverse target sequences with different semantics. <ref type="figure">Fig. 1</ref> shows different questions that can be generated from a given passage.</p><p>Encoder-decoder models <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms <ref type="bibr" target="#b8">(Fan et al., 2018;</ref><ref type="bibr" target="#b49">Vijayakumar et al., 2018)</ref> or mixture of decoders <ref type="bibr" target="#b46">Shen et al., 2019)</ref>. These methods promote diversity at the decoding step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences.</p><p>In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversification stage leverages content selection to map the source to multiple sequences, where each mapping is modeled by focusing on different tokens in the source (oneto-many mapping). The generation stage uses a standard encoder-decoder model to generate a target sequence given each selected content from the source (one-to-one mapping). We present a generic module called SELECTOR that is specialized for diversification. This module can be used as a plug-and-play to an arbitrary encoder-decoder model for generation without architecture change.</p><p>The SELECTOR module leverages a mixture of experts <ref type="bibr" target="#b17">(Jacobs et al., 1991;</ref><ref type="bibr" target="#b7">Eigen et al., 2014)</ref> to identify diverse key contents to focus on during generation. Each mixture samples a sequential latent variable modeled as a binary mask on every source sequence token. Then an encoder-decoder model generates multiple target sequences given these binary masks along with the original source tokens. Due to the non-differentiable nature of discrete sampling, we adopt stochastic hard-EM for training SELECTOR. To mitigate the lack of ground truth annotation for the mask (content selection), we use the overlap between the source and target sequences as a simple proxy for the ground-truth mask.</p><p>We experiment on question generation and abstractive summarization tasks and show that our method achieves the best trade-off between accuracy and diversity over previous models on SQuAD <ref type="bibr" target="#b40">(Rajpurkar et al., 2016)</ref> and <ref type="bibr">CNN-DM (Hermann et al., 2015;</ref><ref type="bibr" target="#b43">See et al., 2017)</ref> datasets. In particular, compared to the recently-introduced mixture decoder <ref type="bibr" target="#b46">(Shen et al., 2019)</ref> that also aims to diversify outputs by creating multiple decoders, our modular method not only demonstrates better accuracy and diversity, but also trains 3.7 times faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Diverse Search Algorithms Beam search, the most commonly used search algorithm for decoding, is known to produce samples that are short, contain repetitive phrases, and share majority of their tokens. Hence several methods are intro-duced to diversify search algorithms for decoding. <ref type="bibr" target="#b10">Graves (2013)</ref>; <ref type="bibr" target="#b4">Chorowski and Jaitly (2017)</ref> tune temperature hyperparameter in softmax function. <ref type="bibr" target="#b49">Vijayakumar et al. (2018)</ref>; <ref type="bibr" target="#b28">Li et al. (2016b)</ref> penalize similar samples during beam search in order to obtain diverse set of samples. <ref type="bibr" target="#b2">Cho (2016)</ref> adds random noise to RNN decoder hidden states. <ref type="bibr" target="#b8">Fan et al. (2018)</ref> sample tokens from top-k tokens at each decoding step. Our method is orthogonal to these search-based strategies, in that they diversify decoding while our method diversifies which content to be focused during encoding. Moreover, our empirical results show diversification with stochastic sampling hurts accuracy significantly.</p><p>Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) <ref type="bibr" target="#b17">(Jacobs et al., 1991;</ref><ref type="bibr" target="#b7">Eigen et al., 2014)</ref> to diversify decoding steps. <ref type="bibr" target="#b55">Yang et al. (2018)</ref> introduce soft mixture of softmax on top of the output layer of RNN language model. ; <ref type="bibr" target="#b46">Shen et al. (2019)</ref> introduce mixture of decoders with uniform mixing coefficient to improve diversity in machine translation. Among these, the closest to ours is the mixture decoder <ref type="bibr" target="#b46">(Shen et al., 2019)</ref> that also adopts hard-EM for training, where a minimum-loss predictor is assigned to each data point, which is also known as multiple choice learning <ref type="bibr" target="#b13">(Guzman-Rivera et al., 2012;</ref><ref type="bibr" target="#b25">Lee et al., 2016)</ref>. While <ref type="bibr" target="#b46">Shen et al. (2019)</ref> makes RNN decoder as a MoE, we make SELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly.</p><p>Variational Autoencoders Variational Autoencoders (VAE) (Kingma and Welling, 2013) are used for diverse generation in several tasks, such as language modeling (Bowman et al., 2016), machine translation <ref type="bibr" target="#b47">Su et al., 2018;</ref><ref type="bibr" target="#b45">Shankar and Sarawagi, 2019)</ref>, and conversation modeling <ref type="bibr" target="#b44">(Serban et al., 2017;</ref><ref type="bibr" target="#b52">Wen et al., 2017;</ref><ref type="bibr" target="#b58">Zhao et al., 2017;</ref><ref type="bibr" target="#b36">Park et al., 2018;</ref><ref type="bibr" target="#b51">Wen and Luong, 2018;</ref><ref type="bibr" target="#b11">Gu et al., 2019)</ref>. These methods sample diverse latent variables from an approximate posterior distribution, but often suffer from a posterior collapse where the sampled latent variables are ignored <ref type="bibr" target="#b0">(Bowman et al., 2016;</ref><ref type="bibr" target="#b36">Park et al., 2018;</ref><ref type="bibr" target="#b22">Kim et al., 2018b</ref>; Figure 2: Overview of diverse sequence-to-sequence generation methods. (a) refers to our two-stage approach described throughout Section. 3, (b) refers to search-based methods <ref type="bibr" target="#b49">(Vijayakumar et al., 2018;</ref><ref type="bibr" target="#b28">Li et al., 2016b;</ref><ref type="bibr" target="#b8">Fan et al., 2018)</ref>, and (c) refers to mixture decoders <ref type="bibr" target="#b46">(Shen et al., 2019;</ref>. <ref type="bibr" target="#b6">Dieng et al., 2018;</ref><ref type="bibr" target="#b54">Xu and Durrett, 2018;</ref><ref type="bibr" target="#b14">He et al., 2019;</ref><ref type="bibr" target="#b41">Razavi et al., 2019)</ref>. This is also observed in our initial experiments and <ref type="bibr" target="#b46">Shen et al. (2019)</ref>, where MoE-based methods significantly outperforms VAE-based methods because of the posterior collapse. Moreover, we observe that sampling mixtures makes training more stable compared to stochastic sampling of latent variables. Furthermore, our latent structure as a sequence of binary variables is different from most VAEs which use a fixed-size continuous latent variable. This gives a finer-grained control and interpretability on where to focus, especially when source sequence is long.</p><p>Diversity-Promoting Regularization Adding regularization to objective functions is used to diversify generation. <ref type="bibr" target="#b27">Li et al. (2016a)</ref> introduce a term maximizing mutual information between source and target sentences. <ref type="bibr" target="#b4">Chorowski and Jaitly (2017)</ref>; <ref type="bibr" target="#b19">Kalyan et al. (2018)</ref> introduce terms enforcing knowledge transfer among similar annotations. Our work is orthogonal to these methods and can potentially benefit from adding these regularization terms to our objective function.</p><p>Content Selection in NLP Selecting important parts of the context has been an important step in NLP applications <ref type="bibr" target="#b42">(Reiter and Dale, 2000)</ref>.  <ref type="bibr" target="#b50">(Vinyals et al., 2015)</ref> for extracting key phrases for question generation and <ref type="bibr" target="#b9">Gehrmann et al. (2018)</ref> use content selector to limit copying probability for abstractive summarization. The main purpose of these approaches is to enhance accuracy, while our method uses diverse content selection to enhance both accuracy and diversity (refer to our empirical results). Additionally, our method allows models to learn how to utilize information from the selected content, whereas Gehrmann et al. (2018) manually limit the copying mechanism on non-selected contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this paper, we focus on sequence generation tasks such as question generation and summarization that have one-to-many relationship. More formally, given a source sequence x = (x 1 . . . x S ) ∈ X , our goal is to model a conditional multimodal distribution for the target sequence p(y|x) that assigns high values on p(y 1 |x) . . . p(y K |x) for K valid mappings x → y 1 . . . x → y K . For instance, <ref type="figure">Fig. 1</ref> illustrates K = 3 different valid questions generated for a given passage.</p><p>For learning a multimodal distribution, it is not appropriate to use a generic encoder-decoder <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> that minimizes for the expected value of the log probability of all the valid mappings.</p><p>This can lead to a suboptimal mapping that is in the middle of the targets but not near any of them. As a solution, we propose to (1) introduce a latent variable called focus that factorizes the distribution into two stages, select and generate (Section 3.1), and (2) independently train the factorized distributions (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Select and Generate</head><p>In order to factorize the multimodal distribution into the two stages (select and generate), we introduce a latent variable called focus. The intuition is that in the select stage we sample several meaningful focus, each of which indicates which part of the source sequence should be considered important. Then in the generate stage, each sampled focus biases the generation process towards being conditioned on the focused content. Formally, we model focus with a sequence of binary variable, each of which corresponds to each token in the input sequence, i.e. m = {m 1 . . . m S } ∈ {0, 1} S . The intuition is that m t = 1 indicates t-th source token x t should be focused during sequence generation. For instance, in <ref type="figure">Fig. 1</ref>, colored tokens (green, red, or blue) show that different tokens are focused (i.e. values are 1) for different focus samples (out of 3). We first use the latent variable m to factorize p(y|x),</p><formula xml:id="formula_0">p(y|x) = E m∼p φ (m|x) [p θ (y|m, x)]<label>(1)</label></formula><p>where p φ (m|x) is selector and p θ (y|x, m) is generator. The factorization separates focus selection from generation so that modeling multimodality (diverse outputs) can be solely handled in the select stage and generate stage can solely concentrate on the generation task itself. We now describe each component in more details.</p><p>Selector In order to directly control the diversity of the SELECTOR's outputs, we model it as a hard mixture of experts (hard-MoE) <ref type="bibr" target="#b17">(Jacobs et al., 1991;</ref><ref type="bibr" target="#b7">Eigen et al., 2014)</ref>, where each expert specializes in focusing on different parts of the source sequences. In <ref type="figure">Fig. 1</ref> and 2, focus produced by each SELECTOR expert is colored differently. We introduce a multinomial latent variable z ∈ Z, where Z = {1 . . . K}, and let each focus m be assigned to one of K experts with uniform prior p(z|x) = 1 K . With this mixture setting, p(m|x) is recovered as follows.</p><formula xml:id="formula_1">p(m|x) = E z∼p(z|x) [p φ (m|x, z)] = 1 K 1...K z p φ (m|x, z)<label>(2)</label></formula><p>We model SELECTOR with a single-layer Bidirectional Gated Recurrent Unit (Bi-GRU) <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> followed by two fully-connected layers and a Sigmoid activation. We feed (current hidden state h t , first and last hidden state h 1 , h S , and expert embedding e z ) to a fully-connected layers (FC). Expert embedding e z is unique for each expert and is trained from a random initialization. From our initial experiments, we found this parallel focus inference to be more effective than auto-regressive pointing mechanism <ref type="bibr" target="#b50">(Vinyals et al., 2015;</ref><ref type="bibr" target="#b48">Subramanian et al., 2018)</ref>. The distribution of the focus conditioned on the input x and the expert id z is the Bernoulli distribution of the resulting values,</p><formula xml:id="formula_2">(h 1 . . . h S ) = Bi-GRU(x) o z t = σ(FC([h t ; h 1 ; h S ; e z ])) p φ (m t |x, z) = Bernoulli(o z t )<label>(3)</label></formula><p>To prevent low quality experts from dying during training, we let experts share all parameters, except for the individual expert embedding e z . We also reuse the word embedding of the generator in the word embedding of SELECTOR to promote cooperative knowledge sharing between SELECTOR and generator. With these parameter sharing techniques, adding a mixture of SELECTOR experts increases only a slight amount of parameters (GRU, FC, and e z ) to sequence-to-sequence models.</p><p>Generator For maximum diversity, we sample one focus from each SELECTOR expert to approximate p φ (m|x). For a deterministic behavior, we threshold o z t with a hyperparameter th instead of sampling from the Bernoulli distribution. This gives us a list of focus m 1 . . . m K coming from K experts. Each focus m z = (m z 1 . . . m z S ) is encoded as embeddings and concatenated with the input embeddings of the source sequence x = (x 1 . . . x S ). An off-the-shelf generation function such as encoder-decoder can be used for modeling p(y|m, x), as long as it accepts a stream of input embeddings. We use an identical generation function with K different focus samples to produce K different diverse outputs. </p><formula xml:id="formula_3">D = {(x (i) , y (i) , m guide (i) )} N i=1 1 for i ∈ {1 . . . N } do / * Selector p φ (m|x, z) E-step * / 2 for z ∈ {1 . . . K} do 3 L (i) z select = − log p φ (m guide (i) |x (i) , z) 4 end 5 z best (i) = argmin z L (i) z select / * Selector p φ (m|x, z) M-step * / 6 φ z best (i) = φ z best (i) − α∇ φ z best (i) L (i) z best (i) select / * Generator p θ (y|x, m) Update * / 7 L (i) gen = − log p θ (y (i) |x (i) , m guide (i) ) 8 θ = θ − α∇ θ L (i) gen 9 end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Marginalizing the Bernoulli distribution in Eq. 3 by enumerating all possible focus is intractable since the cardinality of focus space 2 S grows exponentially with source sequence length S. Policy gradient <ref type="bibr" target="#b53">(Williams, 1992;</ref><ref type="bibr" target="#b56">Yu et al., 2017)</ref> or Gumbel-softmax <ref type="bibr" target="#b18">(Jang et al., 2017;</ref><ref type="bibr" target="#b30">Maddison et al., 2017)</ref> are often used to propagate gradients through a stochastic process, but we empirically found that these do not work well. We instead create focus guide and use it to independently and directly train the SELECTOR and the generator. Formally, a focus guide m guide = (m guide 1 . . . m guide S ) is a simple proxy of whether a source token is focused during generation. We set t-th focus guide m guide t to 1 if t-th source token x t is focused in target sequence y and 0 otherwise. During training, m guide acts as a target for SELECTOR and is a given input for generator (teacher forcing). During inference,m is sampled from SELECTOR and fed to the generator.</p><p>In question generation, we set m guide t to 1 if there is a target question token which shares the same word stem with passage token x t . Then we set m guide t to 0 if x t is a stop word or is inside the answer phrase. In abstractive summarization, we generate focus guide using copy target generation by <ref type="bibr" target="#b9">Gehrmann et al. (2018)</ref>, where they set source document token x t is copied if it is part of the longest possible subsequence that overlaps with the target summary.</p><p>Alg. 1 describes the overall training process, which first uses stochastic hard-EM <ref type="bibr" target="#b33">(Neal and Hinton, 1998;</ref><ref type="bibr" target="#b25">Lee et al., 2016)</ref> for training the SE-LECTOR and then the canonical MLE for training the generator. E-step (line 2-5 in Alg. 1) we sample focus from all experts and compute their losses − log p φ (m guide |x, z). Then we choose an expert z best with minimum loss.</p><formula xml:id="formula_4">z best = argmin z − log p φ (m guide |x, z) (4)</formula><p>M-step (line 6 in Alg. 1) we only update the parameters of the chosen expert z best .</p><p>Training Generator (line 7-8 in Alg.</p><p>1) The generator is independently trained using conventional teacher forcing, which minimizes − log p θ (y|x, m guide ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>We describe our experimental setup for question generation (Section 4.1) and abstractive summarization (Section 4.2). For both tasks, we use an off-the-shelf, task-specific encoder-decoder-based model for the generator and show how adding SE-LECTOR can help to diversify the output of an arbitrary generator. To evaluate the contribution of SELECTOR we additionally compare our method with previous diversity-promoting methods as the baseline (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Question Generation</head><p>Question generation is the task of generating a question from a passage-answer pair. Answers are given as a span in the passage (See <ref type="figure">Fig.1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We conduct experiments on SQuAD <ref type="bibr" target="#b40">(Rajpurkar et al., 2016)</ref> and use the same dataset split of <ref type="bibr" target="#b59">Zhou et al. (2017a)</ref>, resulting in 86,635, 8,965, and 8,964 source-target pairs for training, validation, and test, respectively. Both source passages and target questions are single sentences. The average length of source passage and target question are 32 and 11 tokens.</p><p>Generator We use NQG++ <ref type="bibr" target="#b59">(Zhou et al., 2017a)</ref> as the generator, which is an RNN-based encoderdecoder architecture with copying mechanism .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Abstractive Summarization</head><p>Abstractive summarization is the task of generating a summary sentence from a source document that consists of multiple sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We conduct experiments on the nonanonymized version of CNN-DM dataset <ref type="bibr" target="#b16">(Hermann et al., 2015;</ref><ref type="bibr" target="#b43">See et al., 2017)</ref>, whose training, validation, test splits have size of 287,113, 13,368, and 11,490 source-target pairs, respectively. The average length of the source documents and target summaries are 386 and 55 tokens. Following <ref type="bibr" target="#b43">See et al. (2017)</ref>, we truncate source and target sentences to 400 and 100 tokens during training.</p><p>Generator We use Pointer Generator (PG) <ref type="bibr" target="#b43">(See et al., 2017)</ref> as the generator for summarization, which also leverages RNN-based encoder-decoder architecture and copying mechanism, and uses coverage loss to avoid repetitive phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>For each task, we compare our method with other techniques which promote diversity at the decoding step. In particular, we compare with recent diverse search algorithms including Truncated Sampling <ref type="bibr" target="#b8">(Fan et al., 2018)</ref>, Diverse Beam Search <ref type="bibr" target="#b49">(Vijayakumar et al., 2018)</ref>, and Mixture Decoder <ref type="bibr" target="#b46">(Shen et al., 2019)</ref>. We implement these methods with NQG++ and PG. For each method, we generate K = (3 and 5) hypotheses from each source sequence. For search-based baselines <ref type="bibr" target="#b8">(Fan et al., 2018;</ref><ref type="bibr" target="#b49">Vijayakumar et al., 2018)</ref>, we select the top-k candidates after generation. For mixture models <ref type="bibr" target="#b46">(Shen et al. (2019)</ref> and ours), we conduct greedy decoding from each mixture for fair comparison with search-based methods in terms of speed/memory usage.</p><p>Beam Search This baseline keeps K hypotheses with highest log-probability scores at each decoding step.</p><p>Diverse Beam Search This baseline adds a diversity promoting term to log-probability when scoring hypotheses in beam search, Following Vijayakumar et al. <ref type="formula" target="#formula_0">(2018)</ref>, we use hamming diversity and diversity strength λ = 0.5.</p><p>Truncated Sampling This baseline randomly samples words from top-10 candidates of the distribution at the decoding step <ref type="bibr" target="#b8">(Fan et al., 2018)</ref>.</p><p>Mixture Decoder This baseline constructs a hard-MoE of K decoders with uniform mixing coefficient (referred as hMup in <ref type="bibr" target="#b46">Shen et al. (2019)</ref>) and conducts parallel greedy decoding. All decoders share all parameters but use different embeddings for start-of-sequence token.</p><p>Mixture Selector (Ours) We construct a hard-MoE of K SELECTORs with uniform mixing coefficient that infers K different focus from source sequence. Guided by K focus, generator conducts parallel greedy decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Metrics: Accuracy and Diversity</head><p>We use metrics introduced by previous works <ref type="bibr" target="#b34">(Ott et al., 2018;</ref><ref type="bibr" target="#b49">Vijayakumar et al., 2018;</ref><ref type="bibr" target="#b61">Zhu et al., 2018)</ref> to evaluate the diversity promoting approaches. These metrics are extensions over <ref type="bibr">BLEU-4 (Papineni et al., 2002)</ref> and ROUGE-2 F 1score <ref type="bibr" target="#b29">(Lin, 2004)</ref> and aim to evaluate the trade-off between accuracy and diversity.</p><p>Top-1 metric (⇑) This measures the Top-1 accuracy among the generated K-best hypotheses. The accuracy is measured using a corpus-level metric, i.e., BLEU-4 or ROUGE-2.</p><p>Oracle metric (⇑) This measures the quality of the target distribution coverage among the Top-K generated target sequences <ref type="bibr" target="#b34">(Ott et al., 2018;</ref><ref type="bibr" target="#b49">Vijayakumar et al., 2018)</ref>. Given an optimal ranking method (oracle), this metric measures the upper bound of Top-1 accuracy by comparing the best hypothesis with the target. Concretely, we generate hypotheses {ŷ 1 . . .ŷ K } from each source x and keep the hypothesisŷ best that achieves the best sentence-level metric with the target y. Then we calculate a corpus-level metric with the greedily-selected hypotheses {ŷ (i),best } N i=1 and references {y <ref type="bibr">(i)</ref> </p><formula xml:id="formula_5">} N i=1 .</formula><p>Pairwise metric (⇓) Referred as self- <ref type="bibr" target="#b61">(Zhu et al., 2018)</ref> or pairwise- <ref type="bibr" target="#b34">(Ott et al., 2018)</ref> metric, this measures the within-distribution similarity. This metric computes the average of sentencelevel metrics between all pairwise combinations of hypotheses {ŷ 1 . . .ŷ K } generated from each source sequence x. Low pairwise metric indicates high diversity between generated hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Human Evaluation Setup</head><p>We ask Amazon Mechanical Turkers (AMT) to compare our method with the baselines. For each method, we generate three questions / summaries from 100 passages sampled from SQuAD / CNN-DM test set. For every pair of methods, annotators are instructed to pick a set of questions / summaries that are more diverse. To evaluate accuracy, they see one question / summary selected out of 3 questions / summaries with highest logprobability from each method. They are instructed to select a question / summary that is more coherent with the source passage / document. Each annotator is asked to choose either a better method (resulting in "win" or "lose") or "tie" if their quality is indistinguishable. Diversity and accuracy evaluations are conducted separately, and every pair of methods are presented to 10 annotators 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Implementation details</head><p>For all experiments, we tie the weights <ref type="bibr" target="#b39">(Press and Wolf, 2017)</ref> of the encoder embedding, the decoder embedding, and the decoder output layers. This significantly reduces the number of parameters and training time until convergence. We train up to 20 epochs and select the checkpoint with the best oracle metric. We use Adam (Kingma and Ba, 2015) optimizer with learning rate 0.001 and momentum parmeters β 1 = 0.9 and β 2 = 0.999. Minibatch size is 64 and 32 for question generation and abstractive summarization. All models are implemented in PyTorch <ref type="bibr" target="#b37">(Paszke et al., 2017)</ref> and trained on single Tesla P40 GPU, based on NAVER Smart Machine Learning (NSML) platform <ref type="bibr">(Kim et al., 2018a)</ref>.</p><p>Question Generation Following Zhou et al.</p><p>(2017a), we use 256-dim hidden states for each direction of Bi-GRU encoder, 512-dim hidden states for GRU decoder, 300-dim word embedding initialized from GloVe <ref type="bibr" target="#b38">(Pennington et al., 2014)</ref>, vocabulary of 20,000 most frequent words, 16dim embeddings for three linguistic features (POS, NER and word case) respectively.</p><p>Abstractive Summarization Following <ref type="bibr" target="#b43">See et al. (2017)</ref>, we use 256-dim hidden states for each direction of Bi-LSTM encoder and LSTM decoder, 128-dim word embedding trained from scratch, and vocabulary of 50,000 most frequent words. Following <ref type="bibr" target="#b43">See et al. (2017)</ref>, we train our model to generate concatenation of target summaries and split it with periods.</p><p>SELECTOR The GRU has the same size as the generator encoder, and the dimension of expert embedding e z is 300 for NQG++, and 128 for PG. From simple grid search over [0.1, 0.5], we obtain focus binarization threshold th 0.15. The size of focus embedding for the generator is 16.  <ref type="bibr" target="#b46">(Shen et al., 2019)</ref>. <ref type="table" target="#tab_2">Tables 1 and 2</ref> show that pairwise similarity increases (diversity ⇓) when the number of mixtures increases for Mixture Decoder. While we observe a similar trend for SELECTOR in the question generation task, <ref type="table" target="#tab_2">Table 2</ref> shows the opposite   <ref type="bibr" target="#b43">See et al. (2017)</ref>, and the rest are from our experiments using PG as a generator. Method prefixes are the numbers of generated summaries for each document. Best scores are bolded. effect in the summarization task i.e., pairwise similarity decreases (diversity ⇑) for SELECTOR. The abstractive summarization task on CNN-DM has a target distribution with more modalities than question generation task on SQuAD, which is more difficult to model. We speculate that our SELECTOR improves accuracy by focusing on more modes of the output distribution (diversity ⇑), whereas Mixture Decoder tries to improve the accuracy by concentrating on fewer modalities of the output distribution (diversity ⇓).</p><p>Upper Bound Performance The bottom rows of <ref type="table" target="#tab_2">Tables 1 and 2</ref> show the upper bound performance of SELECTOR by feeding focus guide to generator during test time. In particular, we assume that the mask is the ground truth overlap between input and target sequences at test time. The gap between the oracle metric (top-k accuracy) and the upper bound is very small for question generation. This indicates that the top-k masks for question generation include the ground truth mask. Future work involves improving the content selection stage for the summarization task.  Comparison with State-of-the-art <ref type="table" target="#tab_6">Table 4</ref> compares the performance of SELECTOR with the state-of-the-art bottom-up content selection of <ref type="bibr" target="#b9">Gehrmann et al. (2018)</ref> in abstractive summarization. SELECTOR passes focus embeddings at the decoding step, whereas the bottom-up selection method only uses the masked words for the copy mechanism. We set K, the number of mixtures of SELECTOR, to 1 to directly compare it with the previous work (Bottom-Up <ref type="bibr" target="#b9">(Gehrmann et al., 2018)</ref>). We observe that SELECTOR not only outperforms Bottom-Up in every metric, but also achieves a new state-of-the-art ROUGE-1 and ROUGE-L on CNN-DM. Moreover, our method scores state-of-the-art BLEU-4 in question generation on SQuAD <ref type="table">(Table 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>R-1 R-2 R-L PG <ref type="bibr" target="#b43">(See et al., 2017)</ref> 39.53 17.28 36.38 Bottom-Up <ref type="bibr" target="#b9">(Gehrmann et al., 2018)</ref>    <ref type="bibr" target="#b29">(Lin, 2004)</ref> Efficient Training <ref type="table" target="#tab_7">Table 5</ref> shows that SELEC-TOR trains up to 3.7 times faster than mixture decoder <ref type="bibr" target="#b46">(Shen et al., 2019)</ref>. Training time of mixture decoder linearly increases with the number of decoders, while parallel focus inference of SELEC-TOR makes additional training time negligible.</p><p>Qualitative Analysis To analyze how the generator uses the selected content, we visualize attention heatmap of NQG++ in <ref type="figure" target="#fig_3">Fig.3</ref> for question generation. The figure shows different attention fo rm ed in no ve m be r 19 90 by th e eq ua l m er ge r of sk y te le vi si on an d br iti sh sa te lli te br oa dc as tin g, bs ky b be ca m e th e uk 's la rg es t di gi ta l su bs cr ip tio n te le vi si on co m pa ny <ref type="table">.   who  was  the  merger  of  sky  television  and  british  satellite  broadcasting  ?</ref> fo rm ed in no ve m be r 19 90 by th e eq ua l m er ge r of sk y te le vi si on an d br iti sh sa te lli te br oa dc as tin g, bs ky b be ca m e th e uk 's la rg es t di gi ta l su bs cr ip tio n te le vi si on co m pa ny .</p><p>what team won the uk ?</p><p>fo rm ed in no ve m be r 19 90 by th e eq ua l m er ge r of sk y te le vi si on an d br iti sh sa te lli te br oa dc as tin g, bs ky b be ca m e th e uk 's la rg es t di gi ta l su bs cr ip tio n te le vi si on co m pa ny .  mechanisms depending on three different focuses inferred by different SELECTOR experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce a novel diverse sequence generation method via proposing a content selection module, SELECTOR. Built upon mixture of experts and hard-EM training, SELECTOR identifies different key parts on source sequence to guide generator to output a diverse set of sequences.</p><p>SELECTOR is a generic plug-and-play module that can be added to an existing encoder-decoder model to enforce diversity with a negligible additional computational cost. We empirically demonstrate that our method improves both accuracy and diversity and reduces training time significantly compared to baselines in question generation and abstractive summarization. Future work involves incorporating SELECTOR for other generation tasks such as diverse image captioning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Most recently, Ke et al. (2018); Min et al. (2018) conduct soft-/ hard-selection of key parts from source passages for question answering. Zhou et al. (2017b) use soft gating on the source document encoder for abstractive summarization. Li et al. (2018) guide abstractive summarization models with off-the-shelf keyword extractors. The most relevant work to ours are Subramanian et al. (2018) and Gehrmann et al. (2018). Subramanian et al. (2018) use a pointer network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Dataset size, K: Number of mixtures) Data:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Attention heatmap of NQG++ decoder with three different focus by SELECTOR. This shows focus can guide generator to generate different sequences. Passage tokens are colored when corresponding focus is 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Summarization results: Comparison of di-</cell></row><row><cell>verse generation methods on CNN-DM. The score of</cell></row><row><cell>PG (top row) is from</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3a</head><label>3a</label><figDesc>31.3 19.0 43.9 36.9 19.2 vs. 3-T. Sampling 46.7 35.1 18.2 45.3 36.1 18.6 vs. 3-M. Decoder 47.6 32.5 19.9 41.8 36.0 22.2</figDesc><table><row><cell></cell><cell>Diversity (%)</cell><cell>Accuracy (%)</cell></row><row><cell>Baselines</cell><cell>Win Lose Tie</cell><cell>Win Lose Tie</cell></row><row><cell cols="3">vs. 3-D. Beam 49.7 (a) SQuAD question generation</cell></row><row><cell></cell><cell>Diversity (%)</cell><cell>Accuracy (%)</cell></row><row><cell>Baselines</cell><cell>Win Lose Tie</cell><cell>Win Lose Tie</cell></row><row><cell>vs. 3-D. Beam</cell><cell>50.4 40.9 8.7</cell><cell>46.2 38.5 15.3</cell></row><row><cell cols="2">vs. 3-T. Sampling 48.7 42.0 9.3</cell><cell>50.3 41.2 8.5</cell></row><row><cell cols="3">vs. 3-M. Decoder 49.7 39.6 10.7 46.5 37.5 16.0</cell></row><row><cell cols="3">(b) CNN-DM abstractive summarization</cell></row><row><cell>and 3b show the</cell><cell></cell><cell></cell></row><row><cell>human evaluation in two tasks of questions gen-</cell><cell></cell><cell></cell></row><row><cell>eration and summarization, comparing sequences</cell><cell></cell><cell></cell></row><row><cell>generated by SELECTOR with the diversity-</cell><cell></cell><cell></cell></row><row><cell>promoting baselines: Diverse Beam, Truncated</cell><cell></cell><cell></cell></row><row><cell>Sampling and Mixture Decoder. The table shows</cell><cell></cell><cell></cell></row><row><cell>that our method significantly outperforms all three</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation results baselines in terms of both diversity and accuracy with statistical significance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of single-expert selector with state-of-the-art abstractive summarization methods on CNN-DM. R stands for ROUGE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Training time: Comparison of training time on CNN-DM. See 4.6 for implementation details.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We conducted 12 human evaluations in total: 2 tasks x 3 baselines x 2 criteria. SeeTable 3a and 3b.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research was supported by Allen Distinguished Investigator Award, the Office of Naval Research under the MURI grant N00014-18-1-2670, Samsung GRO, and gifts from Allen Institute for AI and Google. We also thank the members of Clova AI, UW NLP, and the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<editor>M. Dai, Rafal Jozefowicz, and Samy Bengio</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Communicating Agents for Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1150</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent Alignment and Variational Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.1177/096228029400300304</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Avoiding Latent Variable Collapse with Generative Skip Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Factored Representations in a Deep Mixture of Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical Neural Story Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bottom-Up Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generating Sequences With Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DialogWAE : Multimodal Response Generation with Conditional Wasserstein Auto-Encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointing the Unknown Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1014</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple Choice Learning: Learning to Produce Multiple Structured Outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abner</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lagging Inference Networks and Posterior Collapse in Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Spokoyny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sequence to Sequence Mixture Model for Diverse Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive Mixtures of Local Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Focused Hierarchical RNNs for Conditional Sequence Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Laurent Charlin, and Chris Pal</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Nako Sung, and Jung-Woo Ha. 2018a. NSML: Meet the MLaaS platform with a real-world case study</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-Amortized Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1051/0004-6361/201527329</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Guiding Generation for Abstractive Text Summarization based on Key Information Guide Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Diversity-Promoting Objective Function for Neural Conversation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A Simple, Fast Diverse Decoding Algorithm for Neural Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient and Robust Question Answering from Minimal Context over Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization using Sequence-tosequence RNNs and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A View of the EM Algorithm that Justifies Incremental, Sparse, and other Variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Learning in Graphical Models</title>
		<imprint>
			<biblScope unit="page" from="355" to="368" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Analyzing Uncertainty in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BLEU: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wj</forename><surname>Wei-Jing Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Hierarchical Latent Structure for Variational Conversation Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yookoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1162</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using the Output Embedding to Improve Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Preventing Posterior Collapse with δ-VAEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Building Natural Language Generation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Posterior Attention Models for Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mixture Models for Diverse Machine Translation: Tricks of the Trade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Variational Recurrent Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural Models for Key Phrase Extraction and Question Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Diverse Beam Search for Improved Description of Complex Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasath</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Pointer Networks. In NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luong</surname></persName>
		</author>
		<title level="m">Latent Topic Conversational Models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Latent Intention Dialogue Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Tsung Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1022672621406</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spherical Latent Spaces for Stable Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Variational Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Neural Question Generation from Text: A Preliminary Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Selective Encoding for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1101</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Texygen: A Benchmarking Platform for Text Generation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210080</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
