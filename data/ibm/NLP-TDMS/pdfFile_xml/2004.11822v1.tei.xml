<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Pose Estimation using Spatio-Temporal Networks with Explicit Occlusion Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Game AI Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
							<email>bohawkwang@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Game AI Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
							<email>robby.tan@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Yale-NUS College</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D Human Pose Estimation using Spatio-Temporal Networks with Explicit Occlusion Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 3D poses from a monocular video is still a challenging task, despite the significant progress that has been made in the recent years. Generally, the performance of existing methods drops when the target person is too small/large, or the motion is too fast/slow relative to the scale and speed of the training data. Moreover, to our knowledge, many of these methods are not designed or trained under severe occlusion explicitly, making their performance on handling occlusion compromised. Addressing these problems, we introduce a spatio-temporal network for robust 3D human pose estimation. As humans in videos may appear in different scales and have various motion speeds, we apply multi-scale spatial features for 2D joints or keypoints prediction in each individual frame, and multi-stride temporal convolutional networks (TCNs) to estimate 3D joints or keypoints. Furthermore, we design a spatio-temporal discriminator based on body structures as well as limb motions to assess whether the predicted pose forms a valid pose and a valid movement. During training, we explicitly mask out some keypoints to simulate various occlusion cases, from minor to severe occlusion, so that our network can learn better and becomes robust to various degrees of occlusion. As there are limited 3D ground truth data, we further utilize 2D video data to inject a semisupervised learning capability to our network. Experiments on public data sets validate the effectiveness of our method, and our ablation studies show the strengths of our network's individual submodules.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>This paper focuses on 3D human pose estimation from a monocular RGB video. A 3D pose is defined as the 3D coordinates of pre-defined keypoints on humans, such as shoulder, pelvis, wrist, and etc. Recent top-down approaches <ref type="bibr" target="#b10">(Hossain and Little 2018;</ref><ref type="bibr" target="#b27">Wandt and Rosenhahn 2019;</ref><ref type="bibr" target="#b21">Pavllo et al. 2019;</ref><ref type="bibr" target="#b4">Cheng et al. 2019</ref>) have shown promising results, where spatial features from individual frames are extracted to detect a target person and estimate the 2D poses, and temporal context is used to produce consistent 3D predictions. However, we find that existing methods do not fully exploit the spatial and temporal information available in videos. As a result, they suffer from the problem of large variations in sizes and speeds of the target person in wild videos. In this paper, we address this problem. First, we consider multi-scale features both spatially and temporally to deal with persons at various distances with different speeds of motions. We use the High Resolution Network (HR-Net) <ref type="bibr" target="#b23">(Sun et al. 2019</ref>) which exploits multi-scale spatial features to produce one heat map for each keypoint. Unlike most previous works <ref type="bibr" target="#b20">(Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b21">Pavllo et al. 2019</ref>) that only use the peaks in the heat maps, we encode these maps into a latent space to incorporate more spatial information. Then, we apply temporal convolutional networks (TCNs) <ref type="bibr" target="#b21">(Pavllo et al. 2019)</ref> to these latent features with different strides, e.g., 1, 2, 4, and 8, and concatenate them together for prediction of the 3D poses. <ref type="figure" target="#fig_0">Figure 1</ref> shows some expamples of our results.</p><p>Moreover, to reduce the risk of invalid 3D poses, we also utilize a discriminator in our framework like many previ- : Illustration for our framework. We only show two different temporal strides for clarity purpose. KPTS is short for keypoints; KCS is Kinematic Chain Space; TKCS means Temporal KCS.</p><p>ous works <ref type="bibr" target="#b27">Wandt and Rosenhahn 2019;</ref><ref type="bibr" target="#b3">Chen et al. 2019;</ref><ref type="bibr" target="#b33">Zhang et al. 2019)</ref>. Different from these methods' single frame based discriminators, we check the pose validity spatio-temporally. Our main reasoning is that valid poses in individual frames do not necessarily constitute a valid sequence. We extend the spatial KCS (Kinematic Chain Space) <ref type="bibr" target="#b27">(Wandt and Rosenhahn 2019)</ref>, a successful single image descriptor for pose discriminator, and introduce a temporal KCS to represent motions of human joints. This temporal KCS descriptor is used by another TCN to check the validity of the estimated 3D pose sequence. Finally, in order to deal with occlusion, during the training of our TCNs, we mask out some keypoints or frames by setting the corresponding heat maps to zero, as shown in <ref type="figure">Figure  2</ref>. There are two types of works that are similar to ours. One is the partial occlusion modeling by setting coordinates of some keypoints to zero <ref type="bibr" target="#b4">(Cheng et al. 2019)</ref>. The other is human dynamics, which only handles occlusion that happens in the end of a temporal window, since it predicts several future frames from given past frames' information <ref type="bibr" target="#b33">Zhang et al. 2019</ref>). However, our approach can handle both partial and total occlusion cases in individual frames or in a sequence of frames. Hence, our method is more general in handling human 3D pose estimation under occlusion. Moreover, the occlusion module allows us to do semi-supervised learning that utilizes both 3D and 2D datasets.</p><p>As a summary, our contributions are as follows: • Incorporate multi-scale spatial and temporal features for robust pose estimation in video. • Introduce a spatio-temporal discriminator to regularize the validity of a pose sequence. • Perform diverse data augmentation for TCN to deal with different occlusion cases. Experiments on public datasets show the efficacy of our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Within the last few years, pose estimation has been undergoing rapid development with deep learning techniques <ref type="bibr" target="#b24">(Tompson et al. 2014;</ref><ref type="bibr" target="#b25">Toshev and Szegedy 2014;</ref><ref type="bibr" target="#b20">Newell, Yang, and Deng 2016;</ref><ref type="bibr" target="#b1">Cao et al. 2019;</ref><ref type="bibr" target="#b19">Mehta et al. 2017b)</ref>. Researchers keep pushing the frontier of this field from different angles via better utilizing spatial or temporal information, learning human dynamics, pose regularization, and semi-supervised/self-supervised learning.</p><p>To better utilize spatial information, some recent works focused on cross stage feature aggregation or multi-scale spatial feature fusion to maintain the high resolution in the feature maps <ref type="bibr" target="#b2">(Chen et al. 2018;</ref><ref type="bibr" target="#b23">Sun et al. 2019;</ref><ref type="bibr" target="#b13">Kanazawa et al. 2019)</ref>. Although this helps to improve the 2D estimators, there is an inherent ambiguity for inferring 3D human structure from a single 2D image. To overcome this limitation, some researchers further utilized temporal information in video <ref type="bibr" target="#b21">(Pavllo et al. 2019;</ref><ref type="bibr" target="#b10">Hossain and Little 2018;</ref><ref type="bibr" target="#b4">Cheng et al. 2019;</ref><ref type="bibr" target="#b0">Bertasius et al. 2019)</ref>, and showed obvious improvement. However, their fixed temporal scales limit their performance on videos with different motion speeds from the ones in training.</p><p>To regularize predictions to be reasonable 3D human poses, pose discriminators have been proposed <ref type="bibr" target="#b27">Wandt and Rosenhahn 2019;</ref><ref type="bibr" target="#b3">Chen et al. 2019</ref>). These methods utilize the idea of Generative Adversarial Networks (GAN) to check whether the estimated 3D pose is consistent with the pose distribution in the ground truth data. However, most of existing works focused on determining if a given 3D human pose is reasonable. Combining a series of reasonable 3D poses together does not make the whole series a reasonable human motion trajectory. As a result, we propose temporal KCS which checks both the spatial and temporal validity of 3D poses.</p><p>To deal with partial occlusions, some techniques have been designed to recover occluded keypoints from unoc-cluded ones according to the spatial or temporal context <ref type="bibr" target="#b22">(Radwan, Dhall, and Goecke 2013;</ref><ref type="bibr" target="#b23">Rogez, Weinzaepfel, and Schmid 2017;</ref><ref type="bibr">de Bem et al. 2018;</ref><ref type="bibr" target="#b8">Guo and Dai 2018;</ref><ref type="bibr" target="#b4">Cheng et al. 2019)</ref> or scene constraints . Some methods further introduced the concept of "human dynamics" <ref type="bibr" target="#b33">Zhang et al. 2019)</ref>, which predicts future human poses according to single or multiple existing frames in a video without any future frames. In real scenarios, we may have full, partial, or total occlusion for individual or continuous frames. Therefore, we introduce a method to integrate these two categories of methods into one unified framework by explicitly performing occlusion augmentation for all these cases during training. Due to limited 3D human pose data, recent methods suggest to further utilize 2D human pose datasets in a semi-supervised or self-supervised fashion <ref type="bibr" target="#b27">(Wandt and Rosenhahn 2019;</ref><ref type="bibr" target="#b28">Wang et al. 2019;</ref><ref type="bibr" target="#b14">Kocabas, Karagoz, and Akbas 2019;</ref><ref type="bibr" target="#b3">Chen et al. 2019)</ref>. They project estimated 3D pose back to 2D image space so that 2D ground-truth can be used for loss computation. Such approaches reduce the risk of over-fitting on small amount of 3D data. We also adopt this method and combine it with our explicit occlusion augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Our method belongs to the top-down pose estimation category. Given an input video, we first detect and track the persons by any state-of-the-art detector and tracker, such as Mask R-CNN <ref type="bibr" target="#b9">(He et al. 2017)</ref> and PoseFlow <ref type="bibr" target="#b29">(Xiu et al. 2018</ref>). Subsequently, we perform the pose estimation for each person individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale Features for Pose Estimation</head><p>Given a series of bounding boxes for a person in a video, we first normalize the image within each bounding box to a pre-defined fixed size, e.g., 256 × 256, and then apply High Resolution Networks (HRNet) (Sun et al. 2019) to each normalized image patch to produce K heat maps, each of which indicates the possibility of certain human joint's location. The HRNet conducts repeated multi-scale fusions by exchanging the information across the parallel multi-scale subnetworks. Thus, the estimated heat maps incorporate spatial multi-scale features to provide more accurate 2D pose estimations.</p><p>We concatenate the K heat maps in each frame as a Kdimensional image m t , where t is the frame index, and apply an embedding network f E to produce a low dimensional representation as r t = f E (m t ). Such embedding incorporates more spatial information from the whole heat maps than only using maps' peaks as most previous works do. The effectiveness of the embedding is shown in the ablation study in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Given a sequence of heat map embeddings {r t }, we apply TCN to them. As human motions may be fast or slow, we consider multi-scale features in the temporal domain. As shown in <ref type="figure">Figure 2</ref>, we apply TCN with temporal strides of 1, 2, 3, 5, 7 and concatenate these features for the final pose estimation. Such multi-scale features in both spatial and tem-  poral domains enable our networks to deal with various scenarios. <ref type="figure" target="#fig_2">Figure 3</ref> shows an example video clip with fast motion of playing baseball. We observe that multi-scale TCN is able to produce more accurate results than single-scale TCN. We use both 3D dataset Human3.6M (Ionescu et al. 2014) and 2D dataset Penn Action <ref type="bibr" target="#b34">(Zhang, Zhu, and Derpanis 2013)</ref> for training. Human3.6M has multi-view captured videos and 3D ground-truths, while PENN only has 2D ground-truths for visible keypoints. For Human3.6M data, the 3D MSE loss is defined as:</p><formula xml:id="formula_0">L 3d = (X − X 3D ) 2 ,<label>(1)</label></formula><p>where X is our predicted 3D coordinates for all keypoints, and X 3D is the 3D ground truth. As Human3.6M data set provides videos from multiple views, we expect the 3D estimation results from different views should be the same after rotation alignment. So, we define the multi-view loss as:</p><formula xml:id="formula_1">L mv = (R v1→v2 X v1 − X v2 ) 2 ,<label>(2)</label></formula><p>where R v1→v2 is the rotation matrix from viewpoint 1 to viewpoint 2, and is precomputed from the ground-truth camera parameters. The X v1 and X v2 are the predicted 3D results in viewpoints 1 and 2.</p><p>For the 2D dataset, we project the 3D prediction to 2D space assuming orthogonal projection, and the 2D MSE loss is defined as:</p><formula xml:id="formula_2">L 2d = (Orth(X) − X 2D ) 2 ,<label>(3)</label></formula><p>where Orth(·) is the orthogonal projection operator, and X 2D is the 2D ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatio-Temporal KCS Pose Discriminator</head><p>To reduce the risk of generation of unreasonable 3D poses, we introduce a novel spatio-temporal discriminator to check the validity of a pose sequence, rather than just poses in individual frames like previous works <ref type="bibr" target="#b27">Wandt and Rosenhahn 2019;</ref><ref type="bibr" target="#b3">Chen et al. 2019)</ref>. Among all single frame discriminators, the Kinematic Chain Space (KCS) used in <ref type="bibr" target="#b27">(Wandt and Rosenhahn 2019)</ref> is one of the most effective methods. Each bone, defined as the connection between two neighboring human keypoints such as elbow and wrist, is represented as a 3D vector b m , indicating the direction from one keypoint to its neighbor. All such vectors form a 3 × M matrix B, where M is the predefined number of bones for a human structure. They use Ψ = B T B as the features for discriminator, where the diagonal elements in Ψ indicate the square of bone length and other elements represent the weighted angle between two bones as an inner production.</p><p>Inspired by their spatial KCS, we introduce a Temporal KCS (TKCS) defined as:</p><formula xml:id="formula_3">Φ = B T t+i B t+i − B T t B t .</formula><p>(4) where i is the temporal interval between the KCS. The diagonal elements in Φ indicates the bone length changes, and other elements denote the change of angles between two bones. <ref type="figure" target="#fig_3">Figure 4</ref> shows an example of two neighboring bones b 1 and b 2 . The spatial KCS measures the lengths of b 1 and b 2 as well as angles between them, θ 12 . The temporal KCS measures the bone length changes between two frames with temporal interval i, i.e., differences between b t 1 and b t+i 1 as well as b t 2 and b t+i 2 , and the angle change between neighboring bones, i.e., difference between θ t 12 and θ t+i 12 . We concatenate the spatial KCS, temporal KCS, and the predicted keypoint coordinates, and then feed them to a TCN to build a discriminator. Such approach not only considers whether a pose is valid in individual frames, but also checks the validity of transitions across frames. We follow the procedure in the standard GAN to train the discriminator, and use it to produce a regularization loss for our predicted poses as L gen .</p><p>In addition, to increase the robustness under different view angles, we introduce a rotational matrix as an augmentation to the generated 3D pose, as shown in the following equation:</p><p>L gen = L gen (RX), (5) where R is a rotational matrix Rotation(α, β, γ), and α, β, γ are rotational angles along x, y, and z axis, respectively. As the rotational angles along x and z angles should be smaller compared with rotations along y for normal human poses, in our experiments, β is randomly sampled from [−π, π] while α and γ are sampled from [−0.2π, 0.2π].</p><p>The overall loss function for our training is defined as  where w 1 , w 2 , w 3 are set to 0.5, 0.1, 0.01, respectively, and are fixed in all our experiments.</p><formula xml:id="formula_4">L = L 3d + w 1 L mv + w 2 L 2d + w 3 L gen ,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation for Occlusions</head><p>To make our approach capable of dealing with different occlusion cases, we perform data augmentation during the training.</p><p>We use random masking of keypoints to simulate the occluded condition. Three types of occlusion are applied in the training process. The first type is the frame-wise occlusion. Given a sequence of heatmaps produced by the 2D keypoint estimator, we randomly mask several frames by setting their heatmaps to zero, indicating that the whole frame is occluded or has low confidence. Second, the point-wise occlusion is applied by randomly setting certain keypoints' heatmaps to zero. This simulates the scenario that certain keypoints are occluded. Third, we apply area occlusion by setting a virtual occluder area. The heatmaps of keypoints located within this area are set to zero.</p><p>In addition, as the output of 2D pose estimator is not strictly Gaussian distribution, we introduce random noise to the heatmaps of the input sequence. To further improve the robustness under wrong detection cases, the points are randomly shifted or randomly swapped symmetrically. For example, the left knee is swapped with the right knee and the elbow point is sifted by 10 pixels. We expect the trained multi-scale TCN is able to recover the correct 3D pose using context information from partly wrong 2D estimations.</p><p>Note that, when the occlusion masks are all at the end of our TCN receptive field, it degrades to the human dynamics case, i.e., estimation of future poses without any future observation. Therefore, our framework is a more generalized approach for occlusion handling. We could predict human poses from temporal context information with or without meaningful observation in a few frames in the video clip. <ref type="figure" target="#fig_4">Figure 5</ref> demonstrates an example where occlusion augmentation helps to generate robust pose estimation results in a video clip where a target person is occluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experiment Settings</head><p>Data Sets. Human3.6M <ref type="bibr" target="#b10">(Ionescu et al. 2014</ref>) is a large 3D human pose dataset. It has 3.6 million images including eleven actors performing daily-life activities, and seven actors are annotated. The 3D ground-truth is provided by the mocap system, and the intrinsic/extrinsic camera parameters are known. Similar to some existing methods <ref type="bibr" target="#b10">(Hossain and Little 2018;</ref><ref type="bibr" target="#b21">Pavllo et al. 2019;</ref><ref type="bibr" target="#b20">Pavlakos, Zhou, and Daniilidis 2018;</ref><ref type="bibr" target="#b30">Yang et al. 2018)</ref>, we use subjects 1, 5, 6, 7, 8 for training, and the subjects 9 and 11 for evaluation.</p><p>HumanEva-I is a relatively smaller dataset. Following the typical protocol <ref type="bibr" target="#b10">Hossain and Little 2018;</ref><ref type="bibr" target="#b21">Pavllo et al. 2019)</ref>, we use the same data division to train one model for all three actions (Walk, Jog, Box), and use the remaining data for testing. MPI-INF-3DHP <ref type="bibr" target="#b18">(Mehta et al. 2017a</ref>) is a relatively new dataset that is captured in an indoor setting which is similar to the setting of Human3.6M. Following recent methods <ref type="bibr" target="#b12">(Kanazawa et al. 2018;</ref><ref type="bibr" target="#b20">Pavlakos, Zhou, and Daniilidis 2018;</ref><ref type="bibr" target="#b3">Chen et al. 2019</ref>) that report their performance on this dataset, we utilize this dataset for quantitative evaluation. 3DPW <ref type="bibr" target="#b26">(von Marcard et al. 2018</ref>) is a new dataset contains multi-person outdoor scenes. We use the testing set of 3DPW to perform quantitative evaluation following <ref type="bibr" target="#b13">Kanazawa et al. 2019)</ref>.</p><p>Evaluation protocols. We apply a few common evaluation protocols in our experiments. Protocol #1 refers to the Mean Per Joint Position Error (MPJPE) which is the millimeters between the ground-truth and the predicted keypoints. Protocol #2, often called P-MPJPE, refers to the same error after applying alignment between the predicted keypoints and the ground-truth. Percentage of Correct 3D Keypoints (3D PCK) under 150mm radius is used for quantitative evaluation for MPI-INF-3DHP following <ref type="bibr" target="#b18">(Mehta et al. 2017a</ref>). To compare with other human dynamics/pose forecasting methods, mean angle error (MAE) is used following <ref type="bibr" target="#b11">(Jain et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-Parameter Sensitivity Analysis</head><p>We conduct the sensitivity test of four hyper-parameters mentioned in this paper: embedding dimension for encoder, temporal length, temporal strides for TCN, and temporal interval for TKCS. The results are shown in <ref type="table">Table 1</ref>. We find the best parameter settings by fixing three and adjusting the other one. To focus on understanding the influence of each parameter, semi-supervised learning using extra 2D data is disabled here.</p><p>For the embedding dimension, we observe that within a reasonably large range, the performance is not affected significantly. The dimension 64 is insufficient and results in large error. Within the range 256 to 1024, the errors only differ 0.4mm, indicating that the model is insensitive to the setting of embedding dimension.</p><p>For the temporal length, we test the range from 8 to 128. We can observe a steady reduction of errors until saturation at 128. In addition, we adjust the temporal strides and  find out that by adding more strides, the performance is improved and finally reaches 41.2mm with 5 strides compared to 45.4mm for single stride. We also test different temporal intervals for TKCS and observe interval 1 produces the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>We conduct ablation studies to analyze each component of the proposed framework as shown in <ref type="table" target="#tab_3">Table 2</ref>. As the baseline, we build a TCN to regress the 3D keypoints' positions based solely on their 2D coordinates (x, y), which are obtained from the peaks in heatmaps from 2D pose detector. During TCN training, the 3D skeletons are also rotated along x, y, z axes as mentioned before. We use the standard MSE loss for the training. We then add the modules one-by-one to perform ablation studies, including heat maps embedding, multi-stride TCN, multi-view loss, spatial KCS, temporal KCS, and 2D data semi-supervised learning. We see that by adding more modules, the performance steadily improves, validating the effectiveness of our proposed modules. The largest improvements come from multi-stride TCN, spatial KCS, and tem-   poral KCS modules. Temporal multi-scale features increase the capability of the networks to deal with videos with different speeds of motions. Although the spatial KCS constraints the pose validity at individual frames properly, our temporal KCS clearly further improves the performance, which demonstrates that checking the pose validity of a single frame itself is insufficient, and checking the validity of the temporal pose sequence is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head><p>The experiment results on Human 3.6M are shown in <ref type="table" target="#tab_5">Table 3 and Table 4</ref> for Protocol #1 and #2, respectively. The MPJPE is reduced by 2.8mm compared to previous work and yields an error reduction of 6.5% . The P-MPJPE is reduced by 2.1mm and obtained 6.4% error reduction. The performance on actions which already have low error rates is not improved significantly, but for those actions such as photo capturing and sitting down, the errors are reduced by &gt; 5mm. Since in these actions, occlusion happens frequently, more temporal information and effective pose regularization are needed for producing correct estimations. Considering existing methods almost get saturated on this dataset, our improvement is promising.</p><p>We also evaluate our model's potential on human dynamics which is targeted to predict several future frames' 3D skeleton. The performance is shown in <ref type="table">Table 5</ref>. Note that, <ref type="bibr" target="#b5">(Chiu et al. 2019</ref>) uses past 3D ground-truth keypoints as input for prediction, while our method does not use any ground-truth but takes images from video as input to esti-mate the keypoints first, and then predict the future 3D information, which is a more difficult task. Nevertheless, we still achieve similar performance compared with the stateof-the-art, which demonstrates the versatility of the proposed framework. Our method is not designed specifically for human dynamics prediction, but is a more generalized framework for pose estimation with or without observations (due to occlusion) in various scenarios. <ref type="table" target="#tab_9">Table 6</ref> shows our evaluation results on HumanEva-I dataset and an improvement of 0.8mm is achieved in average, which implies an error reduction of 5.6%. For MPI-INF-3DHP dataset, we use only our model trained on Hu-man3.6M dataset, but do not fine-tune or retrain on the 3DHP data set. Following existing methods <ref type="bibr" target="#b5">(Chiu et al. 2019;</ref><ref type="bibr" target="#b17">Martinez, Black, and Romero 2017)</ref>, we evaluate the Percentage of Correct 3D Keypoints (3D PCK) where points error under 150mm is considered correct (as the keypoints definitions are different in Human3.6M and 3DHP, we evaluate only the overlapped keypoints). As shown in <ref type="table" target="#tab_10">Table 7</ref>, even if we do not perform any re-training or fine-tuning, we still achieve an improvement of 1.6% PCK, indicating the effectiveness of our approach.</p><p>As above 3D human pose datasets contain mostly singleperson indoor scenes, we also evaluate our framework on 3DPW, a new outdoor multi-person 3D poses dataset. Following <ref type="bibr" target="#b13">Kanazawa et al. 2019)</ref> we do not train on 3DPW and only use its testing set for quantitative evaluation. The P-MPJPE value of our method on 3DPW testing set is 71.8, which outperforms the re-  <ref type="table">Table 5</ref>: Evaluation on Human3.6M dataset on human dynamics protocol. Mean angle error of predicted 3D poses after different time intervals is used following <ref type="bibr" target="#b17">(Martinez, Black, and Romero 2017;</ref><ref type="bibr" target="#b7">Ghosh et al. 2017</ref>). The milliseconds is the set future time for checking the performance. Best in bold, second best underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Frames</head><p>Multi-Scale Only Results   <ref type="figure">Figure 6</ref> shows the 3D pose estimation results of the proposed framework compared with different baseline results. The first video clip (left four columns) shows a person playing baseball, which contains fast motion of limbs; the second video clip (right four columns) shows a person walking from left to right while some other people passing him, leading to occlusion. We use the same baseline method as  used in the Ablation Studies. The baseline (the second row) fails on both video clips, because it cannot handle fast motion or occlusion. The results in the third row are from the method that uses multi-scale temporal features but without occlusion augmentation and spatio-temporal KCS based discriminator. We observe that it can handle the fast motion case to some extent but fails on the occlusion video, and the generated poses do not always satisfy anthropometrical constraints. The last row shows the results of our whole framework and it demonstrates our method can handle different motion speeds and various types of occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Final Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we present a new method based on three major components: multi-scale temporal features, spatio-temporal KCS pose discriminator, and occlusion data augmentation. Our method can deal with videos with various motion speeds and different types of occlusion. The effectiveness of each component of our method is illustrated in the ablation studies. To compare with the state-of-the-art 3D pose estimation methods, we evaluate the proposed method on four public 3D human pose datasets with commonly used protocols and demonstrate our method's superior performance. Comparison with the human dynamics methods is provided as well to show our method is versatile and potentially can be used for other pose tasks, like pose forecasting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of our 3D human pose estimation under different movement speeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Illustration for our framework. We only show two different temporal strides for clarity purpose. KPTS is short for keypoints; KCS is Kinematic Chain Space; TKCS means Temporal KCS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of single-scale and multi-scale TCN results. Errors are labeled in red circles. The single-scale TCN fails to provide accurate predictions for fast motion frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration for Temporal Kinematic Chain Space (TKCS) between two neighboring bones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of results from models trained with and without occlusion augmentation. Wrong estimations are labeled in red circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on Human3.6M dataset under Protocol #1 and #2. Best in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>MethodDirectDisc. Eat Greet PhonePhoto Pose Purch.Sit SitD. SmokeWait WalkD. Walk WalkT. Avg Fang et al. AAAI (2018) 50.1 54.3 57.0 57.1 66.6 73.3 53.4 55.7 72.8 88.6 60.3 57.7 62.7 47.5 50.6 60.4 Yang et al. CVPR (2018) 51.5 58.9 50.4 57.0 62.1 65.4 49.8 52.7 69.2 85.2 57.4 58.4 43.6 60.1 47.7 58.6 Hossain &amp; Little ECCV (2018) 44.2 46.7 52.3 49.3 59.9 59.4 47.5 46.2 59.9 65.6 55.8 50.4 52.3 43.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5 45.1</cell><cell>51.9</cell></row><row><cell>Li et al. CVPR (2019)</cell><cell cols="15">43.8 48.6 49.1 49.8 57.6 61.5 45.9 48.3 62.0 73.4 54.8 50.6 56.0 43.4 45.5</cell><cell>52.7</cell></row><row><cell>Chen et al. CVPR (2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.0</cell></row><row><cell>Wandt et al. CVPR (2019) *</cell><cell cols="15">50.0 53.5 44.7 51.6 49.0 58.7 48.8 51.3 51.1 66.0 46.6 50.6 42.5 38.8 60.4</cell><cell>50.9</cell></row><row><cell>Pavllo et al. CVPR (2019)</cell><cell cols="15">45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9</cell><cell>46.8</cell></row><row><cell>Cheng et al. ICCV (2019)</cell><cell cols="15">38.3 41.3 46.1 40.1 41.6 51.9 41.8 40.9 51.5 58.4 42.2 44.6 41.7 33.7 30.1</cell><cell>42.9</cell></row><row><cell>Our result</cell><cell cols="15">36.2 38.1 42.7 35.9 38.2 45.7 36.8 42.0 45.9 51.3 41.8 41.5 43.8 33.1 28.6</cell><cell>40.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Quantitative evaluation using MPJPE in millimeter between estimated pose and the ground-truth on Human3.6M under Protocol #1, no rigid alignment or transform applied in post-processing. Best in bold, second best underlined. * indicates ground-truth 2D labels are used.</figDesc><table><row><cell>Method</cell><cell cols="16">DirectDisc. Eat Greet PhonePhoto Pose Purch.Sit SitD. SmokeWait WalkD. Walk WalkT. Avg</cell></row><row><cell>Fang et al. AAAI (2018)</cell><cell cols="15">38.2 41.7 43.7 44.9 48.5 55.3 40.2 38.2 54.5 64.4 47.2 44.3 47.3 36.7 41.7</cell><cell>45.7</cell></row><row><cell>Yang et al. CVPR (2018)</cell><cell cols="15">26.9 30.9 36.3 39.9 43.9 47.4 28.8 29.4 36.9 58.4 41.5 30.5 29.5 42.5 32.2</cell><cell>37.7</cell></row><row><cell cols="16">Hossain &amp; Little ECCV (2018) 36.9 37.9 42.8 40.3 46.8 46.7 37.7 36.5 48.9 52.6 45.6 39.6 43.5 35.2 38.5</cell><cell>42.0</cell></row><row><cell>Kocabas et al. CVPR (2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.0</cell></row><row><cell>Li et al. CVPR (2019)</cell><cell cols="15">35.5 39.8 41.3 42.3 46.0 48.9 36.9 37.3 51.0 60.6 44.9 40.2 44.1 33.1 36.9</cell><cell>42.6</cell></row><row><cell>Wandt et al. CVPR (2019) *</cell><cell cols="15">33.6 38.8 32.6 37.5 36.0 44.1 37.8 34.9 39.2 52.0 37.5 39.8 34.1 40.3 34.9</cell><cell>38.2</cell></row><row><cell>Pavllo et al. CVPR (2019)</cell><cell cols="15">34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5 37.4 33.8 37.8 25.6 27.3</cell><cell>36.5</cell></row><row><cell>Cheng et al. ICCV (2019)</cell><cell cols="15">28.7 30.3 35.1 31.6 30.2 36.8 31.5 29.3 41.3 45.9 33.1 34.0 31.4 26.1 27.8</cell><cell>32.8</cell></row><row><cell>Our result</cell><cell cols="15">26.2 28.1 31.1 28.4 28.5 32.9 29.7 31.0 34.6 40.2 32.4 32.8 33.1 26.0 26.1</cell><cell>30.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Quantitative evaluation using P-MPJPE in millimeter between estimated pose and the ground-truth on Human3.6M under Protocol #2. Procrustes alignment to the ground-truth is used in post-processing. Best in bold, second best underlined. * indicates ground-truth 2D labels are used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.11 1.39 1.55 1.39 1.31 1.49 1.86 1.76 2.01 0.92 1.03 1.15 1.38 1.77 1.11 1.20 1.38 1.53 1.73 Martinez et al. (2017) 0.32 0.54 0.72 0.86 0.96 0.25 0.42 0.64 0.94 1.30 0.33 0.60 1.01 1.23 1.83 0.34 0.74 1.04 1.43 1.75 Chiu et al. (2019) 0.25 0.41 0.58 0.74 0.77 0.20 0.33 0.53 0.84 1.14 0.26 0.48 0.88 0.98 1.66 0.30 0.66 0.98 1.39 1.74 Our result 0.29 0.48 0.65 0.79 0.92 0.25 0.39 0.58 0.87 1.02 0.34 0.44 0.90 1.07 1.52 0.33 0.63 0.90 1.30 1.77</figDesc><table><row><cell>Actions</cell><cell>Walking</cell><cell>Eating</cell><cell>Smoking</cell><cell>Discussion</cell></row><row><cell>Milliseconds</cell><cell cols="4">80 160 320 560 1000 80 160 320 560 1000 80 160 320 560 1000 80 160 320 560 1000</cell></row><row><cell>Ghosh et al. (2017)</cell><cell>1.00 1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Examples of results from our whole framework compared with different baseline results. First row shows the images from two video clips; second row shows the 3D results that uses baseline approach described in Ablation Studies; third row shows the 3D results that uses multi-scale temporal features without occlusion augmentation and spatio-temporal KCS; last row shows the results of the whole framework. Wrong estimations are labeled in red circles.</figDesc><table><row><cell>Frame 54</cell><cell>Frame 80</cell><cell>Frame 88</cell><cell>Frame 91</cell><cell>Frame 532</cell><cell>Frame 542</cell><cell>Frame 550</cell><cell>Frame 560</cell></row><row><cell>Figure 6: Method</cell><cell>Walking</cell><cell>Jogging</cell><cell>Avg</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pavlakos et al. (2018)*</cell><cell cols="3">18.812.7 29.2 23.515.414.5 18.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hossain et al. (2018)</cell><cell cols="3">19.113.6 43.9 23.216.915.5 22.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wang et al. (2019)</cell><cell cols="3">17.213.4 20.5 27.919.520.9 19.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pavllo et al. (2019)</cell><cell cols="3">13.410.2 27.2 17.113.113.8 15.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cheng et al. (2019)</cell><cell cols="3">11.710.1 22.8 18.711.411.0 14.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our result</cell><cell cols="3">10.611.8 19.3 15.811.512.2 13.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Evaluation on HumanEva-I dataset under Protocol #2. Legend: (*) uses extra depth annotations for ordinal supervision. Best in bold, second best underlined. sults, 157.0 and 80.1, reported in (Martinez et al. 2017; Kanazawa et al. 2019).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Evaluation on MPI-INF-3DHP dataset using 3D PCK. Best in bold, second best underlined. Only overlapped keypoints with Human3.6M are used for evaluation.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning temporal pose estimation from sparsely-labeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep fully-connected part-based models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>; R.; Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACML</title>
		<editor>WACV. de Bem,</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="342" />
		</imprint>
	</monogr>
	<note>Action-agnostic human pose forecasting</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning human motion models for long-term predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="458" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Occluded joints recovery in 3d human pose estimation based on distance matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
	<note>Exploiting temporal information for 3d human pose estimation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3d human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2891" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1888" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="614" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose machines with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose Flow: Efficient online pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8410" to="8419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Predicting 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
