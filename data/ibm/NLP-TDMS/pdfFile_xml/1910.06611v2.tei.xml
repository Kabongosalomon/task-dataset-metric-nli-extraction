<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing the Transformer With Explicit Relational Encoding for Math Problem Solving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Swiss AI Lab IDSIA</orgName>
								<orgName type="institution">USI / SUPSI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Fernandez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">The Swiss AI Lab IDSIA</orgName>
								<orgName type="institution">USI / SUPSI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing the Transformer With Explicit Relational Encoding for Math Problem Solving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems. Pretrained models and code are available online 1 . * Work partially done while at Microsoft Research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we propose a variation of the Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> that is designed to allow it to better incorporate structure into its representations. We test the proposal on a task where structured representations are expected to be particularly helpful: math word-problem solving, where, among other things, correctly parsing expressions and compositionally evaluating them is crucial. Given as input a free-form math question in the form of a character sequence like Let r(g) be the second derivative of 2 * g ** 3/3 -21 * g ** 2/2 + 10 * g. Let z be r <ref type="bibr">(7)</ref>. Factor -z * s + 6 -9 * s ** 2 + 0 * s + 6 * s ** 2., the model must produce an answer matching the specified target character-sequence -(s + 3) * (3 * s -2) exactly. Our proposed model is trained end-to-end and infers the correct answer for novel examples without any task-specific structural biases.</p><p>We begin by viewing the Transformer as a kind of Graph Neural Network (e.g., <ref type="bibr" target="#b6">Gori et al., 2005;</ref><ref type="bibr" target="#b5">Goller and Küchler, 1995;</ref><ref type="bibr" target="#b1">Battaglia et al., 2018)</ref>. For concreteness, consider the encoder component of a Transformer with H heads. When the h th head of a cell t of layer l issues a query and as a result concentrates its self-attention distribution on another cell t in layer l, we can view these two cells as joined by an edge in an information-flow graph: the information content at t in effect passes via this edge to affect the state of t. The strength of this attention can be viewed as a weight on this edge, and the index h of the head can be viewed as a label. Thus, each layer of the Transformer can be viewed as a complete, directed, weighted, labeled graph. Prior NLP work has interpreted certain edges of these graphs in terms of linguistic relations (Sec. 8), and we wish to enrich the relation structure of these graphs to better support the explicit representation of relations within the Transformer.</p><p>Here we propose to replace each of the discrete edge labels 1, . . . , H, with a relation vector: we create a bona fide representational space for the relations being learned by the Transformer. This makes it possible for the hidden representation at each cell to approximate the vector embedding of a symbolic structure built from the relations generated by that cell. This embedding is a Tensor-Product Representation (TPR; <ref type="bibr" target="#b22">Smolensky, 1990)</ref> in an end-to-end-differentiable TPR system <ref type="bibr" target="#b20">(Schlag and Schmidhuber, 2018;</ref><ref type="bibr" target="#b21">Schmidhuber, 1993</ref>) that learns "internal spotlights of attention" <ref type="bibr" target="#b21">(Schmidhuber, 1993)</ref>. TPRs provide a general method for embedding symbol structures in vector spaces. TPRs support compositional processing by directly encoding constituent structure: the representation of a structure is the sum of the representation of its constituents. The representation of each constituent is built compositionally from two vectors: one vector that embeds the content of the constituent, the 'filler' -here, the vector returned by attention -and a second vector that embeds the structural role it fills -here, a relation conceptually labeling an edge of the attention graph. The vector that embeds a filler and the vector that embeds the role it fills are bound together by the tensor product to form the tensor that embeds the constituent that they together define. <ref type="bibr">2</ref> The relations here, and the structures they define, are learned unsupervised by the Transformer in service of a task; post-hoc analysis is then required to interpret those roles.</p><p>In the new model, the TP-Transformer, each head of each cell generates a key-, value-and queryvector, as in the Transformer, but additionally generates a role-vector (which we refer to in some contexts as a 'relation vector'). The query is interpreted as seeking the appropriate filler for that role (or equivalently, the appropriate string-location for fulfilling that relation). Each head binds that filler to its role via the tensor product (or some contraction of it), and these filler/role bindings are summed to form the TPR of a structure with H constituents (details in Sec. 2).</p><p>An interpretation of an actual learned relation illustrates this (see <ref type="figure" target="#fig_1">Fig. 3</ref> in Sec. 5.2). One head of our trained model can be interpreted as partially encoding the relation second-argument-of. The toplayer cell dominating an input digit seeks the operator of which the digit is in the second-argument role. That cell generates a vector r t signifying this relation, and retrieves a value vector v t describing the operator from position t that stands in this relation. The result of this head's attention is then the binding of filler v t to role r t ; this binding is added to the bindings resulting from the cell's other attention heads.</p><p>On the Mathematics Dataset (Sec. 3), the new model sets a new state of the art for the overall accuracy (Sec. 4). Initial results of interpreting the learned roles for the arithmetic-problem module show that they include a good approximation to <ref type="bibr">2</ref> The tensor product operation (when the role-embedding vectors are linearly independent) enables the sum of constituents representing the structure as a whole to be uniquely decomposable back into individual pairs of roles and their fillers, if necessary. the second-argument role of the division operator and that they distinguish between numbers in the numerator and denominator roles (Sec. 5).</p><p>More generally, it is shown that Multi-Head Attention layers not only capture a subspace of the attended cell but capture nearly the full information content (Sec. 6.1). An argument is provided that multiple layers of standard attention suffer from the binding problem, and it is shown theoretically how the proposed TP-Attention avoids such ambiguity (Sec. 6.2). The paper closes with a discussion of related work (Sec. 8) and a conclusion (Sec. 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The TP-Transformer</head><p>The TP-Transformer's encoder network, like the Transformer's encoder <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>, can be described as a 2-dimensional lattice of cells (t, l) where t = 1, ..., T are the sequence elements of the input and l = 1, ..., L are the layer indices with l = 0 as the embedding layer. All cells share the same topology and the cells of the same layer share the same weights. More specifically, each cell consists of an initial layer normalization (LN) followed by a TP-Multi-Head Attention (TPMHA) sublayer followed by a fully-connected feed-forward (FF) sub-layer. Each sub-layer is followed by layer normalization (LN) and by a residual connection (as in the original Transformer). Our cell structure follows directly from the official TensorFlow source code by <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> but with regular Multi-Head Attention replaced by our TPMHA layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TP-Multi-Head Attention</head><p>The TPMHA layer of the encoder consists of H heads that can be applied in parallel. Every head h, 1 ≤ h ≤ H applies separate affine transformations W</p><p>(1)</p><p>The filler of the attention head t, l, h is</p><formula xml:id="formula_0">v h t,l = T i=1 v h i,l α h,i t,l ,<label>(2)</label></formula><p>i.e., a weighted sum of all T values of the same layer and attention head (see <ref type="figure">Fig. 1</ref>). Here α h,i t,l ∈ (0, 1) is a continuous degree of match given by the softmax of the dot product between the query vector at position t and the key vector at position i:</p><formula xml:id="formula_1">α h,i t,l = exp(q h t,l · k h i,l 1 √ d k ) T i =1 exp(q h t,l · k h i ,l 1 √ d k )<label>(3)</label></formula><p>The scale factor 1 √ d k can be motivated as a variance-reducing factor under the assumption that the elements of q h t,l and k h t,l are uncorrelated variables with mean 0 and variance 1, in order to initially keep the values of the softmax in a region with better gradients <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>.</p><p>Finally, we bind the fillerv h t,l with our relation vector r h t,l , followed by an affine transformation W</p><formula xml:id="formula_2">(o) h,l ∈ R dz×d k , b (o)</formula><p>h,l ∈ R dz before it is summed up with the other heads' bindings to form the TPR of a structure with H constituents: this is the output of the TPMHA layer.</p><formula xml:id="formula_3">TPMHA(z t,l , z 1:T,l ) = h W (o) h,l (v h t,l r h t,l ) + b (o) h,l<label>(4)</label></formula><p>Note that, in this binding, to control dimensionality, we use a contraction of the tensor product, pointwise multiplication : this is the diagonal of the tensor product. For discussion, see the Appendix. <ref type="figure">Figure 1</ref>: A simplified illustration of our TP-Attention mechanism for one head at position t in layer l. The main difference from standard Attention is the additional role representation that is element-wise multiplied with the filler/value representation.</p><p>It is worth noting that the l th TPMHA layer returns a vector that is quadratic in the inputs z t,l to the layer: the vectors v h i,l that are linearly combined to formv h t,l (Eq. 2), and r h t,l , are both linear in the z i,l (Eq. 1), and they are multiplied together to form the output of TPMHA (Eq. 4). This means that, unlike regular attention, TPMHA can increase, over successive layers, the polynomial degree of its representations as a function of the original input to the Transformer. Although it is true that the feed-forward layer following attention (Sec. 2.2) introduces its own non-linearity even in the standard Transformer, in the TP-Transformer the attention mechanism itself goes beyond mere linear re-combination of vectors from the previous layer. This provides further potential for the construction of increasingly abstract representations in higher layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feed-Forward Layer</head><p>The feed-forward layer of a cell consists of an affine transformation followed by a ReLU activation and a second affine transformation:</p><formula xml:id="formula_4">FF(x) = W (g) l ReLU(W (f ) l x+b (f ) l )+b (g) l (5) Here, W (f ) l ∈ R d f ×dz , b (f ) l ∈ R d f , W (g) l ∈ R dz×d f , b (g) l</formula><p>∈ R dz and x is the function's argument. As in previous work, we set d f = 4d z .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Decoder Network</head><p>The decoder network is a separate network with a similar structure to the encoder that takes the hidden states of the encoder and auto-regressively generates the output sequence. In contrast to the encoder network, the cells of the decoder contain two TPMHA layers and one feed-forward layer. We designed our decoder network analogously to <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> where the first attention layer attends over the masked decoder states while the second attention layer attends over the final encoder states. During training, the decoder network receives the shifted targets (teacher-forcing) while during inference we use the previous symbol with highest probability (greedy-decoding). The final symbol probability distribution is given bŷ</p><formula xml:id="formula_5">yt = softmax(E Tẑt ,L )<label>(6)</label></formula><p>whereẑt ,L is the hidden state of the last layer of the decoder at decoding stept of the output sequence and E is the shared symbol embedding of the encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Mathematics Dataset</head><p>The Mathematics Dataset <ref type="bibr" target="#b19">(Saxton et al., 2019)</ref> is a large collection of math problems of various types, including algebra, arithmetic, calculus, numerical comparison, measurement, numerical factorization, and probability. Its main goal is to investigate the capability of neural networks to reason formally. Each problem is structured as a character-level sequence-to-sequence problem. The input sequence is a free-form math question or command like What is the first derivative of 13 * a ** 2 -627434 * a + 11914106? from which our model correctly predicts the target sequence 26 * a -627434. Another example from a different module is Calculate 66.6 * 12.14. which has 808.524 as its target sequence.</p><p>The dataset is structured into 56 modules which cover a broad spectrum of mathematics up to university level. It is procedurally generated and comes with 2 million pre-generated training samples per module. The authors provide an interpolation dataset for every module, as well as a few extrapolation datasets as an additional measure of algebraic generalization.</p><p>We merge the different training splits train-easy, train-medium, and train-hard from all modules into one big training dataset of 120 million unique samples. From this dataset we extract a characterlevel vocabulary of 72 symbols, including start-ofsentence, end-of-sentence, and padding symbols 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate our trained model on the concatenated interpolation and extrapolation datasets of the pre-generated files, achieving a new state of the art (see <ref type="table" target="#tab_0">Table 1</ref>). A more detailed comparison of the interpolation and extrapolation performance for every module separately can be found in the supplementary material. Throughout 1.0 million training steps, the interpolation error on the held-out data was strictly decreasing. We trained on one machine with 8 P100 Nvidia GPUs for 10 days. Preliminary experiments of 2.0 million training steps indicates that the interpolation accuracy of the TP-Transformer can be further improved to at least 84.24%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>The TP-Transformer uses the same hyperparameters as the regular Transformer (d z = 512, d f = 2048, H = 8, L = 6). Due to the use of the TP-Attention this results in a larger number of trainable weights. For this reason we also include two hyper-parameter settings with fewer trainable weights. TP-Transformer B shrinks the hiddenstate size and filter size from a multiple of 64 to a multiple of 60 (d z = 480, d f = 1920) which results in 1.2 million fewer trainable weights than the baseline and the TP-Transformer C shrinks the filter size more aggressively down to a total of 14.2 million trainable weights ( 32% fewer weights) by massively reducing the filter size while keeping the hidden state size the same (d z = 512, d f = 512).</p><p>We initialize the symbol embedding matrix E from N (0, 1), W <ref type="bibr">(p)</ref>  <ref type="figure">from N (1, 1)</ref>, and all other matrices W (·) using the Xavier uniform initialization as introduced by <ref type="bibr" target="#b4">(Glorot and Bengio, 2010)</ref>. We were not able to train the TP-Transformer, nor the regular Transformer, using the learning rate and gradient clipping scheme described by <ref type="bibr" target="#b19">(Saxton et al., 2019)</ref>. Instead we proceed as follows:</p><p>The gradients are computed using PyTorch's Autograd engine and their gradient norm is clipped at 0.1. The optimizer we use is also Adam, but with a smaller learning rate = 1 × 10 −4 , beta1 = 0.9, beta2 = 0.995. We train with a batch size of 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Interpreting the Learned Structure</head><p>We report initial results of analyzing the learned structure of the encoder network's last layer after training the TP-Transformer for 700k steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Interpreting the Learned Roles</head><p>To this end, we sample 128 problems from the interpolation dataset of the arithmetic mixed module and collect the role vectors from a randomly chosen head. We use k-means with k = 20 to cluster the role vectors from different samples and different time steps of the final layer of the encoder. Interestingly, we find separate clusters for digits in the numerator and denominator of fractions. When there is a fraction of fractions we can observe that these assignments are placed such that the second fraction reverses, arguably simplifying the division of fractions into a multiplication of fractions (see <ref type="figure" target="#fig_0">Fig. 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Interpreting the Attention Maps</head><p>In <ref type="figure" target="#fig_1">Fig. 3</ref>   Blue and gold rectangles respectively highlight numerator and denominator roles. They were discovered manually. Note how their placement is correctly swapped in rows 2, 3, and 4, where a number in the denominator of a denominator is treated as if in a numerator. Role-cluster 9 corresponds to the role ones-digit-of-a-numerator-factor, and 6 to ones-digit-of-a-denominator-factor; other such roles are also evident. layer of the encoder. Gold boxes are overlaid to highlight most-relevant portions. The row above the attention mask indicates the symbols that take information to the symbol in the bottom row. In each case, they take from '/'. Seen most simply in the first example, this attention can be interpreted as encoding a relation second-argument-to holding between the querying digits and the '/' operator.</p><p>The second and third examples show that several numerals in the denominator can participate in this relation. The third display shows how a numeratornumeral (-297) intervening between two denominator-numerals is skipped for this relation.</p><p>6 Deficits of Multi-Head Attention Layers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multi-Head Attention Subspaces Capture Virtually All Information</head><p>It was claimed by <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> that Multihead attention allows the model to jointly attend to information from different representation subspaces at different positions. In this section, we show that in our trained models, an individual at-tention head does not access merely a subset of the information in the attended cell but instead captures nearly the full information content. Let us consider a toy example where the attention layer of cell t,l only attends to cell t ,l . In this setting, the post-attention representation simplifies and becomes</p><formula xml:id="formula_6">z t,l + W (o) l (W (v) l z t ,l + b (v) l ) + b (o) l = z t,l + o(v(z t ,l ))<label>(7)</label></formula><p>where o and v are the respective affine maps (see Sec. 2.1). Note that even though W</p><formula xml:id="formula_7">(v) l</formula><p>is a projection into an 8 times smaller vector space, it remains to be seen whether the hidden state loses information about z t ,l . We empirically test to what extent the trained Transformer and TP-Transformer lose information. To this end, we randomly select n = 100 samples and extract the hidden state of the last layer of the encoder z t,6 , as well as the value representation v h (z t,6 ) for every head. We then train an affine model to reconstruct z t,6 from v h (z t,6 ), the value vector of the single head h:</p><formula xml:id="formula_8">z t,6 = W h v h (z t,6 ) + b h e = 1 n (ẑ t,6 − z t,6 ) 2<label>(8)</label></formula><p>For both trained models, the TP-Transformer and the regular Transformer, the mean squared error e averaged across all heads is only 0.017 and 0.009 respectively. Note that, because of layer normalization, the relevant quantities are scaled to √ d z . This indicates that the attention mechanism incorporates not just a subspace of the states it attends to, but affine transformations of those states that preserve nearly the full information content. In such a case, the attention mechanism can be interpreted as the routing of multiple local information source into one global tree structure of local representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The Binding Problem of Stacked Attention Layers</head><p>The binding problem refers to the problem of binding features together into objects while keeping them separated from other objects. It has been studied in the context of theoretical neuroscience <ref type="bibr" target="#b14">(von der Malsburg, 1981</ref><ref type="bibr" target="#b15">(von der Malsburg, , 1994</ref> but also with regards to connectionist machine learning models <ref type="bibr" target="#b8">(Hinton et al., 1984)</ref>. The purpose of a binding mechanism is to enable the fully distributed representation of symbolic structure (like a hierarchy of features) which has recently resurfaced as an important direction for neural network research (Lake and <ref type="bibr" target="#b12">Baroni, 2017;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2018;</ref><ref type="bibr" target="#b24">van Steenkiste et al., 2019;</ref><ref type="bibr" target="#b16">Palangi et al., 2017;</ref><ref type="bibr" target="#b26">Tang et al., 2018)</ref>. In this section, we describe how the standard attention mechanism is ill suited to capture complex nested representations, and we provide an intuitive understanding of the benefit of our TP-Attention. We understand the attention layer of a cell as the means by which the subject (the cell state) queries all other cells for an object. We then show how a hierarchical representation of multiple queries becomes ambiguous in multiple standard attention layers.</p><p>Consider the string (a/b)/(c/d). A good neural representation captures the hierarchical structure of the string such that it will not be confused with the similar-looking but structurally different string (a/d)/(c/b). Our TP-Attention makes use of a binding mechanism in order to explicitly support complex structural relations by binding together the object representations receiving high attention with a subject-specific role representation. Let us continue with a more technical example. Consider a simplified Transformer network where every cell consists only of a singlehead attention layer with a residual connection: no feed-forward layer or layer normalization, and let us assume no bias terms in the maps o l and v l introduced in the previous section (Eq. 7). In this setting, assume that cell a,l only attends to cell b,l , and cell c,l only attends to cell d,l where a, b, c, d are distinct positions of the input sequence. In this case</p><formula xml:id="formula_9">z a,l+1 = z a,l + o l (v l (z b,l )) z c,l+1 = z c,l + o l (v l (z d,l ))<label>(9)</label></formula><p>Suppose now that, for hierarchical grouping, the next layer cell e,l+1 attends to both cell a,l+1 and cell c,l+1 (equally, each with attention weight 1 2 ). This results in the representation z e,l+2 = z e,l+1 <ref type="figure">z d,l )</ref>)))/2 (10)</p><formula xml:id="formula_10">+ o l+1 (v l+1 (z a,l+1 + z c,l+1 ))/2 = z e,l+1 + o l+1 (v l+1 (z a,l + z c,l + o l (v l (z b,l )) + o l (v l (</formula><p>Note that the final representation is ambiguous in the sense that it is unclear by looking only at Eq. 10 whether cell a,l has picked cell b,l or cell d,l .</p><p>Either scenario would have led to the same outcome, which means that the network would not be able to distinguish between these two different structures (as in confusing (a/b)/(c/d) with (a/d)/(c/b)). In order to resolve this ambiguity, the standard Transformer must recruit other attention heads or find suitable non-linear maps in between attention layers, but it remains uncertain how the network might achieve a clean separation.</p><p>Our TP-Attention mechanism, on the other hand, specifically removes this ambiguity. Now Eqs. 9 and 10 become:</p><formula xml:id="formula_11">z a,l+1 = z a,l + o l (v l (z b,l ) r a,l ) z c,l+1 = z c,l + o l (v l (z d,l ) r c,l ) z e,l+2 = z e,l+1 + o l+1 (v l+1 (z a,l + z c,l + o l (v l (z b,l ) r a,l ) + o l (v l (z d,l ) r c,l )))/2<label>(11)</label></formula><p>Note that the final representation is not ambiguous anymore. Binding the filler symbols v l (z) (our objects) with a subject-specific role representation r as described in Eq. 4 breaks the structural symmetry we had with regular attention. It is now simple for the network to specifically distinguish the two different structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Hadamard-Product Attention as an</head><p>Optimal Approximation of Tensor-Product Attention</p><p>In Eq. 4 for TPMHA, we have a sum over all H heads of an affine-transformed product of a value vector v h and a role vector r h . (Throughout this discussion, we leave the subscripts t, l implicit, as well as the over-bar on v h in Eq. 4.) In a hypothetical, full-TPR formulation, this product would be the tensor product v h ⊗ r h , although in our actual proposed TP-Transformer, the Hadamard (elementwise) product v h r h (the diagonal of v h ⊗ r h ) is used. The appropriateness of the compression from tensor product to Hadamard product can be seen as follows.</p><p>In the hypothetical full-TPR version of TPMHA, attention would return the sum of H tensor products. This tensor A would have rank at most H, potentially enabling a substantial degree of compression across all tensors the model will compute over the data of interest. Given the translationinvariance built into the Transformer via positioninvariant parameters, the same compression must be applied in all positions within a given layer l, although the compression may vary across heads. For the compression of A we will need more than H components, as this decomposition needs to be optimal over all all tensors in that layer for all data points.</p><p>In detail, for each head h, the compression of the tensor A h = v h ⊗ r h (or matrix A h = v h r h ) is to dimension d k , which will ultimately be mapped to dimension d z (to enable addition with z via the residual connection) by the affine transformation of Eq. 4 . The optimal d k -dimensional compression for head h at layer l would preserve the d k dominant dimensions of variance of the attentiongenerated states for that head and layer, across all positions and inputs: a kind of singular-value decomposition retaining those dimensions with the principal singular values. The compression of A h ,Â h , will lie within the space spanned by these d k tensor products</p><formula xml:id="formula_12">m h c ⊗ n h c , i.e.,Â h = d k c=1ã h c m c ⊗ n c ; in ma- trix form,Â h = M hÃh N h , whereÃ h is the d k × d k diagonal matrix with elementsã h c .</formula><p>Thus the d k dimensions {ã h c } of the compressed matrix A h that approximates A h are given by: </p><formula xml:id="formula_13">A h = M hÂh N h ≈ M h A h N h = M h v h r h N h = (M h v h )(N h r h ) (12) a h c = Ã h cc ≈ [M h v h ] c [N h r h ] c = [ṽ h r h ] c<label>(13)</label></formula><formula xml:id="formula_14">whereṽ h = M h v h ,r h = N h r h . Now from Eq. 1, v h = W h,(v) z + b h,(v) , sõ v h = M h (W h,(v) z + b h,(v) ). Thus by chang- ing the parameters W h,(v) , b h,(v) to W h,(v) = M h W h,(v) ,b h,(v) = M h b h,(v) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Several recent studies have shown that the Transformer-based language model BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> captures linguistic relations such as those expressed in dependency-parse trees. This was shown for BERT's hidden activation states <ref type="bibr" target="#b7">(Hewitt and Manning, 2019;</ref><ref type="bibr" target="#b27">Tenney et al., 2019)</ref> and, most directly related to the present work, for the graph implicit in BERT's attention weights <ref type="bibr">(Coenen et al., 2019;</ref><ref type="bibr" target="#b13">Lin et al., 2019)</ref>. Future work applying the TP-Transformer to language tasks (like those on which BERT is trained) will enable us to study the connection between the explicit relations {r h t,l } the TP-Transformer learns and the implicit relations that have been extracted from BERT.</p><p>Multiplicative states have been used in neural networks before <ref type="bibr" target="#b10">(Ivakhnenko and Lapa, 1965;</ref><ref type="bibr" target="#b11">Ivakhnenko, 1971</ref>). The Hadamard-Product Attention bears similarity to neural network gates which have been shown to be effective mechanism to represent complex states in recurrent models (Hochreiter and <ref type="bibr" target="#b9">Schmidhuber, 1997)</ref> and feed-forward models <ref type="bibr" target="#b23">(Srivastava et al., 2015)</ref>. Recent work presents the Gated Transformer-XL model which incorporates a gating layer after the Multi-Head Attention layer of the regular Transformer <ref type="bibr">(Parisotto et al., 2019)</ref>. Unlike the regular Transformer, the Gated Transformer-XL is shown to be stable in a reinforcement learning domain while matching or outperforming recurrent baselines. The main difference is that the TP-Attention binds the values of different heads before summation whereas the Gated Transformer-XL gates simply the output of the attention-layer.</p><p>Very recent work proposes a Transformer variation which merges the self-attention layer and the fully-connected layer into a new all-attention layer <ref type="bibr" target="#b25">(Sukhbaatar et al., 2019)</ref>. To achieve this, the selfattention layer is extended with keys and values which are learned by gradient descent instead of using neural network activations. The feed-forward layers are then removed. Our binding-problem argument (Sec. 6.2) applies to this architecture even more so than to the regular Transformer since there are no non-linear maps in-between attention lay-ers that could possibly learn to resolve ambiguous cases.</p><p>Previous work used Multi-Head Attention also in recurrent neural networks in order to perform complex relational reasoning . In their work, Multi-Head Attention is used to allow memories to interact with each other. They demonstrate benefits on program evaluation and memorization tasks. Technically, the TP-Attention is not limited to the Transformer architecture and as such the benefits could possibly carry over to any connectionist model that makes use of Multi-Head Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have introduced the TP-Transformer, which enables the powerful Transformer architecture to learn to explicitly encode structural relations using Tensor-Product Representations. On the novel and challenging Mathematics Dataset, TP-Transformer beats the previously published state of the art and our initial analysis of this model's final layer suggests that the TP-Transformer naturally learns to cluster symbol representations based on their structural position and relation to other symbols. fortunately, the compute requirements of training on the Mathematics Dataset currently makes using the full tensor product infeasible, unless the vector representations of symbols and roles are reduced to dimensions that proved to be too small for the task. When future compute makes it possible, we expect that expanding from the diagonal to the full tensor product will provide further improvement in performance and interpretability.   <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed Test Accuracy</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Samples of correctly processed problems from the arithmetic mixed module. '#' and '%' are the startand end-of-sentence symbols. The colored squares indicate the k-means cluster of the role-vector assigned by one head in the final layer in that position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>TP-Transformer attention maps for three examples as described in section 5.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Denote these principal directions by {m h c ⊗ n h c |c = 1, . . . , d k }, and let M h and N h respectively be the d k × d k matrices with the orthonormal vectors {m h c } and {n h c } as columns. (Note that orthonormality implies that M h M h = I and N h N h = I, with I the d k × d k identity matrix.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Test accuracy on the interpolation test-set of the Mathematics dataset. TPT refers to the TP-Transformer variations as introduced in section 4.1. TF refers to our implementation of the Transformer<ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Test accuracy on the extrapolation test-set of the Mathematics dataset. TPT refers to the TP-Transformer variations as introduced in section 4.1. TF refers to our implementation of the Transformer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>we display three separate attention weight vectors of one head of the last TP-Transformer Weights Steps Train Interpolation Extrapolation acc &gt;95% acc &gt;95% LSTM with thinking steps(Saxton et al.Model accuracy averaged over all modules. A sample is correct if all characters of the target sequence have been predicted correctly. The column "&gt;95%" counts how many of the 56 modules achieve over 95% accuracy. TP-Transformer B and C differ from the standard hyper-parameters in order to reduce the total number of weights.</figDesc><table><row><cell></cell><cell>) 18M</cell><cell>500k</cell><cell>-</cell><cell>57.00%</cell><cell>6</cell><cell>41.00%</cell><cell>1</cell></row><row><cell>Transformer (Saxton et al.)</cell><cell>30M</cell><cell>500k</cell><cell>-</cell><cell cols="3">76.00% 13 50.00%</cell><cell>1</cell></row><row><cell>Transformer (ours)</cell><cell cols="6">44.2M 1000k 86.60% 79.54% 16 53.28%</cell><cell>2</cell></row><row><cell>TP-Transformer (ours)</cell><cell cols="6">49.1M 1000k 89.01% 81.92% 18 54.67%</cell><cell>3</cell></row><row><cell>TP-Transformer B (ours)</cell><cell cols="6">43.0M 1000k 87.53% 80.52% 16 52.04%</cell><cell>1</cell></row><row><cell>TP-Transformer C (ours)</cell><cell cols="6">30.0M 1000k 86.33% 79.02% 14 54.71%</cell><cell>1</cell></row><row><cell>See section 4.1 for more details.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and analogously for the role parameters W h,(r) , b h,(r) , we convert our original hypothetical TPR attention tensor A h to its optimal d k -dimensional approximation, in which the tensor product of the original vectors v h , r h is replaced by the Hadamard product of the linearly-transformed vectorsṽ h ,r h . Therefore, in the proposed model, which deploys the Hadamard product, learning simply needs to converge to the parameters W h,(v) ,b h,(v) rather than the parameters W h,(v) , b h,(v)  .</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that<ref type="bibr" target="#b19">(Saxton et al., 2019)</ref> report a vocabulary size of 95, but this figure encompasses characters that never appear in the pre-generated training and test data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is a vector, and should not be confused with the inner product v · r which is a scalar: the inner product is the sum of all the elements of the Hadamard product.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgments</head><p>We thank the anonymous reviewers for their valuable comments. This research was supported by an European Research Council Advanced Grant (no: 742870).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A General Considerations</head><p>In the version of the TP-Transformer studied in this paper, binding of relations r to their values v is not done by the tensor product, v ⊗ r, as in full TPRs. Rather, a contraction of the full TPR is used: the diagonal, which is the elementwise or Hadamard product v r. 4 To what extent does Hadamard-product binding share relevant properties with tensor-product binding?</p><p>A crucial property of the tensor product for its use in vector representations of structure is that a structure like a/b is not confusable with b/a, unlike the frequently-used bag-of-words encoding: in the BOW encoding of a/b, the pair of arguments to the operator are encoded simply as a + b, where a and b are respectively the vector encodings of a and b. Obviously, this cannot be distinguished from the BOW encoding of the argument pair in b/a, b + a. (Hence the name, symbol "bag", as opposed to symbol "structure".)</p><p>In a tensor-product representation of the argument pair in a/b, we have a r n + b r d , where r n and r d are respectively distinct vector embeddings of the numerator (or first-argument) and denominator (or second-argument) roles, and is the tensor product. This is distinct from a r d +b r n , the embedding of the argument-pair in b/a. (In Sec. 6.2 of the paper, an aspect of this general property, in the context of attention models, is discussed. In Sec. 5, visualization of the roles and the per-role-attention show that this particular distinction, between the numerator and denominator roles, is learned and used by the trained TP-Transformer model.)</p><p>This crucial property of the tensor product, that a r n + b r d = a r d + b r n , is shared by the Hadamard product: if we now take to represent the Hadamard product, the inequality remains true. To achieve this important property, the full tensor product is not required: the Hadamard product is the diagonal of the tensor product, which retains much of the product structure of the tensor product. In any application, it is an empirical question how much of the full tensor product is required to successfully encode distinctions between bindings of symbols to roles; in the TP-Transformer, it turns out that the diagonal of the tensor product is sufficient to get improvement in performance over having no symbol-role-product structure at all. Un-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noukhovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harm</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12889</idno>
		<title level="m">Systematic generalization: What is required and can it be learned? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pearce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02715</idno>
		<title level="m">Fernanda Viégas, and Martin Wattenberg. 2019. Visualizing and measuring the geometry of BERT</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning Task-Dependent Distributed Representations by Backpropagation Through Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Küchler</surname></persName>
		</author>
		<idno>AR-95-02</idno>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Institut für Informatik, Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">AR-Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Joint Conference on Neural Networks</title>
		<meeting>the IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<title level="m">Distributed representations. Carnegie-Mellon University</title>
		<meeting><address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Cybernetic Predicting Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekseȋ</forename><surname>Grigorê¹evich Ivakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin Grigorévich</forename><surname>Lapa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<publisher>CCM Information Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Polynomial theory of complex systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksey</forename><surname>Grigorievitch Ivakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="364" to="378" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00350</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Open sesame: Getting inside BERT&apos;s linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Frank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01698</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The correlation theory of brain function (internal report 81-2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Von Der Malsburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
		<respStmt>
			<orgName>Max Planck Intitute for Biophysical Chemistry</orgName>
		</respStmt>
	</monogr>
	<note>Goettingen: Department of Neurobiology</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The correlation theory of brain function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Von Der Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Models of neural networks II</title>
		<editor>E. Domany, J. L. van Hemmen, and K. Schulten</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="95" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08432</idno>
		<title level="m">Deep learning of grammaticallyinterpretable representations through questionanswering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06764</idno>
		<title level="m">Seb Noury, et al. 2019. Stabilizing transformers for reinforcement learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relational recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7299" to="7310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysing mathematical reasoning abilities of neural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to reason with third order tensor products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9981" to="9993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On decreasing the ratio between learning complexity and number of time-varying variables in fully recurrent nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks</title>
		<meeting>the International Conference on Artificial Neural Networks<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="460" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tensor product variable binding and the representation of symbolic structures in connectionist systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<idno type="DOI">10.1016/0004-3702(90)90007-M</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="159" to="216" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01035</idno>
		<title level="m">A perspective on objects and systematic generalization in model-based rl</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Augmenting self-attention with persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01470</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia R De</forename><surname>Sa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12456</idno>
		<title level="m">Learning distributed representations of symbolic structure using binding and unbinding operations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05950</idno>
		<title level="m">BERT rediscovers the classical NLP pipeline</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
