<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Increasing Trustworthiness of Deep Neural Networks via Accuracy Monitoring *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaolei</forename><surname>Ren</surname></persName>
							<email>sren@ece.ucr.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Increasing Trustworthiness of Deep Neural Networks via Accuracy Monitoring *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inference accuracy of deep neural networks (DNNs) is a crucial performance metric, but can vary greatly in practice subject to actual test datasets and is typically unknown due to the lack of ground truth labels. This has raised significant concerns with trustworthiness of DNNs, especially in safety-critical applications. In this paper, we address trustworthiness of DNNs by using post-hoc processing to monitor the true inference accuracy on a user's dataset. Concretely, we propose a neural network-based accuracy monitor model, which only takes the deployed DNN's softmax probability output as its input and directly predicts if the DNN's prediction result is correct or not, thus leading to an estimate of the true inference accuracy. The accuracy monitor model can be pre-trained on a dataset relevant to the target application of interest, and only needs to actively label a small portion (1% in our experiments) of the user's dataset for model transfer. For estimation robustness, we further employ an ensemble of monitor models based on the Monte-Carlo dropout method. We evaluate our approach on different deployed DNN models for image classification and traffic sign detection over multiple datasets (including adversarial samples). The result shows that our accuracy monitor model provides a close-to-true accuracy estimation and outperforms the existing baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have achieved unprecedentedly high classification accuracy and found success in numerous applications, including image classification, speech recognition, and nature language processing. Nonetheless, training an error-free or 100% accurate DNN is impossible in most practical cases. Inference accuracy is a crucial metric for quantifying the performance of DNNs. Typically, the reported inference accuracy of a DNN is measured offline on test datasets with labels, but this can significantly differ * Accepted by the AISafety workshop co-located with <ref type="bibr">IJCAI-PRICAI 2020.</ref> from the true accuracy on a user's dataset because of, e.g., data distribution shift away from the training dataset or even adversarial modification to the user's data <ref type="bibr" target="#b0">[Che et al., 2019;</ref><ref type="bibr" target="#b3">Kull et al., 2019;</ref><ref type="bibr">Malinin and Gales, 2018]</ref>. Moreover, obtaining the true accuracy is very challenging in practice due to the lack of ground-truth labels.</p><p>The unknown inference accuracy has further decreased the transparency of already hard-to-explain DNNs and raised significant concerns with their trustworthiness, especially in safety-critical applications. Consequently, studies on increasing trustworthiness of DNNs have been proliferating. For example, many studies have considered out-of-distribution (OOD) detection and adversarial sample detection, since OOD and adversarial samples often dramatically decrease inference accuracy of DNNs <ref type="bibr" target="#b2">[Hendrycks and Gimpel, 2017;</ref><ref type="bibr" target="#b0">Che et al., 2019;</ref><ref type="bibr" target="#b3">Lee et al., 2018;</ref><ref type="bibr" target="#b4">Liang et al., 2018]</ref>. While these efforts can offer an increased assurance of DNNs to users to some extent, they do not provide a quantitative measure of actual classification accuracy, which is a more direct and sensible measure of the target DNN's performance. Some other studies propose (post-hoc) processing to quantify/estimate the prediction confidence of a <ref type="bibr">DNN [Guo et al., 2017;</ref><ref type="bibr" target="#b3">Kull et al., 2019;</ref><ref type="bibr">Snoek et al., 2019]</ref>. Nonetheless, they typically require the target DNN's training/validation dataset to train a (sometimes complicated) new transformation model for confidence calibration, and do not transfer well to new unseen datasets. The accuracy of a target DNN on a user's operational dataset can also be estimated via selective random sampling, but it can suffer from a high estimation variance .</p><p>Contribution. In this paper, we propose a simple yet effective post-hoc method -accuracy monitoring -which increases the trustworthiness of DNN classification results by estimating the true inference accuracy on an actual (possibly OOD/adversarial) dataset. Concretely, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we propose a neural network-based accuracy monitor model, which only takes the deployed DNN's softmax probability output as its input and directly predicts if the DNN's prediction result is correct or not. Thus, over a sequence of prediction samples from a user's dataset, our accuracy monitor can form an estimate of the target DNN's true inference accuracy. Furthermore, we employ an ensemble of monitoring models based on the Monte-Carlo dropout method, providing a robust estimate of the target DNN's true accuracy. Utilizing as little information as the target DNN's softmax probability output for accuracy estimation provides better transferability than more complicated calibration methods <ref type="bibr" target="#b3">[Kull et al., 2019]</ref>. Specifically, we can pre-train an accuracy monitor model based on a labeled dataset relevant to the target application of interest (e.g., public datasets for image classification). Then, for model transfer, we can selectively label a small amount (1% in our work) of data from the user's test dataset with active learning via an entropy acquisition function <ref type="bibr" target="#b0">[Beluch et al., 2018]</ref>, and re-train our monitor models on the selectively labeled data using transfer learning. In addition, without the need of accessing the target DNN's training/validation datasets, our accuracy monitoring method can be easily applied as a plug-in module on top of the target DNN to monitor its runtime performance on a variety of datasets. Thus, our method is not restricted to the DNN providers themselves; instead, even an end user can employ our method to monitor the target DNN's accuracy performance on its own, bringing further increased trustworthiness of accuracy monitoring.</p><p>To evaluate the effectiveness of our accuracy monitoring method, we consider different target DNN models for image classification (10 classes and 1000 classes) and for traffic sign detection in autonomous driving, respectively. Our results show that, by only utilizing the prediction class and softmax probability output of the deployed DNN model and labeling 1% of the user's dataset, our method can monitor the healthy of the target DNN models, providing a remarkably accurate estimation of the true classification accuracy on a variety of user's datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Prediction uncertainty estimation. Several methods have been proposed to estimate DNN prediction uncertainty. In <ref type="bibr">[Schulam and Saria, 2019]</ref>, the model uncertainty is estimated with ensemble models via re-sampling the original DNN model parameters based on the Hessian matrix and gradient matrix on the training data. Additionally, <ref type="bibr" target="#b3">[Jiang et al., 2018]</ref> estimates model uncertainty via the similarity between the test data and training data. However, it requires not only the training data but also a white-box target DNN model. Other methods (e.g, MC dropout, ensembles, stochastic variational Bayesian inference, prior networks) are summarized in <ref type="bibr">[Malinin and Gales, 2018;</ref><ref type="bibr">Snoek et al., 2019]</ref>, which also require a white-box model and/or the original training dataset. By contrast, our post-hoc processing method only needs the target DNN's softmax probability output and applies to a variety of datasets, including OOD and adversarial samples.</p><p>Concept/distribution drift detection. After model de-  <ref type="bibr" target="#b4">et al., 2019;</ref><ref type="bibr" target="#b4">Liang et al., 2018;</ref><ref type="bibr" target="#b3">Lee et al., 2018</ref>] study OOD and adversarial detection by setting a threshold to decide if an input data is sufficiently similar to the pre-learnt in-distribution or nonadversarial data distribution. These approaches do not offer a measure of the actual accuracy. Moreover, they require access to the original training and/or validation datasets, which are not needed by our accuracy monitor. Accuracy estimation for the target model. Secondary models are trained to estimate the accuracy of the primary model, but they are trained on the same dataset as the primary model and requires either the original input data <ref type="bibr" target="#b1">[Ghanta et al., 2019a]</ref> or saliency maps <ref type="bibr">[Mohseni et al., 2019]</ref>. In <ref type="bibr" target="#b5">[Nguyen et al., 2018]</ref>, an active testing framework is proposed to estimate model accuracy, with a focus on noisy labeled datasets instead of unlabeled datasets that we consider. Our problem is also related to operational testing , which uses selective random sampling to provide an accuracy estimate for a target DNN on an actual operational dataset prior to DNN deployment. The work <ref type="bibr" target="#b2">[Istrate et al., 2019]</ref> predicts the accuracy of a target DNN architecture on a given dataset, while <ref type="bibr">[Unterthiner et al., 2020]</ref> predicts accuracy based on the target DNN's weights. These studies require a large number of DNN training experiments.</p><p>Prediction confidence via softmax probability. A related study <ref type="bibr" target="#b2">[Hendrycks and Gimpel, 2017]</ref> utilizes the maximum softmax probability of the target DNN for misclassification detection, whereas our approach exploits the softmax probabilities for all classes. Further, an abnormality module is designed to detect OOD data in <ref type="bibr" target="#b2">[Hendrycks and Gimpel, 2017]</ref>, for which a decoder is required and trained with a white-box target model. In <ref type="bibr" target="#b2">[Guo et al., 2017]</ref>, temperature scaling is proposed to calibrate the original softmax probability, but a labeled validation set is required to learn the hyperparameter T . Likewise, <ref type="bibr" target="#b3">[Kull et al., 2019]</ref> advances the temperature scaling method by training a sophisticated Dirichlet distribution for better confidence calibration. These methods are sensitive to and do not transfer well to a user's datasets with OOD/adversarial samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>We consider a deployed target DNN model that performs classification tasks with C classes. The DNN provides softmax probabilities denoted as p(x) = M Θ d (x), where x represents the input data, Θ d denotes target DNN's parameters (not required by the accuracy monitor), and p(x) ∈ R C . Thus, the predicted class isỹ = arg max k∈{1,2...C} {p k (x)}. The empirical accuracy Acc of a deployed DNN model M Θ d on a user's dataset (x i , y i ) ∈ D U can be calculated as follows  </p><formula xml:id="formula_0">Acc = 1 |D U | (xi,yi)∈D U I (y i =ỹ i ) ,<label>(1)</label></formula><p>where I(·) is the Boolean indicator function. The exact value of Acc cannot be possibly obtained without knowing all the true class y i , which is often the case in practice (e.g., a user employs a classifier due to the high cost of manually labeling its data). It can also significantly differ from the accuracy value evaluated based on the DNN model provider's test dataset due to data distribution disparity.</p><p>In this paper, we leverage a simple plug-in accuracy monitor model to estimate the empirical accuracy Acc without all the true labels for user's dataset. Specifically, the neural network-based monitor model s(p(x)) = M Θa (p(x)) parameterized by Θ a takes the target DNN's softmax probabilities p(x) = M Θ d (x) as its input and outputs a softmax probability/score s(p(x)) to indicate the likelihood of correct classification for data x. Then, if the probability of correct classification is greater than or equal to a threshold th s , the target DNN's classification is considered correct and otherwise wrong. By default, we use s(p(x)) ≥ th s = 0.5 in order for a classification result to be considered correct. Thus, the accuracy of the deployed DNN on the user's dataset estimated by our monitor model is</p><formula xml:id="formula_1">Acc = 1 |D U | (xi,yi)∈D U I [s(p(x)) ≥ th s ] .<label>(2)</label></formula><p>Our problem formulation is similar to that for the existing confidence calibration techniques <ref type="bibr" target="#b3">[Kull et al., 2019;</ref><ref type="bibr" target="#b2">Guo et al., 2017]</ref> that focus on estimating the probability of correct/wrong prediction for each individual sample. Nonetheless, our key goal is to make the estimated average accuracy Acc as close to the true empirical accuracy Acc as possible. This allows the application of our method in even OOD/adversarial datasets, while still offering an important view of the average accuracy performance of the target DNN.</p><p>Note finally that our accuracy monitoring method does not require a white-box target DNN model and can be applied on top of the target DNN to monitor its accuracy performance, either by the DNN model provider or by an end user (provided that it has access to a relevant labeled dataset, not necessarily the target DNN's training/validation dataset). <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the flow of our DNN accuracy monitor, including three phases. First, monitor models are pre-trained over a labeled dataset that shares the same application as the user's dataset. Then, monitor models are re-trained with a small t% of labeled data from the user's dataset using active learning. Finally, multiple monitor models are provided to approximate Bayesian neural networks via MC dropout, achieving a more robust accuracy estimation. Algorithm 1 describes the steps of our proposed method. Next, we provide details of the three phases for accuracy monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design of DNN Accuracy Monitoring</head><p>Training phase. To pre-train initial monitor models, the accuracy monitor can leverage a labeled dataset D R , which can be the target DNN's training/validation dataset (if the DNN provider wants to monitor its own model's accuracy) or a different dataset relevant to the target application (if the DNN user wants to monitor the accuracy by itself but does not have the target DNN's original training/validation dataset). For example, if the target DNN is developed by one entity but later provided to another user as a black-box model for image classification, CIFAR10, CINIC10 or ImageNet2012 can be used by the user to pre-train its own accuracy monitor models. We run the target DNN on the labeled dataset and obtain prediction softmax probabilities p R (x) produced by the target DNN. Meantime, the correct/wrong result CW R (x) of the target DNN can also be obtained by comparing the DNN's predicted class with the true data label. Then, based on p R (x) and CW R , we can train B monitor models</p><formula xml:id="formula_2">M Θ (b) a .</formula><p>Transfer with active learning. Due to the possible distribution differences between the chosen labeled dataset D R and the user's actual dataset D U , the monitor models pre-trained solely on the D R may not provide a satisfactory accuracy for the target DNN as shown in Section 5. To address this issue, we need to transfer the monitor models into the user's dataset. In the transfer learning phase, we freeze the weights of all layers in the monitor models except for the last two layers. Only the weights of the last two layers will be updated during transfer learning. Due to expensive labeling cost, we Algorithm 1: DNN Accuracy Monitoring</p><formula xml:id="formula_3">Input: A labeled dataset D R , user dataset D U , target model M Θ d (x)</formula><p>, the MC dropout model number B, data labeling budget t%. 1. Obtain softmax probabilities for D R and D U .</p><formula xml:id="formula_4">p R (x) ← M Θ d (x) for x ∈ D R ; CW R (x) ← I (ỹ = y) for (x, y) ∈ D R ; p U (x) ← M Θ d (x) for x ∈ D U ; 2. Train monitor models with p R and CW R . for b = 1 to B do Initialize Θ (b) a for a monitor model M Θ (b) a ; Train M Θ (b) a with (p R (x), CW R (x)); s (b) (p U (x)) ← M Θ (b) a (p(x)) for x ∈ D U ; end 3. Actively label dataset D U s from user's dataset D U Calculate Shannon entropy E(x) based on s (b) (p U (x)) and average over B monitor models for (x, y) ∈ D U ; D U s ← (x, y) ∈ D U |E(x) among the top t% ; p U s (x) ← M Θ d (x) for (x, y) ∈ D U s ; CW U s (x) ← I (ỹ = y) for (x, y) ∈ D U s ; 4.</formula><p>Transfer learning and accuracy estimation.</p><formula xml:id="formula_5">for b = 1 to B do Transfer M Θ (b) a with (p U s (x), CW U s (x)); s (b) (p U (x)) ← M Θ (b) a (p U (x)) for x ∈ D U \ D U s ; end return Average Acc from Eqn. (2);</formula><p>only sample a small amount of user's dataset (denoted as D U s ) from D U , and only D U s are manually labeled. To minimize the size of D U s , entropy-based active learning <ref type="bibr" target="#b0">[Beluch et al., 2018]</ref> is utilized during the transfer. Specifically, we calculate the average entropy of softmax probabilities produced by the monitor models, and label t% of user's data with the greatest entropy.</p><p>Note that while labeling user's data, only the user's data label y and deployed DNN's softmax probabilities p(x) (instead of the raw data x) are utilized by the monitor models. Moreover, by doing so, the accuracy monitor actually performs accuracy estimation of the target DNN model over a low-dimension softmax probability representation of x, which effectively facilities transfer learning to user's dataset. As shown in our experiments, by labeling only 1% of the user's dataset, the monitor models can produce a highly accurate estimation of the target DNN's average accuracy.</p><p>Robust accuracy estimation with MC dropout. Estimating accuracy for the target DNN by a single monitor model may not be robust because of the indispensable uncertainty in deep learning. Based on <ref type="bibr" target="#b1">[Gal and Ghahramani, 2016]</ref>, we employ the MC dropout method to approximate a Bayesian neural network and provide more robust accuracy estimation. Specifically, we train an ensemble of monitor models in the training phase using the same labeled dataset but different initialized weights and dropout layers. Then, we transfer the trained models using the same dataset D U s . When estimating the target DNN's classification accuracy, multiple estimated accuracies can be obtained from the ensemble. The mean of the results is considered as the monitor's assessment on the deployed DNN's classification accuracy over the user's dataset. Moreover, the standard deviation (std) can also be provided to represent the uncertainty of estimated accuracy by the ensemble of monitor models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first evaluate the effectiveness of our accuracy monitoring method on two image classification applications: smallscale image classification with 10 classes, and large-scale image classification with 1000 classes. Then, we consider a mission-critical application -traffic sign detection for autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Our accuracy monitor model is trained as a neural network with dropout layers using Tensorflow and Keras <ref type="bibr" target="#b0">[Abadi et al., 2016]</ref>. The weight parameter Θ a is trained via minimizing binary cross-entropy loss using Adam [Kingma and Ba, 2015] with a learning rate α = 0.001. The input of the monitor model is the softmax probabilities p(x) produced by the target DNN, while the output represents if the classification is correct or not for an input image x with a softmax score s(p(x)), which will then be averaged over multiple samples to form an estimate of the average accuracy.</p><p>Dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Approaches and Metrics</head><p>The following baselines and metrics are considered.</p><p>RS: With random sampling (RS), u% of user's data is randomly sampled and manually labeled. Then, the accuracy on the sampled user's dataset is considered as the overall accuracy. We also run RS for 100 times, and highlight the accuracy range achieved by 100 runs. Note, however, that in practice the RS is only performed once for each test dataset.</p><p>MP and MP*: In the MP approach considered in <ref type="bibr" target="#b2">[Hendrycks and Gimpel, 2017]</ref>, no manual labeling where th M P is a threshold, then the classification for x is considered correct and otherwise wrong. In our experiment, the threshold th M P is determined based on the same labeled dataset used to train our monitor models to achieve the best accuracy estimation during the validation. Alternatively, we can also use the maximum softmax probability on the use's dataset to estimate the target DNN's accuracy as M P *= (x,y)∈D U M P (x), and we use MP* to represent this approach. Entropy: The prediction entropy Entropy(p(x)) can be calculated from softmax probability of the target DNN model. Then, the target DNN's classification for x is considered correct if Entropy &lt; th En , where th En is the entropy threshold decided by the monitor according to its chosen labeled ataset, and wrong otherwise.</p><p>Temperature scaling (TS): By using temperature scaling, the softmax probability can be calibrated from the logits with a hyper-parameter T . According to <ref type="bibr" target="#b2">[Guo et al., 2017]</ref>, given the logit output z i , the model accuacy can be estimated as T S = max σ SM (z i /T ), where σ SM is the softmax function and T is called the temperature. Usually, the temperature T is obtained via minimizing the Negative log likelihood (NLL) on the target DNN's validation set. Here, we use the actively labeled user's data samples as the validation set.</p><p>Perfect confidence calibration: This is an oracle that gives the true accuracy of the target DNN and no practical confidence calibration methods (e.g., <ref type="bibr" target="#b3">[Kull et al., 2019]</ref>) can outperform.</p><p>Metrics: Our main performance metric is the estimated average accuracy of the target DNN. Additionally, we also consider AUPR (Area Under the Precision-Recall Curve) to isolate the effects of different thresholds th s . The value of threshold-less AUPR varies from positive class ratio p (random guess) to 1.0 (perfect classification), and measures a model's capability of distinguishing between correct/wrong classification. The higher AUPR, the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Result on 10-class Image Classification</head><p>For 10-class image classification, the target model is a VGG16 model trained on <ref type="bibr">CIFAR-10 [Geifmany, 2018]</ref>. We evaluate the performance of our proposed method on four datasets shown in <ref type="table" target="#tab_2">Table 1</ref>. The dataset sizes are 10k (CIFAR-10), 90k (CINIC-10), 8k (STL-10) and 10k (AD-10). The Input <ref type="formula" target="#formula_0">(10)</ref> Dense <ref type="formula">(500)</ref> Dense <ref type="formula" target="#formula_0">(100)</ref> Dropout <ref type="formula">(</ref> reported inference accuracy of the target VGG16 model is 93.56% measured on CIFAR-10, while the inference accuracies for other datasets are 76.17% (CINIC-10), 63.04% (STL-10), and 37.80% (AD-10), indicating a significant accuracy degradation due to OOD/adversarial data. First, we train an ensemble of 20 monitor models on 9000 images from a public dataset (i.e., CINIC-10 training dataset in our experiment) and the structure of monitor model is shown in <ref type="figure">Fig. 3(a)</ref>, including two hidden dense layers and one dropout layer.</p><p>In the training phase, each monitor model is trained over 200 epochs with Adam optimizer. <ref type="figure">Fig. 3(b)</ref> shows the training and validation loss for a monitor model in training phase. Then, two hidden layers are frozen to perform transfer learning as shown in <ref type="figure">Fig. 3(a)</ref>. To improve the transfer efficiency, an active learning approach is utilized to select 1% samples with the highest entropy from the user's test dataset. In the prediction phase, robust estimation and its uncertainty are provided by the ensemble of monitor models.</p><p>The estimated accuracy results are summarized in <ref type="table" target="#tab_2">Table 1</ref>, compared with baseline approaches. Our method can provide much more accurate estimate of the target DNN's inference accuracy on user's test datasets. While our monitor models are trained on CINIC-10, with transfer learning on only 1% of the user's dataset, we can still accurately estimate the target DNN's inference accuracy when user's dataset is STL-10.</p><p>The inference accuracy via the RS approach exhibits a large variance with 1% labeled data, and at least 10% labeled samples are required to achieve a small estimation error. For temperature scaling method, the estimated accuracy still deviates from the true accuracy with a large gap. For MP-based and entropy-based approaches, the estimated accuracy varies greatly with threshold values. Although in theory one can always find a threshold with which the resulting estimated accuracy coincides with the true accuracy z% = Acc, such a threshold is not very meaningful, since it simply says samples with top z% maximum softmax probability or entropy are correct. As shown in <ref type="table" target="#tab_2">Table 1</ref>, our method has a similar AUPR value with the baseline approaches, demonstrating that the overall capability of distinguishing correct/wrong classification is comparable among different methods. Nonetheless, AUPR is not as an intuitive metric as average accuracy, which our accuracy monitor is specifically designed for. Also, AUPR is only applicable for methods with variable thresholds (e.g., MP, Entropy and TS) as provided in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>In addition, we also evaluate the performance of monitor model on small-batch datasets to see if our monitor models can track the true empirical accuracy of the target DNN on user's time-varying datasets. Specifically, we randomly select 500 images as a batch from STL-10 with replacement, and repeat to have a total of 100 batches each having 500 images. We show the results in <ref type="figure">Fig. 4</ref> and demonstrate our accuracy monitor can closely track the empirical true accuracy, whereas the baseline approaches cannot. Even 20% RS (i.e., randomly label 100 images for each batch) and temperature scaling algorithms cannot provide a good accuracy estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Result on 1000-class Image Classification</head><p>For image classification with 1000 classes, two target models (MobileNet and ResNet-50) are applied on ImageNet2012's validation dataset. The original validation set includes 50k images. We randomly split ImageNet2012 into 3 datasets: training dataset with 20k, ImageNet A with 20k images and ImageNet B with 10k images. The reported accuracies on ImageNet dataset are 70.40% for MobileNet and 74.90% for ResNet-50, respectively. For MobileNet, the true accuracies on test datasets are 68.59% (ImageNet A) and 67.91% (Im-ageNet B), respectively. For ResNet-50, the true accuracies are 68.36% (ImageNet A) and 67.47% (ImageNet B), respectively. The true accuracies vary due to the distribution shift.</p><p>For the 1000-class target model, the softmax probability p(x) includes 1000 values. Therefore, the monitor model structure is changed accordingly with 1000 input nodes and 1000 hidden nodes in hidden layers. Other settings remain the same. baseline approaches for large-scale image classification. Similarly, the RS's estimated accuracy exhibits a high variation and at least 10% labeled data are required to achieve a similar performance as the monitor model. Due to distribution similarity between the training dataset and ImageNet A/B which are all selected from ImageNet2012, the MP-based and entropy-based approaches (with thresholds optimized based on the training dataset) offer a reasonable estimate of the true accuracy, but they are still worse than our monitor model. Similarly, temperature scaling has a higher estimation error due to limited (1%) labeled samples. Our accuracy monitor exhibits a slightly larger estimation error on 1000-class models than the 10-class case. One possible reason is the higher dimensions in the softmax probability, which may require more complex feature extraction layers instead of simple fully-connected layers in our current experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Result on Traffic Sign Detection</head><p>We now consider traffic sign detection in safety-critical autonomous driving on the GTSD dataset, including 40k samples grouped into 43 categories/classes <ref type="bibr" target="#b2">[Houben et al., 2013]</ref>. We train a CNN on GTSD training dataset (27k samples) using 50 epochs via Adam optimizer. The CNN includes convolution layers, dropout layers, and fully connected layers. We evaluate the proposed method and baseline approaches on four test datasets generated from GTSD, including the original test dataset (GTSD-D1), augmented test dataset (GTSD-D2), out-of-distribution dataset (GTSD-OOD), and adversarial dataset (GTSD-AD). Specifically, GTSD-D1 includes 10k samples randomly selected from the GTSD test dataset, while GTSD-D2 includes 10k augmented samples from the GTSD test dataset. The augmentation operations and parameters for GTSD-D2 are random rotation within <ref type="table">Table 3</ref>: Performance of our method and baseline algorithms on traffic sign detection. The mean/std values are provided for our method. Target DNN: a CNN model trained on GTSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Estimated Accuracy AUPR GTSD-D1 GTSD-D2 GTSD-OOD GTSD-AD GTSD-D1 GTSD-D2 GTSD-OOD GTSD-AD As for GTSD-OOD, it includes 12k OOD samples from CIFAR-10 and 18k samples from the augmented dataset GTSD-D2. The OOD samples from CIFAR-10 are resized into (30, 30, 3) using tf.image.resize function with default parameters, and they are treated with NULL label, indicating not belonging to any of the 43 classes in GTSD. The GTSD-AD dataset includes 15k normal samples and 15k adversarial samples. The normal samples are randomly selected from the augmented dataset GTSD-D2, while adversarial samples are generated with DeepFool. The reported inference accuracy measured on GTSD-D1 is 97.34%, while the inference accuracies for the other datasets are 84.01% (GTSD-D2), 51.47% (GTSD-OOD) and 42.91% (GTSD-AD), respectively.</p><p>For the target DNN, the softmax probability vector p(x) includes 43 elements. The structure of our monitor model in <ref type="figure">Fig. 3(a)</ref> is modified to include 100 and 50 hidden nodes in two hidden layers, respectively. First, we pre-train an ensemble of 20 monitor models on a public dataset (for which we choose GTSD-D2 in our evaluation). In the training phase, each monitor model is trained over 200 epochs with Adam optimizer. Then, when applied to different datasets, the weights in the first two layers are frozen to perform transfer learning. Other settings remain the same.</p><p>The estimated accuracies by different methods are summarized in <ref type="table">Table 3</ref>. The results show that our method still outperforms the considered baselines, providing a much more accurate estimate of the target DNN's inference accuracy on user's datasets. With pre-trained monitor models and only 1% labeled data, we can accurately estimate the target DNN's inference accuracy when applied to different datsets (GTSD-OOD or GTSD-AD). Also, the estimated accuracy by RS exhibits a high variation with 1% labeled data and at least 10% labeled data is required to achieve a similar performance as our method. Additionally, the baselines provide an estimated accuracy with a large error. For instance, the MP* and TS methods often provide a higher estimated accuracy than the true accuracy.</p><p>To sum up, the results on GTSD further demonstrates the effectiveness of our proposed method for accuracy estimation of a target DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, to increase the trustworthiness of DNN classifi-cation results, we propose a post-hoc method for monitoring the prediction performance of a target DNN models and estimating its empirical inference accuracy on user's (possibly OOD/adversarial) dataset. The monitor model only takes the softmax probability produced by the target DNN model as its input. Thus, it can be easily employed as a plug-in module on top of a target DNN to monitor its accuracy. Importantly, by active learning with a small amount of labeled data from user's datasets, our monitor model can produce a very accurate estimate of inference accuracy of the target DNN model. Our experiment results on different datasets validate the effectiveness and efficiency of the proposed method for image classification and traffic sign detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Accuracy monitoring for a deployed/target DNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>DNN accuracy monitoring: Training, transferring and accuracy estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.</head><label></label><figDesc>The datasets includeCIFAR-10 [Krizhevsky,  2009],CINIC-10 [Darlow et al., 2018],STL-10 [Coates  et al., 2011], ImageNet2012 [Russakovsky et al., 2015 and German Traffic Sign Detection (GTSD)<ref type="bibr" target="#b2">[Houben et al., 2013]</ref>. In addition, we also consider a user's dataset with adversarial images for 10-class classification and GTSD classification, denoted as AD-10, and GTSD-AD, respectively. The adversarial images are generated using DeepFool<ref type="bibr" target="#b5">[Moosavi-Dezfooli et al., 2016]</ref> policy with "Foolbox" package<ref type="bibr" target="#b5">[Rauber et al., 2017]</ref>.Target DNN model. The target DNN model for 10-class image classification is VGG16[Simonyan and Zisserman,  2015], whileMobileNet [Howard et al., 2017]  andResNet- 50 [He et al., 2016]  are used as the target DNNs for 1000class image classification. The target model for GTSD is a native convolutional neural network (CNN) trained on the GTSD training dataset. The accuracy monitor estimates the classification accuracy achieved by these DNNs on the above datasets (which can be OOD with respect to the DNNs' original training datasets).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) The monitor model structure for 10-class image classification. (b) Loss during monitor model training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ployment, some studies indirectly tackle the problem of model accuracy monitoring via concept/data distribution drift detection in the absence of labels. In<ref type="bibr" target="#b5">[Pinto et al., 2019]</ref>, an automatic concept drift detection algorithm SAMM is developed with no labeled test data by utilizing the feature distance between test data and reference data. Other approaches include ML Health<ref type="bibr" target="#b2">[Ghanta et al., 2019b] and</ref> MD3  [Sethi and Kantardzic, 2017]. Moreover, [Che</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance of our method and baseline algorithms on 10-class image classification. The mean/std values are provided for our method. Target DNN: VGG16 trained on CIFAR-10</figDesc><table><row><cell>Method</cell><cell>CIFAR-10</cell><cell cols="2">Estimated Accuracy CINIC-10 STL-10</cell><cell>AD-10</cell><cell cols="4">AUPR CIFAR-10 CINIC-10 STL-10 AD-10</cell></row><row><cell>Our method</cell><cell>0.9313/0.0123</cell><cell>0.7691/0.0138</cell><cell>0.6343/0.0371</cell><cell>0.3866/0.0322</cell><cell>0.9270</cell><cell>0.8645</cell><cell cols="2">0.7966 0.8935</cell></row><row><cell>MP</cell><cell>0.8907</cell><cell>0.7574</cell><cell>0.7105</cell><cell>0.5035</cell><cell>0.9341</cell><cell>0.8595</cell><cell cols="2">0.7922 0.8918</cell></row><row><cell>Entropy</cell><cell>0.8943</cell><cell>0.7662</cell><cell>0.7165</cell><cell>0.5380</cell><cell>0.9352</cell><cell>0.8645</cell><cell cols="2">0.7966 0.8859</cell></row><row><cell>TS</cell><cell>0.9727</cell><cell>0.4066</cell><cell>0.8803</cell><cell>0.8618</cell><cell>0.9343</cell><cell>0.8607</cell><cell cols="2">0.7964 0.8922</cell></row><row><cell>MP*</cell><cell>0.9756</cell><cell>0.9443</cell><cell>0.9319</cell><cell>0.7881</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RS (1%)</cell><cell cols="4">[0.8879,0.9852] [0.6500,0.7340] [0.5274,0.7382] [0.2800,0.5100]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RS (10%)</cell><cell cols="4">[0.9207,0.9516] [0.7340,0.7930] [0.5976,0.6618] [0.3400,0.4080]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">is needed; instead, the maximum softmax probability</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">M P (x) = max k∈{1,2...C} {p k (x)} produced by the target</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">DNN model is utilized: if M P (x) ≥ th M P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of our method and baseline algorithms on 1000-class image classification. The mean/std values are provided for our method. Target DNN: MobileNet/ResNet-50 model.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Estimated Accuracy</cell><cell></cell><cell></cell><cell cols="2">AUPR</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">MobileNet</cell><cell cols="2">ResNet-50</cell><cell cols="2">MobileNet</cell><cell cols="2">ResNet-50</cell></row><row><cell></cell><cell>ImageNet A</cell><cell>ImageNet B</cell><cell>ImageNet A</cell><cell cols="5">ImageNet B ImageNet A ImageNet B ImageNet A ImageNet B</cell></row><row><cell cols="5">Our method 0.6933/0.0202 0.6796/0.0235 0.6862/0.0240 0.6719/0.0219</cell><cell>0.7192</cell><cell>0.7245</cell><cell>0.7066</cell><cell>0.7175</cell></row><row><cell>MP</cell><cell>0.7203</cell><cell>0.7004</cell><cell>0.6765</cell><cell>0.6757</cell><cell>0.7182</cell><cell>0.7221</cell><cell>0.7050</cell><cell>0.7103</cell></row><row><cell>Entropy</cell><cell>0.7032</cell><cell>0.7131</cell><cell>0.6724</cell><cell>0.6694</cell><cell>0.7052</cell><cell>0.7015</cell><cell>0.6907</cell><cell>0.7050</cell></row><row><cell>TS</cell><cell>0.8094</cell><cell>0.8086</cell><cell>0.7771</cell><cell>0.8044</cell><cell>0.7123</cell><cell>0.7197</cell><cell>0.7059</cell><cell>0.7150</cell></row><row><cell>MP*</cell><cell>0.7550</cell><cell>0.7539</cell><cell>0.7633</cell><cell>0.7638</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RS (1%)</cell><cell cols="4">[0.6197,0.7512] [0.5866,0.7754] [0.6631,0.7029] [0.6457,0.7049]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RS (10%)</cell><cell cols="4">[0.6652,0.7057] [0.6492,0.7073] [0.6696,0.6987] [0.6525,0.6959]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the U.S. National Science Foundation under grants CNS-1551661, ECCS-1610471, and  CNS-1910208.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep verifier networks: Verification of deep discriminative models with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<idno>arXiv:1911.07421</idno>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<editor>Coates et al., 2011] Adam Coates, Andrew Ng, and Honglak Lee</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR. CINIC-10 is not ImageNet or CIFAR-10</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghahramani ; Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Ghanta</surname></persName>
		</author>
		<ptr target="https://github.com/geifmany/cifar-vgg" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>OpML</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Marc Schlipsing, and Christian Igel. Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02808</idno>
		<idno>arXiv:1704.04861</idno>
	</analytic>
	<monogr>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<meeting><address><addrLine>Salmen</addrLine></address></meeting>
		<imprint>
			<publisher>Sebastian Houben</publisher>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<meeting><address><addrLine>Ba; Lee, Kibok Lee, Honglak Lee</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Uncertainty and Robustness in Deep Learning</title>
		<editor>Mohseni et al., 2019] Sina Mohseni, Akshay Jagadeesh, and Zhangyang Wang</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Malinin and Gales</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the reliable detection of concept drift from streaming unlabeled data. Expert Systems with Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11448</idno>
	</analytic>
	<monogr>
		<title level="m">Sylvain Gelly, Olivier Bousquet, and Ilya Tolstikhin. Predicting neural network accuracy from weights</title>
		<editor>NeurIPS</editor>
		<imprint>
			<publisher>Jasper Snoek</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">211252</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Asian Conference on Pattern Recognition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
