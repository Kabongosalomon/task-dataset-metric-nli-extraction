<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACCURATE AND SCALABLE VERSION IDENTIFICATION USING MUSICALLY-MOTIVATED EMBEDDINGS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furkan</forename><surname>Yesiler</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Dolby Laboratories</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Emilia Gómez † § † Music Technology Group</orgName>
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Joint Research Centre</orgName>
								<orgName type="institution">European Commission</orgName>
								<address>
									<settlement>Sevilla</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">IEEE Service Center / 445 Hoes Lane</orgName>
								<address>
									<postBox>P.O. Box 1331 / Piscataway</postBox>
									<postCode>08855-1331</postCode>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ACCURATE AND SCALABLE VERSION IDENTIFICATION USING MUSICALLY-MOTIVATED EMBEDDINGS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The version identification (VI) task deals with the automatic detection of recordings that correspond to the same underlying musical piece. Despite many efforts, VI is still an open problem, with much room for improvement, specially with regard to combining accuracy and scalability. In this paper, we present MOVE, a musically-motivated method for accurate and scalable version identification. MOVE achieves state-of-the-art performance on two publicly-available benchmark sets by learning scalable embeddings in an Euclidean distance space, using a triplet loss and a hard triplet mining strategy. It improves over previous work by employing an alternative input representation, and introducing a novel technique for temporal content summarization, a standardized latent space, and a data augmentation strategy specifically designed for VI. In addition to the main results, we perform an ablation study to highlight the importance of our design choices, and study the relation between embedding dimensionality and model performance.</p><p>Index Terms-Cover song identification, deep learning, music embedding, network encoder.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Version identification (VI) commonly refers to the task of determining, by computational means, whether two audio renditions correspond to the same underlying musical composition <ref type="bibr" target="#b0">[1]</ref>. Being more challenging than traditional audio fingerprinting <ref type="bibr" target="#b1">[2]</ref>, VI goes beyond near-exact duplicate detection to embrace additional perceptual differences that, despite having a contrasting imprint in the signal, convey the same musical entity <ref type="bibr" target="#b2">[3]</ref>. Such is the case of changes in instrumentation, musical key, tempo, timing, structure, or lyrics, to name a few <ref type="bibr" target="#b0">[1]</ref>. Besides digital rights management, VI has application to music organization, retrieval, navigation, and understanding.</p><p>Traditional VI systems generally approach the task with a pipeline consisting of three main stages <ref type="bibr" target="#b3">[4]</ref>. Firstly, as many other content-based retrieval methods, VI systems use feature extraction to obtain relevant information from the audio signal. Representations like predominant melody, pitch class profiles (PCP), or the constant-Q transform (CQT) have proven useful for this initial step <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. Secondly, traditional VI systems use various postprocessing strategies for achieving transposition, tempo, timing, or structure invariance <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Thirdly, for estimating similarity between pairs of songs, VI systems use segmentation strategies or local alignment methods, which also introduce invariance with regard to musical piece structure <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Further approaches have * Work done while at Telefónica Research, Barcelona. explored combining the information obtained from different features and/or different alignment schemes with early or late fusion techniques <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. These, together with some previous solutions, achieve good performance in different evaluation contexts but, nonetheless, have difficulties in scaling to datasets above tens of thousands of songs <ref type="bibr" target="#b14">[15]</ref>. With the release of the SHS dataset <ref type="bibr" target="#b15">[16]</ref>, researchers explored scalable approaches based on audio hashprints, the 2D Fourier transform, or motif-finding strategies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, but those achieved a limited success compared to their predecessors.</p><p>Recent deep learning approaches for VI aim to provide systems that are both accurate and scalable. In general, they focus on learning accurate, low-dimensional embeddings of recordings for, later, estimating similarities with basic distance metrics, with the intention to exploit existing scalable nearest-neighbor libraries. Xu et al. <ref type="bibr" target="#b18">[19]</ref> and Yu et al. <ref type="bibr" target="#b19">[20]</ref> train their convolutional networks in a multi-class classification fashion, where each version group (or clique) is considered a unique class, and use PCP and CQT as their inputs, respectively. For evaluation, they use the representations obtained from the penultimate layer of their network as embeddings. Beyond classification-based strategies, deep metric learning approaches with contrastive and triplet losses are becoming popular for VI. Qi et al. <ref type="bibr" target="#b20">[21]</ref> use a convolutional network with PCPs as input and a triplet loss as the objective function. As an alternative to using PCP variants, Doras &amp; Peeters <ref type="bibr" target="#b21">[22]</ref> use a 2D predominant melody representation as input to their convolutional network, which is also trained with a triplet loss but using an online semi-hard triplet mining strategy.</p><p>In this paper, we propose a music embedding method that allows for both accurate and scalable VI. We call it MOVE: musicallymotivated version embeddings. MOVE achieves state-of-the-art results on two publicly-available benchmark sets and, since it is based on Euclidean distances, allows for efficient retrieval and indexing using existing libraries. The architecture of MOVE introduces a number of improvements, including (1) a relatively novel input representation that has not been explored in the context of deep metric learning for VI, (2) a multi-channel adaptive attention mechanism that is an alternative to previously-used temporal aggregation strategies, and (3) a non-parametric batch normalization at the last layer to yield a standardized embedding space. The training of MOVE, like other recent VI systems, is done with a triplet loss. However, in contrast to those, it uses an online hard triplet mining strategy. In order to learn invariances with respect to the modifiable musical characteristics, MOVE is trained with a VI-specific data augmentation strategy. To gain insight, we perform an ablation study and also investigate the role of embedding dimensionality. To enable further research, we evaluate our method on publicly-available datasets and provide our code at https://github.com/furkanyesiler/move. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MUSICALLY-MOTIVATED VERSION EMBEDDINGS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Input</head><p>We use as input a relatively novel PCP variant: crema-PCP. This representation is constructed by using the output of an intermediate step of the crema chord estimation model <ref type="bibr" target="#b22">[23]</ref>. For each frame, the crema model estimates the root, the bass, and the pitch classes, which are later combined to output a single chord. Specifically, crema-PCP is constructed by taking the sigmoid activation values of pitch classes for each frame, and considering them as the energy values of each pitch class <ref type="bibr" target="#b22">[23]</ref>. Although being a fairly new approach, crema-PCP has been shown to outperform elaborate PCP representations in some benchmarking experiments <ref type="bibr" target="#b14">[15]</ref>. We use the pre-trained model available at https://github.com/bmcfee/crema (version 0.1.0) and denote the obtained output by X ∈ [0, 1] 12×T , where T is the number of frames using non-overlapping windows of 93 ms. For training, we take random patches of T = 1800 frames after applying data augmentation (see below) to a full song. At inference time, we give entire tracks to the model without picking random patches of a particular length (preliminary experiments showed that the belowproposed temporal pooling strategy was also effective with entire tracks at inference time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network architecture</head><p>MOVE consists of 5 convolutional blocks with PReLU activation functions and no padding, interleaved by two different pooling layers ( <ref type="figure" target="#fig_0">Figure 1</ref>). A linear layer followed by non-parametric batch normalization produces the final embedding. With the current best setup, the total number of parameters is 6.3 M. We now motivate and present the key components of MOVE. Transposition-invariant architecture -Following the strategy proposed by Xu et al. <ref type="bibr" target="#b18">[19]</ref>, we increase the dimension of the crema-PCP inputs X from 12×T to 23×T by concatenating two copies of X in the pitch dimension and removing the last pitch class. The first convolutional layer, with a kernel size of 12×180 traverses the input, going through all possible transpositions in the pitch dimension, and the subsequent max-pooling layer, with a kernel size of 12×1, keeps the transposition with the highest activation value (convolutions in MOVE have no padding).</p><p>Expanding the receptive field -The 4 convolutional blocks after max-pooling are designed to encode higher-level information and to increase the receptive field of the model <ref type="figure" target="#fig_0">(Figure 1</ref>). On the one hand, with the layers that have no dilation, we aim to encode higherlevel nonlinearities without expanding the temporal context. On the other hand, with the layers that have dilations 20 and 13, we increase the receptive field, which after max-pooling is less than 17 s, to approximately 30 s. Notice that this temporal span could be already sufficient to detect musical piece versions, at least from a human perspective. However, to process an even larger time span, and to be able to deal with different lengths T at test time, we still perform an additional step. Summarizing temporal content -We consider the convolutional part of our network as a feature extractor that processes the input to obtain a representation that is invariant to the modifiable musical characteristics mentioned in Section 1. In order to summarize the values of each feature in the temporal dimension, unlike previous approaches that use average-or max-pooling variants <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, we propose a multi-channel adaptive attention mechanism, which combines multi-channel temporal attention <ref type="bibr" target="#b23">[24]</ref> with auto-pool <ref type="bibr" target="#b24">[25]</ref>. The first idea is to let the network compute (and learn) the importance of each time step independently for each feature with an attention-like mechanism <ref type="bibr" target="#b23">[24]</ref>. The second idea is to apply a non-linear, learnable pooling function with a scaling parameter before the softmax function <ref type="bibr" target="#b24">[25]</ref> such that, depending on the value of such parameter, the function pivots between average-and max-pooling. In practice, temporal summarization is done by calculating channel-wise attention weights, which correspond to the first half of the filters of the last convolutional layer, using the auto-pool function, and utilizing the result to weight the last half of the filters of the same layer. Splitting the hidden representation channel-wise into two halves,</p><formula xml:id="formula_0">H = [Ha H b ], this corresponds to H = T t=1 σ(αHa) H b ,</formula><p>where the sum is taken across the temporal dimension, σ corresponds to the softmax function, α is a learnable parameter which we initialize to 0 (equivalent to average-pooling), and is the elementwise product. Standardizing embedding components -For deep metric learning approaches using a triplet loss, it is highly important to take into account the volume of the hyper-dimensional space where the embeddings lie, specially during training. For instance, if the magnitude of the distances and the margin are disproportionate, the training process may not be able to structure the latent space in an effective way. With these motivations in mind, we propose to use non-parametric batch normalization after the linear layer that finalizes the encoding process. By doing so, we aim to obtain zero-mean and unit-variance components in our embeddings, yielding a statistically-standardized latent space volume. This, together with dimension-normalized Euclidean distances, may also allow us to develop some intuition regarding the loss values and the corresponding margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training strategy</head><p>MOVE is trained by minimizing the triplet loss</p><formula xml:id="formula_1">L = max (D (XA, XP ) − D (XA, XN ) + m, 0) (1) using D(Xi, Xj) = 1 d f (Xi) − f (Xj) 2 ,</formula><p>where corresponds to the Euclidean norm and f (X) denotes an embedding of size d produced by our model. Equation 1 aims to make the distance between an anchor A and a positive example P smaller than the distance between the same anchor A and a negative example N under a margin m. We now present our decisions regarding training data, data augmentation, triplet mining, and hyperparameters. Training data -We use a private collection of 97,905 songs that are divided into 17,999 cliques. The annotations of the songs are under the Creative Commons BY-NC 3.0 license, and obtained with the API of secondhandsongs.com. The related metadata can be found at our repository. For training and validation, we created two disjoint sets of cliques, with 14,499 cliques containing 83,905 songs and 3,500 cliques containing 14,000 songs, respectively. All audio files are encoded in MP3 format and their sample rate is 44.1 kHz. Data augmentation -In order to enhance the learning of MOVE, we apply to each example a data augmentation function specifically designed for VI. Based on the modifiable musical characteristics specified in Section 1 and elsewhere, such function sequentially and independently applies transposition in the pitch dimension, time stretching, and time warping with probabilities 1, 0.3, and 0.3, respectively. Transposition uses the octave-equivalent characteristics of PCP representations and randomly rolls X in the pitch dimension between 0 and 11 bins. Time stretching uses one-dimensional interpolations in the temporal domain, with a random factor between 0.7 and 1.5. Time warping consists of three mutually-exclusive functions, which either silence, duplicate, or remove frames with probabilities 0.3, 0.4, and 0.3, respectively (silence corresponds to zeroing-out the entire frame). Once selected, these functions are applied on a per-frame basis with a probability of 0.1, 0.15, and 0.1, respectively. All random numbers are sampled using a uniform distribution. Triplet mining -As discussed in previous works that employ a triplet loss, the characteristics of the triplets in each mini-batch may have drastic effects on learning performance <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. For our model, we employ an online hard triplet mining strategy <ref type="bibr" target="#b26">[27]</ref>. In our implementation, we choose 16 unique cliques and 4 songs per clique, forming a mini-batch of 64. For the cliques that have less than 4 songs, we choose among the already chosen songs of the same clique. Within a mini-batch, we consider all the examples as anchors (A), and select the positive/negative example that has the maximum/minimum distance to the anchor (P and N , respectively; Equation 1). Although Schroff et al. <ref type="bibr" target="#b25">[26]</ref> point out that the hardest examples may lead to local minima early in the training, our triplets can be considered "moderate" <ref type="bibr" target="#b26">[27]</ref>, as they are selected only from the current mini-batch, and therefore do not strictly correspond to the hardest triplets in the dataset. This presumably avoids the aforementioned local minima.</p><p>Hyper-parameters and optimization -We train our network for 120 epochs with plain stochastic gradient descent, using an initial learning rate of 0.1 and decreasing it by a factor of 5 at epochs 80 and 100. An epoch is completed when our data loader goes through all possible cliques. However, an important detail to note is that we include the cliques with size between 6 and 9 twice, the ones with size between 10 and 13 three times, and the ones with size 14 or above four times. This is done to increase the probability of every song being introduced to the network at least once per epoch. The margin value m for the triplet loss is 1. As mentioned, we use patches of T = 1800 frames for training and an initial auto-pool parameter α = 0. If not already specified in <ref type="figure" target="#fig_0">Figure 1</ref>, the remaining hyperparameters and implementation details can be found at our GitHub repository 1 . We study the impact of the embedding dimension d in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation methodology</head><p>For studying the effect of the embedding dimension and performing the ablation study, we train with a subset of our training set with 8,817 cliques and 44,909 songs in total, and report the performance scores on our validation set. For comparison to the previous work, we utilize the entire training set. To report performance, we use mean average precision (MAP) and mean rank of the first relevant result (MR1). For all the experiments presented in this section, we use the models obtained after the last epochs.</p><p>For comparing the performance of MOVE with the state of the art, we use two additional datasets. The first dataset, the benchmark subset of Da-TACOS <ref type="bibr" target="#b14">[15]</ref>, contains a total of 15,000 songs, with 1,000 cliques of 13 songs each, and 2,000 songs not belonging to any other clique (acting as noise and not queried). The second dataset, YouTubeCovers (YTC) <ref type="bibr" target="#b27">[28]</ref>, contains 50 cliques with 7 songs each, and comes split into a training and a test set with 250 and 100 songs each, respectively. To compare the performance of our model on YTC with previous works, we follow their approach of only querying the test set to retrieve the versions in the reference set <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b28">29]</ref>. Moreover, in this case, we remove from our training data the 17 cliques that overlap with YTC. After that, both Da-TACOS and YTC do not contain any overlapping cliques with respect to our training/validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Effect of the embedding dimension</head><p>For any embedding system, the size of the embeddings d is a crucial hyper-parameter, as it can have an important effect on model performance. Therefore, we decide to study the model performance on the validation set with respect to it <ref type="figure" target="#fig_1">(Figure 2</ref>). For this set of experiments, we consider d = {128, 256, 512, 1 k, 2 k, 4 k, 8 k, 16 k, 32 k}. We observe that performance continues to increase with the embedding dimensionality until it saturates at d = 16 k. We can place a knee in the curve between d = 512 and d = 2 k. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation study</head><p>We now analyze the performance of the main components of our network by comparing them to their potential alternatives ( <ref type="table">Table 1)</ref>.</p><p>With that, we aim to quantify the importance of each decision. The first aspect we assess is the effect of the proposed data augmentation strategy <ref type="bibr" target="#b0">(1)</ref>. We find that removing data augmentation yields a relative decrease of 6% in MAP. The second aspect that we evaluate is the importance of the transposition-invariant architecture explained in Section 2.2 <ref type="bibr" target="#b1">(2)</ref>. As an alternative, we consider the case where we do not pre-process the input by changing its shape, and remove the max-pooling layer after the first convolution. Although trained with a much smaller learning rate (10 −4 ) and the Adam optimizer, the model was not able to properly learn an effective representation, even though multiple transpositions were present in the data augmentation function. The third aspect we consider is temporal summarization <ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref>. We observe that the introduction of the auto-pool parameter α to multi-channel attention does not really change the results <ref type="bibr" target="#b2">(3)</ref>. In contrast, substituting the proposed multi-channel attention by auto-, max-, or average-pool clearly has an impact <ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref>. The final aspect we analyze is the effect of the triplet mining strategy <ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref>. To do so, we train our network with online semi-hard <ref type="bibr" target="#b6">(7)</ref> and random (8) mining strategies. For semi-hard mining, we pick a random positive example for each anchor and then select a negative example that satisfies the condition D (XA, XN ) ≤ D (XA, XP ). In case no such negative example exists, we pick a random one. For random mining, we randomly select one positive and one negative example for each anchor. We see that semi-hard and random mining produce a relative MAP decrease of 5 and 26%, respectively.</p><p>Overall, our ablation study shows that all introduced variations have a positive impact in performance. The only exception is the mixing of the auto-pool parameter with multi-channel attention, which nonetheless does not substantially affect the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison with the state-of-the-art</head><p>Finally, we compare the performance of MOVE with the state of the art ( <ref type="table" target="#tab_1">Table 2</ref>). The results on Da-TACOS show that MOVE clearly outperforms all considered VI systems. Importantly, this does not only happen for systems that, like MOVE, use a single input representation and alignment, but also for complex systems that employ early or late fusion strategies. The relative MAP difference with respect to LateFusion <ref type="bibr" target="#b13">[14]</ref>, the most competing system, is over 10%.</p><p>We also see that, although the best performance is achieved with a MAP MR1 Results on Da-TACOS 2DFTM <ref type="bibr" target="#b16">[17]</ref> 0.275 155 SiMPle <ref type="bibr" target="#b17">[18]</ref> 0.332 142 Dmax <ref type="bibr" target="#b13">[14]</ref> 0.322 132 Qmax <ref type="bibr" target="#b9">[10]</ref> 0.365 113 Qmax* <ref type="bibr" target="#b29">[30]</ref> 0.373 104 EarlyFusion <ref type="bibr" target="#b11">[12]</ref> 0.426 116 LateFusion <ref type="bibr" target="#b13">[14]</ref> 0  <ref type="table" target="#tab_1">Table 2</ref>). However, we caution about the use of YTC to report VI performance, as differences measured with this dataset may not be significant due to the relatively small number of query and reference tracks (cf. <ref type="bibr" target="#b0">[1]</ref>). As an example, MOVE with d = 4 k shows a similar result as the setting with d = 16 k on YTC, while in larger datasets, the latter clearly outperforms the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this work, we have proposed MOVE, a method for accurate and scalable version identification using musically-motivated embeddings. MOVE achieves state-of-the-art performance on two publicly-available benchmark sets for VI. After motivating the components of its architecture and training strategy, both designed while incorporating a certain degree of domain knowledge, we performed an ablation study to justify our decisions. We have also studied the relation between the embedding size and the performance of our model. As future work, we plan to investigate different input representations. Since some early and late fusion methods incorporate several musical dimensions to outperform their isolated components, we intend to explore possibilities where we can mimic the same idea to improve MOVE's performance. Moreover, considering that our method outperforms traditional VI systems that are built with a certain notion of similarity in mind (for instance, local alignment between tonal features), a future study investigating the similarity concept learned by our model could provide meaningful insight regarding the links that bind various versions originated from the same musical composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGMENTS</head><p>This work is supported by the MIP-Frontiers project, the European Union's Horizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No. 765068, and by TROMPA, the Horizon 2020 project 770376-2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Block diagram of MOVE's architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>MAP with respect to embedding dimension d on validation data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of state-of-the-art VI systems (best results are highlighted in bold). Results on Da-TACOS are taken from<ref type="bibr" target="#b14">[15]</ref>.relatively large embedding dimension of 16 k, a smaller embedding size of 4 k can still outperform the state of the art. The results on YTC support the claim that MOVE achieves a new state-of-the-art performance (</figDesc><table><row><cell>.454</cell><cell>177</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/furkanyesiler/move</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Identification of versions of the same musical composition by processing audio descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Spain</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universitat Pompeu Fabra</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review of audio fingerprinting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Batlle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haitsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of VLSI Signal Processing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Audio content-based music retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grosche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Music Processing</title>
		<editor>M. Müller, M. Goto, and M. Schedl</editor>
		<meeting><address><addrLine>Wadern, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Dagstuhl Publishing</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="157" to="174" />
		</imprint>
	</monogr>
	<note>of Dagstuhl Follow-Ups</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A combining approach to cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Osmalskyj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Belgium</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Liege</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A mid-level melody-based representation for calculating audio similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Music Information Retrieval</title>
		<meeting>of the Int. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="280" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient index-based audio matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="382" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data driven and discriminative projections for large-scale cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Music Information Retrieval</title>
		<meeting>of the Int. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying &apos;cover songs&apos; with chroma features and dynamic programming beat tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Poliner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing</title>
		<meeting>of the IEEE Int. Conf. on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">IV</biblScope>
			<biblScope unit="page" from="1429" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Known-artist live song ID: a hashprint approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Prätzlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</title>
		<meeting>of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="427" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross recurrence quantification for cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Andrzejak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">93017</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The song remains the same: identifying versions of the same piece using tonal descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Music Information Retrieval</title>
		<meeting>of the Int. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="180" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Early MFCC and HPCP fusion for robust cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Tralie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR</title>
		<meeting>of the Int. Soc. for Music Information Retrieval Conf. (ISMIR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="294" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal similarity between musical streams for cover version detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Foucard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Durrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>of the IEEE Int. Conf. on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="5514" to="5517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fusing similarity functions for cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2629" to="2652" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Da-TACOS: a dataset for cover song identification and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yesiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tralie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Correya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tovstogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</title>
		<meeting>of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The million song dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Music Information Retrieval</title>
		<meeting>of the Int. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="628" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale cover song recognition using the 2D Fourier Transform magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. on Music Information Retrieval Conf. (ISMIR)</title>
		<meeting>of the Int. Soc. on Music Information Retrieval Conf. (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="241" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SiMPle: assessing music similarity using subsequences joins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chin-Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</title>
		<meeting>of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="23" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Key-invariant convolutional neural network toward efficient cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Multimedia and Expo (ICME)</title>
		<meeting>of the IEEE Int. Conf. on Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal pyramid pooling convolutional neural network for cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>of the Int. Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4846" to="4852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Triplet convolutional network for music version identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<editor>Multimedia Modeling, K. Schoeffmann, T. H. Chalidabhongse, C. W. Ngo, S. Aramvith, N. E. O&apos;Connor, Y.-S. Ho, M. Gabbouj, and A. Elgammal</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="544" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cover detection using dominant melody embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</title>
		<meeting>of the Int. Soc. for Music Information Retrieval Conf. (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured training for largevocabulary chord recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Soc. for Music Information Retrieval Conf. (ISMIR</title>
		<meeting>of the Int. Soc. for Music Information Retrieval Conf. (ISMIR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards a universal neural network encoder for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence Research and Development</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
	<note>Artificial Intelligence and Applications</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive pooling operators for weakly labeled sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2180" to="2193" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FaceNet: a unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Computer Vision Conference (ICCV)</title>
		<meeting>of the IEEE Int. Computer Vision Conference (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>ArXiv: 1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Music shapelets for fast cover song recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Music Information Retrieval</title>
		<meeting>of the Int. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="441" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cover song identification with 2D Fourier transform sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. on Acoustics, Speech and Signal Processing</title>
		<meeting>of IEEE Int. Conf. on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="616" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised detection of cover song sets: accuracy improvement and original identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laurier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sordo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Music Information Retrieval</title>
		<meeting>of the Int. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Summarizing and comparing music data and its application on cover song identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">V</forename><surname>Falcão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Andrade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Soc. for Music Information Retrieval Conf. (ISMIR)</title>
		<meeting>of Int. Soc. for Music Information Retrieval Conf. (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="732" to="739" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
