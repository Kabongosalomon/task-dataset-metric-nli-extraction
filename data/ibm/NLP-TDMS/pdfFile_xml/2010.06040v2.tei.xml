<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IMPROVING SELF-SUPERVISED PRE-TRAINING via A FULLY-EXPLORED MASKED LANGUAGE MODEL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhi</forename><surname>Zheng</surname></persName>
							<email>mizheng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<addrLine>365 AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
							<email>dishen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<addrLine>365 AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<addrLine>365 AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
							<email>lin.xiao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Dynamics</orgName>
								<address>
									<addrLine>365 AI</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IMPROVING SELF-SUPERVISED PRE-TRAINING via A FULLY-EXPLORED MASKED LANGUAGE MODEL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Masked Language Model (MLM) framework has been widely adopted for selfsupervised language pre-training. In this paper, we argue that randomly sampled masks in MLM would lead to undesirably large gradient variance. Thus, we theoretically quantify the gradient variance via correlating the gradient covariance with the Hamming distance between two different masks (given a certain text sequence). To reduce the variance due to the sampling of masks, we propose a fully-explored masking strategy, where a text sequence is divided into a certain number of non-overlapping segments. Thereafter, the tokens within one segment are masked for training. We prove, from a theoretical perspective, that the gradients derived from this new masking schema have a smaller variance and can lead to more efficient self-supervised training. We conduct extensive experiments on both continual pre-training and general pre-training from scratch. Empirical results confirm that this new masking strategy can consistently outperform standard random masking. Detailed efficiency analysis and ablation studies further validate the advantages of our fully-explored masking strategy under the MLM framework. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Large-scale pre-trained language models have attracted tremendous attention recently due to their impressive empirical performance on a wide variety of NLP tasks. These models typically abstract semantic information from massive unlabeled corpora in a self-supervised manner. Masked language model (MLM) has been widely utilized as the objective for pre-training language models. In the MLM setup, a certain percentage of words within the input sentence are masked out, and the model learns useful semantic information by predicting those missing tokens.</p><p>Previous work found that the specific masking strategy employed during pre-training plays a vital role in the effectiveness of the MLM framework . Specifically,  introduce entity-level and phrase-level masking strategies, which incorporate the prior knowledge within a sentence into its masking choice. Moreover,  propose to mask out random contiguous spans, instead of tokens, since they can serve as more challenging targets for the MLM objective.</p><p>Although effective, we identify an issue associated with the random sampling procedure of these masking strategies. Concretely, the difficulty of predicting each masked token varies and is highly dependent on the choice of the masking tokens. For example, predicting stop words such as "the" or "a" tends to be easier relative to nouns or rare words. As a result, with the same input sentence, randomly sampling certain input tokens/spans, as a typical masking recipe, will result in undesirable large variance while estimating the gradients. It has been widely demonstrated that large gradient variance typically hurts the training efficiency with stochastic gradient optimization algorithms <ref type="bibr" target="#b40">(Zhang &amp; Xiao, 2019;</ref><ref type="bibr" target="#b36">Xiao &amp; Zhang, 2014;</ref><ref type="bibr" target="#b13">Johnson &amp; Zhang, 2013)</ref>. Therefore, we advocate that obtaining gradients with a smaller variance has the potential to enable more sample-efficient learning and thus accelerate the self-supervised learning stage.</p><p>In this paper, we start by introducing a theoretical framework to quantify the variance while estimating the training gradients. The basic idea is to decompose the total gradient variance into two terms, where the first term is induced by the data sampling process and the second one relates to the sampling procedure of masked tokens. Theoretical analysis on the second variance term demonstrates that it can be minimized by reducing the gradient covariance between two masked sequences. Furthermore, we conduct empirical investigation on the correlation between the gradient's covariance while utilizing two masked sequences for training and the Hamming distance between these sequences. We observed that that the gradients' covariance tends to decrease monotonically w.r.t the sequences' Hamming distance.</p><p>Inspired by the observations above, we propose a fully-explored masking strategy, which maximizes the Hamming distance between any of two sampled masks on a fixed text sequence. First, a text sequence is randomly divided into multiple non-overlapping segments, where each token (e.g. subword, word or span) belongs to one of them. While the model processes this input, several different training samples are constructed by masking out one of these segments (and leaving the others as the contexts). In this manner, the gradient w.r.t. this input sequence can be calculated by averaging the gradients across multiple training samples (produced by the same input sequence). We further verify, under our theoretical framework, that the gradients obtained with such a scheme tend to have smaller variance, and thus can improve the efficiency of the pre-training process.</p><p>We evaluate the proposed masking strategies on both continued pre-training <ref type="bibr" target="#b11">(Gururangan et al., 2020)</ref> and from-scratch pre-training scenarios. Specifically, Computer Science (CS) and News domain corpus <ref type="bibr" target="#b11">(Gururangan et al., 2020)</ref> are leveraged to continually pre-train RoBERTa models, which are then evaluated by fine-tuning on downstream tasks of the corresponding domain. It is demonstrated that the proposed fully-explored masking strategies lead to pre-trained models with stronger generalization ability. Even with only a subset of the pre-training corpus utilized in <ref type="bibr" target="#b11">(Gururangan et al., 2020)</ref>, our model consistently outperforms reported baselines across four natural language understanding tasks considered. Besides, we also show the effectiveness of our method on the pre-training of language models from scratch. Moreover, the comparison between fully-explored and standard masking strategies in terms of their impacts on the model learning efficiency further validates the advantages of the proposed method. Extensive ablation studies are further conducted to demonstrate the robustness of the proposed masking scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Self-supervised Language Pre-training Self-supervised learning has been demonstrated as a powerful paradigm for natural language pre-training in recent years. Significant research efforts have been devoted to improve different aspects of the pre-training recipe, including training objective <ref type="bibr" target="#b7">Clark et al., 2019;</ref><ref type="bibr" target="#b3">Bao et al., 2020;</ref>, architecture design , the incorporation of external knowledge , etc. The idea of self-supervised learning has also been extended to generation tasks and achieves great results <ref type="bibr" target="#b28">(Song et al., 2019;</ref><ref type="bibr" target="#b9">Dong et al., 2019)</ref>. Although impressive empirical performance has been shown, relatively little attention has been paid to the efficiency of the pre-training stage. ELECTRA <ref type="bibr" target="#b7">(Clark et al., 2019)</ref> introduced a discriminative objective that is defined over all input tokens. Besides, it has been showed that incorporating language structures  or external knowledge  into pre-training could also help the language models to better abstract useful information from unlabeled samples.</p><p>In this work, we approach the training efficiency issue from a different perspective, and argue that the masking strategies, as an essential component within the MLM framework, plays a vital role especially in efficient pre-training. Notably, our fully-explored masking strategies can be easily combined with different model architectures for MLM training. Moreover, the proposed approach can be flexibly integrated with various tokenization choices, such as subword, word or span . A concurrent work  also shared similar motivation as this work, although they have a different solution and their method requires additional computation to generate the masks, and yet is outperformed by the proposed fully-explored masking (see <ref type="table" target="#tab_1">Table 2</ref>).</p><p>Domain-specific Continual Pre-training The models mentioned above typically abstract semantic information from massive, heterogeneous corpora. Consequently, these models are not tailored to any specific domains, which tends to be suboptimal if there is a domain of interest beforehand. <ref type="bibr" target="#b11">Gururangan et al. (2020)</ref> showed that continual pre-training (on top of general-purpose LMs) with in-domain unlabeled data could bring further gains to downstream tasks (of that particular domain). One challenge inherent in continual pre-training is that in-domain data are usually much more lim- <ref type="figure">Figure 1</ref>: Illustration of the proposed fully-explored masking strategy with a specific example. In this case, the input sequence has been divided into 4 exclusive segments, where different colors indicate which segment a certain token belongs to.</p><p>ited, compared to domain-invariant corpora. As a result, how to efficiently digest information from unlabeled corpus is especially critical while adapting large pre-trained language models to specific domains. To this end, we specifically consider the continual pre-training scenario to evaluate the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>In this section, we first review the MLM framework that is widely employed for natural language pre-training. Motivated by the gradient variance analysis of MLM in section 3.2, we present the fully-explored masking strategy, which serves as a simple yet effective solution to reduce the gradient variance during training. Connections between our method and variance reduction theory are further drawn, which provides a theoretical foundation for the effectiveness of the proposed strategy. Finally, some specific implementation details are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BACKGROUND: THE MLM FRAMEWORK</head><p>Let V denote the token vocabulary and x = (x 1 , . . . , x n ) denote a sentence of n tokens, where x i ∈ V for i = 1, . . . , n. Let m = (m 1 , . . . , m n ) denote a binary vector of length n, where m i ∈ {0, 1}, representing the mask over a sentence. Specifically, m i = 1 means the token x i is masked and m i = 0 if x i is not masked. We use m • x to denote a masked sentence, that is,</p><formula xml:id="formula_0">(m • x) i = [MASK] if m i = 1, x i if m i = 0.</formula><p>In addition, let m be the complement of m; in other words, m i = 0 if m i = 1 and m i = 1 if m i = 0. Naturally, m • x denotes a sentence with the complement mask m.</p><p>For a typical language model with parameters θ, its loss function over a sentence x ∈ V n and a mask m ∈ {0, 1} n as</p><formula xml:id="formula_1">(θ; x, m) = − log P (m • x | θ, m • x) = − i : mi=1 log P (x i | θ, m • x),<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">P (x i | θ, m • x)</formula><p>is the probability of the model correctly predicting x i given the masked sentence m • x. If m i = 0, it always has P (x i | θ, m • x) = 1 as the ground-truth x i is not masked.</p><p>We will focus on masks with a fixed length. Let τ be an integer satisfying 0 ≤ τ ≤ n. The set of possible masks of length τ is defined as M(τ ) ,</p><formula xml:id="formula_3">M(τ ) = m ∈ {0, 1} n | n i=1 m i = τ , which has a cardinality |M(τ )| = n τ = n! τ !(n−τ )! .</formula><p>Therefore, the average loss function over a sentence x with masks of length τ is,</p><formula xml:id="formula_4">L(θ; x) = E m∼Unif(M(τ )) (θ; x, m) = 1 n τ m∈M(τ ) (θ; x, m).<label>(2)</label></formula><p>Let's consider P D as the probability distribution of sentence in a corpus D ⊂ V n . The overall loss function for training the masked language model over corpus D is</p><formula xml:id="formula_5">L(θ) E x∼P D L(θ; x) = E x∼P D E m∼Unif(M(τ )) (θ; x, m).<label>(3)</label></formula><p>During each step of the training process, it randomly samples a mini-batch of sentences S t ⊂ D. For each x ∈ S t , we randomly pick a subset of masks K t (x) ⊂ M(τ ), independently across different x. Thus, the mini-batch stochastic gradient is</p><formula xml:id="formula_6">g t (θ) = 1 S x∈St 1 K m∈Kt(x) ∇ θ (θ; x, m).<label>(4)</label></formula><p>where |S t | = S and |K t (x)| = K for all t. Clearly we have E[g t (θ)] = ∇L(θ). In the following sections, it first gives the variance of g t (θ) which is an important factor to influence model training efficiency <ref type="bibr" target="#b36">(Xiao &amp; Zhang, 2014;</ref><ref type="bibr" target="#b40">Zhang &amp; Xiao, 2019)</ref>, and then it presents the proposed fullyexplored masking strategy to help reduce the gradient variance of the masked language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ANALYSIS: GRADIENT VARIANCE OF MLM</head><p>According to the law of total variance <ref type="bibr" target="#b35">(Weiss, 2005)</ref>, the variance of the mini-batch stochastic gradient Var St,Kt (g t ) can be decomposed as follows,</p><formula xml:id="formula_7">Var St,Kt (g t ) = E St Var Kt (g t ) | S t + Var St E Kt [g t | S t )] ,<label>(5)</label></formula><p>where for simplicity g t indicates g t (θ) as in eqn. 4, the first term captures the variance due to the sampling of masks, and the second term is the variance due to the sampling of mini-batch sentences.</p><p>In this work, we focus on the analysis of the first term in eqn. 5: the variance due to the sampling of masks. Denote g(m) = ∇ θ (θ; x, m) for any fixed sentence x. Consider a subset of random masks (m 1 , . . . , m K ) and the K-masks gradient is defined as the average of them:</p><formula xml:id="formula_8">g(m 1 , . . . , m K ) = 1 K K k=1 g(m k ).<label>(6)</label></formula><p>Theorem 1. The Variance of K-masks gradient: Var g(m 1 , . . . , m K ) is</p><formula xml:id="formula_9">1 K Var g(m 1 ) + 1 − 1 K Cov g(m 1 ), g(m 2 ) .<label>(7)</label></formula><p>where,</p><formula xml:id="formula_10">Cov g(m 1 ), g(m 2 ) = E g(m 1 ) −ḡ T g(m 2 ) −ḡ ,<label>(8)</label></formula><formula xml:id="formula_11">andḡ = E m∼Unif(M(τ )) g(m) = 1 n τ m∈M(τ ) g(m).<label>(9)</label></formula><p>The detailed proof of Theorem 1 is given in Appendix A.2. In the theorem 1, it indicates that the variance of K-masks gradient can be reduced by decreasing the gradient covariance between different masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VARIANCE REDUCTION: FULLY-EXPLORED MASKING</head><p>Intuitively, if we consider the two random masks m 1 and m 2 are totally overlapped, the gradient covariance between them should be maximized. It motivates us to consider the correlation between gradient covariance and Hamming distance between these two masks. Thus, we have the following assumption: Assumption 1. The covariance Cov g(m 1 ), g(m 2 ) is monotone decreasing in term of the Hamming distance between m 1 and m 2 .</p><p>To verify the Assumption 1, we sample a small set of CS domain sentences from S2ORC dataset (Gururangan et al., 2020) as the fixed mini-batch for our analysis, then calculate gradient covariance Cov g(m 1 ), g(m 2 ) of mask pairs (m 1 , m 2 ) with different Hamming distances H(m 1 , m 2 ) using this mini-batch. In <ref type="figure">Figure 2</ref>, the center of gradient covariance distribution is shifting to left (lower value) as Hamming distance increases. In <ref type="figure">Figure 3</ref>, we also observe that the average gradient covariance is decreasing in term od Hamming distance. As shown in <ref type="figure">Figure 2</ref>, 3, Assumption 1 holds for both RoBERTa-base model  and RoBERTa-base model after continually pre-trained on CS domain corpus. We propose the fully-explored masking strategy which restricts masks sampled from M(τ ) to be non-overlapping, denoted as M FE (τ ) for simplicity:</p><formula xml:id="formula_12">m 1 , . . . , m K ∼ M FE (τ ), ∀ i =j H(m i , m j ) = 2τ</formula><p>(10) With the fully-explored masking strategy, it can be easily approved that expectation of gradient over M FE (τ ) is an unbiased estimation of the expectation of gradient over M(τ ) as in the Lemma 2. In the Lemma 3, it states that the Theorem 1 is still hold for fully-explored masking strategy, which indicates that the variance of K-masks gradient can be reduced by restricting the masks sampling from M FE (τ ). Lemma 2. The expectation of gradient over M FE (τ ) equals to the expectation of gradient over M(τ ).</p><p>Proof. The joint distributions of (m 1 , . . . , m K ) sampling from M FE (τ ) is different from the i.i.d. case due to the non-overlapping restriction. However, the marginal distributions of m k are still the same uniform distribution over M(τ ). Therefore, we still have E g(m k ) =ḡ, ∀ k=1,...,K and as a consequence E g(m 1 , . . . , m K ) =ḡ.</p><p>Lemma 3. The derivation of K-masks gradient variance in Eqn.7 holds for both M FE (τ ) and M(τ ).</p><p>The detailed proof of Lemma 3 can be seen in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">IMPLEMENTATION DETAILS</head><p>The details of fully-explored masking algorithm is illustrated in Algorithm 1. In practice, a text sequence S i is tokenized into subword pieces <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> with the maximum sequence length n set as 512 in the experiments. To understand the performance of fully-explored masking strategy at different granularity, the text sequence S i is masked at both subword level <ref type="bibr" target="#b8">(Devlin et al., 2018;</ref> and span level . The details about other hyperparameters, i.e., masking-ratio and number of splits K will be discussed in experiment section.</p><p>Algorithm 1: Fully-explored Masking Language Model Input: Language corpus D = {S 1 , ..., S T }, |S i | = n; Masking ratio τ n ; Number of sampling masks K, where K * τ n ≤ 1; Initial model parameters θ 0 ; Output: model parameters θ * foreach S i ∈ S do Sample K split masking vectors (m 1 , ..., m K ) from M FE (τ ) as in Eqn.10. Calculate the gradient g(m 1 , ..., m K ) as in Eqn. 6. Update model parameters θ i+1 = Optimizer(θ i , g(m 1 , ..., m K )) end return θ * = θ T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the proposed fully-explored masking strategy for natural language pretraining in two distinct settings: i) continual pre-training, where a given pre-trained model is further adapted leveraging domain-specific unlabeled corpus; ii) pre-training from scratch, where largescale corpus such as Wikipedia and BookCorpus are employed to pre-train a model from the beginning. We also compare the training efficiency of FE-MLM and MLM frameworks to validate our theoretical findings. Ablation studies and analysis are further conducted regarding the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL SETTINGS</head><p>For the continual pre-training scenario, we consider unlabeled corpus from two different domains, i.e., computer science (CS) papers and news text from RealNews, introduced by Gururangan et al. <ref type="bibr">(2020)</ref>. As to the downstream tasks, ACL-ARC citation intent <ref type="bibr" target="#b15">Jurgens et al. (2018)</ref> and SciERC relation classification <ref type="bibr" target="#b23">Luan et al. (2018)</ref> are utilized for the CS domain. While for the News domain, HyperPartisan news detection <ref type="bibr" target="#b16">Kiesel et al. (2019)</ref> and AGNews <ref type="bibr" target="#b41">Zhang et al. (2015)</ref> are employed to facilitate the comparison with <ref type="bibr" target="#b11">Gururangan et al. (2020)</ref>.</p><p>Following <ref type="bibr" target="#b11">(Gururangan et al., 2020)</ref> for a fair comparison, RoBERTa  is leveraged as the initial model for continual pre-training, where the same training objective is optimized on the domain-specific corpus. We choose a batch size of 48, and the model is trained using Adam <ref type="bibr" target="#b17">Kingma &amp; Ba (2014)</ref>, with a learning rate of 1 × 10 −4 . It is worth noting that we observe, in our initial experiments, that downsampling only 72k documents from the total of 2.22M used by <ref type="bibr" target="#b11">Gururangan et al. (2020)</ref> can result in similar performance on downstream tasks. This happens in the News domain as well, where we randomly sample 623k documents out of 11.90M. The model is continually pre-trained for around 40k and 20k steps on the CS and News domain, respectively. One important hyerparameter under the FE-MLM framework is the number of split the input sequence is divided into, where we use 4 as the default setting. The sensitivity of the proposed algorithm w.r.t this hyperparameter is further investigated (see <ref type="figure" target="#fig_1">Figure 4</ref>).</p><p>For the general pre-training experiments, we employ BERT as the baseline model. Wikiepdia and BookCorpus <ref type="bibr" target="#b43">(Zhu et al., 2015)</ref> are used as the pre-training corpus, with a total size of 16G. We adopt the same tokenization (i.e., WordPiece embeddings) as BERT, which consists of 30,522 tokens in the vocabulary. The model is optimized using Adam with the learning rate set as 1 × 10 −4 . A batch size of 256 is employed, and we train the model for 1M step. The resulting model is evaluated on the GLUE benchmark <ref type="bibr" target="#b32">(Wang et al., 2018)</ref>, which comprises 9 natural language understanding (NLU) tasks such as textual entailment <ref type="figure">(MNLI, RTE)</ref>, question-answer entailment (QNLI), question paraphrase (QQP), paraphrase (MRPC), sentimnt analysis (SST-2), linguistic acceptability (CoLA) and textual similarity (STS-B). The HuggingFace codebase 1 is used in our implementation for both settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACL-ARC</head><p>SciERC HyperPartisan AGNews RoBERTa <ref type="bibr" target="#b11">(Gururangan et al., 2020)</ref> 63.0 ± 5.8 77.3 ± 1.9 86.6 ± 0.9 93.9 ± 0.2 DAPT <ref type="bibr" target="#b11">(Gururangan et al., 2020)</ref> 75.4 ± 2.5 80.8 ± 1.5 88.2 ± 5.9 93.9 ± 0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTAL RESULTS</head><p>Continual Pre-training Evaluation We applied our fully-explored MLM framework to both subword and span masking scenarios. The results for the RoEBRTa model continually pre-trained on the CS and News domains are presented in <ref type="table" target="#tab_0">Table 1</ref>. It can be observed that the continual pre-training stage can benefit the downstream tasks on both domains(compared with fine-tuning the RoBERTa model directly). Besides, the baseline numbers based on our implementation is on par with or even better than those reported in Gururangan et al. <ref type="formula" target="#formula_4">(2020)</ref>, even though we downsample the original unlabeled corpus (as described in the previous section).</p><p>More importantly, in the subword masking case, our FE-MLM framework consistently exhibits better empirical results on the downstream tasks. Note that to ensure fair comparison, the same computation is taken for both MLM and FE-LMLM training. This indicates that the models pre-trained using the FE-MLM approach have been endowed with stronger generalization ability, relative to standard MLM training. Similar trend is also observed in the span masking experiments, demonstrating that the proposed method can be naturally and flexibly integrated with different masking schemes. Besides, we found that subword masking tends to work better than span masking in the CS domain, whereas the opposite is true as to the News domain. This may be attributed to the different nature of the unlabeled corpus from two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Avg. Score ELMo, <ref type="bibr" target="#b24">(Peters et al., 2018)</ref> 71.2 GPT, <ref type="bibr" target="#b25">(Radford et al., 2018)</ref> 78.8 BERT-base, <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref> 82.2 BERT-base (ReEval) 82.5 MAP-Net,  82.1 BERT-base (FE-MLM) 83.6 <ref type="table">Table 3</ref>: The comparison between the FE-MLM model with several baseline methods, based on the averaged score (on the dev set) across different tasks from the GLUE benchmark.</p><p>General Pre-training Evaluation We also evaluate the FE-MLM framework on the pretraining experiments with general-purpose unlabeled corpus. Specifically, we follow the same setting as BERT, except that the proposed fullyexplored masking strategy is applied (the same amount of computation is used for the baseline and our method). The corresponding results are shown in <ref type="table" target="#tab_1">Table 2</ref>. It can be found that the FE-MLM approach, while fine-tuned on the GLUE benchmark, exhibits better results on 7 out of 9 NLU datasets than the MLM baseline. This demonstrates the wide applicability of the proposed FE-MLM framework across different pre-training settings.</p><p>We further compare the averaged score over 9 GLUE datasets with other methods, and the numbers are summarized in <ref type="table">Table 3</ref>. It is worth noting that the BERT-based (ReEval) baseline is obtained by fine-tuning the BERT model released by <ref type="bibr" target="#b8">Devlin et al. (2018)</ref> on each GLUE datasets, with the results on the dev sets averaged. Another BERT-base number is reported by <ref type="bibr" target="#b7">Clark et al. (2019)</ref>, which is pretty similar our re-evaluation one. MAsk proposal network (MAP-Net) is proposed by , which shares the same motivation of reducing the gradient variance during the masking stage. However, we approach the problem with a distinct strategy based upon extensive theoretical analysis. We found that BERT-base model improved with the FE-MLM training significantly outperform BERT-base model and Mask Proposal Network, further demonstrating the effectiveness of proposed approach. Training Efficiency Although previous results has demonstrated that our model at the end of pre-training process exhibits stronger generalization ability, it is still unclear how the proposed FE-MLM framework influence the training efficiency during training. In this regard, we examine the intermediate models obtained with both MLM and FE-MLM training by fine-tuning and evaluating them on the ACL-ARC dataset. Specifically, the RoBERTa-base setting is used here, which is continually pre-trained on the unlabeled corpus from the CS domain. As shown on the left side of <ref type="figure" target="#fig_1">Figure 4</ref>, FE-MLM beats MLM at different steps of pre-training. More importantly, the performance of the FE-MLM model improves much faster at the early stage (i.e., less than around 15,000 steps), indicating that the model is able to extract useful semantic information (from unlabeled corpus) more efficiently with the proposed masking strategy. This observation further highlights the advantage and importance of reducing gradient variance under the MLM framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDIES</head><p>The Effect of Masking Split Number The number of masking split the input sentence is divided into is a vital hyperparameter for the FE-MLM approach. Therefore, we investigate its impact on the performance of resulting models. Concretely, the setting of continual pre-training on the CS domain is employed, where the RoBERTa model is pre-trained with the FE-MLM objective. Different split number is explored, including 2, 4, 6, 8, and 12.5% of all the tokens are masked within each split. The results are visualized on the right side of <ref type="figure" target="#fig_1">Figure 4</ref>. We found that the downstream task performance (on both ACL-ARC and SciERC datasets) is fairly stable w.r.t. different split numbers. This may relate to our non-overlapping sampling strategy, which helps the model to explore various position in the sentence as efficiently as possible, so that the model exhibits strong performance even with only two splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we identified that under the MLM framework, the procedure of randomly sampling masked tokens will give rise to undesirably large variance while estimating the training gradients. Therefore, we introduced a theoretical framework to quantify the gradient variance, where the connection between gradient covariance and the Hamming distance between two different masked sequences are drawn. Motivated by these observations, we proposed a fully-explored masking strategy, where a text sequence is divided into multiple non-overlapping segments. During training, all tokens in one segment are masked out, and the model is asked to predict them with the other segments as the context. It was demonstrated theoretically that the gradients obtained with such a novel masking strategy have a smaller variance, thus enabling more efficient pre-training. Extensive experiments on both continual pre-training and general pre-training from scratch showed that the proposed masking strategy consistently outperforms standard random masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 DATASET STATISTICS</p><p>The statistics of the datasets within the GLUE benchmark are summarized in <ref type="table" target="#tab_3">Table 4</ref>.  Proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Var g(m 1 , . . . , m K ) = E g(m 1 , . . . , m K ) −ḡ 2</p><formula xml:id="formula_13">= E 1 K K k=1 g(m k ) −ḡ 2 = 1 K 2 E K k=1 g(m k ) −ḡ 2 = 1 K 2 E   K k=1 g(m k ) −ḡ 2 + k =l g(m k ) −ḡ T g(m l ) −ḡ   = 1 K 2   K k=1</formula><p>Var g(m k ) + Var g(m 1 ) = · · · = Var g(m K )</p><p>Cov g(m k ), g(m l ) = Cov g(m 1 ), g(m 2 ) , ∀ k = l.</p><p>(13) Therefore we have the following variance decomposition:</p><p>Var g(m 1 , . . . , m K ) = 1 K Var g(m 1 ) + 1 − 1 K Cov g(m 1 ), g(m 2 ) .</p><p>A.3 PROOF OF LEMMA 3</p><p>Proof. The joint distribution of the pairs (m k , m l ) sampling from M FE (τ ) are different from the i.i.d. case, it can be shown (by symmetry) that the identity equation 13 also holds. Considering the fact that the derivation in equation 11 holds for any sampling strategy, we conclude that the variance decomposition in equation 14 still holds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>The distributions of gradient covariance Cov g(m 1 ), g(m 2 ) for different Hamming distances H(m 1 , m 2 ) based on a small CS domain corpus. Left: gradient covariance distribution of selected parameters in RoBERTa-base model; Right: gradient covariance distribution of selected parameters in RoBERTa-base model after continually pre-trained on CS domain corpus. Empirical analysis of the correlation between gradient covariance Cov g(m 1 ), g(m 2 ) and Hamming distance H(m 1 , m 2 ) based on a small CS domain corpus. For a sequence of length 512, two masks m 1 , m 2 are randomly sampled with 128 masked tokens, their Hamming distance satisfying 0 ≤ H(m 1 , m 2 ) ≤ 256. Left: gradient covariance calculated based on RoBERTa-base model; Right: gradient covariance calculated based on RoBERTa-base model after continually pretrained on CS domain corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Left: the efficiency comparison between the standard MLM and fully-explored MLM approach. Specifically, the RoBERTa-base model and continual pre-training setting (on the CS domain) are employed. The corresponding models are evaluated on the ACL-ARC dataset (at different training steps). Right: the effect of split number (under the FE-MLM framework) on the generalization ability of pre-trained models, evaluated on two datasets in the CS domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>pair k = l, Cov g(m k ), g(m l ) = E g(m k ) −ḡ T g(m l ) −ḡ ,Since m 1 , . . . , m K are i.i.d. samples from the uniform distribution over M(τ ), we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>2Table 1 :</head><label>1</label><figDesc>MLM + Subword (our implementation) 75.34 ± 2.54 81.51 ± 1.05 91.00 ± 2.66 94.05 ± 0.16 FE-MLM + Subword 76.24 ± 1.86 82.40±0.86 92.35 ± 3.49 94.02 ± 0.09 The empirical results on continual pre-training setting, where RoBERTa and DAPT (RoBERTa continually pre-trained with the standard MLM objective) is leveraged as our baseline to facilitate comparison with<ref type="bibr" target="#b11">(Gururangan et al., 2020)</ref>. Specifically, ACL-ARC and SciERC are evaluated with the continually pre-trained model with CS domain corpus, while HyperPartisann and AGNews are based upon models trained with News domain corpus.</figDesc><table><row><cell cols="2">MLM + Span (our implementation)</cell><cell>76.63 ± 1.65 81.33 ± 1.16</cell><cell cols="2">91.72 ± 3.26</cell><cell>93.94 ± 0.05</cell></row><row><cell cols="2">FE-MLM + Span</cell><cell>78.06±2.31 81.99 ± 0.79</cell><cell cols="2">93.22±3.31</cell><cell>94.13±0.04</cell></row><row><cell>Model</cell><cell cols="5">MNLI-m/mm SST-2 QNLI QQP RTE MRPC CoLA STS-B</cell></row><row><cell>BERT (MLM)</cell><cell>84.37/84.85</cell><cell>92.78 91.01 91.09 63.54</cell><cell>87.01</cell><cell>59.65</cell><cell>87.89</cell></row><row><cell>BERT (FE-MLM)</cell><cell>85.09/84.63</cell><cell>93.23 91.01 91.16 68.59</cell><cell>87.99</cell><cell>61.32</cell><cell>89.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The results on the dev sets of GLUE benchmarks, where MLM and FE-MLM are compared with the BERT-base model as the testbed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>GLUE benchmark dataset statistics summary.</figDesc><table /><note>A.2 PROOF OF THEOREM 1</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/huggingface/transformers</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weighted transformer network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1711.02132</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Variance reduction in sgd by distributed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06481</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Finbert: Financial sentiment analysis with pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dogu</forename><surname>Araci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10063</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12804</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05333</idno>
		<title level="m">Variance-reduced language pretraining via a mask proposal network</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Train no evil: Selective masking for task-guided pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<title level="m">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<title level="m">Spanbert: Improving pre-training by representing and predicting spans</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring the evolution of a scientific field through citation frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raine</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semeval-2019 task 4: Hyperpartisan news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payam</forename><surname>Adineh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="829" to="839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">S2orc: The semantic scholar open research corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mass: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variance reduction for stochastic gradient optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04577</idno>
		<title level="m">Structbert: Incorporating language structures into pre-training for deep language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A course in probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="385" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A proximal stochastic gradient method with progressive variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">Christopher</forename><surname>Siy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08097</idno>
		<title level="m">Finbert: A pretrained language model for financial communications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Defending against neural fake news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9054" to="9065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A stochastic composite gradient method with incremental variance reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernie</surname></persName>
		</author>
		<title level="m">Enhanced language representation with informative entities. ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
