<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Computación</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Computación</orgName>
								<orgName type="institution">Instituto Politécnico Nacional</orgName>
								<address>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal sentiment analysis is a very actively growing field of research. A promising area of opportunity in this field is to improve the multimodal fusion mechanism. We present a novel feature fusion strategy that proceeds in a hierarchical fashion, first fusing the modalities two in two and only then fusing all three modalities. On multimodal sentiment analysis of individual utterances, our strategy outperforms conventional concatenation of features by 1%, which amounts to 5% reduction in error rate. On utterance-level multimodal sentiment analysis of multi-utterance video clips, for which current state-of-the-art techniques incorporate contextual information from other utterances of the same clip, our hierarchical fusion gives up to 2.4% (almost 10% error rate reduction) over currently used concatenation. The implementation of our method is publicly available in the form of open-source code.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The implementation of our method is publicly available in the form of opensource code. 1 This paper is structured as follows: Section 2 briefly discusses important previous work in multimodal feature fusion; Section 3 describes our method in details; Section 4 reports the results of our experiments and discuss their implications; finally, Section 5 concludes the paper and discusses future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In recent years, sentiment analysis has become increasingly popular for processing social media data on online communities, blogs, wikis, microblogging platforms, and other online collaborative media <ref type="bibr">[2]</ref>. Sentiment analysis is a branch of affective computing research <ref type="bibr" target="#b0">[3]</ref> that aims to classify text -but sometimes also audio and video <ref type="bibr" target="#b1">[4]</ref> -into either positive or negative -but sometimes also neutral <ref type="bibr" target="#b2">[5]</ref>. Most of the literature is on English language but recently an increasing number of works are tackling the multilinguality issue <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b4">7]</ref>, especially in booming online languages such as Chinese <ref type="bibr" target="#b5">[8]</ref>. Sentiment analysis techniques can be broadly categorized into symbolic and sub-symbolic approaches: the former include the use of lexicons <ref type="bibr" target="#b6">[9]</ref>, ontologies <ref type="bibr" target="#b7">[10]</ref>, and semantic networks <ref type="bibr" target="#b8">[11]</ref> to encode the polarity associated with words and multiword expressions; the latter consist of supervised <ref type="bibr" target="#b9">[12]</ref>, semi-supervised <ref type="bibr" target="#b10">[13]</ref> and unsupervised <ref type="bibr" target="#b11">[14]</ref> machine learning techniques that perform sentiment classification based on word cooccurrence frequencies. Among these, the most popular recently are algorithms based on deep neural networks <ref type="bibr" target="#b12">[15]</ref> and generative adversarial networks <ref type="bibr" target="#b13">[16]</ref>.</p><p>While most works approach it as a simple categorization problem, sentiment analysis is actually a suitcase research problem <ref type="bibr" target="#b14">[17]</ref> that requires tackling many NLP tasks, including word polarity disambiguation <ref type="bibr" target="#b15">[18]</ref>, subjectivity detection <ref type="bibr" target="#b16">[19]</ref>, personality recognition <ref type="bibr" target="#b17">[20]</ref>, microtext normalization <ref type="bibr" target="#b18">[21]</ref>, concept extraction <ref type="bibr" target="#b19">[22]</ref>, time tagging <ref type="bibr" target="#b20">[23]</ref>, and aspect extraction <ref type="bibr" target="#b21">[24]</ref>. Sentiment analysis has raised growing interest both within the scientific community, leading to many exciting open challenges, as well as in the business world, due to the remarkable benefits to be had from financial <ref type="bibr" target="#b22">[25]</ref> and political <ref type="bibr" target="#b23">[26]</ref> forecasting, e-health <ref type="bibr" target="#b24">[27]</ref> and e-tourism <ref type="bibr" target="#b25">[28]</ref>, user profiling <ref type="bibr" target="#b26">[29]</ref> and community detection <ref type="bibr" target="#b27">[30]</ref>, manufacturing and supply chain applications <ref type="bibr" target="#b28">[31]</ref>, human communication comprehension <ref type="bibr" target="#b29">[32]</ref> and dialogue systems <ref type="bibr" target="#b30">[33]</ref>, etc.</p><p>In the field of emotion recognition, early works by De Silva et al. <ref type="bibr" target="#b31">[34]</ref> and Chen et al. <ref type="bibr" target="#b32">[35]</ref> showed that fusion of audio and visual systems, creating a bimodal signal, yielded a higher accuracy than any unimodal system. Such fusion has been analyzed at both feature level <ref type="bibr" target="#b33">[36]</ref> and decision level <ref type="bibr" target="#b34">[37]</ref>.</p><p>Although there is much work done on audio-visual fusion for emotion recognition, exploring contribution of text along with audio and visual modalities in multimodal emotion detection has been little explored. Wollmer et al. <ref type="bibr" target="#b35">[38]</ref> and Rozgic et al. <ref type="bibr" target="#b37">[39]</ref> fused information from audio, visual and textual modalities to extract emotion and sentiment. Metallinou et al. <ref type="bibr" target="#b39">[40]</ref> and Eyben et al. <ref type="bibr" target="#b40">[41]</ref> fused audio and textual modalities for emotion recognition. Both approaches relied on a feature-level fusion. Wu and Liang <ref type="bibr" target="#b41">[42]</ref> fused audio and textual clues at decision level. Poria et al. <ref type="bibr" target="#b42">[43]</ref> uses convolutional neural network (CNN) to extract features from the modalities and then employs multiple-kernel learning (MKL) for sentiment analysis. The current state of the art, set forth by Poria et al. <ref type="bibr">[1]</ref>, extracts contextual information from the surrounding utterances using long short-term memory (LSTM). Poria et al. <ref type="bibr" target="#b0">[3]</ref> fuses different modalities with deep learning-based tools. Zadeh et al. <ref type="bibr" target="#b43">[44]</ref> uses tensor fusion. Poria et al. <ref type="bibr" target="#b44">[45]</ref> further extends upon the ensemble of CNN and MKL.</p><p>Unlike existing approaches, which use simple concatenation based early fusion <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b45">46]</ref> and non-trainable tensors based fusion <ref type="bibr" target="#b43">[44]</ref>, this work proposes a hierarchical fusion capable of learning the bimodal and trimodal correlations for data fusion using deep neural networks. The method is end-to-end and, in order to accomplish the fusion, it can be plugged into any deep neural network based multimodal sentiment analysis framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>In this section, we discuss our novel methodology behind solving the sentiment classification problem. First we discuss the overview of our method and then we discuss the whole method in details, step by step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Unimodal Feature Extraction</head><p>We extract utterance-level features for three modalities. This step is discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Multimodal Fusion</head><p>Problems of early fusion. The majority of the work on multimodal data use concatenation, or early fusion ( <ref type="figure" target="#fig_0">Fig. 1</ref>), as their fusion strategy. The problem with this simplistic approach is that it cannot filter out and conflicting or redundant information obtained from different modalities. To address this major issue, we devise an hierarchical approach which proceeds from unimodal to bimodal vectors and then bimodal to trimodal vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Features Textual Features</head><p>Video Features Trimodal fusion. We fuse the three bimodal features to obtain trimodal feature as depicted in <ref type="figure" target="#fig_3">Fig. 3</ref>. This step is discussed in details in Section 3.4. Addition of context. We also improve the quality of feature vectors (both unimodal and multimodal) by incorporating information from surrounding utterances using RNN. We model the context using gated recurrent unit (GRU) as depicted in <ref type="figure" target="#fig_4">Fig. 4</ref>. The details of context modeling is discussed in Section 3.3 and the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully-Connected</head><p>Classification. We classify the feature vectors using a softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unimodal Feature Extraction</head><p>In this section, we discuss the method of feature extraction for three different modalities: audio, video, and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Textual Feature Extraction</head><p>The textual data is obtained from the transcripts of the videos. We apply a deep Convolutional Neural Networks (CNN) <ref type="bibr" target="#b46">[47]</ref> on each utterance to extract textual features. Each utterance in the text is represented as an array of pretrained 300-dimensional word2vec vectors <ref type="bibr" target="#b47">[48]</ref>. Further, the utterances are truncated or padded with null vectors to have exactly 50 words.</p><p>Next, these utterances as array of vectors are passed through two different convolutional layers; first layer having two filters of size 3 and 4 respectively  The output of the second max-pooling layer is fed to a fully-connected layer with 500 neurons with a rectified linear unit (ReLU) <ref type="bibr" target="#b48">[49]</ref> activation, followed by softmax output. The output of the penultimate fully-connected layer is used as the textual feature. The translation of convolution filter over makes the CNN learn abstract features and with each subsequent layer the context of the features expands further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Audio Feature Extraction</head><p>The audio feature extraction process is performed at 30 Hz frame rate with 100 ms sliding window. We use openSMILE <ref type="bibr" target="#b49">[50]</ref>, which is capable of automatic pitch and voice intensity extraction, for audio feature extraction. Prior to feature extraction audio signals are processed with voice intensity thresholding and voice normalization. Specifically, we use Z-standardization for voice normaliza- tion. In order to filter out audio segments without voice, we threshold voice intensity. OpenSMILE is used to perform both these steps. Using openSMILE we extract several Low Level Descriptors (LLD) (e.g., pitch , voice intensity) and various statistical functionals of them (e.g., amplitude mean, arithmetic mean, root quadratic mean, standard deviation, flatness, skewness, kurtosis, quartiles, inter-quartile ranges, and linear regression slope). "IS13-ComParE" configuration file of openSMILE is used to for our purposes. Finally, we extracted total 6392 features from each input audio segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Visual Feature Extraction</head><p>To extract visual features, we focus not only on feature extraction from each video frame but also try to model temporal features across frames. To achieve this, we use 3D-CNN on the video. 3D-CNNs have been successful in the past, specially in the field of object classification on 3D data <ref type="bibr" target="#b50">[51]</ref>. Its state-of-the-art performance on such tasks motivates its use in this paper.</p><p>Let the video be called vid ∈ R 3×f ×h×w , where 3 represents the three RGB channels of an image and f, h, and w denote the cardinality, height, and width of the frames, respectively. A 3D convolutional filter, named f lt ∈ R fm×3×f d ×f h ×fw , is applied to this video, where, similar to a 2D-CNN, the fil-ter translates across the video and generates the convolution output conv out ∈</p><formula xml:id="formula_0">R fm×3×(f −f d +1)×(h−f h +1)×(w−fw+1)</formula><p>. Here, f m , f d , f h , and f w denote number of feature maps, depth of filter, height of filter, and width of filter, respectively.</p><p>Finally, we apply max-pooling operation to the conv out , which selects the most relevant features. This operation is applied only to the last three dimensions of conv out . This is followed by a dense layer and softmax computation. The activations of this layer is used as the overall video features for each utterance video.</p><p>In our experiments, we receive the best results with filter dimensions f m = 32</p><p>and f d , f h , f w = 5. Also, for the max-pooling, we set the window size as 3×3×3 and the succeeding dense layer with 300 neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Context Modeling</head><p>Utterances in the videos are semantically dependent on each other. In other words, complete meaning of an utterance may be determined by taking preceding utterances into consideration. We call this the context of an utterance.</p><p>Following Poria et al.</p><p>[1], we use RNN, specifically GRU 3 to model semantic dependency among the utterances in a video.</p><p>Let the following items represent unimodal features:</p><formula xml:id="formula_1">f A ∈ R N ×d A (acoustic features), f V ∈ R N ×d V (visual features), f T ∈ R N ×d T (textual features),</formula><p>where N = maximum number of utterances in a video. We pad the shorter videos with dummy utterances represented by null vectors of corresponding length. For each modality, we feed the unimodal utterance features f m (where </p><formula xml:id="formula_2">m ∈ {A, V, T }) (discussed in Section 3.2) of a video to GRU m with output size D m , which is defined as z m = σ(f mt U mz + s m(t−1) W mz ), r m = σ(f mt U mr + s m(t−1) W mr ), h mt = tanh(f mt U mh + (s m(t−1) * r m )W mh ), F mt = tanh(h mt U mx + u mx ), s mt = (1 − z m ) * F mt + z m * s m(t−1) , where U mz ∈ R dm×Dm , W mz ∈ R Dm×Dm , U mr ∈ R dm×Dm , W mr ∈ R Dm×Dm , U mh ∈ R dm×Dm , W mh ∈ R Dm×Dm , U mx ∈ R dm×Dm , u mx ∈ R Dm , z m ∈ R Dm , r m ∈ R Dm , h mt ∈ R Dm , F mt ∈ R Dm ,</formula><formula xml:id="formula_3">we define F m = GRU m (f m ), where F m ∈ R N ×Dm .</formula><p>Thus, the context-aware multimodal features can be defined as</p><formula xml:id="formula_4">F A = GRU A (f A ), F V = GRU V (f V ), F T = GRU T (f T ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multimodal Fusion</head><p>In this section, we use context-aware unimodal features F A , F V , and F T to a unified feature space.</p><p>The unimodal features may have different dimensions, i.e.,</p><formula xml:id="formula_5">D A = D V = D T .</formula><p>Thus, we map them to the same dimension, say D (we obtained best results with D = 400), using fully-connected layer as follows:</p><formula xml:id="formula_6">g A = tanh(F A W A + b A ), g V = tanh(F V W V + b V ), g T = tanh(F T W T + b T ), where W A ∈ R D A ×D , b A ∈ R D , W V ∈ R D V ×D , b V ∈ R D , W T ∈ R D T ×D , and b T ∈ R D .</formula><p>We can represent the mapping for each dimension as</p><formula xml:id="formula_7">g x =         c x 11 c x 21 c x 31 · · · c x D1 c x 12 c x 22 c x 32 · · · c x D2 . . . . . . . . . · · · . . . c x 1N c x 2N c x 3N · · · c x DN        </formula><p>, where x ∈ {V, A, T } and c x lt are scalars for all l = 1, 2, . . . , D and t = 1, 2, . . . , N . Also, in g x the rows represent the utterances and the columns the feature values. We can see these values c x lt as more abstract feature values derived from fundamental feature values (which are the components of f A , f V , and f T ). For example, an abstract feature can be the angriness of a speaker in a video. We can infer the degree of angriness from visual features (f V ; facial muscle movements), acoustic features (f A , such as pitch and raised voice), or textual features (f T , such as the language and choice of words). Therefore, the degree of angriness can be represented by c x lt , where x is A, V , or T , l is some fixed integer between 1 and D, and t is some fixed integer between 1 and N . Now, the evaluation of abstract feature values from all the modalities may not have the same merit or may even contradict each other. Hence, we need the network to make comparison among the feature values derived from different modalities to make a more refined evaluation of the degree of anger. To this end, we take each bimodal combination (which are audio-video, audio-text, and video-text) at a time and compare and combine each of their respective abstract feature values (i.e. c V lt with c T lt , c V lt with c A lt , and c A lt with c T lt ) using fully-connected layers as follows:</p><formula xml:id="formula_8">i V A lt = tanh(w V A l .[c V lt , c A lt ] + b V A l ),<label>(1)</label></formula><formula xml:id="formula_9">i AT lt = tanh(w AT l .[c A lt , c T lt ] + b AT l ),<label>(2)</label></formula><formula xml:id="formula_10">i V T lt = tanh(w V T l .[c V lt , c T lt ] + b V T l ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_11">w V A l ∈ R 2 , b V A l is scalar, w AT l ∈ R 2 , b AT l is scalar, w V T l ∈ R 2 , and b V T l</formula><p>is scalar, for all l = 1, 2, . . . , D and t = 1, 2, . . . , N . We hypothesize that it will enable the network to compare the decisions from each modality against the others and help achieve a better fusion of modalities.</p><p>Bimodal fusion. Eqs.</p><p>(1) to (3) are used for bimodal fusion. The bimodal fused features for video-audio, audio-text, video-text are defined as</p><formula xml:id="formula_12">f V A = (f V A1 , f V A2 , . . . , f V A(N ) ), where f V At = (i V A 1t , i V A 2t , . . . , i V A Dt ), f AT = (f AT 1 , f AT 2 , . . . , f AT (N ) ), where f AT t = (i AT 1t , i AT 2t , . . . , i AT Dt ), f V T = (f V T 1 , f V T 2 , . . . , f V T (N ) ), where f V T t = (i V T 1t , i V T 2t , . . . , i V T Dt ).</formula><p>We further employ GRU m ( Section 3.3) (m ∈ {V A, V T, T A}), to incorporate contextual information among the utterances in a video with</p><formula xml:id="formula_13">F V A = (F V A1 , F V A2 , . . . , F V A(N ) ) = GRU V A (f V A ), F V T = (F V T 1 , F V T 2 , . . . , F V T (N ) ) = GRU V T (f V T ), F T A = (F T A1 , F T A2 , . . . , F T A(N ) ) = GRU T A (f T A ),</formula><p>where</p><formula xml:id="formula_14">F V At = (I V A 1t , I V A 2t , . . . , I V A D2t ), F V T t = (I AT 1t , I AT 2t , . . . , I AT D2t ), F T At = (I V T 1t , I V T 2t , . . . , I V T D2t ),</formula><p>F V A , F V T , and F T A are context-aware bimodal features represented as vectors and I m nt is scalar for n = 1, 2, . . . , D 2 , D 2 = 500, t = 1, 2, . . . , N , and m = VA,VT,TA.</p><p>Trimodal fusion. We combine all three modalities using fully-connected layers as follows:</p><formula xml:id="formula_15">z lt = tanh(w AV T l .[I V A lt , I AT lt , I V T lt ] + b AV T l ),</formula><p>where w AV T l ∈ R 3 and b AV T l is a scalar for all l = 1, 2, . . . , D 2 and t = 1, 2, . . . , N . So, we define the fused features as</p><formula xml:id="formula_16">f AV T = (f AV T 1 , f AV T 2 , . . . , f AV T (N ) ),</formula><p>where f AV T t = (z 1t , z 2t , . . . , z D2t ), z nt is scalar for n = 1, 2, . . . , D 2 and t = 1, 2, . . . , N .</p><p>Similarly to bimodal fusion (Section 3.4), after trimodal fusion we pass the fused features through GRU AV T to incorporate contextual information in them, which yields</p><formula xml:id="formula_17">F AV T = (F AV T 1 , F AV T 2 , . . . , F AV T (N ) ) = GRU AV T (f AV T ),</formula><p>where F AV T t = (Z 1t , Z 2t , . . . , Z D3t ), Z nt is scalar for n = 1, 2, . . . , D 3 , D 3 = 550, t = 1, 2, . . . , N , and F AV T is the context-aware trimodal feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Classification</head><p>In order to perform classification, we feed the fused features F mt (where m = AV, V T, T A, or AV T and t = 1, 2, . . . , N ) to a softmax layer with C = 2 outputs. The classifier can be described as follows:</p><formula xml:id="formula_18">P = softmax(W softmax F mt + b softmax ), y = argmax j (P[j]),</formula><p>where W softmax ∈ R C×D , b softmax ∈ R C , P ∈ R C , j = class value (0 or 1), and y = estimated class value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training</head><p>We employ categorical cross-entropy as loss function (J) for training,</p><formula xml:id="formula_19">J = − 1 N N i=1 C−1 j=0 y ij log P i [j],</formula><p>where N = number of samples, i = index of a sample, j = class value, and</p><formula xml:id="formula_20">y ij =      1, if expected class value of sample i is j 0, otherwise.</formula><p>Adam <ref type="bibr" target="#b52">[53]</ref> is used as optimizer due to its ability to adapt learning rate for each parameter individually. We train the network for 200 epochs with early stopping, where we optimize the parameter set  </p><formula xml:id="formula_21">θ = m∈M   j∈{z,r,h} {U mj , W mj } ∪ {U mx , u mx }   ∪ m∈M2 D2 i=1 {w m i } ∪ D3 i=1 {w AV T i } ∪ m∈M1 {W m , b m } ∪ {W sof tmax , b sof tmax }, where M = {A, V, T, V A, V T,</formula><formula xml:id="formula_22">f i A ← AudioF eatures(u i ) 5: f i V ← V ideoF eatures(u i ) 6: f i T ← T extF eatures(u i ) 7:</formula><p>for m ∈ {A, V, T } do 8:</p><p>Fm = GRUm(fm) 9: Fusion: 10:</p><p>g A ← M apT oSpace(F A ) dimensionality equalization 11:</p><formula xml:id="formula_23">g V ← M apT oSpace(F V ) 12: g T ← M apT oSpace(F T ) 13: f V A ← BimodalF usion(g V , g A ) bimodal fusion 14: f AT ← BimodalF usion(g A , g T ) 15: f V T ← BimodalF usion(g V , g T ) 16: for m ∈ {V A, AT, V T } do 17: Fm = GRUm(fm) 18: f AV T ← T rimodalF usion(F V A , F AT , F V T )</formula><p>trimodal fusion <ref type="bibr" target="#b16">19</ref>:</p><formula xml:id="formula_24">FAV T = GRUAV T (fAV T ) 20: for i:[1,N ] do softmax classification 21:ŷ i = argmax j (sof tmax(F i AV T )[j])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>T estM odel(V ) 23: procedure MapToSpace(xz) for modality z 24:</p><p>gz ← tanh(Wzxz + bz) 25:</p><p>return gz 26: procedure BimodalFusion(gz 1 , gz 2 ) for modality z 1 and z 2 , where z 1 = z 2 27:</p><p>for i:[1,D] do 28:</p><formula xml:id="formula_25">f i z 1 z 2 ← tanh(w z 1 z 2 i .[g i z 1 , g i z 2 ] + b z 1 z 2 i ) 29: fz 1 z 2 ← (f 1 z 1 z 2 , f 2 z 1 z 2 , . . . , f D z 1 z 2 ) 30:</formula><p>return fz 1 z 2 31: procedure TrimodalFusion(fz 1 , fz 2 , fz 3 ) for modality combination z 1 , z 2 , and z 3 , where z 1 = z 2 = z 3 32:</p><p>for i:[1,D] do 33:</p><formula xml:id="formula_26">f i z 1 z 2 z 3 ← tanh(w i .[f i z 1 , f i z 2 , f i z 3 ] + b i ) 34: fz 1 z 2 z 3 ← (f 1 z 1 z 2 z 3 , f 2 z 1 z 2 z 3 , . . . , f D z 1 z 2 z 3 ) 35:</formula><p>return fz 1 z 2 z 3 36: procedure TestModel(V ) 37:</p><p>Similarly to training phase, V is passed through the learnt models to get the features and classification outputs. Section 3.6 mentions the trainable parameters (θ).  Only the videos by the first eight speakers are considered for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We compare our method with the following strong baselines.</p><p>Early fusion. We extract unimodal features (Section 3.2) and simply concatenate them to produce multimodal features. Followed by support vector machine (SVM) being applied on this feature vector for the final sentiment classification.</p><p>Method from <ref type="bibr" target="#b42">[43]</ref>. We have implemented and compared our method with the approach proposed by Poria et al. <ref type="bibr" target="#b42">[43]</ref>. In their approach, they extracted visual features using CLM-Z, audio features using openSMILE, and textual features using CNN. MKL was then applied to the features obtained from concatenation of the unimodal features. However, they did not conduct speaker independent experiments.</p><p>In order to perform a fair comparison with <ref type="bibr" target="#b42">[43]</ref>, we employ our fusion method on the features extracted by Poria et al. <ref type="bibr" target="#b42">[43]</ref>.</p><p>Method from [1]. We have compared our method with <ref type="bibr" target="#b45">[46]</ref>, which takes advantage of contextual information obtained from the surrounding utterances. This context modeling is achieved using LSTM. We reran the experiments of Poria et al. <ref type="bibr" target="#b45">[46]</ref> without using SVM for classification since using SVM with neural networks is usually discouraged. This provides a fair comparison with our model which does not use SVM.</p><p>Method from <ref type="bibr" target="#b43">[44]</ref>. In <ref type="bibr" target="#b43">[44]</ref>, they proposed a trimodal fusion method based on the tensors. We have also compared our method with their. In particular, their dataset configuration was different than us so we have adapted their publicly available code 5 and employed that on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Setting</head><p>We considered two variants of experimental setup while evaluating our model.</p><p>HFusion. In this setup, we evaluated hierarchical fusion without context-aware features with CMU-MOSI dataset. We removed all the GRUs from the model described in Sections 3.3 and 3.4 forwarded utterance specific features directly to the next layer. This setup is depicted in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>CHFusion. This setup is exactly as the model described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results and Discussion</head><p>We discuss the results for the different experimental settings discussed in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Hierarchical Fusion (HFusion)</head><p>The results of our experiments are presented in <ref type="table">Table 2</ref>. We evaluated this setup with CMU-MOSI dataset (Section 4.1.1) and two feature sets: the feature set used in <ref type="bibr" target="#b42">[43]</ref> and the set of unimodal features discussed in Section 3.2. <ref type="table">Table 2</ref>: Comparison in terms of accuracy of Hierarchical Fusion (HFusion) with other fusion methods for CMU-MOSI dataset; bold font signifies best accuracy for the corresponding feature set and modality or modalities, where T stands for text, V for video, and A for audio. SOT A 1 = Poria et al. <ref type="bibr" target="#b42">[43]</ref>, SOT A 2 = Zadeh et al. <ref type="bibr" target="#b43">[44]</ref> Modality Combination <ref type="bibr" target="#b42">[43]</ref>  Our model outperformed <ref type="bibr" target="#b42">[43]</ref>, which employed MKL, for all bimodal and trimodal scenarios by a margin of 1-1.8%. This leads us to present two observations. Firstly, the features used in <ref type="bibr" target="#b42">[43]</ref> are inferior to the features extracted in our approach. Second, our hierarchical fusion method is better than their fusion method.</p><p>It is already established in the literature <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b56">56]</ref> that multimodal analysis outperforms unimodal analysis. We also observe the same trend in our experiments where trimodal and bimodal classifiers outperform unimodal classifiers.</p><p>The textual modality performed best among others with a higher unimodal classification accuracy of 75%. Although other modalities contribute to improve the performance of multimodal classifiers, that contribution is little in compare to the textual modality.</p><p>On the other hand, we compared our model with early fusion (Section 4.2) for aforementioned feature sets (Section 3.2). Our fusion mechanism consistently outperforms early fusion for all combination of modalities. This supports our hypothesis that our hierarchical fusion method captures the inter-relation among the modalities and produce better performance vector than early fusion. Text is the strongest individual modality, and we observe that the text modality paired with remaining two modalities results in consistent performance improvement.</p><p>Overall, the results give a strong indication that the comparison among the abstract feature values dampens the effect of less important modalities, which was our hypothesis. For example, we can notice that for early fusion T+V and T+A both yield the same performance. However, with our method text with video performs better than text with audio, which is more aligned with our expectations, since facial muscle movements usually carry more emotional nuances than voice.</p><p>In particular, we observe that our model outperformed all the strong baselines mentioned above. The method by <ref type="bibr" target="#b42">[43]</ref> is only able to fuse using concatenation. Our proposed method outperformed their approach by a significant margin; thanks to the power of hierarchical fusion which proves the capability of our method in modeling bimodal and trimodal correlations. However on the other hand, the method by <ref type="bibr" target="#b43">[44]</ref> is capable of fusing the modalities using a tensor. Interestingly our method also outperformed them and we think the reason is the capability of bimodal fusion and use that for trimodal fusion.</p><p>Tensor fusion network is incapable to learn the weights of the bimodal and trimodal correlations in the fusion. Tensor Fusion is mathematically formed by an outer product, it has no learn-able parameters. Wherein our method learns the weights automatically using a neural network (Equation 1,2 and 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Context-Aware Hierarchical Fusion (CHFusion)</head><p>The results of this experiment are shown in <ref type="table" target="#tab_3">Table 3</ref>. This setting fully utilizes the model described in Section 3. We applied this experimental setting for two datasets, namely CMU-MOSI (Section 4.1.1) and IEMOCAP (Section 4.1.2).</p><p>We used the feature set discussed in Section 3.2, which was also used by Poria et al. <ref type="bibr">[1]</ref>. As expected our method outperformed the simple early fusion based fusion by <ref type="bibr" target="#b42">[43]</ref>, tensor fusion by <ref type="bibr" target="#b43">[44]</ref>. The method by Poria et al.</p><p>[1] used a scheme to learn contextual features from the surrounding features. However, as a method of fusion they adapted simple concatenation based fusion method by <ref type="bibr" target="#b42">[43]</ref>. As discussed in Section 3.3, we employed their contextual feature extraction framework and integrated our proposed fusion method to that. This has helped us to outperform Poria et al.</p><p>[1] by significant margin thanks to the hierarchical fusion (HFusion).</p><p>CMU-MOSI. We achieve 1-2% performance improvement over the state of the art [1] for all the modality combinations having textual component. For A+V modality combination we achieve better but similar performance to the state of the art. We suspect that it is due to both audio and video modality being significantly less informative than textual modality. It is evident from the unimodal performance where we observe that textual modality on its own performs around 21% better than both audio and video modality. Also, audio and video modality performs close to majority baseline. On the other hand, it is important to notice that with all modalities combined we achieve about 3.5% higher accuracy than text alone.</p><p>For example, consider the following utterance: so overall new moon even with the bigger better budgets huh it was still too long. The speaker discusses her opinion on the movie Twilight New Moon. Textually the utterance is abundant with positive words however audio and video comprises of a frown which is observed by the hierarchical fusion based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEMOCAP.</head><p>As the IEMOCAP dataset contains four distinct emotion categories, in the last layer of the network we used a softmax classifier whose out- put dimension is set to 4. In order to perform classification on IEMOCAP dataset we feed the fused features F mt (where m = AV, V T, T A, or AV T and t = 1, 2, . . . , N ) to a softmax layer with C = 4 outputs. The classifier can be described as follows:</p><formula xml:id="formula_27">P = softmax(W softmax F mt + b softmax ), y = argmax j (P[j]),</formula><p>where W softmax ∈ R 4×D , b softmax ∈ R 4 , P ∈ R 4 , j = class value (0 or 1 or 2 or 3), andŷ = estimated class value.</p><p>Here as well, we achieve performance improvement consistent with CMU-MOSI. This method performs 1-2.4% better than the state of the art for all the modality combinations. Also, trimodal accuracy is 3% higher than the same for textual modality. Since, IEMOCAP dataset imbalanced, we also present the f-score for each modality combination for a better evaluation. One key observation for IEMOCAP dataset is that its A+V modality combination performs significantly better than the same of CMU-MOSI dataset. We think that this is due to the audio and video modality of IEMOCAP being richer than the same of CMU-MOSI. The performance difference with another strong baseline <ref type="bibr" target="#b43">[44]</ref> is even more ranging from 2.1% to 3% on CMU-MOSI dataset and 2.2% to 5% on IEMOCAP dataset. This again confirms the superiority of the hierarchical fusion in compare to <ref type="bibr" target="#b43">[44]</ref>. We think this is mainly because of learning the weights of bimodal and trimodal correlation (representing the degree of correlations) calculations at the time of fusion while Tensor Fusion Network (TFN) just relies on the non-trainable outer product of tensors to model such correlations for fusion. Additionally, we present class-wise accuracy and f-score for IEMOCAP for trimodal (A+V+T) scenario in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">HFusion vs. CHFusion</head><p>We compare HFusion and CHFusion models over CMU-MOSI dataset. We observe that CHFusion performs 1-2% better than HFusion model for all the modality combinations. This performance boost is achieved by the inclusion of utterance-level contextual information in HFusion model by adding GRUs in different levels of fusion hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Multimodal fusion strategy is an important issue in multimodal sentiment analysis. However, little work has been done so far in this direction. In this paper, we have presented a novel and comprehensive fusion strategy. Our method outperforms the widely used early fusion on both datasets typically used to test multimodal sentiment analysis methods. Moreover, with the addition of context modeling with GRU, our method outperforms the state of the art in multimodal sentiment analysis and emotion detection by significant margin.</p><p>In our future work, we plan to improve the quality of unimodal features, especially textual features, which will further improve the accuracy of classification. We will also experiment with more sophisticated network architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Utterance-level early fusion, or simple concatenationBimodal fusion. We fuse the utterance feature vectors for each bimodal combination, i.e., T+V, T+A, and A+V. This step is depicted inFig. 2 and discussedin details in Section 3.4. We use the penultimate layer forFig. 2 as bimodalfeatures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Utterance-level bimodal fusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Utterance-level trimodal hierarchical fusion. 2 with 50 feature maps each and the second layer has a filter of size 2 with 100 feature maps. Each convolutional layer is followed by a max-pooling layer with window 2 × 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Context-aware hierarchical fusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>T A, AV T }, M 1 = {A, V, T }, and M 2 = {V A, V T, T A}. Algorithm 1 summarizes our method. 4 4. Experiments 4.1. Dataset Details Most research works in multimodal sentiment analysis are performed on datasets where train and test splits may share certain speakers. Since, each individual has an unique way of expressing emotions and sentiments, finding generic and person-independent features for sentiment analysis is crucial. Table 1 shows the train and test split for the datasets used.Dataset Train Test pos. neg. happy anger sad neu. pos. neg. happy anger sad neu. pos. = positive, neg. = negative, neu. = neutral</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc>strongly positive) by five annotators. We took the average of these five annotations as the sentiment polarity and considered only two classes (positive and negative). Given every individual's unique way of expressing sentiments, real world applications should be able to model generic person independent features and be robust to person variance. To this end, we perform person-independent experiments to emulate unseen conditions. Our train/test splits of the dataset are completely disjoint with respect to speakers. The train/validation set consists of the first 62 individuals in the dataset. The test set contains opinionated videos by rest of the 31 speakers. In particular, 1447 and 752 utterances are used for training and test respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4. 1</head><label>1</label><figDesc>.2. IEMOCAP IEMOCAP [55] contains two way conversations among ten speakers, segmented into utterances. The utterances are tagged with the labels anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, and other. We consider the first four ones to compare with the state of the art [1] and other works. It contains 1083 angry, 1630 happy, 1083 sad, and 1683 neutral videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Class distribution of datasets in both train and test splits.</figDesc><table><row><cell>4.1.1. CMU-MOSI</cell></row><row><cell>CMU-MOSI dataset [54] is rich in sentimental expressions, where 89 people</cell></row><row><cell>review various topics in English. The videos are segmented into utterances where</cell></row><row><cell>each utterance is annotated with scores between −3 (strongly negative) and +3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of Context-Aware Hierarchical Fusion (CHFusion) in terms of accuracy (CHFusionacc) and f-score (for IEMOCAP: CHFusion f sc ) with the state of the art for CMU-MOSI and IEMOCAP dataset; bold font signifies best accuracy for the corresponding dataset and modality or modalities, where T stands text, V for video, A for audio. SOT A 1 = Poria et al.<ref type="bibr" target="#b42">[43]</ref>, SOT A 2 = Zadeh et al.<ref type="bibr" target="#b43">[44]</ref>. CHFusionacc and CHFusion f sc are the accuracy and f-score of CHFusion respectively. SOT A 2 CHFusion acc SOT A 1 SOT A 2 CHFusion acc CHFusion f sc</figDesc><table><row><cell cols="2">Modality SOT A 1 T</cell><cell cols="2">CMU-MOSI 76.5%</cell><cell cols="2">IEMOCAP 73.6%</cell><cell>-</cell></row><row><cell>V</cell><cell></cell><cell>54.9%</cell><cell></cell><cell>53.3%</cell><cell></cell><cell>-</cell></row><row><cell>A</cell><cell></cell><cell>55.3%</cell><cell></cell><cell>57.1%</cell><cell></cell><cell>-</cell></row><row><cell>T+V</cell><cell cols="2">77.8% 77.1%</cell><cell>79.3%</cell><cell>74.1% 73.7%</cell><cell>75.9%</cell><cell>75.6%</cell></row><row><cell>T+A</cell><cell cols="2">77.3% 77.0%</cell><cell>79.1%</cell><cell>73.7% 71.1%</cell><cell>76.1%</cell><cell>76.0%</cell></row><row><cell>A+V</cell><cell cols="2">57.9% 56.5%</cell><cell>58.8%</cell><cell>68.4% 67.4%</cell><cell>69.5%</cell><cell>69.6%</cell></row><row><cell cols="3">A+V+T 78.7% 77.2%</cell><cell>80.0%</cell><cell>74.1% 73.6%</cell><cell>76.5%</cell><cell>76.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Class-wise accuracy and f-score for IEMOCAP dataset for trimodal scenario.</figDesc><table><row><cell>Metrics</cell><cell cols="4">Classes Happy Sad Neutral Anger</cell></row><row><cell>Accuracy</cell><cell>74.3</cell><cell>75.6</cell><cell>78.4</cell><cell>79.6</cell></row><row><cell>F-Score</cell><cell>81.4</cell><cell>77.0</cell><cell>71.2</cell><cell>77.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://github.com/senticnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Figure adaptedfrom [52] with permission. 3 LSTM does not perform well</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Implementation of this algorithm is available at http://github.com/senticnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/A2Zadeh/TensorFusionNetwork</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work was partially supported by the Instituto Politécnico Nacional via grant SIP 20172008 to A. Gelbukh.  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distinguishing between facts and opinions for sentiment analysis: Survey and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Welsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilingual sentiment analysis: From formal to informal and scarce resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cornforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="499" to="527" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilingual sentiment analysis: state of the art and independent comparison of techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dashtipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hawalah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="757" to="771" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Multi-grained Aspect Target Sequence for Chinese Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge-Based Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lexicon generation for emotion analysis of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bandhakavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wiratunga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Massie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deepak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OntoSenticNet: A Commonsense Ontology for Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dragoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering conceptual primitives for sentiment analysis by means of context embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SenticNet</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1795" to="1802" />
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical learning theory and ELM for big social data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for big social data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="1662" to="1673" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning word representations for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="843" to="851" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recent trends in deep learning based natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Generative Model for category text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="page" from="301" to="315" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentiment Analysis is a Big Suitcase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word Polarity Disambiguation Using Bayesian Model and Opinion-Level Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="380" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian network based extreme learning machine for subjectivity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of The Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1780" to="1797" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning-based document modeling for personality detection from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Phonetic-Based Microtext Normalization for Twitter Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Satapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guerreiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICDM</publisher>
			<biblScope unit="page" from="407" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A graph-based approach to commonsense concept extraction and semantic similarity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Olsher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>WWW</publisher>
			<biblScope unit="page" from="565" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Time expression analysis and recognition using syntactic token types and general heuristic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="5876" to="5883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Natural Language Based Financial Forecasting: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="73" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Challenges of sentiment analysis for dynamic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="70" to="75" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Sentic Computing for Patient Centered Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eckl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IEEE ICSP</publisher>
			<biblScope unit="page" from="1279" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sentiment analysis in TripAdvisor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valdivia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Luzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What Men Say, What Women Hear: Finding Gender-Specific Meaning Shades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garimella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="62" to="67" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning community embedding with community detection and node embedding on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CIKM</publisher>
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Adaptive Two-Stage Feature Selection for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE SMC</publisher>
			<biblScope unit="page" from="1238" to="1243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multiattention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="5642" to="5649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Augmenting End-to-End Dialogue Systems with Commonsense Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="4970" to="4977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facial emotion recognition using multi-modal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICICS</title>
		<meeting>ICICS</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal human emotion/expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the Third IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kessous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="33" to="48" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recognizing affect from linguistic information in 3D continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="192" to="205" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Youtube movie reviews: Sentiment analysis in an audio-visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ensemble of SVM trees for multimodal emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rozgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal &amp; Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<publisher>APSIPA ASC</publisher>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asia-Pacific</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Audio-visual emotion recognition using gaussian mixture models for face and voice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Symposium on ISM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On-line emotion recognition in a 3-D activation-valence-time continuum using acoustic and linguistic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-B</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Convolutional MKL based multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICDM</publisher>
			<biblScope unit="page" from="439" to="448" />
			<pubPlace>Barcelona</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<title level="m">Tensor Fusion Network for Multimodal Sentiment Analysis</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1114" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ensemble application of convolutional neural networks and multiple kernel learning for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="217" to="230" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>EMNLP</publisher>
			<biblScope unit="page" from="2539" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rate-coded restricted Boltzmann machines for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing system</title>
		<editor>T. Leen, T. Dietterich, V. Tresp</editor>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="908" to="914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Opensmile: the Munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<title level="m">Multimodal Sentiment Analysis in Social Media using Deep Learning with Convolutional Neural Networks, Master&apos;s thesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>CIC, Instituto Politécnico Nacional</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">IEMOCAP: Interactive emotional dyadic motion capture database, Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Utterance-Level Multimodal Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACL</publisher>
			<biblScope unit="page" from="973" to="982" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
