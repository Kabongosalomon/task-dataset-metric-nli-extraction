<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Encode Position for Transformer with Continuous Dynamical Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
							<email>xqliu@cs.ucla.edurofu.yu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UCLA § UT Austin ‡ Amazon Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCLA § UT Austin ‡ Amazon Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
							<email>inderjit@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UCLA § UT Austin ‡ Amazon Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
							<email>chohsieh@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UCLA § UT Austin ‡ Amazon Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Encode Position for Transformer with Continuous Dynamical Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new way of learning to encode position information for non-recurrent models, such as Transformer models. Unlike RNN and LSTM, which contain inductive bias by loading the input tokens sequentially, non-recurrent models are less sensitive to position. The main reason is that position information among input units is not inherently encoded, i.e., the models are permutation equivalent; this problem justifies why all of the existing models are accompanied by a sinusoidal encoding/embedding layer at the input. However, this solution has clear limitations: the sinusoidal encoding is not flexible enough as it is manually designed and does not contain any learnable parameters, whereas the position embedding restricts the maximum length of input sequences. It is thus desirable to design a new position layer that contains learnable parameters to adjust to different datasets and different architectures. At the same time, we would also like the encodings to extrapolate in accordance with the variable length of inputs. In our proposed solution, we borrow from the recent Neural ODE approach, which may be viewed as a versatile continuous version of a ResNet. This model is capable of modeling many kinds of dynamical systems. We model the evolution of encoded results along position index by such a dynamical system, thereby overcoming the above limitations of existing methods. We evaluate our new position layers on a variety of neural machine translation and language understanding tasks, the experimental results show consistent improvements over the baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer based models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref> have become one of the most effective approaches to model sequence data of variable lengths. Transformers have shown wide applicability to many natural language processing (NLP) tasks such as language modeling <ref type="bibr">[4]</ref>, neural machine translation (NMT) <ref type="bibr" target="#b0">[1]</ref>, and language understanding <ref type="bibr" target="#b1">[2]</ref>. Unlike traditional recurrent-based models (e.g., RNN or LSTM), Transformer utilizes a non-recurrent but self-attentive neural architecture to model the dependency among elements at different positions in the sequence, which leads to better parallelization using modern hardware and alleviates the vanishing/exploding gradient problem in traditional recurrent models. <ref type="bibr" target="#b6">[7]</ref> prove that the design of self-attentive architecture leads to a family of permutation equivalence functions. Thus, for applications where the ordering of the elements matters, how to properly encode position information is crucial for Transformer based models. There have been many attempts to encode position information for the Transformer. In the original Transformer paper <ref type="bibr" target="#b0">[1]</ref>, a family of pre-defined sinusoidal functions was adapted to construct a set of embeddings for each position. These fixed position embeddings are then added to the word embeddings of the input sequence accordingly. To further construct these position embeddings in a more data-driven way, many recent Transformer variants such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> include these embeddings as learnable model parameters in the training stage. This data-driven approach comes at the cost of the limitation of a fixed maximum length of input sequence L max and the computational/memory overhead of additional L max × d parameters, where L max is usually set to 512 in many applications, and d is the dimension of the embeddings. <ref type="bibr" target="#b8">[9]</ref> propose a relative position representation to reduce the number of parameters to (2K + 1)d by dropping the interactions between tokens with a distance greater than K. In addition to just the input layer, <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr">[5]</ref> suggest that the injection of position information to every layer leads to even better performance for the Transformer. An ideal position encoding approach should satisfy the following three properties: 1. Inductive: the ability to handle sequences longer than any sequence seen in the training time. 2. Data-Driven: the position encoding should be learnable from the data. <ref type="bibr" target="#b2">3</ref>. Parameter Efficient: number of trainable parameters introduced by the encoding should be limited to avoid increased model size, which could hurt generalization. In <ref type="table" target="#tab_0">Table 1</ref>, we summarize some of the existing position encoding approaches in terms of these three properties.</p><p>In this paper, we propose a new method to encode position with minimum cost. The main idea is to model position encoding as a continuous dynamical system, so we only need to learn the system dynamics instead of learning the embeddings for each position independently. By doing so, our method enjoys the best of both worlds -we bring back the inductive bias, and the encoding method is freely trainable while being parameter efficient. To enable training of this dynamical system with backpropagation, we adopt the recent progress in continuous neural network <ref type="bibr" target="#b10">[11]</ref>, officially called Neural ODE. In some generative modeling literature, it is also called the free-form flow model <ref type="bibr" target="#b11">[12]</ref>, so we call our model FLOw-bAsed TransformER (FLOATER). We highlight our contributions as follows:</p><p>• We propose FLOATER, a new position encoder for Transformer, which models the position information via a continuous dynamical model in a data-driven and parameter-efficient manner. • Due to the use of a continuous dynamic model, FLOATER can handle sequences of any length. This property makes inference more flexible. • With careful design, our position encoder is compatible with the original Transformer; i.e., the original Transformer can be regarded as a special case of our proposed position encoding approach. As a result, we are not only able to train a Transformer model with FLOATER from scratch but also plug FLOATER into most existing pre-trained Transformer models such as BERT, RoBERTa, etc. • We demonstrate that FLOATER consistent improvements over baseline models across a variety of NLP tasks ranging from machine translations, language understanding, and question answering.</p><p>2 Background and Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Importance of Position Encoding for Transformer</head><p>We use a simplified self-attentive sequence encoder to illustrate the importance of position encoding in the Transformer. Without position encoding, the Transformer architecture can be viewed as a stack of N blocks B n : n = 1, . . . , N containing a self-attentive A n and a feed-forward layer F n . By dropping the residual connections and layer normalization, the architecture of a simplified Transformer encoder </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inductive Data-Driven Parameter Efficient</head><p>Sinusoidal <ref type="bibr" target="#b0">[1]</ref> Embedding <ref type="bibr" target="#b1">[2]</ref> Relative <ref type="bibr" target="#b8">[9]</ref> This paper can be represented as follows.</p><formula xml:id="formula_0">Encode(x) = B N • B N −1 • · · · • B 1 (x), (1) B n (x) = F n • A n (x) ,<label>(2)</label></formula><formula xml:id="formula_1">where x = [x 1 , x 2 , . . . , x L ] ∈ R L×d ,</formula><p>L is the length of the sequence and d is the dimension of the word embedding. A n (·) and F n (·) are the self-attentive and feed-forward layer in the n-th block B n (·), respectively. Each row of A 1 (x) can be regarded as a weighted sum of the value matrix V ∈ R L×d , with the weights determined by similarity scores between the key matrix K ∈ R L×d and query matrix Q ∈ R L×d as follows:</p><formula xml:id="formula_2">A 1 (x) = Softmax QK √ d V , Q = [q 1 , q 2 , ..., q L ] , q i = W q x i + b q , K = [k 1 , k 2 , ..., k L ] , k i = W k x i + b k , V = [v 1 , v 2 , ..., v L ] , v i = W v x i + b v ,<label>(3)</label></formula><p>W q/k/v and b q/k/v are the weight and bias parameters introduced in the self-attentive function A 1 (·). The output of the feed-forward function F 1 (·) used in the Transformer is also a matrix with L rows. In particular, the i-th row is obtained as follows.</p><formula xml:id="formula_3">the i-th row of F 1 (x) = W 2 σ(W 1 x i + b 1 ) + b 2 ,<label>(4)</label></formula><p>where W 1,2 and b 1,2 are the weights and biases of linear transforms, and σ(·) is the activation function. It is not hard to see from <ref type="formula" target="#formula_2">(3)</ref> and (4) that both A 1 (·) and F 1 (·) are permutation equivalent. Thus, we can conclude that the entire function defined in (1) is also permutation equivalent, i.e., Π × Encode(x) = Encode (Π × x) for any L × L permutation matrix Π. This permutation equivalence property restricts the Transformer without position information from modeling sequences where the ordering of elements matters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Position Encoding in Transformer</head><p>As mentioned in Section 1, there are many attempts to inject position information in self-attentive components. Most of them can be described in the following form:</p><formula xml:id="formula_4">B n (x) = F n • A n • Φ n (x), n ∈ {1, ..., N },<label>(5)</label></formula><p>Self-Attention  where Φ n (x) is a position encoding function. <ref type="bibr" target="#b0">[1]</ref> propose to keep Φ n (x) = x, ∀n ≥ 2 and inject position information only at the input block with a family of pre-defined sinusoidal functions:</p><formula xml:id="formula_5">Φ 1 (x) = x + p (1) , where p (1) = [p (1) 1 , p<label>(1)</label></formula><p>2 , ..., p</p><p>L ] is a position embedding matrix with the i-th row corresponding to the i-th position in the input sequence. In particular, the j-th dimension of the i-th row is defined as follows.</p><formula xml:id="formula_7">p (1) i [j] =    sin(i · c j d ) if j is even, cos(i · c j−1 d ) if j is odd,<label>(6)</label></formula><p>where c = 10 −4 .</p><p>[10] and <ref type="bibr">[5]</ref> observe better performance by further injecting the position information at each block, i.e., Φ n (x) = x + p (n) as follows:</p><formula xml:id="formula_8">p (n) i [j] =    sin(i · c j d ) + sin(n · c j d ) if j is even, cos(i · c j−1 d ) + cos(n · c j−1 d ) if j is odd.<label>(7)</label></formula><p>Note that for the above two approaches, position encoding functions Φ n (·) are fixed for all the applications. Although no additional parameters are introduced in the model, both approaches are inductive and can handle input sequences of variable length. Many successful variants of pre-trained Transformer models, such as BERT <ref type="bibr" target="#b1">[2]</ref> and RoBERTa <ref type="bibr" target="#b7">[8]</ref>, include the entire embedding matrix p (1) ∈ R L×d in Φ 1 (x) as training parameters. As the number of training parameters needs to be fixed, the maximum length of a sequence, L max , is required to be determined before the training. Although it lacks the inductive property, this data-driven approach is found to be effective for many NLP tasks. Note that, unlike the fixed sinusoidal position encoding, there is no attempt to inject a learnable position embedding matrix at each block for Transformer due to a large number of additional parameters (N L max d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FLOATER: Our Proposed Position Encoder</head><p>We introduce our method in three steps. In the first step, we only look at one Transformer block, and describe how to learn the position representation driven by a dynamical system; in the second step, we show how to save parameters if we add position signals to every layer; lastly, we slightly change the architecture to save trainable parameters further and make FLOATER "compatible" with the original Transformer <ref type="bibr" target="#b0">[1]</ref>. The compatibility means our model is a strict superset of the vanilla Transformer so that it can be initialized from the Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Position Encoding with Dynamical Systems</head><p>Position representations in Transformer models are a sequence of vectors {p i ∈ R d : i = 1, ..., L} to be added to the sequence of the input representations {x i : i = 1, ..., L}. Existing position encoding approaches either apply a fixed sinusoidal function to obtain {p i }, or include them as uncorrelated learnable parameters. Both of them fail to capture the dependency or dynamics among these position representations {p i }. In this paper, we propose to use a dynamical system to model these position representations; that is, there is a "latent force" denoted by h i that drives the changes from p i to p i+1 .</p><p>To encourage smoothness, we consider p(t) : R + → R d as the continuous version of the discrete sequence {p i }. In particular, our proposed continuous dynamical system is characterized as follows:</p><formula xml:id="formula_9">p(t) = p(s)+ t s h(τ, p(τ ); θ h ) dτ, 0 ≤ s ≤ t &lt; ∞,<label>(8)</label></formula><p>together with an initial vector p(0), where h(τ, p(τ ); θ h ) is a neural network parameterized by θ h and takes the previous state (τ, p(τ )). Notice that the domain of p(·) is R + . The position sequence {p i } can be obtained by taking p(·) on a series of points {t i : 0 ≤ t 1 &lt; · · · ≤ t L }: p i = p(t i ). One simple strategy is to set t i = i · ∆t so that the points are equidistant, where ∆ is a hyperparameter (e.g., ∆ = 0.1). With this strategy, we are implicitly assuming the position signals evolve steadily as we go through each token in a sentence. In general, {t i } can be any monotonically increasing series, which allows us to extend our work to more applications where the elements in the sequence are not always observed with the same interval. More discussions about the applicability for this general setting is included in the Supplementary material. For the NLP applications discussed in this paper, we choose</p><formula xml:id="formula_10">t i = i · ∆t.</formula><p>Eq. <ref type="formula" target="#formula_9">(8)</ref> is equivalent to an ODE problem dp(t) dt = h(t, p(t); θ h ), which is guaranteed to have a unique solution under mild conditions <ref type="bibr">[13]</ref>. We follow the efficient approach by <ref type="bibr" target="#b10">[11]</ref> to calculate the gradients of θ h with respect to the overall training loss, which allows us to include this parameterized dynamical position encoder into the end-to-end training of Transformer models. More details can be found in the Supplementary material. Our dynamical system <ref type="formula" target="#formula_9">(8)</ref> is quite flexible to admit the standard sinusoidal position encoding (6) as a special case:</p><formula xml:id="formula_11">p i+1 [j] − p i [j] =    sin (i + 1) · c j d − sin i · c j d</formula><p>if j is even</p><formula xml:id="formula_12">cos (i + 1) · c j−1 d − cos i · c j−1 d if j is odd =    i+1 i c − j d cos(τ · c j d ) dτ if j is even i+1 i −c − j−1 d sin(τ · c j−1 d ) dτ if j is odd,<label>(9)</label></formula><p>This indicates that for simple sinusoidal encoding, there exists a dynamical system h(·) which is also sinusoidal function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Sharing among Blocks</head><p>As mentioned in Section 2, injecting position information to each block for Transformer leads to better performance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">5]</ref> in some language understanding tasks. Our proposed position encoder FLOATER <ref type="bibr" target="#b7">(8)</ref> can also be injected into each block. The idea is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. Typically there are 6 blocks in sequence-to-sequence Transformer and 12 or 24 blocks in BERT. We add a superscript (n) to denote dynamics at n-th block:</p><formula xml:id="formula_13">p (n) (t) = p (n) (s) + t s h (n) (τ, p (n) (τ ); θ (n) h ) dτ.</formula><p>As we can imagine, having N different dynamical models h (n) (·; θ (n) h ) for each block can introduce too many parameters and cause significant training overhead. Instead, we address this issue by sharing parameters across all the blocks, namely</p><formula xml:id="formula_14">θ (1) h = θ (2) h = · · · = θ (N ) h .<label>(10)</label></formula><p>Note that <ref type="formula" target="#formula_5">(10)</ref> does not imply that all the p (n) t are the same, as we will assign different initial values for each block, that is p (n 1 ) (0) = p (n 2 ) (0) for n 1 = n 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-Base Transformer-Large</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>En-De</head><p>En  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compatibility and Warm-start Training</head><p>In this section, we change the way to add position encoding so that our FLOATER can be directly initialized from Transformer. As an example, we use the standard Transformer model, which has a fixed sinusoidal encoding at the input block and no position encoding at deeper levels. Note that this technique can be extended to other variants of Transformers with different position encoding methods, such as embedding matrix. We first examine the standard Transformer model, the query matrix Q (n) at block-n is</p><formula xml:id="formula_15">q ∼ (n) i = W (n) q x i + p ∼ (n) i + b (n) q ,<label>(11)</label></formula><p>where W </p><formula xml:id="formula_16">q (n) i = W (n) q x i + p i + b (n) q = W (n) q (x i + p ∼ (n) i ) + b (n) q Eq. (11) + W (n) q (p i − p ∼ (n) i ) Extra bias term depends on i = q ∼ (n) i + b (n) q,i .<label>(12)</label></formula><p>It is easy to see that the changing the position embedding from {p ∼ (n) i } to {p (n) i } is equivalent to adding a position-aware bias vector b (n) q,i into each self-attentive layers {A n (·)}. As a result, we can instead apply <ref type="bibr" target="#b7">(8)</ref> to model the dynamics of b (n) q . In particular, we have the following dynamical system:</p><formula xml:id="formula_17">b (n) q (t) = b (n) q (0) + t 0 h (n) (τ, b (n) q (τ ); θ h ) dτ.<label>(13)</label></formula><p>After that, we set b</p><formula xml:id="formula_18">(n) q,i = b (n) q (i · ∆t)</formula><p>. We can see that if h(·) = 0 and b (n)</p><formula xml:id="formula_19">q (0) = 0, then b (n) q</formula><p>≡ 0. This implies (12) degenerates to <ref type="bibr" target="#b10">(11)</ref>. Note that (13) has the same form as <ref type="bibr" target="#b7">(8)</ref>, except that we are now modeling the bias terms b q,i in (3). We will apply the same technique to K and V .</p><p>To summarize, our model has a tight connection to the original Transformer: if we set all dynamical models to zero, which means h(τ, p(τ ); θ h ) ≡ 0, then our FLOATER model will be equivalent to the original Transformer with the sinusoidal encoding. The same trick also works for Transformer with position embedding such as BERT <ref type="bibr" target="#b1">[2]</ref>. We strive to make our model compatible with the original Transformer due to the following reasons. First of all, the original Transformer is faster to train as it does not contain any recurrent computation; this is in contrast to our dynamical model <ref type="bibr" target="#b7">(8)</ref>, where the next position p i+1 depends on the previous one p i . By leveraging the compatibility of model architecture, we can directly initialize FLOATER model from a pre-trained Transformer model checkpoint and then fine-tune for the downstream task for a few more epochs. By doing so, we enjoy all the benefits of our FLOATER model but still maintain an acceptable training budget. Likewise, for models such as BERT or Transformer-XL, we already have well-organized checkpoints out of the box for downstream tasks. These models are costly to train from scratch, and since our goal is to examine whether our proposed position representation method can improve over the original one, we decided to copy the weights layer by layer for attention as well as FFN layers, and randomly initialize the dynamical model h(τ, p(τ ); θ h ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section, we perform experiments to see if FLOATER can improve over the existing position encoding approaches for a given Transformer model on various NLP tasks. Thus, all the metrics reported in this paper are computed from a single (not ensemble) Transformer model over each evaluation NLP task. Albeit lower than top scores on the leaderboard, these metrics are able to reveal more clear signal to judge the effectiveness of the proposed position encoder. All our codes to perform experiments in this paper are based on the Transformer implementations in the fairseq <ref type="bibr" target="#b13">[14]</ref> package. Implementation details can be found in the Supplementary material. Our experimental codes will be made publicly available.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neural Machine Translation</head><p>Neural Machine Translation (NMT) is the first application that demonstrates the superiority of a sequence-to-sequence Transformer model over conventional recurrent sequence models. We include the following three additive position encoders: Φ (n) (x) = x + p (n) .</p><p>• Data-driven FLOATER: p (n) is generated by our proposed continuous dynamical models with data-driven parameters described in <ref type="bibr" target="#b7">(8)</ref>. • Pre-defined sinusoidal position encoder: p (n) is constructed by a pre-defined function described in <ref type="bibr" target="#b6">(7)</ref>, which is proposed by <ref type="bibr" target="#b0">[1]</ref> and extended by <ref type="bibr" target="#b9">[10]</ref>. • Length-fixed position embedding: p (n) is included as learnable training parameters. This is first introduced by [1] and adopted in many variants of Transformer <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. To better demonstrate the parameter efficiency brought by FLOATER, for each above encoder, we also include two experimental settings: position encoder at all blocks or only at the input block (i.e., p (n) = 0, ∀n ≥ 2).</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, we present the BLEU scores on WMT14 Ee-De and En-Fr datasets with both Transformerbase and Transformer-large models described in <ref type="bibr" target="#b0">[1]</ref>. Among all the data/model combinations, our proposed FLOATER at all blocks outperforms two other position encoders. On the other hand, we also observe that adding position encoders at all blocks yields better performance than only at the input block. While there is an exception in the fixed-length position embedding approach. We suspect that this phenomenon is due to over-fitting cased by L max dN learnable parameters introduced by this approach. In contrast, our proposed FLOATER is parameter efficient (more discussions in Section 4.3), so the performance can be improved by injecting the position encoder at all the blocks of Transformer without much additional overhead. Pretrained Transformer models such as BERT and RoBERTa have become the key to achieving the state-of-the-art performance for various language understanding and question answering tasks. In this section, we want to evaluate the effectiveness of the proposed FLOATER on these tasks. In particular, we focus on three language understanding benchmark sets, GLUE <ref type="bibr" target="#b15">[16]</ref>, RACE <ref type="bibr" target="#b14">[15]</ref> and SQuAD <ref type="bibr" target="#b16">[17]</ref>. As mentioned in Section 3.3, FLOATER is carefully designed to be compatible with the existing Transformer models. Thus, we can utilize pretrained Transformer models to warm-start a FLOATER model easily to be used to finetune on these NLP tasks. In this paper, we download the same pre-trained RoBERTa model from the official repository as our pretrained Transformer model for all NLP tasks discussed in this section. GLUE Benchmark. This benchmark is commonly used to evaluate the language understanding skills of NLP models. Experimental results in <ref type="table" target="#tab_3">Table 3</ref> show that our FLOATER model outperforms RoBERTa in most datasets, even though the only difference is the choice of positional encoding. RACE benchmark Similar to the GLUE benchmark, the RACE benchmark is another widely used test suit for language understanding. Compared with GLUE, each item in RACE contains a significantly longer context, which we believe requires more important to grasp the accurate position information. Like in GLUE benchmark, we finetune the model from the same pretrained RoBERTa checkpoint. We keep the hyperparameters, such as batch size and learning rate, to also be the same. <ref type="table" target="#tab_4">Table 4</ref> shows the experimental results. We again see consistent improvement of FLOATER across all subtasks. SQuAD benchmark SQuAD benchmark <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> is another challenging task to evaluate the question answering skills of NLP models. In this dataset, each item contains a lengthy paragraph containing facts and several questions related to the paragraph. The model needs to predict the range of characters that answer the questions. In SQuAD-v2, the problem becomes more challenging that the questions might be unanswerable by the context. We follow the same data processing script as BERT/RoBERTa for fair  comparison; more details about the training process are described in the Supplementary material. The experiment results are presented in <ref type="table" target="#tab_5">Table 5</ref>. As we can see, the FLOATER model beats the baseline RoBERTa model consistently across most datasets. The improvement is significant, considering that both models are finetuned from the same pretrained checkpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Understanding and Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">More Discussions and Analysis</head><p>How inductive is FLOATER? FLOATER is designed to be inductive by a data-driven dynamical model <ref type="bibr" target="#b7">(8)</ref>. To see how inductive FLOATER is when comparing to existing approaches, we design the following experiment. We first notice that in WMT14 En-De dataset, 98.6% of the training sentences are shorter than 80 tokens. Based on that, we make a new dataset called En-De short to long (or S2L for brevity): this dataset takes all the short sentences (&lt; 80 tokens) as the training split and all the long sentences (≥ 80 tokens) as the testing split. We further divide the testing split to four bins according to the source length fallen in [80, 100), [100, 120), [120, 140), [140, +∞). BLEU scores are calculated in each bin, and the results are presented in <ref type="figure" target="#fig_3">Figure 2</ref>. Our FLOATER model performs particularly well on long sentences, even though only short sentences are seen by the model during training. This empirical observation supports our conjecture that FLOATER model is inductive: the dynamics learned from shorter sequences can be appropriately generalized to longer sequences.</p><p>Is RNN a good alternative to model the dynamics? Recurrent neural network (RNN) is commonly used to perform sequential modeling. RNN and our continuous dynamical model <ref type="formula" target="#formula_9">(8)</ref>  commonality. Computing the value at the i-th step relies on the results at the (i − 1)-st step. Further, they all contain trainable parameters, allowing them to adapt to each particular task. Lastly, they can be extrapolated to any length as needed. To see if RNN works equally well, we model the sequence {p i } i∈{1,2,... } with RNN models:</p><formula xml:id="formula_20">p i+1 = RNN(z i , p i ),<label>(14)</label></formula><p>where z i ∈ R d in is the input to the RNN model at index i. Recall in RNN language models, z i is the word embedding or hidden feature of the i-th token. In our case, since we apply RNN to learn the encodings as opposed to hidden features, sensible inputs can be scalar value i or vectorized value Vectorize(i) by sinusoidal encoding. We tried both choices on WMT14 En-De data and found that vectorized value generally works better, though not as good as our FLOATER model. Detailed results can be found in <ref type="table" target="#tab_6">Table 6</ref>.</p><p>What does each position encoding look like? To better understand how different position encodings affect the sequence modeling, in <ref type="figure" target="#fig_4">Figure 3</ref>, we visualize the position embedding matrix p obtained from four different position encoding approaches for the Transformer-base backbone on WMT14 En-De dataset. We can see that sinusoidal encoding (3a) is the most structural, while position embedding (3b) is quite chaotic. Our FLOATER model learns position representation completely from data, but still exhibits some regularities (3c). Finally, the RNN model (3d) fails to extract sufficient positional information, probably due to the vanishing gradient problem. Another finding is that by looking at (3b), we observe that the vectors are nearly constant among different large positions (near the bottom of <ref type="figure" target="#fig_4">Figure 3b</ref>, we see patterns of vertical lines with the same color). This phenomenon is due to long sentences in the dataset being scarce, and so the positional information carried by lower indices cannot be extrapolated to higher indices. On the contrary, the dynamical model proposed in this paper enjoys the best of both worlds -it is adaptive to dataset distribution, and it is inductive to handle sequences with lengths longer than the training split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Remarks on Training and Testing Efficiency</head><p>It is not surprising that during the training time, our flow-based method adds a non-negligible time and memory overhead; this is because solving the Neural ODE precisely involves ∼100 times forward and backward propagations of the flow model. Even though we deliberately designed a small flow model (consisting of only two FFN and one nonlinearity layers), stacking them together still increases training time substantially. To make it possible to train big models, we use the following optimizations:</p><p>• Initialize with pretrained models that do not contain flow-based dynamics, as discussed in Section 3.3.</p><p>• From <ref type="formula" target="#formula_9">(8)</ref>, we know that if h(·) is close to zero, then the position information diminishes (derived in appendix). In this way, our model degenerates to the original Transformer. Inspired by this property, we can initialize the FLOATER with smaller weights. Combining with the previous trick, we obtain an informed initialization that incurs lower training loss at the beginning. • We observed that weights in h(·) are more stable and easy to train. Thus, we can separate the weights of h(·) from the remaining parts of the Transformer model. Concretely, we can 1) cache the positional bias vectors for some iterations without re-computing, 2) update the weights of flow models less frequently than other parts of the Transformer, and 3) update the flow models with a larger learning rate to accelerate convergence. • For the RoBERTa model, we adopt an even more straightforward strategy: we first download a pretrained RoBERTa model, plug in some flow-based encoding layers, and re-train the encoding layers on WikiText-103 dataset for one epoch. When finetuning on GLUE datasets, we can choose to freeze the encoding layers. Combining those tricks, we successfully train our proposed models with only 20-30% overhead compared to traditional models, and virtually no overhead when finetuning RoBERTa model on GLUE benchmarks. Moreover, there is no overhead during the inference stage if we store the pre-calculated positional bias vectors in the checkpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have shown that learning position encoding with a dynamical model can be an advantageous approach to improve Transformer models. Our proposed position encoding approach is inductive, data-driven, and parameter efficient. We have also demonstrated the superiority of our proposed model over existing position encoding approaches on various natural language processing tasks such as neural machine translation, language understanding, and question answering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training a Neural ODE model in Transformer</head><p>We discuss the details of training the dynamical model h(τ, p τ ; w h ), recall in our FLOWER model, function h joins in the computational graph implicitly by generating a sequence of position encoding vectors {p 1 , p 2 , . . . , p N }, conditioning on a freely initialized vector p 0 . The generation steps are computed iteratively as follows (suppose we choose the interval between two consecutive tokens to be ∆)</p><formula xml:id="formula_21">p 1 = p 0 + ∆ 0 h(τ, p τ ; w h ) dτ,</formula><formula xml:id="formula_22">p 2 = p 1 + 2∆ ∆ h(τ, p τ ; w h ) dτ, . . . p N = p N −1 + N ∆ (N −1)∆ h(τ, p τ ; w h ) dτ.<label>(15)</label></formula><p>Finally, the loss L of this sequence is going to be a function of all position encoding results L = L(p 0 , p 1 , . . . , p N ), which is further a function of model parameters w h . The question is how to calculate the gradient dL dw h through backpropagation. This question is fully solved in Neural ODE method <ref type="bibr" target="#b10">[11]</ref> with an efficient adjoint ODE solver. To illustrate the principle, we draw a diagram showing the forward and backward propagation in  From <ref type="bibr" target="#b10">[11]</ref>, we know that the gradients d</p><formula xml:id="formula_23">dw h L p s + t s h(τ, p τ ; w h ) dτ dL dw h can be computed by dL dw h = − s t a(τ ) ∂h(τ, p τ ; w h ) ∂w h dτ,<label>(16)</label></formula><p>where a(τ ) defined in τ ∈ [s, t] is called the "adjoint state" of ODE, which can be computed by solving</p><formula xml:id="formula_24">another ODE da(τ ) dτ = −a(τ ) ∂h(τ, p τ ; w h ) ∂p τ .<label>(17)</label></formula><p>Note that the computation of (17) only involves Jacobian-vector product so it can be efficiently calculated by automatic differentiation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation details B.1 Settings of ODE solver</head><p>To setup the ODE server, we need to first choose the numerical algorithms <ref type="bibr" target="#b18">[19]</ref>. We have different setups for different datasets. For neural machine translation problems (WMT14 En-De and En-Fr), we use the more accurate Runge-Kutta scheme with discretization step ∆ 5.0 to solve the adjoint equation (recall that we set the interval of two neighboring tokens to be ∆ = 0.1 globally). While for datasets with long sentences such as GLUE and RACE benchmarks, we found that solving the adjoint equation with high order scheme is too slow, in such case we adopt simple midpoint method with discretization step ∆ 5.0 , and the gradients are calculated by automatic differentiation rather than adjoint method. The third party implementation of ODE solver can be found at https://github.com/rtqichen/ torchdiffeq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Training NMT tasks</head><p>We run the same preprocessing script provided by fairseq <ref type="bibr" target="#b13">[14]</ref>, which is also used in ScalingNMT <ref type="bibr" target="#b19">[20]</ref>. With the standard training script, we first successfully reproduce all the results in Transformer paper <ref type="bibr" target="#b0">[1]</ref>. Based on that we execute the following protocol to get our results:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Cases suitable for non-equidistant discritization</head><p>Although our model allows continuous values of s and t in <ref type="bibr" target="#b7">(8)</ref>, limiting the scope to text modeling tasks, positions are discrete values as {0, 1, 2, . . . }. Once the continuous version of position representation p t is obtained, we simply take the discritized {p 0 , p ∆ , p 2∆ , . . . , } as the actual values to feed into Transformer model, where ∆ is a hyperparameter (e.g. ∆ = 0.1). By choosing positions t equidistantly, we are implicitly assuming the position signal evolves steadily as we go through each token in a sentence. More generally, the dynamics in <ref type="bibr" target="#b7">(8)</ref> can deal with the case in which positions are not integers 0, 1, 2, . . . etc., but arbitrary monotone increasing series t 0 &lt; t 1 &lt; t 2 &lt; . . . which may not be equidistant. In appendix, we exemplify this general situation with several widely deployed tasks; we regard this as a interesting future direction. This makes our model particularly suitable for following scenarios yet traditional position representation may not be good at: • Hierarchical Transformer model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. The model is a direct extension of hierarchical RNN and is often used in long document processing. It works by first running a word-level Transformer model on each sentence to extract the sentence embedding, and then applying a sentence-level Transformer scanning through each sentence embedding sequentially. We argue that when processing at the sentence level, it could be better to set the increment of position index t i+1 − t i proportional to the length of the i-th sentence. This is because longer sentences tend to carry more information, so p i+1 is likely to move farther from p i . • Transformer for time-series events. As measurement time is continuous, time-series data is another scenario when a continuous position makes more sense than a discrete counterpart. More importantly, to predict the future values by modeling historical values observed at irregular time grids, it is better to consider the length of time horizon between two consecutive measures. A successful previous work is the Latent ODE <ref type="bibr" target="#b23">[24]</ref>, except that they use RNN as the backbone, and they model the hidden states rather than position representations with Neural ODE (because RNN itself provides positional bias). In this paper, we are not going to explore the more general cases discussed above. Instead, we decided to leave them as interesting future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FFN</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of our model(FLOATER). The main differences between FLOATER and the original Transformer model are: 1) the position representation is integrated into each block in the hierarchy (there are N blocks in total); and 2) there is a dynamical model (see<ref type="bibr" target="#b7">(8)</ref>) that generates position encoding vectors for each block. The dynamics are solved with a black-box ODE solver detailed in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>q∼</head><label></label><figDesc>are parameters in A n (3); p ∼(n) is the sinusoidal encoding; q∼ (n) i is the i-th row of Q (n) .Here we add a tilde sign to indicate the sinusoidal vectors. Formulas for k similar form and are omitted for brevity. Now we consider the case of FLOATER, where new position encodings p i are added</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>80Figure 2 :</head><label>2</label><figDesc>≤ x &lt; 100 100 ≤ x &lt; 120 120 ≤ x &lt; 140 Comparing BLEU scores of different encoding methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Visualizing the four different position methods. All models are trained using the Transformerbase architecture and En-De dataset. For better visualization, dimension indices are permuted in Figure 3b-3d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>τ, p τ ; w h )dτ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Direction of forward and backward propagation. Here we consider a simplified version where only position encodings p s and p t are in the computational graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparing position representation methods</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of various position encoders on the machine translation task.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on GLUE benchmark</figDesc><table><row><cell>Model</cell><cell cols="8">Single Sentence Similarity and Paraphrase Natural Language Inference CoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE</cell></row><row><cell>Base model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa</cell><cell>63.6</cell><cell>94.8</cell><cell>88.2</cell><cell>91.9</cell><cell>91.2</cell><cell>87.6</cell><cell>92.8</cell><cell>78.7</cell></row><row><cell cols="2">FLOATER 63.4</cell><cell>95.1</cell><cell>89.0</cell><cell>91.7</cell><cell>91.5</cell><cell>87.7</cell><cell>93.1</cell><cell>80.5</cell></row><row><cell cols="2">Large model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa</cell><cell>68.0</cell><cell>96.4</cell><cell>90.9</cell><cell>92.2</cell><cell>92.4</cell><cell>90.2</cell><cell>94.7</cell><cell>86.6</cell></row><row><cell cols="2">FLOATER 69.0</cell><cell>96.7</cell><cell>91.4</cell><cell>92.2</cell><cell>92.5</cell><cell>90.4</cell><cell>94.8</cell><cell>87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="3">Accuracy Middle High</cell></row><row><cell cols="3">Single model on test, large model</cell><cell></cell></row><row><cell>RoBERTa</cell><cell>82.8</cell><cell>86.5</cell><cell>81.3</cell></row><row><cell>FLOATER</cell><cell>83.3</cell><cell>87.1</cell><cell>81.7</cell></row></table><note>Experiment results on RACE benchmark. "Middle" means middle school level English exams, "High" means high school exams. Other details can be found in [15].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Experiment results on SQuAD benchmark. All results are obtained from RoBERTa-large model.</figDesc><table><row><cell>Model</cell><cell>SQuAD 1.1 EM F1</cell><cell cols="2">SQuAD 2.0 EM F1</cell></row><row><cell cols="4">Single models on dev, w/o data augmentation</cell></row><row><cell cols="3">RoBERTa 88.9 94.6 86.5</cell><cell>89.4</cell></row><row><cell cols="3">FLOATER 88.9 94.6 86.6</cell><cell>89.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison on WMT14 En-De data and Transformer-base architecture. Both BLUE scores and the number of trainable parameters inside each position encoder are included.</figDesc><table><row><cell></cell><cell cols="2">BLEU (↑) #Parameters (↓)</cell></row><row><cell>FLOATER</cell><cell>28.57</cell><cell>526.3K</cell></row><row><cell>1-layer RNN + scalar</cell><cell>27.99</cell><cell>263.2K</cell></row><row><cell>2-layer RNN + scalar</cell><cell>28.16</cell><cell>526.3K</cell></row><row><cell>1-layer RNN + vector</cell><cell>27.99</cell><cell>1,050.0K</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GLUE benchmark consists of eight datasets and each have different hyperparameter settings. For hyperparameters such as learning rate, batch size, training iterations, warm-up iterations, etc., we use the same values recommended by official repository of RoBERTa 1 .</p><p>SQuAD benchmark. For this benchmark we wrote our own finetuning code because currently there is no official code available. During the implementation process, we mainly refer to the third-party repositories 2 . We are not able to exactly match the official result reported in RoBERTa paper but quite close (∼0.1 difference in F1). For our FLOWER model, we use the same hyperparameters as RoBERTa.</p><p>RACE benchmark. This benchmark has the longest context and sequence length. We follow the official training script 3 and reproduce the result. Similar to other benchmarks, we then repeat the training process using exactly the same training hyperparameters to make a fair comparison. In this benchmark we freeze the weights w h and only finetune the weights of RoBERTa. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Are transformers universal approximators of sequence-to-sequence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10077</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Betterncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<title level="m">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pollard</surname></persName>
		</author>
		<title level="m">Ordinary Differential Equations: An Elementary Textbook for Students of Mathematics, Engineering, and the Sciences</title>
		<imprint>
			<publisher>Dover Publications</publisher>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01038</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<title level="m">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Numerical recipes in c++. The art of scientific computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">P</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flannery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13164</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">HIBERT: document level pre-training of hierarchical bidirectional transformers for document summarization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Copy tensors from the best performing checkpoint (validation set) to initialize FLOWER model. Initialize weights in the dynamical model with small values</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<title level="m">Half the peak learning rate</title>
		<imprint/>
	</monogr>
	<note>Transformer-base + En-De. the peak learning rate is changed from 7.0 × 10 −4 to 3.5 × 10 −4</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">With the warm-initialized FLOWER checkpoint, retrain on the same dataset for 10 epochs (En-De) or 1 epoch</title>
		<imprint>
			<publisher>En-Fr</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Due to resource constraint (and to show the compatibility to existing models), we initialize our FLOWER model with pretrained RoBERTa, which is similar to NMT task. However, the weights w h in dynamic function h(τ, p τ ; w h ) are not trained in large corpus, given that GLUE/SQuAD/RACE datasets are too small to train dynamics from scratch, we decided to pretrain h alone in WikiText103 [21] data using masked language modeling loss. We have found that when we train w h alone, it only takes a few hours (2x Titan V100) and one epoch to convergence. Once having the pretrained FLOWER model</title>
	</analytic>
	<monogr>
		<title level="m">For GLUE/SQuAD/RACE benchmarks, our experiments are all conducted upon RoBERTa, in which both base and large configurations are available</title>
		<imprint/>
	</monogr>
	<note>we can run following downstream tasks and compare with RoBERTa under the same setting</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
