<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
							<email>yongluli@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
							<email>liangxu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Feng</forename><surname>Wang</surname></persName>
							<email>wangyanfeng@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-Object Interaction (HOI) Detection is an important problem to understand how humans interact with objects. In this paper, we explore Interactiveness Knowledge which indicates whether human and object interact with each other or not. We found that interactiveness knowledge can be learned across HOI datasets, regardless of HOI category settings. Our core idea is to exploit an Interactiveness Network to learn the general interactiveness knowledge from multiple HOI datasets and perform Non-Interaction Suppression before HOI classification in inference. On account of the generalization of interactiveness, interactiveness network is a transferable knowledge learner and can be cooperated with any HOI detection models to achieve desirable results. We extensively evaluate the proposed method on HICO-DET and V-COCO datasets. Our framework outperforms state-of-the-art HOI detection results by a great margin, verifying its efficacy and flexibility. Code is available at https://github.com/DirtyHarryLYL/ Transferable-Interactiveness-Network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-Object Interaction (HOI) detection retrieves human and object locations and infers the interaction classes from still image. As a sub-task of visual relationship <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref>, HOI is strongly related to the human body and object understanding <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b32">33]</ref>. It is crucial for behavior understanding and can facilitate activity understanding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37]</ref>, imitation learning <ref type="bibr" target="#b0">[1]</ref>, etc. Recently, impressive progress has been made by utilizing Deep Neural Networks (DNNs) in this area <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref>. * Cewu Lu is the corresponding author, he is also a member of Department of Computer Science and Engineering, Shanghai Jiao Tong University, MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, and SJTU-SenseTime AI lab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HOI Detection Model</head><p>Interactiveness HOIs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interactiveness Network</head><p>Interactiveness Prior Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HOIs HOI Classifier</head><p>Multiple HOI Datasets (a) (b) <ref type="figure">Figure 1</ref>. Interactiveness Knowledge Learning. (a) HOI datasets contain implicit interactiveness knowledge. We can learn it better by performing explicit interactiveness discrimination, and utilize it to improve the HOI detection performance. (b) Interactiveness knowledge is beyond the HOI categories and can be learned across datasets, which can bring greater performance improvement.</p><p>Generally, human and objects need to be detected first. Given an image and its detections, human and objects are often paired exhaustively <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21]</ref>. HOI detection task aims to classify these pairs as different HOI categories. Previous one-stage methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref> directly classify a pair as specific HOIs. These methods actually predict interactiveness implicitly at the same time, where interactiveness indicates whether a human-object pair is interactive. For example, when a pair is classified as HOI "eat apple", we can implicitly predict that it is interactive.</p><p>Though interactiveness is an essential element for HOI detection, we neglected to study how to utilize it and improve its learning. In comparison to HOI categories, interactiveness conveys more basic information. Such attribute makes it easier for interactiveness to transfer across datasets. Based on this inspiration, we propose a Interactiveness Knowledge learning method as seen in <ref type="figure">Figure 1</ref>. With our framework, interactiveness can be learned across datasets and applied to any specific dataset. By utilizing interactiveness, we take two stages to identify HOIs: we first discriminate a human-object pair as interactive or not and then classify it as specific HOIs. Compared to previous one-stage method <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>, we take advantage of powerful interactiveness knowledge that incorporates more information from other datasets. Thus our method can decrease the false positives significantly. Additionally, after the interactiveness filtering in the first stage, we do not need to handle a large number of non-interactive pairs which are overwhelmingly more than interactive ones.</p><p>In this paper, we proposed a novel two-stage method to classify pairs hierarchically as shown in <ref type="figure">Figure 2</ref>. We introduce an interactiveness network which can be combined with any HOI detection model. We set a hierarchical logical strategy: by utilizing binary interactiveness labels, interactiveness network will bring in a strong supervised constraint which refines the framework in training and learns the interactiveness from multiple datasets. In testing, interactiveness network performs Non-Interaction Suppression (NIS) first. Then the HOI detection model will classify the remaining pairs as specific HOIs, where non-interactive pairs have been decreased significantly. Moreover, if the model classifies a pair as specific HOIs, it should figure out that the pair is interactive simultaneously. Such two-stage prediction will alleviate the learning difficulty and bring in hierarchical predictions. For special attention, interactiveness offers extra information to help HOI classification and is independent of HOI category settings. That means it can be transferred across datasets and utilized to enhance HOI models designed for different HOI settings.</p><p>We perform extensive experiments on HICO-DET <ref type="bibr" target="#b2">[3]</ref>, V-COCO <ref type="bibr" target="#b12">[13]</ref> datasets. Our method cooperated with transferred interactiveness outperforms the state-of-the-art methods by 2.38, 3.06, and 2.17 mAP on three Default category sets on HICO-DET, 4.0 and 3.4 mAP on V-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Visual Relationship Detection. Visual relationship detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28]</ref> aims to detect the objects and classify their relationships simultaneously. In <ref type="bibr" target="#b18">[19]</ref>, Lu et al. proposed a relationship dataset VRD and an approach combined with language priors. Predicates within relationship triplet subject, predicate, object include actions, verbs, spatial and preposition vocabularies. Such vocabulary setting and severe long-tail issue within the dataset make this task quite difficult. Large-scale dataset Visual Genome <ref type="bibr" target="#b15">[16]</ref> is then proposed to promote studies in this problem. Recent works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref> put attention on more effective and efficient visual feature extraction and try to exploit semantic information to refine the relationship detection. Human-Object Interaction Detection. Human-Object Interaction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15]</ref> is essential to understand humancentric interaction with objects. Recently several largescale datasets, such as V-COCO <ref type="bibr" target="#b12">[13]</ref>, HICO-DET <ref type="bibr" target="#b2">[3]</ref>, HCVRD <ref type="bibr" target="#b30">[31]</ref>, were proposed for the exploration of HOI  <ref type="figure">Figure 2</ref>. HOIs within an image can be represented as a HOI graph. Human and object can be seen as nodes, whilst the interactions are represented as edges. Exhaustive pairing of all nodes would import overmuch non-interactive edges and do damage to detection performance. Our Non-Interaction Suppression can effectively reduce non-interactive pairs. Thus the dense graph would be converted to a sparse graph and then be classified. detection. Different from HOI recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20]</ref> which is an image level classification problem, HOI detection needs to detect interactive human-object pairs and classify their interactions at instance level. With the assistance of DNNs and large-scale datasets, recently methods have made significant progress. Chao et al. <ref type="bibr" target="#b2">[3]</ref> proposed a multi-stream model combining visual features, spatial locations to help tackle this problem. To address the long tail issue, Shen et al. <ref type="bibr" target="#b23">[24]</ref> studied zero-shot learning problem and predicted the verb and object separately. In <ref type="bibr" target="#b11">[12]</ref>, an action specific density map estimation method is introduced to locate objects interacted with human. In <ref type="bibr" target="#b20">[21]</ref>, Qi et al. proposed GPNN incorporating DNN and graphical model, which uses message parsing to iteratively update states and classifies all possible pairs/edges. Gao et al. <ref type="bibr" target="#b8">[9]</ref> exploited an instance centric attention module to enhance the information from the interest region and facilitate the HOI classification. Generally, these methods inference in one-stage and may suffer from severe non-interactive pair domination problem. To address this issue, we utilize interactiveness to explicitly discriminate non-interactive pairs and suppress them before HOI classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head><p>HOI representation can be described as a graph model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref> as seen in <ref type="figure">Figure 2</ref>. Instances and relations are expressed as nodes and edges respectively. With exhaustive pairing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9]</ref>, HOI graph G = (V, E) is dense connected, where V includes human node V h and object node V o . Let v h ∈ V h and v o ∈ V o denote the human and object nodes. Thus edges e ∈ E are expressed as</p><formula xml:id="formula_0">e = (v h , v o ) ∈ V h × V o .</formula><p>With n nodes, exhaustive paring will generate a mass of edges. We aim to assign HOI (including no HOI) labels on those edges. Considering that a vast majority of non-interactive edges existing in E should be discarded, our goal is to seek a sparse G * with corrected  <ref type="figure">Figure 3</ref>. Overview of our framework. Interactiveness network P can cooperate with any HOI models (referred as C). P employs human, object and spatial-pose streams to extract features from human and object appearance, spatial locations and human pose information. The outputs of three streams are concatenated and inputted to the interactiveness discriminator. When cooperated with multi-stream C such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> (human, object, and spatial streams), H P and O P in P can share weights (dotted lines) with H C and O C in C during joint training. In this work, these four blocks are all residual blocks <ref type="bibr" target="#b13">[14]</ref>. LIS and NIS will be detailed in Section 4.3 and Section 4.5.</p><p>HOI labeling on its edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>As aforementioned, we introduce Interactiveness Knowledge to advance HOI detection performance. That is, explicitly discriminate the non-interactive pairs and suppress them before HOI classification. From the semantic point of view, interactiveness provides more general information than conventional HOI categories. Since any human-object pair can be assigned binary interactiveness labels according to the HOI annotations, i.e. "interactive" or "non-interactive", interactiveness knowledge can be learned from multiple datasets with different HOI category settings and transferred to any specific datasets.</p><p>To exploit this cue, we proposed interactiveness network (interactiveness predictor, referred as P) which utilizes interactiveness to reduce false positives caused by overmuch non-interactive pair candidates. Some conventional modules are also included, namely, Representation Network R (feature extractor) and Classification Network C (HOI classifier). R is responsible for feature extraction from detected instances. C utilizes node and edge features to perform HOI classification. <ref type="figure">Figure 3</ref> is an overview of our framework which follows the hierarchical classification paradigm. Specifically, we first train P and C jointly to learn the interactiveness and HOIs knowledge. Under usual circumstances, the ratio of non-interactive edges is dominant within inputs. Hence P will bring a strong supervised signal to refine the framework. In testing, P is utilized in two stages. First, P evaluates the interactiveness of edges by exploiting the learned interactiveness knowledge, so we can convert the dense HOI graph to a sparse one. Second, combined with interactiveness score from P, C will process the sparse graph and classify the remaining edges.</p><p>In addition, on account of the generalization ability of interactiveness knowledge, it can be transferred with P across datasets (Section 4.4). Details of the framework architecture are illustrated in Section 4.2 and 4.3. The process of training and testing will be detailed in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Representation and Classification Networks</head><p>Human and Object Detection. In HOI detection, human and object need to be detected first. In this work, we follow the setting of <ref type="bibr" target="#b8">[9]</ref> and employ the Detectron <ref type="bibr" target="#b10">[11]</ref> with ResNet-50-FPN <ref type="bibr" target="#b16">[17]</ref> to prepare bounding boxes and detection scores. Before post-processing, detection results will be filtered by the detection score thresholds first. Representation Network. In previous methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>, R is often modified from object detector such as Fast R-CNN <ref type="bibr" target="#b9">[10]</ref> or Faster R-CNN <ref type="bibr" target="#b21">[22]</ref>. We also exploited a Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> with ResNet-50 <ref type="bibr" target="#b13">[14]</ref> based R here. During training and testing, R is frozen and acts as a feature extractor. Given the detected bounding boxes, we produce human and object features by cropping ROI pooling feature maps according to box coordinates. HOI Classification Network. As for C, multi-stream architecture and late fusion strategy are frequently used and approved effective <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. Follow <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, for our classification network C, we utilize a human stream and an object stream to extract human, object and context features. Within each stream, a residual block <ref type="bibr" target="#b13">[14]</ref> (denoted as H C , O C , seen in <ref type="figure">Figure 3</ref>) with pooling layer and fully connected layers (FCs) are adopted. Moreover, an extra spa- tial stream <ref type="bibr" target="#b2">[3]</ref> is adopted to encode the spatial locations of instances. Its input is a two-channel tensor consisting of a human map and an object map, shown in <ref type="figure" target="#fig_0">Figure 4</ref>. Human and object maps are all 64x64 and obtained from the human-object union box. In the human channel, the value is 1 in the human bounding box and 0 in other areas. The object channel is similar which has value 1 in the object bounding box and 0 elsewhere. Following the late fusion strategy, each stream will first perform HOI classification, then three prediction scores will be fused by element-wise sum in the same proportion to produce the final result of C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Interactiveness Network</head><p>Interactiveness needs to be learned by extracting and combining essential information. The visual appearance of human and object are obviously required. Besides, interactive and non-interactive pairs also have other distinguishing features, e.g. spatial location and human pose information. For example, in the upper image of <ref type="figure" target="#fig_0">Figure 4</ref>, Person 1 and the giraffe far from him are not interactive. Their spatial maps <ref type="bibr" target="#b2">[3]</ref> can provide pieces of evidence to help with classification. Furthermore, pose information is also helpful. In the lower image, although two people are both close to the giraffe, only Person 2 and the giraffe are interactive. The arm of Person 2 is uplift and touching the giraffe. Whilst Person 1 is back on to the giraffe, and his pose is quite different from the typical pose of "feed".</p><p>Based on these reasons, the combination of visual appearance, spatial location and human pose information is key to interactiveness discrimination. Hence P needs to encode these key elements together to learn the interactiveness knowledge. A natural choice is the multi-stream architecture as presented: human, object and spatial-pose streams. Human and Object stream. For human and object appear- ance, we extract ROI pooling features from representation network R, then input them into residual blocks H P and O P , respectively. The architecture of H P and O P are same as H C and O C <ref type="figure">(Figure 3</ref>). Through subsequent global average pooling and FCs, the output features of two streams are denoted as f h and f o , respectively. Spatial-Pose Stream. Different from <ref type="bibr" target="#b2">[3]</ref>, our spatial-pose stream input includes a special 64x64 pose map. Given the union box of each human and his/her paired object, we employ pose estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> to estimate his/her 17 keypoints (in COCO format <ref type="bibr" target="#b17">[18]</ref>). Then, we link the keypoints with lines of different gray value ranging from 0.15 to 0.95 to represent different body parts, which implicitly encodes the pose features. Whilst the other area is set as 0. Finally, we reshape the union box to 64x64 to construct the pose map. We concatenate the pose map with human and object maps which are the same as those in the spatial stream of C. This forms the input for our spatial-pose stream. Next, we exploit two convolutional layers with max pooling and two 1024 sized FCs to extract the feature f sp of three maps. Last, the output will be concatenated with the outputs of human and object streams for interactiveness discrimination. Given a HOI graph G with all possible edges, P will evaluate the interactiveness of pair (v h , v o ) based on learned knowledge, and gives confidence:</p><formula xml:id="formula_1">s P (h,o) = f P (f h , f o , f sp ) * L(s h , s o ),<label>(1)</label></formula><p>where L(s h , s o ) is a novel weight function named Lowgrade Instance Suppressive Function (LIS). It takes the human and object detection scores s h , s o as inputs:</p><formula xml:id="formula_2">L(s h , s o ) = P(s h ) * P(s o ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">P(x) = T 1 + e (k−wx) ,<label>(3)</label></formula><p>P(·) is a part of the logistic function, the value of T, k and w will be determined by data-driven manner. <ref type="figure" target="#fig_1">Figure 5</ref> depicts the curve of P(·) whose domain definition is (0, 1). Bounding boxes will have low weight till their score is higher than a threshold. Previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> often directly multiply detection scores by the final classification score. But they cannot notably emphasize the differentiation between high quality and inaccurate detection results. LIS has the ability to enhance the differentiation between high and low grade object detections as shown in <ref type="figure" target="#fig_1">Figure 5</ref>.</p><p>Weights Sharing Strategy. An additional benefit of our interactiveness network is that, if cooperated with multistream HOI detection model C, P can share the weights of convolutional blocks with the ones in C. As shown in <ref type="figure">Figure 3</ref>, blocks H P and O P can share weights with H C and O C in the joint training. This weights sharing strategy can guarantee information sharing and better optimization of P and C in the multi-task training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Interactiveness Knowledge Transfer Training</head><p>With R, P and C, our framework has two modes of utilization: hierarchical joint training in Default Mode, and interactiveness transfer training in Transfer Learning Mode. Hierarchical Joint Training. In Default Mode, we introduce our hierarchical joint training scheme, as illustrated in <ref type="figure" target="#fig_3">Figure 6</ref> (a). By adding a supervisor P, our framework works in an unconventional training mode. To be specific, the framework is trained with hierarchical classification tasks, i.e. explicit interactiveness discrimination and HOI classification. The objective function of the framework can be expressed as:</p><formula xml:id="formula_4">L = L C + L P ,<label>(4)</label></formula><p>where L C denotes the HOI classification cross entropy loss, while L P is the binary classification cross entropy loss. Different from one-stage methods, additional interactiveness discrimination enforces the model to learn interactiveness knowledge, which can bring more powerful supervised constraints. Namely, when a pair is predicted as specific HOIs such as "cut cake", P must give the prediction "interactive" simultaneously. Experiment results (Section 5.4) prove that interactiveness knowledge learning can effectively refine the training and improve the performance. The framework in Default Mode is called "RP D C D " in the following, where "D" indicates "Default". Interactiveness Knowledge Transfer Training. Noting that P only needs binary labels which are beyond the HOI classes, so interactiveness is transferable and reusable. In Transfer Learning Mode, P can be used as a transferable  In Transfer Learning Mode, P can learn interactiveness knowledge across datasets and cooperates with multiple Cs trained on different datasets. In testing, our framework infers in two stages, i.e. P performs interactiveness discrimination at first, then C classifies the remaining edges/pairs. knowledge learner to learn interactiveness from multiple datasets and be applied to each of them respectively, as illustrated in <ref type="figure" target="#fig_3">Figure 6</ref> (b). On the contrary, C must be trained on a single dataset once a time considering the variety of HOI category settings in different datasets. Therefore, knowledge of the specific HOIs is difficult to transfer. We will compare and evaluate the transferability of interactiveness knowledge and HOI knowledge in Section 5.</p><p>For better representation of the transferability and performance enhancement of interactiveness, we set several transfer learning modes, referred as "RP T n C D ", where "T" indicates "Transfer", and "n" means P learns interactiveness knowledge from "n" datasets: 1) RP T 1 C D : train P on 1 dataset and apply P to another dataset. 2) RP T 2 C D : train P on 2 datasets and apply P to them respectively.</p><p>To compare the transferability of interactiveness knowledge and HOIs knowledge, we set a transfer learning mode "RC T " for C: 3) RC T : train C (without P) on one dataset and apply it to another dataset. For example, we first train and test C on HICO-DET (referred as "RC D "). Second, we replace the last FC layer of C with a FC layer that fits the number of V-COCO HOIs, then finetune C for 1 epoch on V-COCO train set. Last, we test this new C on V-COCO test set. Details of the above modes can be found in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Testing with Non-Interaction Suppression</head><p>After the interactiveness learning, we further utilize P to suppress the non-interactive pair candidates in testing, i.e. Non-Interaction Suppression (NIS). The inference process is based on tree structure as shown in <ref type="figure">Figure 2</ref>. Detected instances in test set will be paired exhaustively, so a dense graph G of human and objects is generated. First, we employ P to compute the interactiveness score of all edges. Next, we suppress the edges that meet NIS conditions, i.e. interaction score s P (h,o) smaller than a certain threshold α.</p><p>Through NIS, we can convert G to G where G denotes the approximate sparse HOI graph. The HOI classification score vector S C (h,o) of (v h , v o ) from C is:</p><formula xml:id="formula_5">S C (h,o) = F C [Γ ; G (v h , v o )],<label>(5)</label></formula><p>where Γ are input features. The final HOI score vector of a pair (v h , v o ) can be obtained by:</p><formula xml:id="formula_6">S (h,o) = S C (h,o) * s P (h,o) .<label>(6)</label></formula><p>Here we multiply interactiveness score s P (h,o) from P by the output of C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first introduce datasets and metrics adopted and then give implementation details of our framework. Next, we report our HOI detection results quantitatively and qualitatively compared with state-of-the-art approaches. Finally, we conduct ablation studies to validate the validity of the components in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Metrics</head><p>Datasets. We adopt two HOI datasets HICO-DET <ref type="bibr" target="#b2">[3]</ref> and V-COCO <ref type="bibr" target="#b12">[13]</ref>. HICO-DET <ref type="bibr" target="#b2">[3]</ref> includes 47,776 images <ref type="bibr" target="#b37">(38,</ref><ref type="bibr">118</ref> in train set and 9658 in test set), 600 HOI categories on 80 object categories (same with <ref type="bibr" target="#b17">[18]</ref>) and 117 verbs, and provides more than 150k annotated human-object pairs. V-COCO <ref type="bibr" target="#b12">[13]</ref> provides 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and <ref type="bibr" target="#b15">16,</ref><ref type="bibr">199</ref> person instances. Each person has annotations for 29 action categories (five of them have no paired object). The objects are divided into two types: "object" and "instrument". Metrics. We follow the settings adopted in <ref type="bibr" target="#b2">[3]</ref>, i.e. a prediction is a true positive only when the human and object bounding boxes both have IoUs larger than 0.5 with reference to ground truth, and the HOI classification result is accurate. The role mean average precision <ref type="bibr" target="#b12">[13]</ref> is used to measure the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We employ a Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> with ResNet-50 <ref type="bibr" target="#b13">[14]</ref> as R and keep it frozen. C consists of three streams similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, extracting features Γ from instance appearance, spatial location as well as context. Within human and object streams, a residual block <ref type="bibr" target="#b13">[14]</ref> with global average pooling and four 1024 sized FCs are used. Relatively, the spatial stream is composed of two convolutional layers with max pooling, and two 1024 sized FCs. Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, we use the late fusion strategy in C. P also consists of three streams (seen in <ref type="figure">Figure 3)</ref>. A residual block <ref type="bibr" target="#b13">[14]</ref> with global average pooling, and two 1024 sized FCs are adopted in human and object streams. Residual blocks within these two streams will share weights with those in C. Spatial-Pose stream consists of two convolutional layers with max pooling and two 1024 sized FCs. The outputs of three streams are concatenated and passed through two 1024 sized FCs to perform interactiveness discrimination. For a fair comparison, we adopt the object detection results and COCO <ref type="bibr" target="#b17">[18]</ref> pre-trained weights from <ref type="bibr" target="#b8">[9]</ref> which are provided by authors. Since NIS and LIS can suppress non-interactive pairs, we set detection confidence thresholds lower than <ref type="bibr" target="#b8">[9]</ref>, i.e. 0.6 for human and 0.4 for object. The image-centric training strategy <ref type="bibr" target="#b21">[22]</ref> is also applied. In other words, pair candidates from one image make up the mini-batch. We adopt SGD and set an initial learning rate as 1e-4, weight decay as 1e-4, momentum as 0.9. In training, the ratio of positive and negative samples is 1:3. We jointly train the framework for 25 epochs. In LIS mentioned in Equation 3, we set T = 8.4, k = 12.0, w = 10.0. In testing, the interactiveness threshold α in NIS is set as 0.1. All experiments are conducted on a single Nvidia Titan X GPU.</p><formula xml:id="formula_7">Test Set Method P-Train Set C-Train Set HICO-DET RP D C D HICO-DET HICO-DET RP T 1 C D V-COCO HICO-DET RP T 2 C D HICO-DET, V-COCO HICO-DET HICO-DET RC D - HICO-DET RC T - V-COCO V-COCO RP D C D V-COCO V-COCO RP T 1 C D HICO-DET V-COCO RP T 2 C D HICO-DET, V-COCO V-COCO V-COCO RC D - V-COCO RC T - HICO-DET</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and Comparisons</head><p>We compare our method with five state-of-the-art HOI detection methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref> on HICO-DET, and four methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref> on V-COCO. The HOI detection result is evaluated with mean average precision. For HICO-DET, we follow the settings in <ref type="bibr" target="#b2">[3]</ref>: Full (600 HOIs), Rare (138 HOIs), Non-Rare (462 HOIs) in Default and Known Object mode. For V-COCO, we evaluate AP role (24 actions with roles). More details can be found in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>. Default Mode. From <ref type="table">Table 2</ref>, we can find that the RP D C D has already outperformed compared methods. We respectively achieve 17.03 and 19.17 mAP on Default and Know Object Full sets on HICO-DET. In particular, we boost the performance of 2.97 and 4.18 mAP on Rare sets. To illustrate, as the generalization ability of interactiveness is beyond HOI category settings, information scarcity and learning difficulty of rare categories is alleviated. So the performance difference between rare and nonrare categories is accordingly reduced. Results on V-COCO are shown in <ref type="table">Table 3</ref>. RP D C D also achieves superior performance and outperforms state-of-the-art method <ref type="bibr" target="#b8">[9]</ref>  We also evaluate the transferability of HOIs knowledge. In comparison with RC D , RC T shows a significant performance decrease of 3.14 and 4.7 mAP on two datasets, as shown in <ref type="table">Table 2</ref> and 3. It proves that interactiveness is more suitable and easier to transfer than HOIs knowledge. Non-Interaction Reduction. The non-interactive pairs reduction effect after employing NIS are shown in <ref type="table">Table  4</ref>. In default mode RP D C D , NIS shows obvious effectiveness. With interactiveness transferred from multiple datasets, RP T 2 C D achieves better suppressive effect and discards 70.94% and 73.62% non-interactive pairs respectively on two datasets, thus bringing more performance gain. Meanwhile, RP T 1 C D also performs well and suppresses a certain amount of non-interactive pair candidates. This suggests the good transferability of interactiveness. Visualized Results. Representative predictions are shown in <ref type="figure" target="#fig_4">Figure 7</ref>. We can find that our model is capable of detecting various kinds of complicated HOIs such as multiple interactions within one pair, one person performing multiple interactions with different objects, one object interacted with multiple persons, multiple persons performing different interactions with multiple objects. <ref type="figure" target="#fig_5">Figure 8</ref> shows the visualized effects of NIS. We can see that NIS effectively distinguish the non-interactive pairs  Method AP role Gupta et al. <ref type="bibr" target="#b12">[13]</ref> 31.8 InteractNet <ref type="bibr" target="#b11">[12]</ref> 40.0 GPNN <ref type="bibr" target="#b20">[21]</ref> 44.0 iCAN w/ late(early) <ref type="bibr" target="#b8">[9]</ref>  <ref type="table">Table 3</ref>. Results comparison on V-COCO <ref type="bibr" target="#b12">[13]</ref>. D indicates the default mode, and T means the transfer learning model. and suppress them in extremely difficult scenarios, such as a person performing a confusing action and the tennis ball, a crowd of people with ties. In the bottom-left corner we show an even harder sample. When the subject and object are the left hand and right hand, C predicts wrong HOI "type on keyboard". C may mistake the left hand for the keyboard because they are too close. However, P accurately figures out that two hands are non-interactive. These results prove that the one-stage method would yield many false positives without interactiveness and NIS.</p><formula xml:id="formula_8">44.7 (45.3) RC D 43.2 RP D C D 47.8 RC T 38.5 RP T 1 C D 48.3 RP T 2 C D 48.7</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Studies</head><p>In mode RP D C D , we analyze the significance of Lowgrade Instance Suppressive, Non-Interaction Suppression and the three streams within P (seen in <ref type="table">Table 5</ref>). Non-Interaction Suppression NIS plays a key role to reduce the non-interactive pairs. We evaluate its impact by removing NIS during testing. In other words, we directly use the S (h,o) from Equation 6 as the final predictions without NIS. Consequently, the model shows an obvious performance degradation, which proves the importance of NIS. Low-grade Instance Suppressive LIS suppress the lowgrade object detections and reward the high-grade ones. By removing L(s h , s o ) in Equation 1, we observe a degradation in <ref type="table">Table 5</ref>. This suggests that LIS is capable of distinguishing the low-grade detections and improves the performance hit-obj skateboard-instr ski-instr work_on-instr wear-tie cut-obj ski-instr hit-obj hold-apple type_on-keyboard hold-handbag sit_on-chair Without NIS, C would generate false positive predictions for these non-interactive pairs in one-stage inference, which are shown by the purple texts below the images. Even some extremely hard scenarios can be discovered and suppressed, such as mis-groupings between person and object close to each other, person and object in clutter scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Set</head><p>Method Reduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HICO-DET</head><p>RP D C D -65.96% RP T 1 C D -62.24% RP T 2 C D -70.94%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V-COCO</head><p>RP D C D -65.98% RP T 1 C D -59.51% RP T 2 C D -73.62% without using more costly superior object detector.</p><p>NIS &amp; LIS Without NIS and LIS both, our method only takes effect in the joint training of P and C. As we can see in <ref type="table">Table 5</ref>, performance degrades greatly but still outperforms other methods, which indicates the enhancement brought by P in the hierarchical joint training. Three Streams. By keeping one stream in P each time, we evaluate their contributions as shown in <ref type="table">Table 5</ref>. We can find that spatial-pose stream is the largest contributor, but we still need appearance features from the other two streams to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel method to learn and utilize the implicit interactiveness knowledge, which is general and beyond HOI categories. Thus, it can be transferred across datasets. With interactiveness knowledge, we exploit an interactiveness network to perform Non-interaction Suppression before HOI classification in inference. Extensive experiment results show the efficacy of interactiveness. By combining our method with existing detection models, we achieve state-of-the-art results on HOI detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Inputs of the spatial-pose stream. Three kinds of maps are included: pose map, human map and object map. Person 2 in two images both have interaction "feed" with giraffes. But two pairs of Person 1 and giraffe are all non-interactive. Their poses and locations are helpful for the interactiveness discrimination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>The illustration of P(·) within Low-grade Suppressive Function. Its input is object detection score. High-grade detected objects will be emphasized and distinguished with low-grade ones. In addition, P(0) = 5.15E − 05 and P(1) = 9.99E − 01.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>The schemes for training and testing. (a) In Default Mode, P and C are first trained jointly with weights sharing on the same dataset. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of sample HOI detections. Subjects and objects are represented with blue and red bounding boxes. While interactions are marked by green lines linking the box centers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Visualized effects of NIS. Green lines mean accurate HOIs, while purple lines mean non-interactive pairs which are suppressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Mode settings in experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Non-interactive pairs reduction after performing NIS. Results of ablation studies. Human, object, spatial-pose stream are representated as H, O and S-P stream.</figDesc><table><row><cell></cell><cell cols="2">HICO-DET</cell><cell>V-COCO</cell></row><row><cell>Method</cell><cell cols="2">Default Full KO Full</cell><cell>AP role</cell></row><row><cell>RP D C D</cell><cell>17.03</cell><cell>19.17</cell><cell>47.8</cell></row><row><cell>w/o NIS</cell><cell>15.86</cell><cell>17.35</cell><cell>46.2</cell></row><row><cell>w/o LIS</cell><cell>16.35</cell><cell>18.83</cell><cell>47.4</cell></row><row><cell>w/o NIS &amp; LIS</cell><cell>15.45</cell><cell>17.31</cell><cell>45.8</cell></row><row><cell>H Stream Only</cell><cell>14.91</cell><cell>16.21</cell><cell>44.5</cell></row><row><cell>O Stream Only</cell><cell>15.28</cell><cell>16.89</cell><cell>45.2</cell></row><row><cell>S-P Stream Only</cell><cell>15.73</cell><cell>17.46</cell><cell>46.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of robot learning from demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Argall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and autonomous systems</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting the location of interactees in novel human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing human actions in still images: a study of bag-of-features and partbased representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pairwise bodypart attention for recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ican: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10437</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<idno>2018. 3</idno>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07333</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing actions from still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pehlivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning models for actions and person-object interactions with transfer to question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of action classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04979</idno>
		<title level="m">Zoom-net: Mining deep feature interactions for visual relationship recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09892</idno>
		<title level="m">Care about you: towards large-scale human-centric visual relationship detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SRDA: Generating Instance Segmentation Annotation via Scanning, Reasoning and Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pose Flow: Efficient Online Pose Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly and semi supervised human body part parsing via pose-guided knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond holistic object recognition: Enriching image understanding with part states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Leonidas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09961</idno>
		<title level="m">Deep RNN Framework for Visual Sequential Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00324</idno>
		<title level="m">CrowdPose: Efficient Crowded Scenes Pose Estimation and A New Benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
