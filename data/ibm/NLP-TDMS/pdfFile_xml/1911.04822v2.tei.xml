<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Capsule Network-based Model for Learning Node Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event, Ireland. ACM</publisher>
				<availability status="unknown"><p>Copyright Virtual Event, Ireland. ACM</p>
				</availability>
				<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Nguyen</surname></persName>
							<email>nguyendinhtu@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Dat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
							<email>dinh.phung@monash.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Dat</roleName><forename type="first">Quoc</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><forename type="middle">Phung</forename><surname>Nguyen</surname></persName>
							<email>dai.nguyen@monash.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Capsule Network-based Model for Learning Node Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</title>
						<meeting>the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20) <address><addrLine>New York, NY, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event, Ireland. ACM</publisher>
							<biblScope unit="volume">7</biblScope>
							<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3340531.3417455</idno>
					<note>ACM Reference Format:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies → Neural networks;</term>
					<term>Information systems → Social networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we focus on learning low-dimensional embeddings for nodes in graph-structured data. To achieve this, we propose Caps2NE -a new unsupervised embedding model leveraging a network of two capsule layers. Caps2NE induces a routing process to aggregate feature vectors of context neighbors of a given target node at the first capsule layer, then feed these features into the second capsule layer to infer a plausible embedding for the target node. Experimental results show that our proposed Caps2NE obtains state-of-the-art performances on benchmark datasets for the node classification task. Our code is available at: https://github.com/daiquocnguyen/ Caps2NE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Numerous real-world and scientific data are represented in forms of graphs, e.g. data from knowledge graphs, recommender systems, social and citation networks as well as telecommunication and biological networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. Recent years have witnessed many successful downstream applications of utilizing the graph-structured data such as for improving information extraction and text classification systems <ref type="bibr" target="#b12">[13]</ref>, traffic learning and forecasting <ref type="bibr" target="#b4">[5]</ref> and for advertising and recommending relevant items to users <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>. This is largely boosted by a surge of methodologies that learn embedding representations to encode graph structures <ref type="bibr" target="#b2">[3]</ref>. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM '20, October 19-23, 2020, Virtual Event, Ireland © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-6859-9/20/10. . . $15.00 https://doi.org/10.1145/3340531. <ref type="bibr">3417455</ref> One of the most important tasks in learning graph representations is to learn low-dimensional embeddings for nodes in the graphstructured data <ref type="bibr" target="#b25">[26]</ref>. These embedding vectors can then be used in a downstream task such as node classification, i.e., using the learned node embeddings as feature inputs to train a classifier to predict node labels <ref type="bibr" target="#b9">[10]</ref>.</p><p>A simple and effective approach is to treat each node as a word token and each graph as a text collection; hence we can apply a word embedding model such as Word2Vec <ref type="bibr" target="#b14">[15]</ref> to learn node embeddings such as DeepWalk <ref type="bibr" target="#b17">[18]</ref> and Node2Vec <ref type="bibr" target="#b7">[8]</ref>. Recent work has developed deep neural networks (DNN) for the node classification task, e.g., GCN <ref type="bibr" target="#b12">[13]</ref>, GraphSAGE <ref type="bibr" target="#b9">[10]</ref> and GAT <ref type="bibr" target="#b20">[21]</ref>. We see that the DNN-based approaches are showing state-of-the-art performances, but not well-efficient to exploit the structural dependencies among nodes.</p><p>In this paper, inspired by the advanced capsule networks <ref type="bibr" target="#b18">[19]</ref>, we present Caps2NE -a new unsupervised embedding model that adapts capsule network to learn node embeddings. Caps2NE aims to capture h-hops context neighbors to predict a target node. In particular, Caps2NE consists of two capsule layers with connections from the first to the second layer, but no connections within layers. The first layer constructs capsules to encapsulate context neighbors. Then a routing process is used to aggregate the feature information from capsules in the first layer to only one capsule in the second layer. After that, the second layer produces a continuous vector which is used to infer an embedding for the target node. Note that encapsulating the context neighbors into the corresponding capsules aims to preserve node properties more efficiently. And the routing process aims to generate high-level features to infer plausible node embeddings effectively.</p><p>Our main contributions are as follows:</p><p>• We investigate the advanced use of capsule networks for the graph-structured data and propose a new embedding model Caps2NE to learn node embeddings. • We evaluate the performance of the proposed Caps2NE on benchmark datasets for the node classification task. • The experimental results show that that our Caps2NE produces state-of-the-art accuracy results on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED CAPS2NE</head><p>This section presents our Caps2NE model. In particular, we detail how to sample data from an input graph, then how to construct Caps2NE to learn node embeddings.  <ref type="figure">Figure 1</ref>: Processes in our Caps2NE with q = 6, d = 4, k = 3 for an illustration purpose. Note that in this illustration, we use numbered subscripts to denote nodes themselves, not indexes of nodes or capsules. The indexes of capsules are fixed from 1 to (q − 1), not depending on the indexes of the context neighbors. With v be the target node 3, we have</p><formula xml:id="formula_0">C v = {v 1 = 1, v 2 = 2, v 3 = 4, v 4 = 5, v 5 = 6}. Definition 1. A network graph G is defined as G = (V, E), in which V is a set of nodes, E ⊆ {(v, v ′ )|v, v ′ ∈ V} is a set of edges, and each node v ∈ V is associated with a feature vector x v ∈ R d .</formula><p>We aim to learn a node embedding o v for each node v.</p><p>Sampling input pairs. We follow Perozzi et al. <ref type="bibr" target="#b17">[18]</ref> to uniformly sample a number T of random walks of length q for every node in V. From each random walk, we randomly sample a target node v, treat (q − 1) remaining nodes as the context neighbors of node v, and construct an input pair of (C v , v), where we denote C v be the list of context neighbors v i of the target node v (here, i ∈ {1, 2, ..., q − 1} and |C v | = q − 1). <ref type="figure">Figure 1</ref> shows an example of a graph consisting of 6 nodes. If we sample a random walk of length q = 6 for node 1 such as {1, 2, 3, 4, 5, 6} and select node 3 as the target node v, then the remaining nodes {1, 2, 4, 5, 6} are treated as the context neighbors of node 3, i.e.,</p><formula xml:id="formula_1">C v = {v 1 = 1, v 2 = 2, v 3 = 4, v 4 = 5, v 5 = 6}.</formula><p>Definition 2. A capsule is a group of neurons. A capsule layer is a group of capsules without connections among capsules in the same layer <ref type="bibr" target="#b18">[19]</ref>. Two continuous capsule layer is connected using a routing process.</p><p>Constructing Caps2NE. We build our Caps2NE with two capsule layers. In the first layer, we construct (q − 1) capsules, where the feature vector of each context neighbor v i is encapsulated by the i-th corresponding capsule (with i ∈ {1, 2, ..., q − 1}). In the second layer, we construct one capsule to produce a vector representation which is then used to infer an embedding for the target node v.</p><p>The first capsule layer consists of (q − 1) capsules, in which the i-th capsule use a non-linear squashing function to transform the feature vector</p><formula xml:id="formula_2">x v i of the context neighbor v i into u (i) v i as: u (i) v i = squash x v i = ∥x v i ∥ 2 1 + ∥x v i ∥ 2 x v i ∥x v i ∥<label>(1)</label></formula><p>The squashing function ensures that the orientation of each feature vector is unchanged while its length is scaled down to below 1. Vectors u (i) v i are then linearly transformed using weight matrices W i ∈ R k ×d to produce vectorsû</p><formula xml:id="formula_3">(i) v i ∈ R k . These vectorsû (i)</formula><p>v i are weighted to sum up to return a vector s v ∈ R k for the capsule in the second layer (recall that the second layer consists of only one capsule). This capsule then performs the non-linear squashing function to produce a vector e v ∈ R k . Formally, we have:</p><formula xml:id="formula_4">e v = squash (s v ) ; s v = i c iû (i) v i ;û (i) v i = W i u (i) v i (2)</formula><p>where c i are coupling coefficients determined by the routing process as presented in Algorithm 1. Here, c i aims to weight u (i) v i of the i-th capsule in the first layer.</p><p>As we use one capsule in the second layer, we make two differences in our routing process in Algorithm 1: (i) we apply softmax in a direction from all capsules in the previous layer to each of capsules in the next layer, (ii) thus, we propose a new update rule</p><formula xml:id="formula_5">(b i ←û (i) v i · e v ) instead of employing (b i ← b i +û (i) v i · e v )</formula><p>originally used by Sabour et al. <ref type="bibr" target="#b18">[19]</ref>.</p><p>Algorithm 1: The Caps2NE routing process.</p><formula xml:id="formula_6">1 for i = 1, 2, ..., q-1 do 2 b i ← 0 3 for iteration = 1, 2, ..., m do 4 c ← softmax (b) 5 s v ← i c iû (i) v i 6 e v ← squash (s v ) 7 for i = 1, 2, ..., q-1 do 8 b i ←û (i) v i · e v</formula><p>Learning model parameters. The vector representation e v is then used to infer the final embedding o v ∈ R k of the target node v, as shown in <ref type="bibr">Equation 3</ref>. We learn all model parameters (including the node embeddings o v ) by minimizing the sampled softmax loss function <ref type="bibr" target="#b10">[11]</ref> applied to the target node v as:</p><formula xml:id="formula_7">L Caps2NE (v) = − log exp(o T v e v ) v ′ ∈V ′ exp(o T v ′ e v )<label>(3)</label></formula><p>where V ′ is a subset sampled from V.</p><p>Algorithm 2: The Caps2NE learning process.</p><formula xml:id="formula_8">1 Input: A network graph G = (V, E) 2 for v ∈ V do 3</formula><p>SAMPLE T random walks of length q starting at v 4 for each random walk do 5 SAMPLE a node v as a target node <ref type="bibr" target="#b5">6</ref> C v ← Remaining nodes</p><formula xml:id="formula_9">7 for i = 1, 2, ..., q-1 do 8 u (i) v i ← squash x v i ∀v i ∈ C v 9 e v ← ROUTING u (i) v i q−1 i=1 10 o v ← e v</formula><p>We briefly represent the general learning process of our proposed Caps2NE model in Algorithm 2 whose main steps 3, 7-9 and 10 are previously detailed in parts "Sampling input pairs", "Constructing Caps2NE" and "Learning model parameters", respectively. We illustrate our model in <ref type="figure">Figure 1</ref> where the length q of random walks, the dimension size d of the feature vectors and the dimension size k of output node embeddings are equal to 6, 4 and 3, respectively. Thus, the first capsule layer has 5 capsules, each with 4 neurons, and the second capsule layer has 1 capsule with 3 neurons. For the target node 3 in the illustration, the vector output of the capsule in the second layer is used to infer the embedding of node 3. Our Caps2NE aims to aggregate feature information from the context neighbors (i.e., k-hops neighbors) to infer the target node 3; hence this helps our proposed model to infer the structural dependencies among nodes to produce the plausible node embeddings effectively.</p><p>Algorithm 3: The inference process for new nodes.</p><formula xml:id="formula_10">1 Input: A graph G = (V, E), a trained model Caps2NE t r ained , a set V t est of new nodes. 2 for v ∈ V t est do 3 SAMPLE Z pairs {p j } Z j=1 of (C v , v) 4 for j ∈ {1, 2, ..., Z } do 5 e (v, j) ← Caps2NE t r ained p j 6 o v ← AVERAGE {e (v, j) } Z j=1</formula><p>Inferring embeddings for new nodes in the inductive setting. Algorithm 3 shows how we infer an embedding for a new node v adding to an existing graph. After training our model, we generate random walks of length q to extract Z pairs of (C v , v). We use each of these pairs as an input for our trained model and then collect the output vector e from the second capsule layer. Thus, we obtain Z vectors associated with node v and then average them into an embedding representation of v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL RESULTS ON PPI, POS, AND BLOGCATALOG 3.1 Datasets and data splits</head><p>PPI <ref type="bibr" target="#b1">[2]</ref> is a subgraph of the Protein-Protein Interaction network for Homo Sapiens, and its node labels represent biological states. POS <ref type="bibr" target="#b13">[14]</ref> is a co-occurrence network of words from the Wikipedia dump, and its node labels represent the part-of-speech tags. BLOGCATA-LOG <ref type="bibr" target="#b24">[25]</ref> is a social network of relationships of the bloggers listed on the BlogCatalog website, and its node labels represent bloggers' interests. PPI, POS and BLOGCATALOG are given without node features, in which each node is assigned with one or more class labels. These datasets are used for the multi-label node classification task. <ref type="table" target="#tab_1">Table 1</ref> presents the statistics of these benchmark datasets. A certain fraction γ of nodes is provided to train a classifier which is then used to predict the labels of the remaining nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training protocol</head><p>We only use the transductive setting for these three datasets. We uniformly sample 64 random walks (T = 64) of length 10 (q = 10) for each node in the graph. In each random walk, we rotationally select each node in the walk as a target node and 9 remaining nodes as its context nodes. We also run up to 50 training epochs and use the batch size to 128, the embedding size k = 128 and |V ′ | = 256 in Equation 3. We vary the Adam initial learning rate lr ∈ {1e −5 , 5e −5 , 1e −4 }. Nodes are given without pre-computed features, hence we set the size d of feature vectors x v i to 128 (d = 128), and these vectors are randomly initialized uniformly, and updated during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation protocol</head><p>We follow the same experimental setup used for the multi-label node classification task from Perozzi et al. <ref type="bibr" target="#b17">[18]</ref> and Duran and Niepert <ref type="bibr" target="#b5">[6]</ref> where we uniformly sample a fraction γ of nodes at random as training set for learning a one-vs-rest logistic regression classifier. The learned node embeddings after each Caps2NE training epoch are used as input feature vectors for this logistic regression classifier. We use default parameters for learning this classifier from Perozzi et al. <ref type="bibr" target="#b17">[18]</ref>. The classifier is then used to categorize the remaining nodes. We monitor the Micro-F1 and Macro-F1 scores of the classifier after each Caps2NE training epoch, for which the best model is chosen by using 10-fold cross-validation for each fraction value. We repeat this manner 10 times for each fraction value, and then compute the averaged Micro-F1 and Macro-F1 scores. We show final scores w.r.t. each value γ ∈ {10%, 50%, 90%}. The baseline results are taken from Duran and Niepert <ref type="bibr" target="#b5">[6]</ref>.  </p><formula xml:id="formula_11">(Macro-F1) γ = 10% γ = 50% γ = 90% γ = 10% γ = 50% γ = 90% γ = 10% γ = 50% γ = 90%<label>DeepWalk</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Overall results</head><p>We show in <ref type="table" target="#tab_2">Table 2</ref> the Micro-F1 and Macro-F1 scores on test sets in the transductive setting. Especially, on POS, Caps2NE produces a new state-of-the-art Macro-F1 score for each of the three fraction values γ , the highest Micro-F1 score when γ = 90% and the second highest Micro-F1 scores when γ ∈ {10%, 50%}. Caps2NE obtains new highest F1 scores on PPI and BLOGCATALOG when γ = 10% and γ = 90%, respectively. On PPI, Caps2NE also achieves the highest Macro-F1 score when γ = 50% and the second highest Micro-F1 score when γ = 90%. On BLOGCATALOG, Caps2NE also achieves the second highest Macro-F1 scores when γ ∈ {10%, 50%}. In short, from <ref type="table" target="#tab_2">Table 2</ref>, Caps2NE obtains top performances on these three datasets: producing the highest scores in 9 over 18 comparison groups (3 datasets × 3 values of the fraction γ × 2 metrics), the second highest scores in 5/18 groups and competitive scores in the remaining 4 groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS ON CORA, CITESEER, AND PUBMED 4.1 Datasets and data splits</head><p>CORA, CITESEER <ref type="bibr" target="#b19">[20]</ref> and PUBMED <ref type="bibr" target="#b15">[16]</ref> are citation networks where each node represents a document (here, each node is associated with a class labeling the main topic of the document), and each edge represents a citation link between two documents. Each node is also associated with a feature vector of a bag-of-words, i.e. the feature vectors x v i in the first capsule layer (Equation 1) are precomputed based on bag-of-words features and fixed during training. <ref type="table" target="#tab_4">Table 3</ref> presents the statistics of these three benchmark datasets.</p><p>Duran and Niepert <ref type="bibr" target="#b5">[6]</ref> show that the experimental setup used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref> is not fair to show the effectiveness of existing models when these models are evaluated using the fixed &amp; pre-split training, validation and test sets from the Planetoid model <ref type="bibr" target="#b22">[23]</ref>. Therefore, for a fair comparison, we follow the same experimental setup used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>. In particular, for each dataset, we uniformly sample 20 random nodes for each class as training data, 1000 different random nodes as a validation set and 1000 different random nodes as a test set. We then repeat this manner 10 times to produce 10 data splits of training-validation-test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training protocol</head><p>Transductive setting. We set the embedding size k to 128 (k = 128) and the number of samples in the sampled softmax loss function to 256 (|V ′ | = 256 in Equation 3). We also set the batch size to 64 for both CORA and CITESEER and to 128 for PUBMED. We use a fixed walk length q = 10 for uniformly sampling T random walks starting from each node. We may get slightly better results when we rotationally selecting each node in the random walk as a target node. But we aim to save training time due to the limitation of computation resources, thus we only select target nodes at indexes of {3, 4, 5, 6}. We optimize the loss function using the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> and select the initial learning rate lr ∈ {1e −5 , 5e −5 , 1e −4 }. We vary the number T of random walks T ∈ {8, 16, 32, 64} and the number m of iterations in the routing process (Algorithm 1) m ∈ {1, 3, 5, 7}.</p><p>We run up to 50 epochs and evaluate the model for each epoch to choose the best model on the validation set. We use the same values of hyper-parameters above for all data splits. Inductive setting. We use the same inductive setting as used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> where we firstly remove all nodes in the test set from the original graph before training phase, thus these nodes are unseen/new in the testing/evaluating phase. We then apply the standard training process on the remaining of the graph. Here, we use the same set of hyper-parameters tuned for the transductive setting to train Caps2NE in the inductive setting. After training, we infer the embedding for each node v in the test set as in Algorithm 3 using a fixed value Z = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation protocol</head><p>We also follow the same setup used in Duran and Niepert <ref type="bibr" target="#b5">[6]</ref> use to evaluate our Caps2NE. For each of 10 data splits, the learned node embeddings after each Caps2NE training epoch are used as input features for learning a L2-regularized logistic regression classifier <ref type="bibr" target="#b6">[7]</ref> on the training set.We monitor the node classification accuracy on the validation set for every Caps2NE training epoch and then choose the model that produces the highest accuracy on the validation set to compute the accuracy on the test set. We finally report the average of the accuracies across 10 test sets from the 10 data splits. We compare Caps2NE with strong baseline models BoW (Bag-of-Words), DeepWalk, DeepWalk+BoW, EP-B <ref type="bibr" target="#b5">[6]</ref>, Planetoid, GCN and GAT. As reported in <ref type="bibr" target="#b8">[9]</ref>, GraphSAGE obtained low accuracies on CORA, PUBMED and CITESEER, thus we do not include GraphSAGE as a strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Overall results</head><p>Transductive setting. <ref type="table">Table 4</ref> reports the experimental results of our proposed Caps2NE and other baselines. BoW is evaluated by directly using the bag-of-words feature vectors for learning the classifier. DeepWalk+BoW concatenates the learned embedding of a node from DeepWalk with the BoW feature vector of the node. As discussed in Duran and Niepert <ref type="bibr" target="#b5">[6]</ref>, the experimental setup used to evaluate GCN and GAT is not fair for existing models when they are evaluated using the fixed &amp; pre-split training, validation and test sets from Yang et al. <ref type="bibr" target="#b22">[23]</ref>. Thus we report results, and also fine-tune and re-evaluate GAT, using the same experimental setup used in Duran and Niepert <ref type="bibr" target="#b5">[6]</ref>. The results of other baselines (e.g., BoW, DeepWalk+BoW, EP-B, Planetoid and GCN) are taken from Duran and Niepert <ref type="bibr" target="#b5">[6]</ref>. <ref type="table">Table 4</ref>: Accuracies on the CORA, CITESEER and PUBMED test sets in the transductive setting. "Unsup" denotes unsupervised graph embedding models, where the best score is in bold while the second best score is in underline. "Semi" denotes a group of semi-supervised models using node labels from the training set together with feature vectors of nodes from the entire dataset during training. Caps2NE obtains the highest scores on CORA and CITESEER and the second highest score on PUBMED against other unsupervised baseline models. In addition, we also compare our unsupervised Caps2NE to the semi-supervised models GCN, Planetoid and GAT, for which Caps2NE works better than GCN and Planetoid on these three datasets, and outperforms GAT on CITESEER. <ref type="table">Table 5</ref>: Accuracies on the CORA, CITESEER and PUBMED test sets in the inductive setting. "Unsup" denotes unsupervised graph embedding models, where the best score is in bold while the second best score is in underline. "Sup" denotes a group of supervised models using node labels from the training set during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>CORA CITESEER PUBMED Inductive setting: <ref type="table">Table 5</ref> reports the experimental results of our Caps2NE and other baselines in the inductive setting. Note that the inductive setting is used to evaluate the models when we do not access nodes in the test set during training. This inductive setting was missed in the original GCN and GAT papers which relied on the semi-supervised training process. Regarding Cora and Citeseer in the inductive setting, many neighbors of test nodes also belong to the test set, thus these neighbors are unseen during training and then become new nodes in the testing/evaluating phase. <ref type="table">Table 4</ref> also shows that under the inductive setting, Caps2NE produces new stateof-the-art scores of 76.54% and 69.84% on CORA and CITESEER respectively, and also obtains the second highest score of 78.98% on PUBMED. As previously discussed in the last paragraph in the "The proposed Caps2NE" section, we re-emphasize that our unsupervised Caps2NE model notably outperforms the supervised models GCN and GAT for this inductive setting. In particular, Caps2NE achieves 4+% absolute higher accuracies than both GCN and GAT on the three datasets, clearly showing the effectiveness of Caps2NE to infer embeddings for unseen nodes.</p><p>Discussion. EP-B is the best model on PUBMED: (i) EP-B simultaneously learns word embeddings on texts from all nodes. Then the embeddings of words from each node are averaged into a new feature vector which is then used to reconstruct the node embedding. (ii) On PUBMED, neighbors of unseen nodes in the test set are frequently present in the training set. Therefore, these are reasons why on PUBMED, EP-B obtains higher performance than Caps2NE and other models (but, note that we only make use of the bag-of-words feature vectors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation analysis on the routing update</head><p>The routing process presented in Algorithm 1 can be considered as an attention mechanism to compute the coupling coefficient c i which is used to weight the output of the i-th capsule in the first layer. Sabour et al. <ref type="bibr" target="#b18">[19]</ref>  m Accuracy CORA CITESEER PUBMED <ref type="figure">Figure 2</ref>: Effects of the Adam initial learning rate lr (left <ref type="figure">figure)</ref>, the number T of random walks sampled for each node (central <ref type="figure">figure)</ref>, and the number m of iterations in the routing process (right figure) on the validation sets in the transductive setting.   the high order variant among different nodes. Therefore, we propose to use the new update rule (b i ←û (i) v i · e v ) as this new rule generally helps obtain a higher performance for each setup. <ref type="table" target="#tab_9">Table 6</ref> shows a comparison between the accuracy results of these two update rules on the CORA validation sets w.r.t each data split and the number m (m &gt; 1) of routing iterations.</p><formula xml:id="formula_12">use (b i ← b i +û (i) v i · e v )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effects of hyper-parameters</head><p>Figures 2 and 3 presents effects of the Adam initial learning rate lr , the number T of random walks sampled for each node and the number m of iterations in the routing process on the validation sets in the transductive and inductive settings respectively. In these experiments, for the 10 data splits of each dataset, we apply the same value of one hyper-parameter and then tune other hyper-parameters.</p><p>We find that in general using lr = 1e −4 produces the top scores on the validation sets to both transductive and inductive settings. We also find that we generally obtain high accuracies with a high value of T at either 32 or 64. However, there is an exception in the inductive setting, where using T = 16 produces the highest accuracy on CITESEER. A possible reason might come from the fact that CITESEER is more sparse than CORA and PUBMED: the average number of neighbors per node on CITESEER is 1.4 which is substantially smaller than 2.0 on CORA and 2.2 on PUBMED.</p><p>Furthermore, using m = 1 usually obtains the top performances in both the settings. But we also note that the best configurations of hyper-parameters over 10 data splits are not always relied on using m = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we present a new unsupervised embedding model Caps2NE based on the capsule network to learn node embeddings from the graph-structured data. Our proposed Caps2NE aims to effectively use context neighbors in random walks to infer plausible embeddings for target nodes. Experimental results show that Caps2NE obtains state-of-the-art performances on benchmark datasets for the node classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Effects of the Adam initial learning rate lr (left figure), the number T of random walks sampled for each node (central figure), and the number m of iterations in the routing process (right figure) on the validation sets in the inductive setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the experimental datasets.</figDesc><table><row><cell>Dataset</cell><cell>|V |</cell><cell>|E |</cell><cell>#Classes</cell></row><row><cell>PPI</cell><cell>3,890</cell><cell>76,584</cell><cell>50</cell></row><row><cell>POS</cell><cell>4,777</cell><cell>184,812</cell><cell>40</cell></row><row><cell cols="3">BLOGCATALOG 10,312 333,983</cell><cell>39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Multi-label classification results on PPI, POS and BLOGCATALOG. 10% γ = 50% γ = 90% γ = 10% γ = 50% γ = 90% γ = 10% γ = 50% γ = 90%</figDesc><table><row><cell>Method</cell><cell></cell><cell>POS</cell><cell></cell><cell></cell><cell>PPI</cell><cell></cell><cell cols="3">BLOGCATALOG</cell></row><row><cell cols="2">(Micro-F1) γ = DeepWalk 45.02</cell><cell>49.10</cell><cell>49.33</cell><cell>17.14</cell><cell>23.52</cell><cell>25.02</cell><cell>34.48</cell><cell>38.11</cell><cell>38.34</cell></row><row><cell>LINE</cell><cell>45.22</cell><cell>51.64</cell><cell>52.28</cell><cell>16.55</cell><cell>23.01</cell><cell>25.28</cell><cell>34.83</cell><cell>38.99</cell><cell>38.77</cell></row><row><cell>Node2Vec</cell><cell>44.66</cell><cell>48.73</cell><cell>49.73</cell><cell>17.00</cell><cell>23.31</cell><cell>24.75</cell><cell>35.54</cell><cell>39.31</cell><cell>40.03</cell></row><row><cell>EP-B</cell><cell>46.97</cell><cell>49.52</cell><cell>50.05</cell><cell>17.82</cell><cell>23.30</cell><cell>24.74</cell><cell>35.05</cell><cell>39.44</cell><cell>40.41</cell></row><row><cell>Our Caps2NE</cell><cell>46.01</cell><cell>50.93</cell><cell>53.92</cell><cell>18.52</cell><cell>23.15</cell><cell>25.08</cell><cell>34.31</cell><cell>38.35</cell><cell>40.79</cell></row><row><cell>Method</cell><cell></cell><cell>POS</cell><cell></cell><cell></cell><cell>PPI</cell><cell></cell><cell cols="3">BLOGCATALOG</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the experimental datasets. d is the dimension size of the feature vectors.</figDesc><table><row><cell>Dataset</cell><cell>|V |</cell><cell>|E |</cell><cell cols="2">#Classes d</cell></row><row><cell>CORA</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell></row><row><cell cols="2">CITESEER 3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell></row><row><cell>PUBMED</cell><cell cols="2">19,717 44,338</cell><cell>3</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>for the image classification task, but this might not be well-suited for graph-structured data because of</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell><cell></cell><cell></cell><cell>CORA</cell><cell cols="2">CITESEER</cell><cell>PUBMED</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell>80.46</cell><cell>80.33</cell><cell></cell><cell>80</cell><cell cols="2">79.73</cell><cell>80.51</cell><cell>79.75</cell><cell></cell><cell>80</cell><cell>80.48</cell><cell>79.98</cell><cell>80.21</cell><cell>79.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78.66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>75</cell><cell>77.9</cell><cell>73.79</cell><cell>76.38</cell><cell>76.53</cell><cell>Accuracy</cell><cell>75</cell><cell>74.74</cell><cell>76.24</cell><cell cols="2">76.16</cell><cell>76.37</cell><cell>75</cell><cell>76.34</cell><cell>76.09</cell><cell>76.23</cell><cell>76.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell>71.9</cell><cell>72.28</cell><cell>72.22</cell><cell></cell><cell cols="2">71.45</cell><cell>71.87</cell><cell>71.88</cell><cell cols="2">72.03</cell><cell></cell><cell>71.99</cell><cell>71.94</cell><cell>72</cell><cell>72.04</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">1e −5</cell><cell>5e −5</cell><cell>1e −4</cell><cell></cell><cell>8</cell><cell></cell><cell>16</cell><cell>32</cell><cell>64</cell><cell></cell><cell></cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>lr</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>T</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Accuracy results on the CORA validation sets w.r.t each data split and each value m &gt; 1 of routing iterations for the transductive and inductive settings. Regarding Algorithm 1 when m &gt; 1, "Ours" denotes our update rule (b i ←û Sab." denotes the update rule (b i ← b i +û · e v ) originally used by Sabour et al.<ref type="bibr" target="#b18">[19]</ref>.80.2 80.5 80.0 79.9 79.4 70.4 70.1 70.4 69.9 70.4 68.8 8th 81.8 82.1 82.1 81.5 82.3 81.2 69.6 69.0 68.7 67.8 69.7 67.5 9th 79.3 79.4 79.7 78.1 78.6 77.8 71.2 70.8 71.5 71.7 72.2 70.1 10th 78.8 79.3 79.7 78.9 79.4 78.7 70.3 69.7 69.5 68.8 69.9 68.3 Overall 79.98 80.06 80.21 79.59 79.96 79.16 68.85 68.55 68.74 68.03 69.10 67.42</figDesc><table><row><cell>(i) v i · e v ),</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This research was partially supported by the ARC Discovery Projects DP150100031 and DP160103934.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The BioGRID interaction database: 2008 update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobby-Joe</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Reguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorrie</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Livstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Oughtred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lackner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="637" to="677" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>JÃijrg BÃd&apos;hler</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: problems, techniques and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1616" to="1637" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02590</idno>
		<title level="m">A Tutorial on Network Embeddings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Henrickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhai</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07007</idno>
		<title level="m">High-Order Graph Convolutional Recurrent Neural Network: A Deep Learning Framework for Network-Scale Traffic Learning and Forecasting</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Graph Representations with Embedding Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5119" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Node2Vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03984</idno>
		<title level="m">SPINE: Structural Identity Preserved Inductive Network Embedding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On Using Very Large Target Vocabulary for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="http://www.mattmahoney.net/text/text.html" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Querydriven Active Surveying for Collective Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Galileo Mark Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Self-Attention Network based Node Embedding Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepWalk: Online Learning of Social Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dik Lun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting Semi-supervised Learning with Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://socialcomputing.asu.edu" />
		<title level="m">Social Computing Data Repository at ASU</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Network representation learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daokun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
