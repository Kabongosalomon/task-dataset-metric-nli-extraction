<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renkun</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Ronny</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
						</author>
						<title level="a" type="main">GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Changes in neural architectures have fostered significant breakthroughs in language modeling and computer vision. Unfortunately, novel architectures often require re-thinking the choice of hyperparameters (e.g., learning rate, warmup schedule, and momentum coefficients) to maintain stability of the optimizer. This optimizer instability is often the result of poor parameter initialization, and can be avoided by architecture-specific initialization schemes. In this paper, we present GradInit, an automated and architecture agnostic method for initializing neural networks. Gra-dInit is based on a simple heuristic; the variance of each network layer is adjusted so that a single step of SGD or Adam results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. Gra-dInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also enables training the original Post-LN Transformer for machine translation without learning rate warmup under a wide range of learning rates and momentum coefficients. Code is available at https: //github.com/zhuchen03/gradinit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A major problem when applying novel deep learning models in new domains is optimizer instability resulting from poor parameter initialization. Carefully crafted initializations that prevent gradient explosion/vanishing in back propagation were an important part of early successes with feed-forward networks <ref type="bibr" target="#b12">(Glorot &amp; Bengio, 2010;</ref><ref type="bibr" target="#b14">He et al., 2015)</ref>. Even when cleverly designed initialization rules are used, complex models with many layers or multiple branches often still suffer from instability, requiring carefully tuned hyperparameters for the training process to converge. For example, the Post-LN Transformer <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> can not converge without learning rate warmup using the default initialization <ref type="bibr" target="#b31">(Xiong et al., 2020;</ref><ref type="bibr" target="#b17">Huang et al., 2020;</ref><ref type="bibr" target="#b22">Liu et al., 2020b)</ref>; RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> have to tune the β 2 paramter of Adam for stability when the batch size is large. While normalization layers <ref type="bibr" target="#b18">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b0">Ba et al., 2016)</ref> are proposed to address the instability issue in part, recent innovations have shown that architecture-specific initializations, which are carefully derived to maintain stability, can mitigate these problems on their own, without normalization layers <ref type="bibr" target="#b34">(Zhang et al., 2019;</ref><ref type="bibr" target="#b9">De &amp; Smith, 2020;</ref><ref type="bibr" target="#b17">Huang et al., 2020;</ref><ref type="bibr" target="#b4">Brock et al., 2021a;</ref>. Unfortunately, the reliance on analytically derived initializations makes it difficult to realize the benefits of these methods when performing architecture search, when training networks with branched or heterogeneous components, or when proposing altogether new architectures.</p><p>In this work, we propose a simple method for learning the initialization of a network. Typically, initialization schemes draw each initial parameter independently from zero-mean Gaussian distributions, with the variance of the distributions set to pre-determined values depending on the dimensions of the layers. Rather than using a rule-based choice for the variance, our method rescales such initialized parameter blocks (e.g. convolution kernels) by a learned scalar coefficient. This small set of coefficients is optimized to make the first step of a stochastic optimizer (e.g. Adam) as effective as possible at minimizing the training loss, while preventing the initial gradient norm from exploding. This process takes into account the direction and step size of the optimizer. It also accounts for stochasticity by crafting an initialization loss using two different minibatches of data. Finally, after the initialization coefficients have been learned, the random network parameters are re-scaled and optimization proceeds as normal.</p><p>Our proposed method, GradInit, is architecture agnostic, and works with both Adam and SGD optimizers. In the arXiv:2102.08098v1 <ref type="bibr">[cs.</ref>LG] 16 Feb 2021 vision domain, we show it accelerates the convergence and test performance of a variety of deep architectures, from the purely feed-forward VGG net to ResNet-110 and DenseNet-100, with or without Batch Normalization. It is efficient and scalable, finding good initializations within less than 1% of the total training iterations and time in our experiments, and it improves the initialization of ResNet-50 on ImageNet to obtain better test accuracy. In the language domain, GradInit enables training the original unmodified Post-LN Transformer <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> using either Adam or SGD without learning rate warmup for machine translation, which was believed to be difficult <ref type="bibr" target="#b31">(Xiong et al., 2020;</ref><ref type="bibr" target="#b35">Zhang et al., 2020)</ref>. Finally, by visualizing the initial norms and gradient variances of the weights before and after GradInit is applied, we show with experiments that GradInit is a useful tool for identifying potential causes for instability at initialization, and we summarize useful design principles for initializing complex architectures that are learned by GradInit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Controlling the norms of network parameters at initialization has proven to be an effective approach to speeding up and stabilizing training. <ref type="bibr" target="#b12">Glorot &amp; Bengio (2010)</ref> studied how the variance of features evolves with depth in feed-forward, linear neural networks by assuming both activations and weight matrices are independent and identical random variables. They developed a technique in which the variance of each filter scales with its fan-in (the number of input neurons).This style of analysis was later generalized to the case of ReLU networks <ref type="bibr" target="#b14">(He et al., 2015)</ref>. These two analyses are most effective for feed-forward networks without skip connections or normalization layers. Based on the orthogonal initialization scheme <ref type="bibr" target="#b28">(Saxe et al., 2013)</ref>, <ref type="bibr" target="#b25">Mishkin &amp; Matas (2016)</ref> proposed an iterative procedure to rescale the orthogonally initialized weights of each layer so that the activations of that layer have unit variance. However, this method fails to prevent the blowup of activations with depth for ResNets <ref type="bibr" target="#b8">(Dauphin &amp; Schoenholz, 2019)</ref>.</p><p>For more complex architectures, normalization layers <ref type="bibr" target="#b18">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b0">Ba et al., 2016)</ref> and skip connections <ref type="bibr" target="#b15">(He et al., 2016)</ref> stabilized training dynamics and improved the state-of-the-art. Similarly, learning rate warmup is a common trick for training large Transformers <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref>. These methods make training tractable for some models, but do not eliminate the high initial gradient variance that destabilizes training when the network is deep <ref type="bibr" target="#b34">(Zhang et al., 2019;</ref><ref type="bibr" target="#b9">De &amp; Smith, 2020;</ref><ref type="bibr" target="#b4">Brock et al., 2021a)</ref> or when the normalization layer is not carefully positioned <ref type="bibr" target="#b31">(Xiong et al., 2020)</ref>.</p><p>Several authors have proposed better initializations for networks with skip connections. This is often achieved by replacing the normalization layers with some simple scaling or bias operations, and scaling the weight matrices in each layer so that the variance of activations does not increase with depth <ref type="bibr" target="#b34">(Zhang et al., 2019;</ref><ref type="bibr" target="#b9">De &amp; Smith, 2020;</ref><ref type="bibr" target="#b4">Brock et al., 2021a;</ref>. Similar analysis has been applied to self attention in Transformers <ref type="bibr" target="#b17">(Huang et al., 2020)</ref>. Without removing the normalization layers, another approach is to introduce carefully initialized learnable scale factors to the skip connections <ref type="bibr" target="#b22">(Liu et al., 2020b)</ref> or the residual branches <ref type="bibr" target="#b1">(Bachlechner et al., 2020)</ref> to stabilize the initial parameter updates. However, such analyses are often restricted to one specific architecture such as ResNets.</p><p>Recently, <ref type="bibr" target="#b8">(Dauphin &amp; Schoenholz, 2019)</ref> proposed a taskagnostic and automatic initialization method, MetaInit, for any task and neural network achitecture, by optimizing the wegiht norms to minimize the "gradient quotient", which measures the effect of curvature near the initial parameters, on minibatches of random Gaussian samples. However, training data is usually accessible to us for most tasks of interest; we simply utilize the training data for our algorithm. MetaInit also involves the gradient of a Hessian-vector product, which requires computing a "gradient of the gradient" multiple times in tandem, and is very computationally intensive. Our proposed method distinguishes itself from MetaInit in the following ways: (i) Our method is more efficient. MetaInit involves computing third-order derivatives, results in long computing times and high memory usage. The memory overhead of MetaInit is more of an issue for networks with normalization layers. For the relatively smallscale CIFAR-10 problem with batch size 64, MetaInit requires 3 GPUs (RTX 2080Ti), while the proposed GradInit needs just one. (ii) Our method takes the stochasticity of minibatches into account (Section 3.3). MetaInit utilizes the local curvature evaluated on a single minibatch, which fails to capture the variance of the loss/gradient between two different stochastic minibatches. (iii) Our method considers the training dynamics of different optimization algorithms including the learning rate and the direction of the gradient step, and effectively handles different optimizers including SGD and Adam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Overview We aim to develop an initialization scheme applicable to any network architecture. The proposed method initializes each tensor of network parameters with a (fixed) array of random numbers drawn from a Gaussian distribution, multiplied by a (trainable) scalar that controls the norm of the initial tensor. By learning a unique scale factor for each parameter tensor in each layer, we hope to stabilize the training process. We aggregate these scale factors in an initialization vector m, and then optimize this vector so that the first step taken by an optimizer A (either SGD or Adam) descends as quickly as possible while keeping the size of gradient within prescribed bounds.</p><p>Setup We are interested in finding better initializations for the parameters of each parameterized linear operation (layer) of the neural network. <ref type="bibr">1</ref> We begin by filling the weight matrices {W i } and bias vectors {b i } of the network with values drawn from independent zero-mean Gaussian distributions, where i = 1 . . . M denotes the layer index, and the variance of the Gaussian distributions are decided by the fan-in and fan-out of the layer to approximately preserve the variance of the activations and gradients <ref type="bibr" target="#b12">(Glorot &amp; Bengio, 2010;</ref><ref type="bibr" target="#b14">He et al., 2015)</ref>. These weights and biases are not modified by the proposed algorithm. For each i, we pair W i and b i with learnable non-negative scalars α i and β i that control the scaling of the layer at initialization. We use m to denote the vector of scale factors [α 1 , β 1 , · · · , α M , β M ], and let θ m be the tuple</p><formula xml:id="formula_0">[α 1 W 1 , β 1 b 1 , · · · , α M W M , β M b M ] of rescaled parameters. Let L(S; θ) = 1 |S| x∈S (x; θ)</formula><p>be the average loss of the model parameterized by θ on a minibatch of samples S, where |S| is the number of samples in the minibatch. We use g(S; θ) = ∇ θ L(S; θ) as a shorthand for the gradient of θ. During standard training, this gradient is preprocessed/preconditioned by the optimization algorithm A, and then used to update the network parameters. For the first gradient step, SGD uses A[g(S; θ m )] = g(S; θ m ), while Adam uses A[g(S; θ m )] = sign(g(S; θ m )) <ref type="bibr" target="#b2">(Balles &amp; Hennig, 2018)</ref>.</p><p>Algorithm We aim to optimize m to learn a good initialization θ m tailored for a target stochastic gradient method A with learning rate η. In doing do, we consider the first update step θ m − ηA[g(S; θ m )] of the algorithm, which computes a gradient, applies the optimizer-specific pre-processing, and then takes a step in the negative gradient direction with learning rate η. We tune the initialization so that the loss after this first gradient step is as low as possible.</p><p>Formally, we optimize the scale factors m through the following constrained optimization objective</p><formula xml:id="formula_1">minimize m L(S; θ m − ηA[g(S; θ m )]), subject to g(S; θ m ) p A ≤ γ,<label>(1)</label></formula><p>where S andS are two different random minibatches, γ &gt; 0 is a constant, and · p A is the p -norm with p = p A chosen according to the optimization algorithm A. For SGD, we choose p SGD = 2 and measure the constraint in the 2 (Euclidean) norm. When Adam is used, we choose p Adam = 1 and measure the constraint in the 1 norm. 1 "Parameterized linear operation" includes, e.g., convolutions, the scale and bias of normalization layers, referred to as "layer" afterwards.</p><p>The problem (1) is solved using a stochastic gradient descent method in which we sample new mini-batches on each iteration. Since the proposed method uses gradient updates to compute the initialization, we dub it GradInit.</p><p>We discuss the choice of this formulation, and a simple algorithm for approximately solving it below. Pseudocode for the proposed method is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Choosing the constraint</head><p>The constraint in (1) is included to prevent the network from minimizing the loss in a trivial way by blowing up the gradients. With this constraint in place, the loss is minimized by choosing the initialization so that the gradient direction is informative and effective, rather than because the gradient magnitude is large.</p><p>From a first-order approximation to the loss function, we expect a gradient step in the direction A[g] to result in a loss decrease of ηA[g] T g. For SGD, the decrease in loss is roughly ηA[g] T g = η g 2 2 . We see that it is possible to exploit the objective by blowing up the 2 -norm of the gradient, and so we prevent this by setting p SGD = 2. For Adam, ηA[g] T g = η sign(g)g = η g 1 . The objective can be exploited by blowing up the 1 -norm, and so we set p Adam = 1 in the constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Handling bias parameters</head><p>In many ML software frameworks and architectures, the biases b i are initialized to 0, or are not present at all in some layers, so β i has no effect on the initialization in this case. However, our approach can be applied to the case where b i = 0 and so we consider bias parameters to maintain generality. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Stochasticity of mini-batching</head><p>The objective in (1) uses two different mini-batches; S is used to compute the gradient, andS is used to compute the loss. The use of two minibatches is important. It is often easy for large overparameterized neural networks to overfit on a single minibatch. In our experiments, we found that if we re-use the minibatch S for evaluating the loss, the network tends to have large gradient norms that can quickly reduce the loss on S by about 50% in only one iteration, but the loss can explode for any other minibatchS. However, ifS is completely different from S, we sometimes find that the optimizer is very slow to minimize the objective in Eq. 1 or it stalls for long periods of time. We find that we get the most reliable behavior using a compromise strategy wherẽ S uses a mixture of 50% samples from S and 50% new randomly sampled training data. Please refer to Appendix B for detailed comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Using stop-gradients to accelerate training</head><p>Obtaining an exact gradient of the objective (1) requires computing a second-order gradient, which is potentially expensive. To accelerate computation, our implementation treats A[g(S; θ m )] in the objective like a constant, i.e., we do not compute gradients through this term. Interestingly, we have found that computing approximate gradients in this way has almost no effect on the performance of the method compared to using exact second-order gradients. In addition to speeding up computation and saving memory, this trick also makes it possible to handle the Adam optimizer since in this case A[g] = sign[g], which is not differentiable.</p><p>Algorithm 1 GradInit, our proposed algorithm for learning the initialization of neural networks. Typical values are α = 10 −2 , β = 0. We do not backpropagate through ηA[g t ] when computing the gradient ∇ mtLt+1 , and so we color this symbol gray. 1: Input: Target optimization algorithm A, target learning rate η, learning rate τ of the scales m, initial parameters θ0, total iterations T , upper bound of the gradient γ, lower bound for the weight and bias scales α, β. 2: m1 ← 1 3: for t = 1 to T do 4:</p><p>Draw random samples St from training set 5:</p><p>Lt ← 1</p><formula xml:id="formula_2">|S t | x k ∈S t (x k ; θm t ) 6: gt ← ∇ θ Lt 7:</formula><p>if gt p A &gt; γ then 8: mt+1 = mt − τ ∇m t gt p A 9: else 10:</p><p>Draw |St|/2 samples to replace |St|/2 samples in St.</p><p>Let this new minibatch beSt.</p><formula xml:id="formula_3">11:Lt+1 ← 1 |S t | x k ∈S t (x k ; θm t − ηA[gt]) 12: mt+1 = mt − τ ∇m tL t+1 13:</formula><p>Clamp mt+1 using α, β</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Solving the constrained problem</head><p>We use a simple trick to enforce the constraint in (1). After computing g(S; θ m ), we test whether the constraint is satisfied. If not, we take a gradient descent step to minimize g(S; θ m ) p A , which involves computing second order derivatives. If the constraint is satisfied, then we instead compute a gradient descent step for the loss using stop-gradients.</p><p>Similar to the case where A is Adam and the first gradient step has a fixed length, for SGD, we find using A[g(S; θ m )] = γg(S; θ m )/ g(S; θ m ) 2 instead of g(S; θ m ) gives better results, so we also adaopt this normalization in all our experiments.</p><p>In addition, we set a lower bound α = 0.01 for all α i , which allows any weight matrix to be small enough, but also prevents the gradient from vanishing so that GradInit can find good scale factors for all layers quickly. In our experiments, we find the only layer that will possibly hit this lower bound is usually the final FC layer (see the figures in Section 4.1). For β i , we simply enforce it to be non-negative, as zero biases do not cause vanishing gradients.</p><p>We find this procedure converges quite reliably in a few thousands (e.g., 2000 iterations for ImageNet) if not a few hundreds (e.g., 390 iterations for CIFAR-10) of iterations, taking less than 1% of the total training time, and that the gradient norm constraint is satisfied for the majority of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate GradInit on benchmark datasets for image classification and machine translation tasks. For image classification, eight different architectures are evaluated for CI-FAR10 <ref type="bibr" target="#b20">(Krizhevsky et al., 2009)</ref>, and ResNet-50 is evaluated for ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009)</ref>. For machine translation, we use GradInit to find good initializations for a Post-LN Transformer without any change to its original architecture on IWSLT-14 De-En <ref type="bibr" target="#b7">(Cettolo et al., 2014)</ref>. We observe that the method can remove the necessity of any form of learning rate warmup without any change to its architecture. We are the first to report such results to the best of our knowledge.</p><p>We conduct our experiments in PyTorch, and use the fairseq library for machine translation . All the experiments on CIFAR-10 and IWSLT-14 DE-EN can run with one single NVIDIA RTX 2080 Ti GPU of 11GB RAM. For ImageNet, we use four of the same GPUs with a total batch size of 256. Due to the difference in the library for training and the number of GPUs used, which affects the BN statistics, our baseline top-1 accuracy on ImageNet is 0.79% lower than <ref type="bibr" target="#b13">(Goyal et al., 2017)</ref>.</p><p>GradInit starts from Kaiming initialization <ref type="bibr" target="#b14">(He et al., 2015)</ref> for all the Conv and FC layers for image classification, and the default Xavier initialization <ref type="bibr" target="#b12">(Glorot &amp; Bengio, 2010)</ref> for machine translation. If normalization layers are used in the network architecture, their scale and bias coefficients are initialized to one and zero, respectively. We optimize the scale</p><formula xml:id="formula_4">factors {α i , β i } M i=1</formula><p>with Adam <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref> using the default momentum parameters (β 1 = 0.9, β 2 = 0.999) and clamp the gradient of each α i , β i to be no larger than one in all experiments. MetaInit was originally designed to be task-agnostic, and learns to initialize the network with random samples as inputs. Here, for fair comparisons, we also feed training data to MetaInit, as this should intuitively improve MetaInit for the specific task, and use Adam with the same gradient clipping to optimize the weight norms for MetaInit. Originally, MetaInit <ref type="bibr" target="#b8">(Dauphin &amp; Schoenholz, 2019)</ref> uses signSGD with momentum <ref type="bibr" target="#b3">(Bernstein et al., 2018)</ref>, but we found using Adam with the hyperparameters above can give better results for MetaInit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Datasets with Various Architectures</head><p>The introduction of Batch Normalization (BN) <ref type="bibr" target="#b18">(Ioffe &amp; Szegedy, 2015)</ref> and skip connections makes it relatively easy to train common CNNs for image classification to achieve high accuracy. Despite this, we show that GradInit can further accelerate training convergence and improve test performance for most architectures we evaluated. The results are given in <ref type="table" target="#tab_0">Table 1</ref>. We visualize the gradient variance at initialization to reveal the amplification effect of BN on gradient variance during back propagation, and show how GradInit learns to rescale the weights of Conv, FC and BN layers to reduce the gradient variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">SETTINGS</head><p>Architectures. On CIFAR-10, we evaluated eight different architectures, aiming to cover all existing constituting blocks for CNNs, and isolating the effect of BN. The base architectures include a popular variant of VGG-19 <ref type="bibr" target="#b29">(Simonyan &amp; Zisserman, 2014)</ref> with BN for CIFAR-10, which includes all the sixteen convolutional layers but only one fully connected layer; a ResNet-110 <ref type="bibr" target="#b15">(He et al., 2016)</ref> with base width 16 and two Conv layers in each residual block; a 28-layer Wide ResNet <ref type="bibr" target="#b32">(Zagoruyko &amp; Komodakis, 2016)</ref> with Widen Factor 10 (WRN-28-10) ; and a DenseNet-100 <ref type="bibr" target="#b16">(Huang et al., 2017)</ref>. To isolate the effect of BN, we also consider removing the BN layers from these three networks and adding learnable bias parameters in their place. On ImageNet, we use the ResNet-50 model with BN.</p><p>To compare with a strong initialization scheme that is tailormade for an architecture family, we consider a 110-layer FixUpResNet <ref type="bibr" target="#b34">(Zhang et al., 2019)</ref>. FixUpResNet removes the BN from ResNet, replacing it with bias parameters and a learnable scale parameter after the second convolutional layer of each block. FixUp initializes the weights of the second convolutional layer in each residual block, and of the final fully connected layer, to zero. It also scales the first convolutional layer in each residual block by 1/ √ M . This causes the gradient to be zero in the first step for all layers except for the final FC layer. When testing GradInit on this architecture, we adopt the non-zero Kaiming initialization to all convolutional and FC layers.</p><p>Hyperparameters. We set A to SGD and η = 0.1 (the same as the base learning rate) for GradInit in all image classification experiments. On CIFAR-10, we train all networks with a batch size of 128, except for DenseNet-100,  <ref type="figure">Figure 1</ref>. Averaged per-dimension weight magnitudes ( Wi /di) and standard deviation of their gradient (σ(gi)) for each layer i of the VGG-19 (w/ BN) on CIFAR-10. The ratio between the weight magnitudes of GradInit and Kaiming Initialization is the learned scale factor of GradInit in each layer. The standard deviation is computed over the minibatches, with a batch size of 128, with the BN in its training mode. This VGG-19 on CIFAR-10 has only one FC layer, but it has the same number of convolutional layers (16) as its ImageNet version. All the weights are indexed from shallow to deep, so the first 16 entries of the Linear Weights are of Conv layers, while the 17th is the FC layer. Due to the magnification effect of BN, σ(g1)/σ(g16) for the Conv layers is higher than it is in VGG-19 without BN, shown in <ref type="figure">Figure 3</ref>. GradInit learns to reduce the magnification effect of BN layers by scaling up all the Conv layers and most of the BN layers, given it has greatly down scaled the last two BN layers and the final FC layer to reduce the variance in the forward pass.  where the recommended batch size is 64. <ref type="bibr">3</ref> We also restrict both the MetaInit and GradInit procedures to fit into the memory limit of a single GPU. GradInit fits with the same batch size as model training, while for MetaInit, we have to reduce its batch size on ResNet-110 and DenseNet-100 by a factor of 2 and 3, respectively. We run GradInit or MetaInit for one epoch on the data, which takes less than 1% of the total training time. For GradInit, we fix the gradient norm constraint γ = 1 in all these experiments. We also fix η = 0.1, the same as the initial learning rate we use for training those networks. Therefore, same as MetaInit, the only hyperparameter that needs to be tuned is the learning rate τ of the scale factors. We do a grid search on τ in the range [10 −3 , 10 −1 ], and report the results with the best average final test accuracy of 4 runs. After the initialization procedures, we use a learning rate of 0.1 and the cosine annealing learning rate schedule without restart <ref type="bibr" target="#b24">(Loshchilov &amp; Hutter, 2016</ref>) to train the model for 200 epochs, where the learning rate decays after each iteration and decays to 0 in 3 https://github.com/gpleiss/efficient_ densenet_pytorch the last iteration. We use random cropping, random flipping and cutout (DeVries &amp; Taylor, 2017) for data augmentation. For WRN-28-10, we additionally use MixUp <ref type="bibr" target="#b33">(Zhang et al., 2018)</ref> for both the baseline and GradInit, to match up with the numbers in MetaInit <ref type="bibr" target="#b8">(Dauphin &amp; Schoenholz, 2019)</ref>. We do not use dropout in any of the experiments here. We set weight decay to 10 −4 in all cases. Due to their high initial gradient variance (see <ref type="figure">Figure 3)</ref>, we have applied gradient clipping (maximum norm is 1) to all non-BN networks with Kaiming Initialization so that they converge under the aforementioned learning rate schedule. We find that this clipping also improves the results of non-BN nets initialized with MetaInit, GradInit and FixUp, except for DenseNet-100 (w/o BN), so we use the same clipping in these settings. We do not apply gradient clipping to networks with BN since it decreases their test accuracy.</p><p>On ImageNet, we train the ResNet-50 model for 90 epochs with a total batch size of 256 on 4 GPUs. We use SGD with a starting learning rate of 0.1 and decay the learning rate by 10 after the 30th and 60th epoch. To fit into the memory, we use a batch size of 128 for GradInit. We simply  <ref type="figure">Figure 3</ref>. Averaged per-dimension weight magnitude ( Wi /di) and standard deviation of their gradient ((σ(gi))) of the VGG-19 (left two) and ResNet-110 (right two) without BN on CIFAR-10, evaluated with a batch size of 128. For VGG-19 (w/o BN), σ(gi) increases at Conv layers with different input and output dimensions during backpropagation. For ResNet-110 without GradInit, the gradient variance is very high due to the cumulative effect of skip connections during the forward pass. In this scenario, to reduce the gradient variance, there is no reason to increase the weights, so GradInit downscales the weights for all layers in both architectures, unlike the case with BN.</p><p>run GradInit for 2000 iterations, which is less than one epoch. Considering ImageNet and CIFAR-10 has 1000 and 10 classes respectively, the cross entropy loss of a random guess on ImageNet is 3 times as large as the loss on CIFAR-10, so a proper initial gradient norm for ImageNet may be 3 times as large as that for CIFAR-10. Therefore, we set γ = 3 for ImageNet. Since τ = 10 −2 worked the best for ResNet-110 (w/ BN) on CIFAR-10, we tried τ ∈ {1 × 10 −3 , 3 × 10 −3 , 5 × 10 −3 , 10 −2 } on ImageNet, and chose τ = 3 × 10 −3 , which maximizes the test accuracy of first epoch. We provide additional details in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">RESULTS AND ANALYSIS</head><p>Kaiming Initialization seems good enough for plain feedforward nets. <ref type="table" target="#tab_0">Table 1</ref> shows that Kaiming initialization, MetaInit, and GradInit all work comparably well for VGG-19 (w/o BN). The results of ResNet-110 (w/o BN) and DenseNet-100 (w/o BN) also validates that Kaiming initialization is stable with the same learning rate for various depths and architectures, as long as gradient clipping is used.</p><p>GradInit further stabilizes feedforward nets with BN. BN does stabilize VGG-19 and allows training without gradient clipping, but with an average first-epoch test accuracy of only 12.57 and an average final test accuracy lower than the version without BN (see <ref type="table" target="#tab_0">Table 1</ref>), it does not seem to eliminate the instability of Kaiming initialization. As shown in <ref type="figure">Figure 1</ref>, its initial gradient variance is still relatively high compared with GradInit. 4 BN could magnify the gradient variance when the variance of its input features (in the forward pass) is smaller than 1 (see Appendix C). GradInit reduces the gradient variance by 4 orders of magnitude compared to Kaiming initialization, resulting in signifi-4 In fact, probably due to the high variance, (Simonyan &amp; Zisserman, 2014) used a learning rate η = 0.01 to train VGG nets. We find η = 0.01 gives higher first-epoch accuracy with Kaiming initialization, but the final test accuracy is not as high as η = 0.1, whether BN is used or not. So we set η = 0.1 in all experiments. cantly higher test accuracy after the first epoch (47.79% vs. 12.57%), which also has an impact on the final test accuracy (95.13% vs. 94.41%). The reduction in gradient variance is achieved mainly by scaling down the weights of the final FC layer and the last 2 BN layers, so that the variance of the activations is reduced in the forward pass. This learned behavior is consistent with the strategy of FixUp, where the final FC layer is initialized to 0. Another source of gradient variance reduction is achieved by increasing the weight norms of the remaining Conv and BN layers, so that the variance of the inputs to the BN layers is increased and the gradient magnifying effect of BN is alleviated in the backward pass. This reduced the ratio σ(g 1 )/σ(g 16 ) from 204.9 to 164.8 for the Conv layers in <ref type="figure">Figure 1</ref>. By contrast, FixUp only reduces the weight norms, which may not always be the best solution for networks with normalization layers.</p><p>The mechanism of GradInit on ResNets. We also gain significant improvements from GradInit for ResNet-110. In ResNets, the skip connections cause the variance of activations to accumulate as the ResNet goes deeper, even for the version with BN <ref type="bibr" target="#b9">(De &amp; Smith, 2020)</ref>, so there is room for improvement through better initialization. The learned layer-wise rescaling patterns of GradInit are even more interesting for ResNet-110. For the case with BN, recall that we have two Conv layers and two BN layers in each residual block. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, GradInit learns to increase the weight norms of all the linear layers except for the final FC layer, instead of decreasing as for the case without BN (see <ref type="figure">Figure 3)</ref>. A more unique pattern is the collaborative behavior of the BN weights, where the second BN in each residual block is usually scaled down while the first BN is always scaled up. In deeper layers, the joint effect of these two BN weights is to downscale the activations and reduce their variance in the forward pass, with a more significant reducing effect as the layers get deeper. Intuitively, the marginal utility of adding a new layer decreases with the depth. Therefore, for deeper layers, GradInit learns to further downscale the residual branch, and prevents the variance from increasing too much in the forward pass. Inside each residual block, increasing the scale factors of the first BN helps to reduce the magnification effect of the second BN on the gradient: forcing the input activations to the second convolution to have variance larger than 1 ensures its variance after the convolution does not go below 1, avoiding the magnification effect that the second BN has on the gradient variance.</p><p>GradInit eliminates the need for gradient clipping for DenseNet-100 (w/o BN). GradiInit stabilizes DenseNet-100 (w/o BN) into a desirable state where no gradient clipping is required for convergence of the network and the test performance is significantly improved. The performance of the baseline may be improved by an adaptive clipping approach <ref type="bibr" target="#b5">Brock et al., 2021b)</ref> instead of a deterministic one, but GradInit eliminates such a requirement. By comparison, MetaInit <ref type="bibr" target="#b8">(Dauphin &amp; Schoenholz, 2019</ref>) also uses gradient clipping for all architectures without BN. However, the efficacy of GradInit is relatively marginal on DeseNet-100 (w/ BN). By design, DenseNet (w/ BN) has smaller gradient variance, as we show in <ref type="figure">Figure 6</ref> in the Appendix. It concatenates, rather than adds, the activations of all previous layers in each Dense Block. Despite the increased memory cost, it avoids the increasing variance issue of ResNet, by normalizing each concatenated feature vector with BN.</p><p>GradInit scales to ImageNet. It accelerates convergence and improves test accuracy of ResNet-50 despite using a smaller batch size for computing the initialization than is used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Post-LN Transformers without Warmup</head><p>For a Transformer model to converge, either an explicit or implicit learning rate warmup stage is needed, especially for the Post-LN architecture. It is observed that the Post-LN architecture tends to give better perforamnce than the Pre-LN one in practice <ref type="bibr" target="#b22">(Liu et al., 2020b)</ref>, but Post-LN Transformers seems to bear higher gradient variance than its Pre-LN counterpart at initializaiton <ref type="bibr" target="#b31">(Xiong et al., 2020)</ref>, and its warmup stage seems inevitable. Here, we show that with a proper initialization, we can do away with the warmup stage for Post-LN Transformer without any modification to the architecture. <ref type="table">Table 2</ref> summarizes the settings and best results of methods for improving the initialization of Post-LN Transformers. We evaluate the stability of GradInit and Admin without warmup under different learning rates and values for Adam's second moment parameter (β 2 ) in <ref type="figure" target="#fig_1">Figure 4</ref>.  <ref type="bibr" target="#b34">(Zhang et al., 2019)</ref>, T-FixUp <ref type="bibr" target="#b17">(Huang et al., 2020)</ref>, Admin <ref type="bibr" target="#b22">(Liu et al., 2020b)</ref>, and our reimplementation of Admin for training the Transformer model on the IWSLT-14 De-EN dataset. "Vanilla" refers to training Post-LN Transformer with standard initialization, and the result is from <ref type="bibr" target="#b22">(Liu et al., 2020b)</ref>. All 3 methods on top start from the same Transformer model as GradInit, with some architectural changes. FixUp removes the Layer Normalization (LN) from the Transformer, and adds learnable scaling and biases, without evaluating whether learning rate warmup can be removed. T-FixUp removes LN without adding additional parameters, and such a structure can be trained successfully without warmup after applying the T-FixUp scale rules to the weights. Admin preserves the layer normalization but adds learnable scaling coefficients w skip to each dimension of the skip connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2017), which is a Post-LN Transformer placing its Layer</head><p>Normalization after the summation of the skip connection and the residual branch. It has a 512-dimensional word embedding layer and 1024 dimensions in its hidden layer of FFN. We also experiment with its variant from <ref type="bibr" target="#b22">(Liu et al., 2020b)</ref>, where a learnable vector w skip is element-wise multiplied with each dimension of the skip connection. Admin initializes w i skip at the i-th layer as the square root of the cumulative variance of all previous layers over the tokens in a minibatch as w i skip = j&lt;i Var[f (x j )], while GradInit simply initializes it as 1. Following <ref type="bibr" target="#b22">(Liu et al., 2020b)</ref>, we use a linearly decaying learning rate schedule that decays from the maximum learning rate η max to 0 as the model trains for 100K iterations. For training with SGD, we fix the momentum to 0.9, and did a grid search for η max from 0.05 to 0.2. We find using η max = 0.15 gives the best results. We also set η = 0.15, γ = 1 for GradInit in this case. We did a grid search on learning rates from {0.01, 0.025, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1} for Admin. We find it achieves the best result with learning rate 0.06, and diverges when η max &gt; 0.06. For training with Adam, we set η = 5 × 10 −4 for the objective of GradInit, and tried η max and β 2 as listed in <ref type="figure" target="#fig_1">Figure 4</ref>. We evaluate the BLEU score every epoch, and report the best BLEU scores throughout training for each method.  The general tendency is that training becomes more unstable when β2 is larger, which is different from the tendency shown in <ref type="bibr" target="#b22">(Liu et al., 2020b)</ref> for the Post-LN Transformer without w skip . This is because we are using Adam instead of RAdam. RAdam has an implicit learning rate warmup stage that lasts longer when larger β2 is used.</p><p>Stability after removing warmup for Adam. <ref type="figure" target="#fig_1">In Figure 4</ref>, the training process becomes more unstable as β 2 grows larger. From the analysis of RAdam <ref type="bibr" target="#b21">(Liu et al., 2020a)</ref>, this is because the variance of the gradient has a stronger impact on the adaptive learning rate when β 2 is closer to 1. Therefore, the largest β 2 &lt; 1 that maintains the performance of the trained model reflects the stability of the initialization. We can see GradInit results in more stable models than Admin in general, though their best performance are almost the same. In addition, we find w skip can help stabilize the model training in extreme hyper parameter settings, e.g., η max = 5 × 10 −4 and β 2 = 0.995 in <ref type="figure" target="#fig_1">Figure 4</ref>, for which GradInit with w skip obtains a good average BLEU score of 36.0, while without w skip only succeeded in obtaining a BLEU score &gt; 35 for one out of four experiments, resulting in an average BLEU score of 8.9. We also find the network is unable to be trained without learning rate warmup if we just fix w skip to its initial value given by Admin and leave the initialization of other parameters unchanged. Nevertheless, with GradInit, we do not need to modify the architecture of Post-LN Transformer to obtain the same good result as Admin. For a closer look at the mechanism, we show the weight norms and gradient variance at initialization of the original Post-LN architecture using GradInit and Xavier initialization in <ref type="figure">Figure 7</ref> in the Appendix. For Xavier initialization, the gradient variance is relatively higher for all the encoder layers, so GradInit downscales the encoder layer weights more in general. For the LN weights, GradInit only downscales the final LN of both the encoder and decoder, which reduces the variance of the encoder and decoder during the forward pass. Another strategy GradInit learns is to downscale the weights of the output projection and the FFN layers, so that the residual branch is relatively down-weighted compared with the skip connection, similar to Admin.</p><p>Removing warmup for SGD without architectural change. Another widely observed phenomenon is that adaptive methods such as Adam seem to be much better than SGD for training Transformer-based language models . In <ref type="table">Table 2</ref>, we show that with GradInit, we can find a good initialization for the Post-LN Transformer on IWSLT-14 DE-EN, such that we can train it using SGD without learning rate warmup nor gradient clipping, and achieves performance close to Adam trained using the same type of learning rate schedule. By comparison, Admin also makes the Transformer trainable with SGD, but the BLEU score is lower than the one initialized with GradInit. By comparing <ref type="figure" target="#fig_3">Figures 7 and 8</ref> in the Appendix, we find Gra-dInit for Adam and SGD adopts different rescaling patterns, with the Adam version depending more on downscaling the residual branches through the FFN and output projection layers than the SGD version, and the SGD version downscaling more in the final FFN block of the decoder. This highlights the importance of considering the optimization algorithm A in GradInit, and also indicates different ways to reduce the initial gradient variance, achieved by introducing new parameters such as w skip and using different rescalings of the parameters, may be required for different optimizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose GradInit, a gradient-based initialization scheme for any architecture. GradInit reinitializes a network by learning a scale factor for each randomly initialized parameter block of a network, so that the training loss evaluated on a different minibatch after one gradient step of a specific stochastic optimizer is minimized. Such a design takes the stochasticity, the learning rate, and the direction of the optimizer into account, allowing us to find better initializations tailored for the optimizer. The initialization learned by GradInit often decreases the gradient variance for most of the parameter blocks. We show that GradInit accelerates the convergence and improves the test performance of a variety of architectures on image classification. It also enables training the Post-LN Transformer without any form of learning rate warmup, even for SGD. GradInit can be a useful tool in the future discovery of better neural architectures that are otherwise discarded due to poor initializations. By analyzing the learned scaling coefficients and their impact on gradient variance, it can also serve a guide to design better initialization schemes for complex architectures.</p><p>GradInit: Learning to Initialize Neural Networksfor for Stable and Efficient Training (Appendix)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hyperparameters of Image Classification</head><p>We give additional details for tuning the hyperparameters of GradInit on CIFAR-10. As in Algorithm 1, each scale factor is initialized to 1 and we set lower bounds α = 0.01, β = 0. For each architecture, we try τ from {10 −3 , 2 × 10 −3 , 5 × 10 −3 , 10 −2 , 2 × 10 −2 , 5 × 10 −2 , 10 −1 }, and report the results of 4 runs with the best τ . We find the best τ for VGG-   <ref type="table">Table 4</ref>. Using GradInit without the gradient norm constraint with different overlapping ratios r to initialize and train a VGG-19 (w/ BN). For both r = 0.5 and r = 1, we tried τ from the range of 1 × 10 −4 to 2 × 10 −2 . The first two rows show the results with the best final test accuracy Acc best among different τ 's, while the last row shows using a larger τ for r = 1.</p><p>The choice ofS, the minibatch on which the objective L(S; θ − ηA[g(S; g)]) is evaluated, has great influence on the results. We have chosenS to have 50% of its samples from S to reduce the variance of the objective. IfS is a completely new batch, i.e., the overlapping ratio r = |S ∩ S|/|S| is 0, then it becomes difficult for GradInit to work with some models with high initial gradient variance. On the other hand, when we use the same minibatch (S = S), the objective does not capture the stochasticity of the optimizer A and can cause undesirable results in some cases. We study the effect of different choices of S through VGG-19 networks on CIFAR-10. We consider two settings.</p><p>In the first setting, we evaluate VGG-19 (w/o BN) initialized with GradInit using different overlapping ratios of S andS. The results are given in <ref type="table" target="#tab_6">Table 3</ref>. As we have shown in <ref type="figure">Figure 3</ref>, VGG-19 (w/o BN) has high initial gradient variance. When r = 0, sometimes the test accuracy after the first epoch is only 10%, which is worse than the baseline without GradInit. This indicates when r = 0, the high variance of L(S; θ − ηA[g(S; g)]) hinders the effectiveness of GradInit. When r = 1, GradInit does effectively reduce the initial gradient variance, achieving lower variance in the first-epoch test accuracy (Acc 0 ) and higher final test accuracy (A test ) than the baseline (Kaiming Initialization in <ref type="table" target="#tab_0">Table 1</ref>), but the result is not as ideal as using r = 0.5. We leave more fine-grained evaluation on the choice of overlapping ratio r as future work.</p><p>In the second setting, we consider removing the gradient norm constraint of GradInit (by setting γ to ∞) while using overlapping ratios r = 1 and r = 0.5 respectively for a VGG-19 (w/ BN) model. We remove the gradient norm constraint to highlight the different degrees of reliance of the two approaches on the gradient constraint. As shown in <ref type="table">Table 4</ref>, when r = 1, we have to use the smallest τ , which results in minimum change to the scale factors, to obtain results that are not significantly worse than the baseline (Kaiming initialization listed in <ref type="table" target="#tab_0">Table 1</ref>). It is easy for these large over-parameterized models to overfit a single minibatch with the scale factors. When r = 1, GradInit learns a greedy strategy, which increases the gradient as much as possible to enable a steeper descent that sometimes can reduce the loss on the same minibatch by more than 50% in just one iteration. The greedy strategy tends to blow up of the gradient norm at initialization, which hinders convergence and results in a higher dependence on the gradient norm constraint γ. However, when we use τ = 0.5, GradInit is able to improve the baseline without any gradient norm constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Magnification Effect of BN</head><p>Intuitively, if we stop the gradient passing through the mean and bias of the BN layer during backpropagation, the BN layer will magnify the gradient variance when the variance of its input features is smaller than 1 in the forward pass. Here we show its magnification effect analytically for the practical case where the gradient is not stopped for the mean and bias of the BN layer. From the input to the output, the layers are usually ordered as Linear, BN, Activation. Without loss of generality, we assume the linear layer before BN is X = ZW + b, where the output features X = [x 1 , ..., x n ] T ∈ R n×d , the input activations Z ∈ R n×k , n is the number of samples and d, k are the dimension of each feature vector. Batch Normalization normalizes each activation vector x i as following</p><formula xml:id="formula_5">y i = γ x i − µ √ σ 2 + + β,<label>(2)</label></formula><p>where all operators are element-wise, γ, β ∈ R d are learnable parameters usually initialized to 1 and 0 respectively, &gt; 0 is a small constant for numerical stability, and </p><formula xml:id="formula_6">σ 2 = 1 n n i=1 (x i − µ) 2 , µ = 1 n n i=1 x i .<label>(3)</label></formula><formula xml:id="formula_7">∂L ∂(αx i ) = γ n √ α 2 σ 2 +   n ∂L ∂y i − n j=1 ∂L ∂y j − y i − β γ n j=1 ∂L ∂y j · y j − β γ   ,<label>(4)</label></formula><p>where, again, all operations are element-wise. Therefore, when α is larger, the variance of the input feature α 2 σ 2 is larger, and the gradient variance becomes smaller after propagated through this BN layer. Since Z remains the same, Var ∂L ∂W becomes smaller. This explains why GradInit learns to enlarge the weights of Conv layers in the VGG-19 (w/ BN) experiments. Further, to simplify the analysis and show its magnification effect on gradient variance when α 2 σ 2 &lt; 1, let γ = 1, β = 0, and we assume each dimension of ∂L ∂yi is i.i.d., and y i is independent from ∂L ∂yi , which is not necessarily a stronger assumption than <ref type="bibr" target="#b12">(Glorot &amp; Bengio, 2010;</ref><ref type="bibr" target="#b14">He et al., 2015)</ref>, then</p><formula xml:id="formula_8">Var ∂L ∂(αx i ) = 1 n 2 (α 2 σ 2 + ) Var   n ∂L ∂y i − n j=1 ∂L ∂y j − y i n j=1 ∂L ∂y j · y j   = 1 n 2 (α 2 σ 2 + ) Var   (n − 1 − y 2 i ) ∂L ∂y i − n j=1,j =i (1 + y i y j ) ∂L ∂y j   ≥ 1 n 2 (α 2 σ 2 + )    (n − 1) 2 Var ∂L ∂y i + n j=1,j =i Var ∂L ∂y j    = n(n − 1) n 2 (α 2 σ 2 + ) Var ∂L ∂y i ,<label>(5)</label></formula><p>where the inequality comes from the assumption that y i is independent from ∂L ∂yi and the fact that Var[(X + a)Y ] ≥ Var[X] + a 2 Var[Y ] (a is a constant ) when X, Y are independent, and the last equality comes from the i.i.d. assumption. Therefore, if is ignorable and α 2 σ 2 &lt; n(n−1) n 2 , we will have</p><formula xml:id="formula_9">Var ∂L ∂(αx i ) &gt; Var ∂L ∂y i ,<label>(6)</label></formula><p>i.e., the BN layer magnifies the gradient variance when α 2 σ 2 is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Weight norms and gradient variances</head><p>In this section, we give weight norms and gradient variances before and after GradInit is applied to DenseNet-100 (w/o BN) and DenseNet-100 (w/ BN) in <ref type="figure">Figure 5</ref> and <ref type="figure">Figure 6</ref>. We also compare the weight norms and gradient variances of the Post-LN Transformer model initialized using GradInit with A set to Adam and SGD respectively in <ref type="figure">Figure 7</ref> and <ref type="figure" target="#fig_3">Figure 8</ref>. Here, GradInit sets A to Adam. The Transformer has 6 Transformer blocks in its encoder and decoder. In each plot, we first list the values for weights in the encoder, and then those in the decoder. Inside each encoder, we first list the weights from the self attention layers and then the those from the FFN layers. Inside each decoder, we first list the weights in the order of self attention, encoder attention and FFN. In general, GradInit reduces the variance for all the weights, except for some of the Query-Projection and Key-Projection weights in the decoder, which are inside the softmax operations in the self attention blocks. The major source of gradient variance reduction comes from downscaling the final LN weights of the decoder, as well as the linear layers of each residual branch (Out-Projection and Value-Projection weights, FFN.FC1 and FFN.FC2 weights) in each block.  <ref type="figure">Figure 7</ref>. Again, in general, GradInit reduces the variance for most of the weights, except for some of the Query-Projection and Key-Projection weights in the decoder, which are inside the softmax operations in the self attention blocks. Different from the patterns in the Adam version, which downscale all the weights in every layer except for the Query-Projection and Key-Projection weights, the SGD version of GradInit mostly reduces the weights in the final Transformer block of the decoder.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Averaged per-dimension weight magnitude ( Wi /di) and standard deviation of their gradient ((σ(gi))) of the Batch Normalization (BN) layers and the linear layers of the ResNet-110 on CIFAR-10. All the layers are indexed from shallow to deep. The linear layers include all Conv layers (2 for each of the residual blocks) and the final FC layer. The ratio between the weight magnitudes of GradInit and Kaiming Initialization is the learned scale factor of GradInit in each layer. The gradient variance is computed with a batch size of 128. GradInit finds a combination of weight norms where the gradient variance is reduced for all layers. Specifically, it learns to further scale down the second BN layer of each residual block in deeper layers, which is a useful strategy, as deeper layers should have less marginal utility for the feature representations, and scaling down those layers helps to alleviate the growth in variance in the forward pass<ref type="bibr" target="#b9">(De &amp; Smith, 2020)</ref>. GradInit also learns to scale up weights of the first BN layer and all the Conv layers in each residual block, which alleviates the magnification effect of the BN layers on the gradient variance during backpropagation, happening if their input features in the forward pass have small variances. The jump on the curves occur when the dimension of the convolutional filters changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>BLEU score of training the Post-LN Transformer without learning rate warmup using Adam on IWSLT-14 DE-EN under different learning rates ηmax (y axis) and β2 (x axis) of Adam. Each result is averaged over 4 experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .Figure 6 .Figure 7 .</head><label>567</label><figDesc>Averaged per-dimension weight magnitudes ( Wi /di) and standard deviation of their gradient (σ(gi)) for each linear layer i in DenseNet-100 (w/o BN). All the layers are indexed from shallow to deep. The linear layers include all convolutional layers and the final fully connected layer. Inside each dense block, each layer concatenates all the preceding features, so their input dimension increases, the weight dimension increases and the weight norm increases. Compared withFigure 3, DenseNet-100 does not significantly increase the gradient variance during backpropagation. The standard deviation of the gradient is reduced by around 10 6 with GradInit, which explains why it is possible to train DenseNet-100 (w/o BN) without gradient clipping after using GradInit. The major source of gradient reduction of GradInit comes from reducing the weights in each layer. Averaged per-dimension weight magnitudes ( Wi /di) and standard deviation of their gradient (σ(gi)) for each (BN or linear) layer i in the DenseNet-100 (w/ BN). All the layers are indexed from shallow to deep. The linear layers include all convolutional layers and the final fully connected layer. The major source of variance reduction comes from down-scaling the final FC layer. Weight norm and averaged per-dimension standard deviation of each weight of the normalization layers and linear layers in the Post-LN Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>of Transformer FFN.FC1 weights (SGD) of Transformer FFN.FC2 weights (SGD) Weight norm and averaged per-dimension standard deviation of each weight of the normalization layers and linear layers in the Post-LN Transformer. Here, GradInit sets A to SGD. The Transformer model and the way each weight is permuted are the same as in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>± 0.83 95.38 ± 0.08 MetaInit 21.49 ± 0.60 95.00 ± 0.14 GradInit 35.62 ± 1.47 95.33 ± 0.03 GradInit † 34.97 ± 0.65 95.38 ± 0.06 First epoch(Acc0)and best test accuracy over all epochs (Acc best ) for models on CIFAR-10 and ImageNet. We report the mean and its standard error of the top-1 test accuracy in 4 experiments with different random seeds. Best results in each group are in bold. † Achieved by usingS ∪ S = ∅. ‡ For DenseNet-100 without BN, we found MetaInit and GradInit to perform better when gradient clipping is not applied. However, Kaiming's initialization will not converge without gradient clipping, so we applied a gradient clipping of 1 to this setting. * The result of MetaInit are taken from<ref type="bibr" target="#b8">(Dauphin &amp; Schoenholz, 2019)</ref>. They use the variant of ResNet without BN from<ref type="bibr" target="#b34">(Zhang et al., 2019)</ref>, and replaced the ReLU activations with Swish<ref type="bibr" target="#b27">(Ramachandran et al., 2017)</ref>, which is not directly comparable with our BN version, but we keep it here for convenience of reference.</figDesc><table><row><cell>Model (#Params)</cell><cell>Method</cell><cell>Acc 0</cell><cell>Acc best</cell></row><row><cell>On CIFAR-10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VGG-19 w/o BN (20.03 M) VGG-19 w/ BN (20.04M) ResNet-110 w/o BN (1.72M) ResNet-110 w/ BN (1.73M) WRN-28-10 w/ BN</cell><cell>Kaiming MetaInit GradInit Kaiming MetaInit GradInit Kaiming MetaInit GradInit Kaiming MetaInit GradInit Kaiming MetaInit  *</cell><cell cols="2">29.14 ± 1.49 94.48 ± 0.09 30.48 ± 0.88 94.62 ± 0.06 29.25 ± 0.55 94.71 ± 0.02 12.57 ± 0.59 94.41 ± 0.09 35.09 ± 0.58 94.64 ± 0.07 47.79 ± 1.79 95.13 ± 0.07 16.05 ± 2.05 94.22 ± 0.09 14.55 ± 2.19 94.19 ± 0.11 36.21 ± 0.78 94.60 ± 0.13 23.15 ± 0.93 94.98 ± 0.16 29.00 ± 1.45 94.76 ± 0.08 38.19 ± 0.85 95.38 ± 0.05 43.10 ± 2.72 97.22 ± 0.05 -97.1</cell></row><row><cell>(36.49M)</cell><cell>GradInit</cell><cell cols="2">46.28 ± 0.44 97.29 ± 0.07</cell></row><row><cell cols="4">FixUpResNet (1.72M) 38.17 DenseNet-100 FixUp w/o BN (0.75M) Kaiming  ‡ 35.54 ± 0.58 93.97 ± 0.09 MetaInit 35.11 ± 0.20 94.43 ± 0.08 GradInit 37.19 ± 1.05 94.85 ± 0.06 DenseNet-100 w/ BN (0.77M) Kaiming 51.23 ± 1.46 95.46 ± 0.06 MetaInit 46.67 ± 4.00 95.47 ± 0.08 GradInit 52.80 ± 0.79 95.50 ± 0.08</cell></row><row><cell>On ImageNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-50 w/ BN</cell><cell>Kaiming MetaInit  *</cell><cell cols="2">14.61 ± 0.24 75.91 ± 0.09 -75.4</cell></row><row><cell>(25.6M)</cell><cell>GradInit</cell><cell cols="2">19.20 ± 0.77 76.21 ± 0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, 10 −1 , 5 × 10 −2 , 5 × 10 −3 , 2 × 10 −2 , 5 × 10 −3 , 10 −2 respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>19 (w/o</cell></row><row><cell cols="4">BN), VGG-19 (w/ BN), ResNet-110 (w/o BN), ResNet-110 (w/ BN), FixUpResNet, DenseNet-100 (w/o BN), DenseNet-100</cell></row><row><cell cols="2">(w/ BN) are 10 −2 B. Mini-batching, continued: Choice ofS</cell><cell></cell><cell></cell></row><row><cell>Model (#Params)</cell><cell>r = |S ∩ S|/|S|</cell><cell>Acc 0</cell><cell>Acc best</cell></row><row><cell>VGG-19 w/o BN (20.03 M)</cell><cell>0 0.5 1</cell><cell cols="2">21.89 ± 4.38 94.52 ± 0.08 29.25 ± 0.55 94.71 ± 0.02 28.69 ± 0.94 94.55 ± 0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>GradInit with the default gradient norm constraint γ = 1 for initializing a VGG-19 (w/o BN) under different overlapping ratios r. We tried τ in the range of 1 −3 to 2 × 10 −2 , choose the τ with the highest Acc best and show the results.</figDesc><table><row><cell>Model (#Params)</cell><cell>r = |S ∩ S|/|S|</cell><cell>τ</cell><cell>g 2</cell><cell>Acc 0</cell><cell>Acc best</cell></row><row><cell>VGG-19 w/ BN (20.04 M)</cell><cell>0.5 1 1</cell><cell cols="4">4 × 10 −3 1 × 10 −4 11.56 ± 0.05 13.81 ± 2.47 94.45 ± 0.07 8.63 ± 0.20 38.37 ± 1.45 94.78 ± 0.08 4 × 10 −3 190.62 ± 7.65 10.30 ± 0.15 93.70 ± 0.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>For most initialization schemes, b is initialized to 0. is often small and ignorable. Under these two assumptions, each y i is invariant to the rescaling of W . Rescaling W changes the scale of x i , σ and µ homogeneously. Therefore, among all the parameters of the network, if we only change W by rescaling it into αW (α &gt; 0), then y i does not change, and consequently, Var[y i ], ∂L ∂yi and Var[ ∂L ∂yi ] do not change, but σ 2 becomes α 2 σ 2 . To see the magnification effect on the gradient variance during backward propagation, we first find the relation between ∂L ∂yi and ∂L ∂(αxi) under different scales α.</figDesc><table><row><cell>In fact,</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">University of Maryland, College Park 2 Google Research. Correspondence to: Chen Zhu &lt;chenzhu@umd.edu&gt;, Zheng Xu &lt;xuzheng@google.com&gt;, Tom Goldstein &lt;tomg@umd.edu&gt;. Preprint.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, PyTorch initializes the biases to non-zero values by default.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">E. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04887</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Rezero is all you need: Fast convergence at large depth</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dissecting adam: The sign, magnitude and variance of stochastic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="404" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compressed optimisation for nonconvex problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Signsgd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Highperformance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign, iwslt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Initializing learning by learning to initialize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metainit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12645" to="12657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization biases residual blocks towards the identity function in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Accurate</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving transformer optimization through better initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volkovs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (Demonstrations)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Why are adaptive methods good for attention models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Karimireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
