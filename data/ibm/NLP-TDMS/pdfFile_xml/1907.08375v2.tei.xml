<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open Set Domain Adaptation: Theoretical Bound and Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Jie</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Feng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Xuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangquan</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Open Set Domain Adaptation: Theoretical Bound and Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Transfer Learning</term>
					<term>Domain Adaptation</term>
					<term>Ma- chine Learning</term>
					<term>Open Set Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim of unsupervised domain adaptation is to leverage the knowledge in a labeled (source) domain to improve a model's learning performance with an unlabeled (target) domain -the basic strategy being to mitigate the effects of discrepancies between the two distributions. Most existing algorithms can only handle unsupervised closed set domain adaptation (UCSDA), i.e., where the source and target domains are assumed to share the same label set. In this paper, we target a more challenging but realistic setting: unsupervised open set domain adaptation (UOSDA), where the target domain has unknown classes that are not found in the source domain. This is the first study to provide a learning bound for open set domain adaptation, which we do by theoretically investigating the risk of the target classifier on unknown classes. The proposed learning bound has a special term, namely open set difference, which reflects the risk of the target classifier on unknown classes. Further, we present a novel and theoretically guided unsupervised algorithm for open set domain adaptation, called Distribution Alignment with Open Difference (DAOD), which is based on regularizing this open set difference bound. The experiments on several benchmark datasets show the superior performance of the proposed UOSDA method compared with the state-of-the-art methods in the literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S TANDARD supervised learning relies on the assumption that both the training and the testing samples are drawn from the same distribution. Unfortunately, this assumption does not hold in many applications since the process of collecting samples is prone to dataset bias <ref type="bibr">[1]</ref>, <ref type="bibr">[2]</ref>. In object recognition, for example, there can be a discrepancy in the distributions between training and testing as a result of the given conditions, the device type, the position, orientation, and so on. To address this problem, unsupervised domain adaptation (UDA) <ref type="bibr">[3]</ref>, <ref type="bibr">[4]</ref> has been proposed as a way of transferring relevant knowledge from a source domain that has an abundance of labeled samples to an unlabeled domain (the target domain).</p><p>The aim of UDA is to minimize the discrepancy between the distributions of two domains. Existing work on UDA falls into two main categories: (1) feature matching, which seeks a new feature space where the marginal distributions or conditional distributions from the two domains are similar <ref type="bibr">[5]</ref>- <ref type="bibr">[7]</ref>, and (2) instance reweighting, which estimates the weights of the source domain so that the distributional discrepancy is minimized <ref type="bibr">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. There is an implicit assumption in most existing UDA algorithms <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b15">[16]</ref> that the source and target domains share the same label set. Under this assumption, UDA is also regarded as unsupervised closed set domain adaptation (UCSDA) <ref type="bibr" target="#b16">[17]</ref>. However, this assumption in UCSDA algorithms is not realistic in an unsupervised setting (i.e., there are no labels in the target domain) since it is not known whether the classes of target samples are from the label set of the source domain. It may be that the target domain contains additional classes (unknown classes) that do not exist in the label set of the source domain <ref type="bibr" target="#b17">[18]</ref>. For example, in the Syn2Real task <ref type="bibr" target="#b18">[19]</ref>, there may be more classes for the real-world objects in the target domain than the synthetic objects contained in the source domain. Therefore, if existing UCSDA algorithms were to be used to solve the UDA problem without the assumption in UCSDA, the potential mismatches between unknown and known classes would likely result in negative transfer <ref type="bibr" target="#b19">[20]</ref> (see <ref type="figure" target="#fig_1">Fig. 2(b)</ref>).</p><p>To address UDA problem without the assumption, Busto et al. <ref type="bibr" target="#b16">[17]</ref> and Saito et al. <ref type="bibr" target="#b17">[18]</ref> recently proposed a new problem setting, unsupervised open set domain adaptation (UOSDA), in which the unlabeled target domain contains unknown classes that do not belong to the label set of the source domain (see <ref type="figure" target="#fig_0">Fig. 1</ref>). There are two key challenges <ref type="bibr" target="#b17">[18]</ref> in addressing the UOSDA problem. The first challenge is that there is not enough knowledge in the target domain to classify the unknown samples. So how should these samples (c) UOSDA algorithm classifies known target samples into the correct known classes and recognizes the unknown target samples as unknown.</p><p>be labeled? The solution is to mine deeper information in the target domain to delineate a boundary between the known and unknown classes. The second challenge in UOSDA is the difference in distributions. The unknown target samples should not be matched when the overall distribution is matched, otherwise negative transfer may occur. Only a small number of algorithms have been proposed to solve the UOSDA problem <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. The first proposed UOSDA algorithm is Assign-and-Transform-Iteratively (ATI) <ref type="bibr" target="#b16">[17]</ref>, which recognizes unknown target samples using constraint integer programming. It then learns a linear map to match the source domain with the target domain by excluding the predicted unknown target samples. However, ATI carries the assumption that the source domain contains unknown classes that are not in the target domain. Hence, the first proposed deep UOSDA algorithm, Open Set Back Propagation (OSBP) <ref type="bibr" target="#b17">[18]</ref> was developed to address the UOSDA problem without this assumption. It rejects unknown target samples by training a binary cross entropy loss.</p><p>Although ATI and OSBP are designed to solve the UOSDA problem, neither is based on a theoretical analysis of UOSDA. Moreover, no work has yet given a learning bound for open set domain adaptation problems. To fill this gap, this paper presents a theoretical exploration of UOSDA. In studying the risk of the target classifier on unknown classes, we discovered the risk is closely related to a special term called open set difference which can be estimated from the unlabeled samples. Minimizing the open set difference helps us to classify unknown target samples, addressing the first challenge.</p><p>Following our theory, we design a principle-guided UOSDA algorithm referred to as Distribution Alignment with Open Difference (DAOD). This algorithm can accurately classify unknown target samples while minimizing the discrepancy between the two domains for known classes. DAOD learns the target classifier by simultaneously optimizing the structural risk function <ref type="bibr" target="#b23">[24]</ref>, the joint distribution alignment, the manifold regularization <ref type="bibr" target="#b24">[25]</ref>, and open set difference. The reason DAOD is able to avoid negative transfer lies in its ability to minimize the open set difference, which enables the unknown target samples to be classified accurately as unknown. By excluding these recognized unknown target samples, the source and target domains can be precisely aligned, addressing the second challenge.</p><p>As mentioned, there is no theoretical work in the literature for open set domain adaptation. The closest theoretical work is by Ben-David et al. <ref type="bibr" target="#b25">[26]</ref>, who gives VC-dimension-based generalization bounds. Unfortunately, this work has several restrictions: 1) the theoretical analysis only covers closed settings; and 2) the work only solves binary classification tasks, rather than the multi-class problems common to open settings. A significant contribution of this paper is that the theoretical work gives a learning bound for open set domain adaptation.</p><p>The contributions of this paper are summarized as follows.</p><p>• We provide the theoretical bound for open set domain adaptation. The closed set domain adaptation theory <ref type="bibr" target="#b25">[26]</ref> is a special case of our theoretical results. To the best of our knowledge, this is the first work on open set domain adaptation theory.</p><p>• We develop an unsupervised novel open set domain adaptation algorithm, Distribution Alignment with Open Difference (DAOD), which is based on the open set learning bound proposed. The algorithm enables the unknown target samples to be separated from samples using open set difference.</p><p>• We conduct 38 real-world UOSDA tasks (including 20 face recognition tasks and 18 object recognition tasks) for evaluating DAOD and existing UOSDA algorithms. Extensive experiments demonstrate that DAOD outperforms the state-ofthe-art UOSDA algorithms ATI and OSBP. This paper is organized as follows. Section II reviews existing work on unsupervised closed set domain adaptation, open set recognition and unsupervised open set domain adaptation. Section III presents the definitions, important notations and our problem. Section IV provides the main theoretical result and our proposed algorithm. Comprehensive evaluation results and analyses are provided in Section V. Lastly, Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we present relevant work related to unsupervised closed set domain adaptation algorithms, open set recognition, and unsupervised open set domain adaptation.</p><p>Closed Set Domain Adaptation. Ben-David et al. <ref type="bibr" target="#b25">[26]</ref> proposed learning bounds for closed set domain adaptation, where the bounds show that the performance of the target classifier depends on the performance of the source classifier and the discrepancy between the source and target domains. Many UCSDA algorithms <ref type="bibr">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> have been proposed based on theoretical bounds with the objective of minimizing the discrepancy between domains. These algorithms can be roughly divided into two categories: feature matching and instance reweighting.</p><p>Feature matching aims to reduce the distribution discrepancy by learning a new feature representation. Transfer Component Analysis (TCA) <ref type="bibr">[5]</ref> learns a new feature space to match distributions by employing the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b28">[29]</ref>. Joint Distribution Adaptation (JDA) <ref type="bibr">[6]</ref> improves TCA by jointly matching marginal distributions and conditional distributions. Adaptation Regularization Transfer Learning (ARTL) <ref type="bibr" target="#b29">[30]</ref> considers a manifold regularization term <ref type="bibr" target="#b24">[25]</ref> to learn the geometric relations between domains, while matching distributions. Joint Geometrical and Statistical Alignment (JGSA) <ref type="bibr" target="#b30">[31]</ref> not only considers the distribution discrepancy but also matches the geometric shift. Recent advances show that deep networks can be successfully applied to closed set domain adaptation tasks. Deep Adaptation Networks (DAN) <ref type="bibr" target="#b31">[32]</ref> considers three adaptation layers for matching distributions and applies Multiple Kernels MMD <ref type="bibr" target="#b32">[33]</ref> for adapting deep representations. Wasserstein Distance Guided Representation Learning <ref type="bibr" target="#b33">[34]</ref> minimizes the distribution discrepancy by employing Wasserstein Distance in neural networks.</p><p>In the other category, instance reweighting algorithms reduce the distribution discrepancy by weighting samples in the source domain. Kernel Mean Matching <ref type="bibr">[8]</ref> defines the weights as the density ratio between the source domain and the target domain. Yu et al. <ref type="bibr" target="#b8">[9]</ref> has provided a theoretical analysis for important instance reweighting algorithms. However, with a very great domain discrepancy, a large number of effective source samples are down-weighted and useful information is lost.</p><p>Unfortunately, the algorithms mentioned above cannot be applied to open set domain adaptation because unknown target samples would be included in the distribution matching process, leading to negative transfer.</p><p>Open Set Recognition. When the source domain and target domain for known classes share the same distribution, open set domain adaptation becomes Open Set Recognition. A common method for handling open set recognition relies on the use of threshold-based classification strategies <ref type="bibr" target="#b34">[35]</ref>. Establishing a threshold for the similarity score means distant samples are removed from the training samples. Open Set Nearest Neighbor (OSNN) <ref type="bibr" target="#b35">[36]</ref> recognizes whether a sample is from an unknown class by comparing the threshold with a ratio: the similarity score of the sample to the two classes most similar to that sample. Another research stream relies on modifying Support Vector Machines <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>. Multi-class open set SVM <ref type="bibr" target="#b38">[39]</ref> uses a multi-class SVM as a basis to learn the unnormalized posterior probability which is used to reject unknown samples.</p><p>Open Set Domain Adaptation. The open set domain adaptation problem was proposed by Assign-and-Transform-Iteratively (ATI) <ref type="bibr" target="#b16">[17]</ref>. Using 2 distance between each target sample and the center of each source class, ATI constructs a constraint integer programming to recognize unknown target samples S u , then learns a linear transformation to match the source domain and target domain excluding S u . However, ATI requires the help of unknown source samples, which are unavailable in our setting. Recently, a deep learning algorithm, Open Set Back Propagation (OSBP) <ref type="bibr" target="#b17">[18]</ref>, is a recent contri-bution to addressing UOSDA. OSBP relies on an adversarial neural network and a binary cross entropy loss to learn the probability of the target samples. It then uses the estimated probability to separate samples of known and unknown classes in the target. However, we have not found any paper that considers the learning bound for open set domain adaptation. In this paper, we aim to fill in the blanks of open set domain adaptation theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head><p>In this section, we formally define the problem setting for this paper and introduce some fundamental concepts to domain adaptation and, therefore, this study. The notations used throughout this paper are summarized in Appendix A. <ref type="table" target="#tab_0">Table I</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definitions and Problem Setting</head><p>Important definitions are presented as follows.</p><formula xml:id="formula_0">Definition 1 (Domain). Given a feature (input) space X ⊂ R d and a label (output) space Y, a domain is a joint distribution P (X, Y ), where random variables X ∈ X , Y ∈ Y.</formula><p>To be exact, a random variable is a measurable map. In Definition 1, X ∈ X and Y ∈ Y mean that the image sets of X and Y are contained in the spaces X and Y respectively. We normally name the random variable X from the feature space X as feature vector while the random variable Y as label. The label Y can either be continuous (in a regression task) or discrete (in a classification task). In this paper, we have fixed it as a discrete variable with a fixed number of items. Based on this definition, we have: Definition 2 (Domains for Open Set Domain Adaptation). Given a feature space X ⊂ R d and the label spaces Y s , Y t , the source and target domains have different joint distributions</p><formula xml:id="formula_1">P (X s , Y s ) and P (X t , Y t ), where the label space Y s ⊂ Y t , and random variables X s , X t ∈ X , Y s ∈ Y s , Y t ∈ Y t .</formula><p>From Definitions 1 and 2, we can see that: 1) X s and X t are from the same space because our focus is on homogeneous situations; and 2) Y s is a subset of Y t . The classes from Y t \Y s are the unknown target classes. The classes from Y s are the known classes. Thus, the problem to be solved is:</p><p>Problem 1 (Unsupervised Open Set Domain Adaptation (UOSDA)). Given labeled samples S drawn from the source domain P (X s , Y s ) i.i.d. and unlabeled samples T X drawn from the target marginal distribution P (X t ) i.i.d., the aim of unsupervised open set domain adaptation is to find a target classifier f t : X → Y t such that 1) f t classifies known target samples into the correct known classes; 2) f t classifies unknown target samples as unknown.</p><p>It is worth noting that, with UOSDA tasks, the algorithm only needs to classify unknown samples as unknown and classify known target samples into the correct known classes. Classifying unknown target samples into correct unknown classes is not necessary. Hence, we consider all unknown target samples are allocated to one big unknown class. Without loss of generality, we assume that</p><formula xml:id="formula_2">Y s = {y c } C c=1 , Y t = {y c } C+1 c=1</formula><p>, where the label y C+1 represents the unknown target classes and the label y c ∈ R (C+1)×1 is a one-hot vector, whose cth coordinate is 1 and other coordinates are 0. The label y c represents the c-th class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Concepts and Notations</head><p>Before introducing our main results, we need to introduce the following necessary concepts and notations in this field. Unless otherwise specified, all the following notations are used consistently throughout the paper without further explanations. More detail on these notations is provided in Appendix A.</p><p>1) Notations for distributions: For the sake of simplicity, we use the notations P X s Y s and P X t Y t to denote the joint distributions P (X s , Y s ) and P (X t , Y t ) respectively and also denote P X s and P X t as the marginal distributions P (X s ) and P (X t ), respectively. P X s |yc and P X t |yc represent the conditional distributions for the c-th class P (X s |Y s = y c ) and P (X t |Y t = y c ), while π t c represents the target class-prior probability for c-th class P (Y t = y c ). Hence, π t C+1 = P (Y t = y C+1 ) is the classprior probability for the unknown target classes.</p><p>Lastly, P X t |Y s represents the target conditional distribution for the known classes P (X t |Y t ∈ Y s ), which can be evaluated by</p><formula xml:id="formula_3">P (X t , Y t ∈ Y s ) P (Y t ∈ Y s ) = C c=1 P (X t |Y t = y c )π t c 1 − π t C+1 .</formula><p>The notation P denotes the corresponding empirical distribution to any distribution P . For example, P X s Y s represents the empirical distribution corresponding to P X s Y s .</p><p>2) Risks and Partial Risks: Risks and partial risks are two important concepts in learning theory, which are briefly explained in the following and later used in our theorems.</p><p>Following the notations in <ref type="bibr" target="#b39">[40]</ref>, we consider a multi-class classification task with a hypothesis space H of the scoring functions</p><formula xml:id="formula_4">h : X → R |Y t | = R C+1 x → [h 1 (x), ..., h C+1 (x)] T ,<label>(1)</label></formula><p>where the output h c (x) indicates the confidence in the prediction of the label y c . Let : R C+1 × R C+1 → R + be a symmetric loss function. Then the risks of h ∈ H w.r.t. under P X s Y s and P X t Y t are given by</p><formula xml:id="formula_5">R s (h) : = E (x,y)∼P X s Y s (h(x), y) = E (h(X s ), Y s )), R t (h) : = E (x,y)∼P X t Y t (h(x), y) = E (h(X t ), Y t )).<label>(2)</label></formula><p>The partial risk of h ∈ H for the known target classes is</p><formula xml:id="formula_6">R t * (h) : = 1 1 − π t C+1 X ×Y s (h(x), y)dP X t Y t (x, y) (3)</formula><p>and the partial risk of h ∈ H for the unknown target classes is</p><formula xml:id="formula_7">R t C+1 (h) : = E x∼P X t |y C+1 (h(x), y C+1 ) = X (h(x), y C+1 )dP X t |y C+1 (x).<label>(4)</label></formula><p>According to (2), (3) and (4), we have</p><formula xml:id="formula_8">R t (h) = π t C+1 R t C+1 (h) + (1 − π t C+1 )R t * (h).<label>(5)</label></formula><p>The proof can be found in Appendix A. Lastly, we denote</p><formula xml:id="formula_9">R s u,C+1 (h) := E x∼P X s (h(x), y C+1 ) = E (h(X s ), y C+1 ), R t u,C+1 (h) := E x∼P X t (h(x), y C+1 ) = E (h(X t ), y C+1 )<label>(6)</label></formula><p>as the risks that the samples are regarded as the unknown classes. Given a risk R(h), it is convenient to use notation R(h) as the empirical risk that corresponds to R(h). Hence, notations R s (h), R s u,C+1 (h) and R t u,C+1 represent the empirical risks corresponding to the risks R s (h), R s u,C+1 (h) and R t u,C+1 (h) respectively.</p><p>3) Discrepancy Distance and Maximum Mean Discrepancy: One challenge of domain adaptation is the mismatch between the distributions of the source and target domains. To mitigate this effect, two famous distribution distances have been proposed as the measures of the distribution difference.</p><p>The first one is discrepancy distance presented as follows.</p><p>Definition 3 (Discrepancy Distance <ref type="bibr" target="#b40">[41]</ref>). Let the hypothesis space H be a set of functions defined in a feature space X , be a loss function and P 1 , P 2 be distributions on space X . The discrepancy distance d H (P 1 , P 2 ) between the distributions P 1 and P 2 over X is</p><formula xml:id="formula_10">sup h,h * ∈H | E x∼P1 (h(x), h * (x)) − E x∼P2 (h(x), h * (x))|.</formula><p>If in the definition is the zero-one loss, the discrepancy distance is known as the H∆H distance <ref type="bibr" target="#b25">[26]</ref>. The discrepancy distance is symmetric and satisfies the triangle inequality, but it does not define a distance in general: d H (P 1 , P 2 ) = 0 does not mean P 1 = P 2 .</p><p>The second distance is Maximum Mean Discrepancy:</p><p>Definition 4 (Maximum Mean Discrepancy <ref type="bibr" target="#b28">[29]</ref>). Given a feature space X and a class of function F (f : X → R). The maximum mean discrepancy between the distributions P 1 and P 2 is</p><formula xml:id="formula_11">MMD[F, P 1 , P 2 ] := sup f ∈F | E x∼P1 f (x) − E x∼P2 f (x)|.</formula><p>To ensure that MMD is a metric, one must identify a function class F that is rich enough to uniquely identify whether P 1 = P 2 . Gretton et al. <ref type="bibr" target="#b28">[29]</ref>, therefore, propose as MMD function class F the unit ball in a reproducing kernel Hilbert space (RKHS) H k <ref type="bibr" target="#b41">[42]</ref> (the subscript k represents the reproducing kernel and is used to distinguish the hypothesis set H from the RKHS H k ).</p><p>For convenience, we have used the notation MMD H k (·, ·) to represent MMD[F, ·, ·], when F = {f ∈ H k : f H k ≤ 1} <ref type="bibr" target="#b28">[29]</ref>. Note that MMD H k is symmetric and satisfies the triangle inequality. When the kernel k is a universal kernel, MMD H k (P 1 , P 2 ) = 0 if and only if P 1 = P 2 , which implies that MMD H k is a metric.</p><p>Though the MMD distance is powerful, it is not convenient to be optimized as a regularization term in shallow domain adaptation algorithms. The projected MMD <ref type="bibr">[5]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> has been proposed to transform the MMD distance into a proper regularization term. Given a scoring function h = [h 1 , ..., h C+1 ] T , where h c ∈ H k , c = 1, ..., C +1, the projected MMD is defined as follows:</p><formula xml:id="formula_12">D h,k (P 1 , P 2 ) = X h(x)dP 1 (x) − X h(x)dP 2 (x) 2 ,</formula><p>where · 2 is the 2 norm. 4) Manifold Regularization: The idea of manifold regularization has a rich machine learning history going back to transductive learning and truly semi-supervised learning <ref type="bibr" target="#b24">[25]</ref>. Manifold regularization is specifically designed to control the complexity as measured by the geometry of the distribution. Given samples {x 1 , ..., x n }, the manifold regularization is</p><formula xml:id="formula_13">n i,j=1 h(x i ) − h(x j ) 2 2 W ij ,</formula><p>where [W ij ] is the pair-wise affinity matrix and W ij estimates the similarity of x i , x j . By the manifold assumption <ref type="bibr" target="#b24">[25]</ref>, if two samples from the support set of the distributions P X s , P X t are close, then the scores of the two samples are similar. To extract geometric relationship between domains, the manifold regularization has been used by many closed set domain adaptation algorithms <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Main Theoretical Result and Open Set Difference</head><p>Before introducing the main theorem, we firstly define open set difference, one of the main contributions of the paper.</p><p>Definition 5 (Open Set Difference). Given risks R s u,C+1 (h) and R t u,C+1 (h) defined in <ref type="bibr">(6)</ref>, the open set difference is</p><formula xml:id="formula_14">∆ o = R t u,C+1 (h) 1 − π t C+1 − R s u,C+1 (h),</formula><p>where π t C+1 is the class-prior probability for the unknown target classes.</p><p>The following theorem provides an open set domain adaptation bound according to discrepancy distance and open set difference.</p><p>Theorem 1. Given a symmetric loss function satisfying triangle inequality and a hypothesis H with a mild condition that the constant vector value function g := y C+1 ∈ H, then for any h ∈ H, we have</p><formula xml:id="formula_15">R t (h) 1 − π t C+1 ≤ Source Risk R s (h) +2 Distribution Discrepancy d H (P X t |Y s , P X s ) + Λ + R t u,C+1 (h) 1 − π t C+1 − R s u,C+1 (h) Open Set Difference∆o ,</formula><p>where R s (h) and R t (h) are the risks defined in (2), R s u,C+1 (h) and R t u,C+1 (h) are the risks defined in (6), R t * (h) is the partial risk defined in (3), and Λ = min h∈H R s (h)+R t * (h). Proof. Here we provide a proof sketch. Detailed proof is given in Appendix A. According to <ref type="bibr">(5)</ref>, we have</p><formula xml:id="formula_16">R t (h) 1 − π t C+1 − R s (h) =R t * (h) − R s (h) + π t C+1 1 − π t C+1 R t C+1 (h).<label>(7)</label></formula><p>Then we can check that</p><formula xml:id="formula_17">R t * (h) − R s (h) ≤ Λ + d H (P X t |Y s , P X s ),<label>(8)</label></formula><formula xml:id="formula_18">π t C+1 1 − π t C+1 R t C+1 (h) ≤ d H (P X t |Y s , P X s ) + ∆ o .<label>(9)</label></formula><p>Combining (8), (9) with (7), we have</p><formula xml:id="formula_19">R t (h) 1 − π t C+1 ≤ R s (h) + 2d H (P X t |Y s , P X s ) + Λ + ∆ o .</formula><p>Remark 1. The condition y C+1 ∈ H can be replaced by a weaker condition that there exists a sequence {h i } +∞ i=1 such that h i converges uniformly to y C+1 . Note that the hypothesis space H used in our algorithm satisfies the weaker condition automatically, thus, the condition y C+1 ∈ H can be removed when we use the H applied in our algorithm.</p><p>The open set difference ∆ o is the crucial term to bound the risk of h on unknown target classes, since</p><formula xml:id="formula_20">R t C+1 (h) ≤ 1 − π t C+1 π t C+1 ∆ o + d H (P X t |Y s , P X s ) . (10)</formula><p>The risk of h on unknown target classes is intimately linked to the open set difference ∆ o :</p><formula xml:id="formula_21">π t C+1 R t C+1 (h) − (1 − π t C+1 )∆ o ≤ d H (P X t |Y s , P X s )</formula><p>. When π t C+1 = 0, Theorem 1 degenerates into the closed set scenario with this theoretical bound <ref type="bibr" target="#b25">[26]</ref> </p><formula xml:id="formula_22">R t (h) ≤ R s (h) + 3d H (P X t , P X s ) + Λ.</formula><p>This is because when π t C+1 = 0, the open set difference is</p><formula xml:id="formula_23">∆ o ≤ d H (P X t |Y s , P X s ) = d H (P X t , P X s ).</formula><p>The significance of Theorem 1 is twofold. First, it highlights that the open set difference ∆ o is the main term for controlling performance in open set domain adaptation. Second, the bound shows a direct connection with closed set domain adaptation theory.</p><p>In addition, the open set difference ∆ o consists of two parts: a positive term R t u,C+1 (h) and a negative term R s u,C+1 (h). A larger positive term implies more target samples are classified as unknown samples. The negative term is used to prevent the source samples from being classified as unknown. According to <ref type="bibr" target="#b9">(10)</ref>, the negative term and the distance discrepancy jointly prevent all target samples from being recognized as unknown classes. In addition, Corollary 1.1 also tells us that the positive term and the negative term can be estimated from unlabeled samples. Using Natarajan Dimension Theory <ref type="bibr" target="#b48">[49]</ref> to bound the source risk R s (h), risks R t u,C+1 (h) and R s u,C+1 (h) by empirical estimates R s (h), R t u,C+1 (h) and R s u,C+1 (h) respectively, we have the following result. Corollary 1.1. Given a symmetric loss function satisfying the triangle inequality and bounded by B, and a hypothesis H ⊂ {h : X → Y t } with conditions: 1) the constant vector value function g := y C+1 ∈ H, 2) the Natarajan dimension of H is d, if a random labeled sample of size n s is generated by source joint distribution P X s Y s -i.i.d. and a random unlabeled sample of size n t is generated by target marginal distribution P X t -i.i.d., then for any h ∈ H and δ ∈ (0, 1) with probability at least 1 − 3δ, we have <ref type="bibr">(6)</ref>. Proof. The proof is given in Appendix D.</p><formula xml:id="formula_24">R t (h) 1 − π t C+1 ≤ R s (h) + 2d H (P X t |Y s , P X s ) + ∆ o + Λ + 4B 8d log n s + 16d log(C + 1) + 2 log 2/δ n s + 2B 8d log n t + 16d log(C + 1) + 2 log 2/δ (1 − π t C+1 ) 2 n t , where X is the feature space, Y t is the target label space, R s (h) is the empirical source risk, R t (h) is the target risk, Λ = min h∈H R s (h) + R t * (h) and empirical open set difference ∆ o = R t u,C+1 (h) 1−π t C+1 − R s u,C+1 (h), here R s (h) are the risks defined in (2), R s u,C+1 (h), R t u,C+1 (h) are the empirical risks corresponding to R s u,C+1 (h), R t u,C+1 (h) defined in</formula><p>Next, we employ the open set difference ∆ o to construct our model -Distribution Alignment with Open Difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithm</head><p>The importance of Theorem 1 is that it tells us the relationships between the three terms (i.e., the source risk, the distribution discrepancy, and the open set difference) and the bound of the open set domain adaptation. Inspired by these relationships, our initial focus is the following optimization problem for unsupervised open set domain adaptation:</p><formula xml:id="formula_25">h * = arg min h∈H R s (h) + λd H ( P X s , P X t |Y s ) + γ 1 1 − π t C+1 R t u,C+1 (h) − R s u,C+1 (h) ,<label>(11)</label></formula><p>where the hypothesis space H is defined as a subset of functional space {h = [h 1 , ..., h C+1 ] T : h c ∈ H k } and λ and γ are two free hyper-parameters.</p><p>As proven by JDA <ref type="bibr">[6]</ref>, ARTL <ref type="bibr" target="#b29">[30]</ref> and MEDA <ref type="bibr" target="#b43">[44]</ref>, incorporating conditional distributions into the original marginal distribution discrepancy can lead to superior domain adaptation performance. Hence, we have also added an additional conditional distribution discrepancy to the optimization problem in <ref type="bibr" target="#b10">(11)</ref>. Hence, the new problem becomes:</p><formula xml:id="formula_26">h * = arg min h∈H R s (h) + λµD 2 h,k ( P X s , P X t |Y s ) + λ(1 − µ) C c=1 D 2 h,k ( P X s |yc , P X t |yc ) + γ 1 1 − π t C+1 R t u,C+1 (h) − R s u,C+1 (h) ,</formula><p>where µ ∈ [0, 1] is the adaptive factor <ref type="bibr" target="#b43">[44]</ref> to convexly combine the contributions from both the empirical marginal distribution alignment and the empirical conditional distribution alignment. Note that the original d H (·, ·) is replaced with the projected MMD D h,k (·, ·) in the new problem, because d H (·, ·) is difficult to estimate. This results in a gap with Theorem 1 where the discrepancy distance is used to measure the distribution discrepancy rather than projected MMD. To mitigate this gap, we also give a similar theoretical bound using MMD distance (see Theorem 4 in Appendix C for details). Specifically, for proving Theorem 4, we need an additional condition that the loss is squared loss (y, y ) = y − y 2 2 . Thus, we use the squared loss to design our algorithm.</p><p>Further, we have added the manifold regularization <ref type="bibr" target="#b24">[25]</ref> to learn the geometric structure of the source and target domains. With this regularization, our algorithm can consistently achieve good performance when the setting degrades into a closed set domain adaptation (i.e., where there are no unknown classes). This is because the state of the art closed set algorithm ARTL <ref type="bibr" target="#b29">[30]</ref> is a special case of DAOD, when there is no open set difference.</p><p>Thus, the optimization problem can be rewritten as follows:</p><formula xml:id="formula_27">h * = arg min h∈H R s (h) + λµD 2 h,k ( P X s , P X t |Y s ) + λ(1 − µ) C c=1 D 2 h,k ( P X s |yc , P X t |yc ) + α R t u,C+1 (h) − γ R s u,C+1 (h) + ρM h (S X , T X ) + σ h 2 k ,<label>(12)</label></formula><p>where α := γ/(1 − π t C+1 ), ρ and σ are three free hyperparameters, T X denotes unlabeled target samples, S X denotes source samples without labels, M h (S X , T X ) is the manifold regularization, and h 2 k is the squared norm of h in H k to avoid over-fitting.</p><p>Next, we show how to formulate equation (12) using given samples. First, following the representer theorem, if the optimization problem (12) has a minimizer h * , then h * can be written as</p><formula xml:id="formula_28">h * (x) = n s +n t i=1 β i k(x i , x), ∀x ∈ X ,</formula><p>where x i ∈ S X ∪ T X and β i ∈ R (C+1)×1 is the parameter. With this form of h * , we explain the computation of terms in <ref type="bibr" target="#b11">(12)</ref>. The notations used in this section are summarized in Appendix A. <ref type="table" target="#tab_0">Table II.</ref> 1) Distribution Alignment: Since there are no labels for the target samples, we cannot directly compute</p><formula xml:id="formula_29">µD 2 h,k ( P X s , P X t |Y s ) + (1 − µ) C c=1 D 2 h,k ( P X s |yc , P X t |yc )<label>(13)</label></formula><p>Therefore, the pseudo target labels are used to help compute P X t |Y s and P X t |yc instead. Given the pseudo target samples for known classes T X,K , the pseudo target samples for c-th class T X,c and the source samples for c-th class S X,c we can compute (13) by the representer theorem and kernel trick as follows:</p><formula xml:id="formula_30">tr(β T KMKβ),<label>(14)</label></formula><p>where</p><formula xml:id="formula_31">β = [β 1 , ..., β n s +n t ] T ∈ R (C+1)×(n s +n t ) , K is the (n s + n t ) × (n s + n t ) kernel matrix [k(x i , x j )], here x i , x j ∈ S X ∪ T X , and M = µM 0 + (1 − µ) C c=1 M c is the MMD matrix: (M 0 ) ij =                      1 (n s ) 2 , x i , x j ∈ S X , 1 (n t K ) 2 , x i , x j ∈ T X,K , 0, x i or x j ∈ T X \ T X,K , − 1 n s n t K , otherwise; (M c ) ij =                              1 (n s c ) 2 , x i , x j ∈ S X,c , 1 (n t c ) 2 , x i , x j ∈ T X,c , − 1 n s c n t c , x i ∈ S X,c , x j ∈ T X,c , − 1 n s c n t c , x j ∈ S X,c , x i ∈ T X,c , 0, otherwise,</formula><p>where n s := |S X |, n t K := |T X,K |, n s c := |S X,c | and n t c := |T X,c |.</p><p>2) Manifold Regularization: The pair-wise affinity matrix is denoted as</p><formula xml:id="formula_32">W ij = sim(x i , x j ), x i ∈ N p (x j ) or x j ∈ N p (x i ) 0, otherwise;</formula><p>where sim(x i , x j ) is the similarity function such as cosine similarity, N p (x i ) denotes the set of p-nearest neighbors to point x i and p is a free parameter. The manifold regularization can then be evaluated as follows:</p><formula xml:id="formula_33">M h (S X , T X ) = n s +n t i,j=1 h(x i ) − h(x j ) 2 2 W ij = C+1 c=1 n s +n t i,j=1 h c (x i )L ij h c (x j ), where x i , x j ∈ S X ∪T X , L is the Laplacian matrix, which can be written as D − W, here D is a diagonal matrix and D ii = n s +n t j=1</formula><p>W ij . Using the representer theorem and kernel trick, the manifold regularization M h (S X , T X ) can be written as</p><formula xml:id="formula_34">tr(β T KLKβ).<label>(15)</label></formula><p>3) Open Set Loss Function: We use a matrix to rewrite the loss function and open set difference. Let the label matrix be Y ∈ R (C+1)×(n s +n t ) :</p><formula xml:id="formula_35">Y ij = 1, x j ∈ S X,i 0, otherwise when i ≤ C,<label>(16)</label></formula><formula xml:id="formula_36">Y ij = 1, x j ∈ T X,C+1 0, otherwise when i = C + 1. (17) The label matrix Y ∈ R (C+1)×(n s +n t ) is Y ij = 1 iff i = C + 1 and x j ∈ S X , otherwise Y ij = 0. (18) Then R s (h) + α R t u,C+1 (h) − γ R s u,C+1 (h) + σ h 2 k = (Y − β T K)A 2 F − γ ( Y − β T K) A 2 F + σtr(β T Kβ)<label>(19)</label></formula><p>where A is a (n s + n t ) × (n s + n t ) diagonal matrix with</p><formula xml:id="formula_37">A ii = 1 n s if x i ∈ S X , A ii = α n t if x i ∈ T X ; A is a (n s + n t ) × (n s + n t ) diagonal matrix with A ii = 1 n s if x i ∈ S X , A ii = 0 if x i ∈ T X , and · F is the Frobenius norm.</formula><p>4) Overall Reformulation: Finally, based on (14), (15), <ref type="bibr" target="#b18">(19)</ref>, the optimization problem in (12) is reformulated as:</p><formula xml:id="formula_38">β * = arg min β∈R (n s +n t )×(C+1) L(β) where L(β) := (Y − β T K)A 2 F − γ ( Y − β T K) A 2 F + tr(β T K(λM + ρL)Kβ) + σtr(β T Kβ).<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training</head><p>There is a negative term in L(β), hence it may be not correct to compute the optimizer by solving the equation ∂L(β) ∂β = 0 directly. Maybe the "minimizer" solved by ∂L(β) ∂β = 0 is a maximum point or a saddle point. Fortunately, the following theorem shows that there exists a unique optimizer that can be solved by ∂L(β) ∂β = 0. Theorem 2. If the coefficient γ of R s u,C+1 (h) is smaller than 1 and the kernel k is universal, then the L(β) defined in <ref type="bibr" target="#b19">(20)</ref> has a unique minimizer, which can be written as:</p><formula xml:id="formula_39">(A 2 − γ A 2 + λM + ρL)K + σI −1 (A 2 Y T − γ A 2 Y T ).<label>(21)</label></formula><p>The proof can be found in Appendix B.</p><p>To compute the true value of (21), it is best to use the groundtruth labels of the target domain. However, our focus is on unsupervised task, which means that it is impossible to obtain any true target labels and, as mentioned, pseudo labels Algorithm 1: DAOD Input: Data S, T X ; #iterations T ; #neighbor p and parameters λ, σ, ρ, α, γ, µ; threshold t; universal kernel function k(·, ·). 1. Y t ← OSNN cv (S, T X , t);% Predict pseudo labels; 2. Compute L, K using S, T X , and Y t ;</p><formula xml:id="formula_40">3. i ← 1; while i &lt; T + 1 do 4. Compute M using S, T X , and Y t ; 5. Compute β by formula (21); 6. Y t ← β T K;%Predict pseudo labels; 7. i ← i + 1; Output: Predicted target labels Y t , classifier β T K.</formula><p>can be used instead. These pseudo labels are generated by applying an open set classifier that has been trained on the source samples to the target samples.</p><p>In this paper, we used Open Set Nearest Neighbor for Class Verification-t (OSNN cv -t) <ref type="bibr" target="#b35">[36]</ref> to help us learn the pseudo labels. We select the two nearest neighbors v, u from the test sample s. If both nearest neighbors have the same label y c , s is classified with the label y c . Otherwise, the following ratio</p><formula xml:id="formula_41">is calculated v − s 2 / u − s 2 , on the assumption that v − s 2 ≤ u − s 2 .</formula><p>If the ratio is smaller than or equal to a predefined threshold t, 0 &lt; t &lt; 1, s is classified with the same label as v. Otherwise, s is recognized as the unknown sample.</p><p>To make the pseudo labels more accurate, we use the iterative pseudo label refinement strategy, proposed by JDA <ref type="bibr">[6]</ref>. The implementation details are demonstrated in Algorithm 1 (https://github.com/fang-zhen/Open-set-domain-adaptation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND EVALUATIONS</head><p>In this section, we first utilized real world datasets to verify the performance of DAOD. We then conducted experiments to examine the behavior of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Real World Datasets</head><p>We evaluated our algorithm on three cross-domain recognition tasks: object recognition (Office-31, Office-Home), and face recognition (PIE). <ref type="table" target="#tab_0">Table I</ref> lists the statistics of these datasets. Office-31 <ref type="bibr" target="#b49">[50]</ref> consists of three real-world object domains: AMAZON (A), DSLR (D) and WEBCAM (W). It has 4,652 images with 31 common categories. This means that there are 6 domain adaptation tasks:</p><formula xml:id="formula_42">A → D, A → W, D → A, W → A, D → W, W → D.</formula><p>Following the standard protocol and for a fair comparison with the other algorithms, we extracted feature vectors from the fully connected layer-7 (fc7) of the AlexNet <ref type="bibr" target="#b50">[51]</ref>. We introduced an open set protocol for this dataset by taking classes 1-10 as the shared classes in alphabetical order. The classes 21-31 were used as the unknown classes in the target domain.</p><p>Office-Home <ref type="bibr" target="#b51">[52]</ref> consists of 4 different domains: Artistic (Ar), Clipart (Cl), Product (Pr) and Real-World (Rw). Each domain contains images from 65 object classes. We constructed 12 OSDA tasks: Ar → Cl, Ar → Pr,..., Rw → Ar.</p><p>In alphabetical order, we used the first 25 classes as the known classes and classes 26-65 as the unknown classes. Following the standard protocol and for a fair comparison with the other algorithms, we extracted feature vectors from ResNet-50.</p><p>PIE <ref type="bibr" target="#b52">[53]</ref> contains 41, 368 facial images of 68 people in various poses, illuminations, and expression changes. The face images are captured by 13 synchronized cameras (different poses) and 21 flashes (different illuminations and/or expressions). We focused on 5 of 13 poses, i.e., PIE1 (C05, left pose), PIE2 (C07, upward pose), PIE3 (C09, downward pose), PIE4 (C27, frontal pose) and PIE5 (C29, right pose). These facial images were cropped to a size of 32 × 32. We took classes 1-20 as the known classes and classes 21-68 as the unknown classes in the target domain. 20 tasks were tested: PIE1→PIE2, PIE1→PIE3,..., PIE5→PIE4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Algorithms</head><p>The baseline algorithms selected for comparison with DAOD were: 1) No Transfer:</p><p>• OSNN <ref type="bibr" target="#b35">[36]</ref>. OSNN recognizes a sample as unknown by computing the ratio of similarity scores to the two most similar classes of the sample and then comparing the ratio with a predefined threshold.</p><p>2) Closed Set:</p><p>• TCA [5] + OSNN. The aim in implementing TCA is to show that if the UCSDA algorithm is used to solve the UOSDA problem, negative transfer will occur, leading to poor performance.</p><p>3) Open Set:</p><p>• JDA [6] + OSNN. We extended JDA into the open set setting. Joint distribution matching is the main step in JDA. Thus, we simply matched the known samples predicted by OSNN when the JDA algorithm was implemented.</p><p>• JGSA [31] + OSNN. We extended JGSA into the open set setting. First, for learning new features, we implemented JGSA using the source samples and the known target samples predicted by OSNN. Then, we used OSNN to predict the pseudo labels. We repeated the process until convergence.</p><p>• ATI [17] + OSNN. ATI was the first UOSDA algorithm, but it requires the unknown source samples to implement. Therefore, to implement ATI under our setting, we used ATI to select the outliers, and then learned the new features for matching the source domain and target domain excluding selected outliers. Lastly, OSNN was used to predict the labels.</p><p>• OSBP <ref type="bibr" target="#b17">[18]</ref>. OSBP utilizes adversarial neural networks and a binary cross entropy loss to learn the probability for the target samples, then uses the estimated probability to recognize the unknown samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Setup</head><p>Before reporting the detailed evaluation results, it is important to explain how DAOD's hyper-parameters are tuned. DAOD has several hyper-parameters: 1) the choice of the kernel function k; 2) the adaptation parameters λ, σ, ρ, p, µ;</p><p>3) the open set parameters α, γ; and 4) #iterations T and the threshold t ∈ (0, 1). Each parameter is discussed one by one next.</p><p>1) The kernel function k : As suggested in <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b43">[44]</ref>, we chose the Gaussian kernel</p><formula xml:id="formula_43">k(a, b) = exp(− a − b 2 2 2r 2 ),<label>(22)</label></formula><p>where the kernel bandwidth r is median( a − b 2 ), ∀a, b ∈ S X ∪ T X .</p><p>2) The adaptive factor µ: The adaptive factor µ expresses the relative importance of the marginal distributions and conditional distributions. Wang et al. <ref type="bibr" target="#b43">[44]</ref> made the first attempt to compute µ by employing A-distance <ref type="bibr" target="#b25">[26]</ref>, which is the special case d 0−1 H of the discrepancy distance d H . According to <ref type="bibr" target="#b25">[26]</ref>, the A-distance can also be defined as the error of building a binary classifier from hypothesis set H to discriminate between the two domains. Wang et al. <ref type="bibr" target="#b43">[44]</ref> used the linear hypothesis set to estimate A-distance. Let (h) be the error of the linear classifier h discriminating source samples S X and target samples T X . Then the A-distance</p><formula xml:id="formula_44">d A (S X , T X ) = 2(1 − (h)).</formula><p>We adopted the same algorithm as <ref type="bibr" target="#b43">[44]</ref> to estimate µ by</p><formula xml:id="formula_45">µ = 1 − d 0 d 0 + C c=1 d c , where d 0 := d A (S X , T X,K ), d c := d A (S X,c , T X,c ) (c = 1, ..., C).</formula><p>Here T X,K is the set of the target samples predicted as known samples. This estimation has to be computed at every iteration of DAOD, since the predicted conditional distributions for the target may vary each time.</p><p>3) The open set parameters α and γ: As shown in Figs. 3 and 4, DAOD is able to achieve consistently good performance within a same range α ∈ [0.2, 0.4] and γ ∈ [0.15, 0.5], which shows the relative stability of DAOD given the correct tuning of these two parameters. Tuning should be done according to the following rules. First, the positive term R t u,C+1 and the negative term R s u,C+1 in the open set difference are inferred from each other. A larger positive term means that more samples are recognized as the unknown classes. A larger negative term implies that more samples are classified as known classes. To ensure that the positive and negative terms balance, the difference |α−γ| should not be too large. Further, the parameter α should be larger than γ, since the positive term's coefficient 1/(1 − π t C+1 ) is larger than 1. In this paper, we set α = 0.4 for all tasks and 1) γ = 0.2 for Office-31, and 2) γ = 0.25 for Office-Home and PIE datasets.</p><p>4) Other hyper-parameters: We ran DAOD with a wide range of parameter values for λ, ρ, p, σ, t and T in Section V-G. The results are shown in <ref type="figure" target="#fig_7">Fig. 4</ref>. These results indicate that DAOD can provide a robust performance with a wide range of hyper-parameter values.</p><p>From our tests, the best choices of parameters were: λ ∈ [50, 100], ρ ∈ [0, 1], p ∈ <ref type="bibr">[2,</ref><ref type="bibr" target="#b31">32]</ref>, σ ∈ [0.2, 1.6], t ∈ [0, 0.9] and DAOD can converge within 10 iterations. To sum up, the performance of DAOD stays robust with a large range of parameter choice. Therefore, the parameters do not need to be significantly fine-tuned in practical applications. In this paper, we fixed p = 10, ρ = 1 and σ = 1, T = 10, t = 0.5 and set 1) λ = 50 for Office-31, and 2) λ = 500 for Office-Home and PIE datasets.</p><p>Although DAOD is easy to use, and its parameters do not have to be fine-tuned, we did explore how to further tune these parameters for research purposes. We chose the parameters according to following the rules: 1) The regularization term h 2 k is very important, so we tended to choose a slightly larger σ (σ = 1) to prevent DAOD from degenerating. 2) We chose ρ by following <ref type="bibr" target="#b24">[25]</ref>. 3) p is set following <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b53">[54]</ref>. 4) distribution alignment is inevitable for DAOD, so we chose a larger λ (λ ≥ 50) to make it count.</p><p>We used two types of accuracy <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> to evaluate DAOD:</p><formula xml:id="formula_46">Acc(OS) = 1 C + 1 C+1 c=1 |x : x from class c f (x) = c| |x : x from class c| ,<label>(23)</label></formula><p>and</p><formula xml:id="formula_47">Acc(OS * ) = 1 C C c=1 |x : x from class c f (x) = c| |x : x from class c| ,<label>(24)</label></formula><p>where f is the predicted classifier. Note that Acc(OS) is the main index for evaluating the performance of the UOSDA algorithms <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Results</head><p>The classification accuracy of the UOSDA tasks is shown in <ref type="table" target="#tab_0">Table II</ref>. The following facts can be observed from this table. 1) The closed set algorithm TCA performed poorly on most tasks, even worse than the standard OSNN algorithm, indicating that negative transfer occurred.</p><p>2) All open set algorithms achieved better classification accuracy than OSNN on most tasks. This is because the source samples and the known target samples have different distributions.</p><p>3) DAOD achieved much better performance Acc(OS) than the six baseline algorithms on most tasks (24 out of 38). The average classification accuracy (Acc(OS), Acc(OS * )) of DAOD on the 38 tasks was 69.3%, 70.4% respectively, gaining a performance improvement of 3.4%, 3.6% compared to the best baseline OSBP. 4) Generally, JDA+OSNN, JGSA+OSNN and ATI+OSNN algorithms did not perform as well as DAOD. A major limitation of these algorithms may be that they omit the selected unknown target samples when they construct a latent space to match the distributions for the known classes. This may result in the unknown samples being mixed with the known samples in the latent space. In DAOD, the negative term R s u,C+1 helps DAOD to avoid the problem suffered by JDA, JGSA and ATI. 5) The performance of the OSPB algorithm was generally worse than that of DAOD. The main reasons may be that: 1) OSBP only matches the marginal distributions, not the joint distributions; 2) OSBP does not keep the unknown target samples away from the known source samples, with the result that many unknown target samples are recognized as known samples. DAOD, however, uses the negative term R s u,C+1 to separate the source samples and unknown target samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Open Set Parameters Analysis</head><p>From our analysis of the open set parameters α and γ, we find that the relationship between α and γ is closely related to another parameter, the difference δ := α − γ.</p><p>We conducted experiments on the Office-31 dataset with α ranging from 0.2 to 1.2 and δ ranging from −0.2 to α. Due to space limitations, the average results on Office-31 are reported in <ref type="figure" target="#fig_3">Fig. 3</ref>. According to <ref type="figure" target="#fig_3">Fig. 3</ref>, we made the following observations: 1) As δ increased, the accuracy of the unknown classes also increased, since a larger positive term R t u,C+1 (h) means that more samples are recognized as unknown.</p><p>2) When δ &lt; 0 (α &lt; γ), for almost all α ∈ [0.2, 1.2], the performance Acc(OS) was poorer than the best baseline algorithm (dashed line). This is because when δ &lt; 0, more samples are recognized as known classes. Our theoretical results support this observation (Theorem 1) since the positive term's coefficient 1/(1 − π t C+1 ) is larger than the negative term's coefficient 1. Thus, δ should be larger than 0 (α &gt; γ).</p><p>3) All figures in <ref type="figure" target="#fig_3">Fig. 3</ref> are similar for almost all α from 0.4 to 1.2, which implies that α may be not the most important factor influencing the performance of DAOD. Rather, the difference δ is likely to be more important. 4) Performance Acc(OS) begins to decrease when δ is larger than 0.25 because more known samples are classified as unknown with a larger δ.  In the figures, the difference δ is not larger than α, since the parameter γ is required to be larger than or equal to 0. If δ &gt; 0, α is larger than γ. If δ &lt; 0, γ is larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Parameter Sensitivity, Ablation Study and Convergence</head><p>Analysis. These studies were conducted on different types of datasets to demonstrate that: 1) a wide range of parameter values can be chosen to obtain satisfactory performance, and 2) the open set difference and distribution alignment term are important and necessary. We evaluated important parameters λ, σ, ρ, p, t, α, γ and T , reporting the average results for datasets Office-31, Office-Home and PIE respectively. The dashed line denotes the results of the best baseline algorithm with each dataset.</p><p>Distribution Alignment λ. We ran DAOD with varying values of λ. <ref type="figure" target="#fig_7">Fig. 4</ref>(a) plots the classification accuracy w.r.t. to different values of λ. From this figure, we can see that: 1) When λ = 0, the performance was the worst than the baseline. 2) After the increasing of the λ from 0 to 50, the performance dramatically increased to equal that of the baseline. 3) From 50 to 1000, DAOD was stable with values of around 0.85, 0.7, and 0.65 on the three datasets. Overall, the performance of DAOD with most values of λ was better than the baselines. We also found that larger values of λ resulted in a better distribution alignment, and, if we chose λ from [50, 1000], we obtained better results than the best baseline algorithm.</p><p>Manifold Regularization ρ. In these experiments, we ran DAOD with varying values of ρ. Larger values of ρ increase the importance of manifold consistency in DAOD. From <ref type="figure" target="#fig_7">Fig.  4(b)</ref>, we can see that: 1) DAOD's performance was steady and consistently good when ρ ∈ [0, 1]. 2) But, after the increasing of ρ from 1 to 5, its performance dramatically dropped below the baseline. 3) Further, DAOD's continued to fall below the baseline from 5 to 50. The reason for this poor performance at ρ ∈ [5, 50] is that when ρ is large, DAOD mainly focuses on the geometric information of the samples and ignores other information. Choosing λ from [0, 1], however, provides the best results.</p><p>#Nearest Neighbors p. We ran DAOD with varying values of p. If p → +∞, two samples which are not at all similar are connected. If p → 0, limited similarity information between samples is captured, thus p should not be too large or too small. <ref type="figure" target="#fig_7">Fig. 4(c)</ref> shows that if p is selected from <ref type="bibr">[2,</ref><ref type="bibr" target="#b31">32]</ref>, the performance of our algorithm is better than the baseline. When p &gt; 32, the performance of PIE was worse than the baseline. One reason may be that when p is large, the samples from different classes are connected, resulting in that samples from different classes share similar scores. From <ref type="figure" target="#fig_7">Fig. 4(c)</ref>, p can be selected from <ref type="bibr">[2,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Regularization σ. We ran DAOD with varying values of σ and plotted the classification accuracy as shown in <ref type="figure" target="#fig_7">Fig.  4(d)</ref>. Theoretically, when σ → 0, the classifier degenerates and overfitting occurs. When σ → +∞, the classifier obtains a trivial result. From <ref type="figure" target="#fig_7">Fig. 4(d)</ref>, we can see that: 1) When σ = 0, the performance was the worst and also much worse than the baseline. 2) However, after increasing of σ from 0 to 0.2, performance dramatically increased commensurate with the baseline. 3) From 0.2 to 1.6, DAOD was stable with values at around 0.85, 0.7 and 0.65 on three datasets. 4) When σ &gt; 1.6, the performance dramatically dropped again to below the baseline. According to <ref type="figure" target="#fig_7">Fig. 4(d)</ref>, we can choose σ ∈ [0.2, 1.6].</p><p>Threshold t. <ref type="figure" target="#fig_7">Fig. 4(b)</ref> shows the classification accuracy with varying values of t. Theoretically, the threshold t is determined by the openness O. When openness O → 1, t → 0. When openness O → 0, t → 1. However, according to <ref type="figure" target="#fig_7">Fig.  Fig. 4</ref>: Parameter sensitivity study, ablation study and convergence analysis of the proposed DAOD algorithm. 4(e), DAOD performed steadily when the threshold t varies from [0, 0.9]. This is because: 1) As the number of iterations T increases, the effect of t tapers off. 2) OSNN cv -t is not sensitive to t.</p><p>Open Set Parameter α. <ref type="figure" target="#fig_7">Fig. 4</ref>(f) plots the classification accuracy w.r.t. different values. Theoretically, when α → 0, the classifier can not recognize unknown samples, whereas, when α → +∞, the classifier classifies all samples as unknown. These conjectures are verified by the results in <ref type="figure" target="#fig_7">Fig. 4(f)</ref>, where the performance reaches its maximal point at around α = 0.3 and then gradually drops as α increases. Performance was worst and lower than the baselines when α &gt; 0.4, because at this parameter setting many samples from known classes are classified as unknown. In general, we can choose α from [0.2, 0.4].</p><p>Open Set Parameter γ. The classification accuracy w.r.t. different values of γ is shown in <ref type="figure" target="#fig_7">Fig. 4(g)</ref>. Theoretically, when γ → +∞, DAOD keeps the unknown target samples away from known source samples. As a result, few samples are classified as unknown classes. When γ → 0, more samples are classified as unknown, and when γ &lt; 0.15, its performance was worse than the baselines. Conversely, as γ increased, DAOD's performance dramatically increased, reaching its maximal value at around γ = 0.3 before gradually dropping again as γ continues to increase. In general, we can choose γ ∈ [0.15, 0.5].</p><p>Ablation Study. 1) α and γ are the two parameters that control the contribution of the open set difference. As shown in <ref type="figure" target="#fig_7">Fig. 4(f)</ref>, setting α closer to 0 reduces the contribution of the open set difference and performance degrades compared to the optimal value of about α = 0.3. Further, as shown in <ref type="figure" target="#fig_7">Fig.  4(g)</ref>, setting γ closer to 0 also reduces the contribution of the open set difference and again performance degrades compared to the optimal value of about γ = 0.3. Therefore, we can safely draw the conclusion that our proposed open set difference is a necessary term for open set domain adaption. 2) λ is the parameter that controls the contribution of the distribution discrepancy. As shown in <ref type="figure" target="#fig_7">Fig. 4(a)</ref>, when λ is 0, performance is much worse than at other values, which shows that this term also makes a significant contribution to the final domain adaptation performance. 3) ρ controls the contribution of the manifold regularization. <ref type="figure" target="#fig_7">Fig. 4(b)</ref> shows there is no significant change in performance when ρ is set in the range 0 to 1. These results indicate that ρ makes no significant contributions to DAOD and may even negatively effect its performance with values from 5 to 50. Though the contribution of manifold regularization is not significant, more experiments in Appendix E show that the manifold regularization is necessary. 4) σ is used to avoid overfitting. As shown in <ref type="figure" target="#fig_7">Fig. 4(d)</ref>, performance drops significantly when σ is set to 0. Thus, the term h 2 k is important to our algorithm.</p><p>Convergence Analysis. The results of the convergence analysis on the number of iterations T are provided in <ref type="figure" target="#fig_7">Fig.  4(h)</ref>. As shown, DAOD reached a steady performance in only a few iterations (T &lt; 10). This is a clear indication of the advantages of DAOD's ability to be trained in unsupervised open set domain adaptation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>To the best of our knowledge, this is the first work to present a theoretical analysis for open set domain adaptation. In deriving a theoretical bound, we discovered a special term, open set difference, which is crucial for recognizing unknown target samples. Using this open set difference, we then constructed an unsupervised open set domain adaptation algorithm, called Distribution Alignment with Open Difference (DAOD). Extensive experiments show that DAOD outperforms several competitive algorithms.</p><p>In the future, we will mainly focus on universal domain adaptation <ref type="bibr" target="#b54">[55]</ref>, which is a unified domain adaptation framework that includes closed set domain adaptation, open set domain adaptation and partial domain adaptation <ref type="bibr" target="#b55">[56]</ref> .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. APPENDIX A: DEFINITION, NOTATIONS AND PROOF FOR THEOREM 1</head><p>The necessary definitions and notations for these Appendices follow, including a restatement of some definitions and notations in the main text. Note that, although some notations and definitions coincide with the definitions and notations defined in the main text of the paper, the notions and definitions introduced here strictly follow probability theory.</p><p>Source domain, target domain and distributions.</p><p>1. Let (Ω, A , P ) be a probability space, where Ω is the original space, A is the σ-algebra on Ω, and P is the probability measure on (Ω, A ). <ref type="figure">(X , B)</ref> be the measure space, where X is the sample space (feature or input) and B is the σ-algebra on X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Let</head><formula xml:id="formula_48">3. Let (Y s , C ) and (Y t , D) be two measure spaces, where the label spaces Y s = {y c } C c=1 , Y t = {y c } C+1 c=1</formula><p>are one-hot set (y C+1 means unknown classes) and C , D are σ-algebras, which consist of all subsets of Y s and Y t respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Denote the random variables</head><formula xml:id="formula_49">X s , X t , Y s , Y t as: X s : (Ω, A ) → (X , B), X t : (Ω, A ) → (X , B), Y s : (Ω, A ) → (Y s , C ), Y t : (Ω, A ) → (Y t , D),<label>(1)</label></formula><p>where the notation X : (Ω, A ) → (Z, E ) means X is a measurable map related to measurable spaces (Ω, A ) and (Z, E ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>The joint random variables are denoted as:</p><formula xml:id="formula_50">X s × Y s : (Ω, A ) → (X × Y s , B ⊗ C ) ω → (X s (ω), Y s (ω)) and X t × Y t : (Ω, A ) → (X × Y t , B ⊗ D) ω → (X t (ω), Y t (ω)),</formula><p>where ⊗ is the direct product of two measurable space (Ω, F 1 ) and (Ω, Notations P X s Y s and P X t Y t denote the source joint distribution and target joint distribution respectively. Hence, given any measurable set U ∈ B ⊗ C and V ∈ B ⊗ D, the value P X s Y s (U ) and P X t Y t (V ) can be computed as follows: 8. The distribution P X t |Y s is defined as follows: For any measurable set B ∈ B,</p><formula xml:id="formula_51">P X s Y s (U ) = P (ω ∈ Ω : X s × Y s (ω) ∈ U ), P X t Y t (V ) := P (ω ∈ Ω : X t × Y t (ω) ∈ V ).</formula><formula xml:id="formula_52">P X t |Y s (B) := P (X t ∈ B|Y t ∈ Y s ) = P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) ∈ Y s ) P (ω ∈ Ω : Y t (ω) ∈ Y s ) .</formula><p>9. The distribution P X t |y C+1 is defined as follows: For any measurable set B ∈ B,</p><formula xml:id="formula_53">P X t |y C+1 (B) := P (X t ∈ B|Y t ∈ y C+1 ) = P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) = y C+1 ) P (ω ∈ Ω : Y t (ω) = y C+1 ) .</formula><p>(2)</p><p>10. The distribution P X t Y t |Y s is defined as follows: For any measurable set V ∈ B ⊗ D,</p><formula xml:id="formula_54">P X t Y t |Y s (V ) := P (X t × Y t ∈ V |Y t ∈ Y s ) = P (ω ∈ Ω : X t × Y t (ω) ∈ V and Y t (ω) ∈ Y s ) P (ω ∈ Ω : Y t (ω) ∈ Y s ) .<label>(3)</label></formula><p>Restatement of the definitions of risks:</p><p>11. Given the loss function : R C+1 × R C+1 → R + and the scoring function h : X → R C+1 , then the source risk R s (h) and the target risk R t (h) are</p><formula xml:id="formula_55">R s (h) := E (h(X s ), Y s ) = X ×Y s (h(x), y)dP X s Y s (x, y), R t (h) := E (h(X t ), Y t ) = X ×Y t (h(x), y)dP X t Y t (x, y).<label>(4)</label></formula><p>12. The class-prior probability for the unknown target classes is</p><formula xml:id="formula_56">π t C+1 = P (w ∈ Ω : Y t (ω) = y C+1 ).<label>(5)</label></formula><p>13. The target risk of h w.r.t. limited to the source label space Y s is:</p><formula xml:id="formula_57">R t * (h) := X ×Y s (h(x), y)dP X t Y t (x, y) 1 − π t C+1 .<label>(6)</label></formula><p>14. The target risk of h w.r.t. for the unknown target classes is:</p><formula xml:id="formula_58">R t C+1 (h) := X (h(x), y C+1 )dP X t |y C+1 (x).<label>(7)</label></formula><p>15. The risks that unlabeled samples are regarded as unknown samples are:</p><formula xml:id="formula_59">R s u,C+1 (h) := E (h(X s ), y C+1 ) = X (h(x), y C+1 )dP X s (x), R t u,C+1 (h) := E (h(X t ), y C+1 ) = X (h(x), y C+1 )dP X t (x)<label>(8)</label></formula><p>Next, we prove several results which will be used in the proof of Theorem 1.</p><formula xml:id="formula_60">Proposition 1. If (h(x), y) is a P X t Y t -measurable function, then R t (h) = (1 − π t C+1 )R t * (h) + π t C+1 R t C+1 (h). Proof for Proposition 1. First consider π t C+1 P X t |y C+1 (B). For any measurable set B ∈ B, π t C+1 P X t |y C+1 (B) = P (w ∈ Ω : Y t (ω) = y C+1 ) · P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) = y C+1 ) P (ω ∈ Ω : Y t (ω) = y C+1 ) = P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) = y C+1 ) = P X t Y t (B × y C+1 ).</formula><p>Hence, π t C+1 dP X t |y C+1 (x) = dP X t Y t (x, y C+1 ).</p><formula xml:id="formula_61">Now consider (1 − π t C+1 )R t * (h) + π t C+1 R t C+1 (h), (1 − π t C+1 )R t * (h) + π t C+1 R t C+1 (h) = X ×Y s (h(x), y)dP X t Y t (x, y) + π t C+1 X (h(x), y C+1 )dP X t |y C+1 (x) = y∈Y s X (h(x), y)dP X t Y t (x, y) + π t C+1 X (h(x), y C+1 )dP X t |y C+1 (x) = y∈Y s X (h(x), y)dP X t Y t (x, y) + X (h(x), y C+1 )dP X t Y t (x, y C+1 ) = y∈Y t X (h(x), y)dP X t Y t (x, y) = X ×Y t (h(x), y)dP X t Y t (x, y) =R t (h).</formula><p>Proposition 2. Given the scoring functions h,h, if (h(x),h(x)) is a P X t -measurable function, then</p><formula xml:id="formula_62">X ×Y t (h(x),h(x))dP X t Y t |Y s (x, y) = X ×Y s (h(x),h(x))dP X t Y t (x, y) 1 − π t C+1 = X (h(x),h(x))dP X t |Y s (x)</formula><p>Proof for Proposition 2. Here we denote P X t |yc , π t c , ∀ c ∈ 1, ...C as: for any B ∈ B,</p><formula xml:id="formula_63">P X t |yc (B) := P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) = y c ) P (ω ∈ Ω : Y t (ω) = y c ) , π t c := P (ω ∈ Ω : Y t (ω) = y c ).<label>(9)</label></formula><p>Firstly, we prove that for any</p><formula xml:id="formula_64">B ∈ B, C c=1 π t c · P X t |yc (B) = P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) ∈ Y s ) = P X t Y t (B × Y s ).<label>(10)</label></formula><p>That is because</p><formula xml:id="formula_65">C c=1 π t c · P X t |yc (B) = C c=1 P (ω ∈ Ω : Y t (ω) = y c ) · P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) = y c ) P (ω ∈ Ω : Y t (ω) = y c )) = C c=1 P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) = y c ) = P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) ∈ Y s ) = P X t Y t (B × Y s ).</formula><p>By Fubini theorem,</p><formula xml:id="formula_66">X ×Y s (h(x),h(x))dP X t Y t (x, y) 1 − π t C+1 = X y∈Y s (h(x),h(x))dP X t Y t (x, y) 1 − π t C+1 = using (10) C c=1 π t c X (h(x),h(x))dP X t |yc (x) 1 − π t C+1<label>(11)</label></formula><p>Secondly, we need to prove that for any B ∈ B,</p><formula xml:id="formula_67">C c=1 π t c · P X t |yc (B) = (1 − π t C+1 ) · P X t |Y s (B).</formula><p>That is because</p><formula xml:id="formula_68">(1 − π t C+1 ) · P X t |Y s (B) = P (w ∈ Ω : Y t (ω) ∈ Y s ) · P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) ∈ Y s ) P (ω ∈ Ω : Y t (ω) ∈ Y s ) =P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) ∈ Y s ) = C i=1 π t c · P X t |yc (B) Hence, C c=1 π t c X (h(x),h(x))dP X t |yc (x) 1 − π t C+1 = X (h(x),h(x))dP X t |Y s (x).<label>(12)</label></formula><p>According to <ref type="bibr" target="#b10">(11)</ref> and <ref type="formula" target="#formula_4">(12)</ref>, we obtain</p><formula xml:id="formula_69">X ×Y s (h(x),h(x))dP X t Y t (x, y) 1 − π t C+1 = X (h(x),h(x))dP X t |Y s (x).<label>(13)</label></formula><p>Next, we prove that</p><formula xml:id="formula_70">X ×Y t (h(x),h(x))dP X t Y t |Y s (x, y) = X ×Y s (h(x),h(x))dP X t Y t (x, y) 1 − π t C+1 .</formula><p>Note that (h(x),h(x)) is not related to variable y, thus we only need to prove that</p><formula xml:id="formula_71">P X t Y t |Y s (B × Y t ) = P X t Y t (B × Y s ) 1 − π t C+1 .</formula><p>where B ∈ B. It is clear that</p><formula xml:id="formula_72">P X t Y t |Y s (B × Y t ) = P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) ∈ Y s ∩ Y t ) P (ω ∈ Ω : Y t (ω) ∈ Y s ) = P X t Y t (B × Y s ) 1 − π t C+1 .</formula><p>hence,</p><formula xml:id="formula_73">X ×Y t (h(x),h(x))dP X t Y t |Y s (x, y) = X ×Y s (h(x),h(x))dP X t Y t (x, y) 1 − π t C+1 .<label>(14)</label></formula><p>According to <ref type="bibr" target="#b12">(13)</ref> and <ref type="formula" target="#formula_4">(14)</ref>, we obtain the result.</p><formula xml:id="formula_74">Proposition 3. If (h(x), y) is a P X t Y t -measurable function, then R t * (h) = X ×Y t (h(x), y)dP X t Y t |Y s (x, y)</formula><p>. Proof for Proposition 3. We first prove that: for any measurable set</p><formula xml:id="formula_75">V ∈ B ⊗ C ⊂ B ⊗ D, we have P X t Y t |Y s (V ) = 1 1 − π t C+1 P X t Y t (V ). Note V ∈ B ⊗ C ⊂ B ⊗ D. P X t Y t |Y s (V ) = P (ω ∈ Ω : X t × Y t (ω) ∈ V and Y t (ω) ∈ Y s ) P (ω ∈ Ω : Y t (ω) ∈ Y s ) = P (ω ∈ Ω : X t × Y t (ω) ∈ V ) P (ω ∈ Ω : Y t (ω) ∈ Y s ) = 1 1 − π t C+1 P X t Y t (V ). Hence X ×Y s (h(x), y)dP X t Y t |Y s (x, y) = 1 1 − π t C+1 X ×Y s (h(x), y)dP X t Y t (x, y).<label>(15)</label></formula><p>In addition, we note that, for any B ∈ B,</p><formula xml:id="formula_76">P X t Y t |Y s (B × y C+1 ) = P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) ∈ y C+1 ∩ Y s ) P (ω ∈ Ω : Y t (ω) ∈ Y s ) = P (ω ∈ Ω : X t (ω) ∈ B and Y t (ω) ∈ ∅) P (ω ∈ Ω : Y t (ω) ∈ Y s ) = 0.</formula><p>Hence,</p><formula xml:id="formula_77">X ×Y s (h(x), y)dP X t Y t |Y s (x, y) = X ×Y t (h(x), y)dP X t Y t |Y s (x, y).<label>(16)</label></formula><p>Combining <ref type="formula" target="#formula_4">(15)</ref> and <ref type="formula" target="#formula_4">(16)</ref>, we have</p><formula xml:id="formula_78">X ×Y t (h(x), y)dP X t Y t |Y s (x, y) = 1 1 − π t C+1 X ×Y s (h(x), y)dP X t Y t (x, y).</formula><p>Next we provide a proof for Theorem 1.</p><p>Proof of Theorem 1. We firstly present the main idea of the proof. According to Proposition 1, we have that</p><formula xml:id="formula_79">R t (h) = (1 − π t C+1 )R t * (h) + π t C+1 R t C+1 (h), then R t (h) 1 − π t C+1 − R s (h) = R t * (h) − R s (h) + π t C+1 1 − π t C+1 R t C+1 (h),<label>(17)</label></formula><p>thus we separate the proof into two main steps. For the first step, we mainly consider that R t * (h) − R s (h). For the second step, we investigate R t C+1 (h). If we can prove that</p><formula xml:id="formula_80">R t * (h) − R s (h) ≤ Λ + d H (P X t |Y s , P X s ),<label>(18)</label></formula><p>and</p><formula xml:id="formula_81">π t C+1 1 − π t C+1 R t C+1 (h) ≤d H (P X t |Y s , P X s ) + R t u,C+1 (h) 1 − π t C+1 − R t u,C+1 (h),<label>(19)</label></formula><p>then combining <ref type="bibr" target="#b17">(18)</ref>, <ref type="bibr" target="#b18">(19)</ref> with <ref type="formula" target="#formula_4">(17)</ref>, we have</p><formula xml:id="formula_82">R t (h) 1 − π t C+1 ≤ R s (h) + 2d H (P X t |Y s , P X s ) + Λ + ∆ o .</formula><p>Step 1. We claim that R t</p><formula xml:id="formula_83">* (h) − R s (h) ≤ Λ + d H (P X t |Y s , P X s ). Firstly, we note that R t * (h) − R s (h) = X ×Y t (h(x), y)dP X t Y t |Y s (x, y) − X ×Y s (h(x), y)dP X s Y s (x, y) ≤ inequality1 R t * (h) + X ×Y t (h(x),h(x))dP X t Y t |Y s (x, y) +R s (h) − X ×Y s (h(x),h(x))dP X s Y s (x, y),<label>(20)</label></formula><p>whereh is any scoring function in H. In inequality 1, we have used following triangle inequality: (h(x), y) − (h(x), y) ≤ (h(x),h(x)). Then according to the definition of P X t Y t |Y s , we can check that</p><formula xml:id="formula_84">X ×Y t (h(x),h(x))dP X t Y t |Y s (x, y) = X (h(x),h(x))dP X t |Y s (x).<label>(21)</label></formula><p>The proof of (21) can be found in Proposition 2. By Fubini's theorem, it is also easy to check that</p><formula xml:id="formula_85">X ×Y s (h(x),h(x))dP X s Y s (x, y) = X (h(x),h(x))dP X s (x).<label>(22)</label></formula><p>Based on <ref type="bibr" target="#b19">(20)</ref>, <ref type="bibr" target="#b20">(21)</ref>, <ref type="bibr" target="#b21">(22)</ref> and the definition of the discrepancy distance, we have</p><formula xml:id="formula_86">R t * (h) − R s (h) ≤ R t * (h) + R s (h) + X (h(x),h(x))dP X t |Y s (x) − X (h(x),h(x))dP X s (x) ≤ R t * (h) + R s (h) + d H (P X t |Y s , P X s ). Hence, R t * (h) − R s (h) ≤miñ h∈H R t * (h) + R s (h) + d H (P X t |Y s , P X s ) =Λ + d H (P X t |Y s , P X s ).<label>(23)</label></formula><p>Step 2. We claim that</p><formula xml:id="formula_87">π t C+1 1−π t C+1 R t C+1 (h) ≤ d H (P X t |Y s , P X s ) + R t u,C+1 (h) 1−π t C+1 − R t u,C+1 (h). We note that R t u,C+1 (h) − (1 − π t C+1 )R s u,C+1 (h) =π t C+1 R t C+1 (h) + (1 − π t C+1 ) X (h(x), y C+1 )dP X t |Y s (x) − (1 − π t C+1 ) X (h(x), y C+1 )dP X s (x),<label>(24)</label></formula><p>this is because the following equation</p><formula xml:id="formula_88">R t u,C+1 (h) = X (h(x), y C+1 )dP X t (x) =π t C+1 X (h(x), y C+1 )dP X t |y C+1 (x) + (1 − π t C+1 ) X (h(x), y C+1 )dP X t |Y s (x) =π t C+1 R t C+1 (h) + (1 − π t C+1 ) X (h(x), y C+1 )dP X t |Y s (x).<label>(25)</label></formula><p>According to the definition of the discrepancy distance and the condition that the constant vector value function g = y C+1 ∈ H, we have</p><formula xml:id="formula_89">X (h(x), y C+1 )dP X t |Y s (x) − X (h(x), y C+1 )dP X s (x) = X (h(x), g(x))dP X t |Y s (x) − X (h(x), g(x))dP X s (x) ≤ d H (P X t |Y s , P X s ).<label>(26)</label></formula><p>Combining <ref type="formula" target="#formula_5">(24)</ref> and <ref type="formula" target="#formula_5">(26)</ref>, we show that</p><formula xml:id="formula_90">π t C+1 R t C+1 (h) ≤(1 − π t C+1 )d H (P X t |Y s , P X s ) + R t u,C+1 (h) − (1 − π t C+1 )R t u,C+1 (h).</formula><p>Hence, we have proved</p><formula xml:id="formula_91">π t C+1 1−π t C+1 R t C+1 (h) ≤ d H (P X t |Y s , P X s ) + R t u,C+1 (h) 1−π t C+1 − R t u,C+1 (h).</formula><p>As mentioned above, combining <ref type="bibr" target="#b17">(18)</ref>, <ref type="bibr" target="#b18">(19)</ref> with <ref type="formula" target="#formula_4">(17)</ref>, we have</p><formula xml:id="formula_92">R t (h) 1 − π t C+1 ≤ R s (h) + 2d H (P X t |Y s , P X s ) + Λ + ∆ o .</formula><p>The proof has been completed. </p><formula xml:id="formula_93">Notation Description Notation Description X feature space P X s Y s , P X t Y t source, target joint distributions Y s , Y t source, target label spaces {y c } C c=1 , {y c } C+1 c=1 P X s , P X t source, target marginal distributions X s , X t random variables on the feature space P X s |yc , P X t |yc P (X s |Y s = y c ), P (X t |Y t = y c ) Y s , Y t</formula><p>random variables on the label spaces  source samples for class c (label y c ) T X,c pseudo target samples for class c (label y c ) T X,K pseudo target samples for known classes n s , n t numbers of sets S X , T X , n s c , n t c , n t K numbers of sets S X,c , T X,c , T X,</p><formula xml:id="formula_94">P X t |Y s P (X t |Y t ∈ Y s ) R s (h), R t (h) source,</formula><formula xml:id="formula_95">K K kernel [k(x i , x j )], here x i , x j ∈ S X ∪ T X M MMD matrix L laplacian matrix Y label matrix Y label matrix A, A diagonal matrics</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. APPENDIX B: PROOF FOR THEOREM 2</head><p>Proof for Theorem 2. The proof of Theorem 2 is divided into three parts. Claim 1: lim</p><formula xml:id="formula_96">β 2 →+∞ L(β) = +∞.</formula><p>It is easy to check that tr(β T K(λM + ρL)Kβ) + σtr(β T Kβ) ≥ 0, since the projected MMD distance and manifold regularization are not negative.</p><formula xml:id="formula_97">Consider (Y − β T K)A 2 F − γ ( Y − β T K) A 2 F .</formula><p>Since the kernel k is universal and γ &lt; 1, the matrix K(A 2 − γ A 2 )K is symmetric and positive definite, which can be written as</p><formula xml:id="formula_98">K(A 2 − γ A 2 )K = OΛO T ,</formula><p>where O is the orthogonal matrix, and Λ is the diagonal matrix, whose diagonal elements {λ i } n s +n t i=1 are positive. Then</p><formula xml:id="formula_99">(Y − β T K)A 2 F − γ ( Y − β T K) A 2 F = tr[(Y − β T K)A 2 (Y − β T K) T ] − γtr[( Y − β T K) A 2 ( Y − β T K) T ] = tr[β T K(A 2 − γ A 2 )Kβ] − 2tr[(YA 2 K − γ Y A 2 K)β] + constant = tr[β T OΛO T β] − 2tr[(YA 2 K − γ Y A 2 K)β] + constant = tr[(β T O)Λ(β T O) T ] − 2tr[(YA 2 K − γ Y A 2 K)β] + constant ≥ ctr[(β T O)I(β T O) T ] − O( β 2 ) + constant = ctr[β T β] − O( β 2 ) + constant,</formula><p>where c is the smallest diagonal element of the diagonal matrix Λ. Therefore,</p><formula xml:id="formula_100">lim β 2 →+∞ L(β) ≥ lim β 2 →+∞ ctrβ T β − O( β 2 ) + constant = lim β 2 →+∞ c β 2 2 − O( β 2 ) + constant = +∞</formula><p>Claim 2: There exist optimizers. In Claim 1, we have proven that lim</p><formula xml:id="formula_101">β 2 →+∞ L(β) = +∞.</formula><p>which implies that there exists a large constant r &gt; 0, such that L(β) &gt; L(0), for any β ∈ R (C+1)×(n s +n t ) \ B r (0), where B r <ref type="figure">(0)</ref> is an open ball with a radius pf r and a center of 0.</p><p>Since L is a continuous function and the closed ball B r (0) is a compact set, we know that there exist optimizers for L and these points must be contained in the open ball B r (0).</p><p>Claim 3: The solution is unique. If a point β 0 is a minimizer, then β 0 satisfies the following equation:</p><formula xml:id="formula_102">∂L ∂β (β 0 ) = 0.</formula><p>If the solution of ∂L ∂β (β 0 ) = 0 is unique, then the solution must be the unique minimizer. We compute ∂L ∂β (β 0 ) = 0,</p><formula xml:id="formula_103">0 = −2(KA 2 Y T − γK A 2 Y T ) + 2σKβ + 2(KA 2 K − γK A 2 K)β + 2K(λM + ρL)Kβ.<label>(27)</label></formula><p>Noting that the solution of (27) is unique and can be written as:</p><formula xml:id="formula_104">β = (A 2 − γ A 2 + λM + ρL)K + σI −1 (A 2 Y T − γ A 2 Y T ).</formula><p>With Claim 1, Claim 2 and Claim 3, Theorem 2 is proven.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPENDIX C: LEARNING BOUND BASED ON MMD</head><p>We give a theoretical bound for OSDA that shows how MMD controls generalization performance in the case of the squared loss (y, y ) = y − y 2 2 . One of the main techniques is that we use MMD distance to bound the discrepancy distance d H . This technique is firstly given by Ghifary et al. <ref type="bibr">[1]</ref>. We firstly review the technique, see Lemma 2.</p><p>Definition 2 and Lemma 1, along with details of the proof, can be found in <ref type="bibr">[1]</ref> (Definition 8 and Lemma 4).</p><p>Definition 2 (Multiplication Operator <ref type="bibr">[1]</ref>). Let C(X ) be the space of continuous functions on the compact set X equipped with the supremum norm · ∞ , Given g ∈ C(X ), define the multiplication operator as the bounded linear operator M g : C(X ) → C(X ) given by:</p><formula xml:id="formula_105">M g (f )(x) = g(x)f (x), f or any f ∈ C(X ).<label>(28)</label></formula><p>Lemma 1. Given g, f ∈ H k , where H k is equipped with a universal kernel, it holds that</p><formula xml:id="formula_106">M g (f ) H k = g · f H k ≤ g ∞ · f H k .</formula><p>The details of Lemma 1 is in <ref type="bibr">[1]</ref>.</p><p>Lemma 2 (Domain Scatter Bounds Discrepancy <ref type="bibr">[1]</ref>). Let H k be an RKHS with a universal kernel. Suppose that = y − y 2 2 is the squared loss, and consider the hypothesis set</p><formula xml:id="formula_107">H = {f ∈ H k , f H k ≤ M and f ∞ ≤ m},</formula><p>where M, m &gt; 0 is a constant. Let P 1 and P 2 be two distributions over X . Then the following inequality holds:</p><formula xml:id="formula_108">d H (P 1 , P 2 ) ≤ 4M mMMD H k (P 1 , P 2 ).<label>(29)</label></formula><p>Proof for Lemma 2. Let f,f ∈ H. Observe that</p><formula xml:id="formula_109">d H (P 1 , P 2 ) = sup f,f ∈H E x∼P1 [(f (x) −f (x)) 2 ] − E x∼P2 [(f (x) −f (x)) 2 ] = sup f,f ∈H M hc (h c ), E x∼P1 ϕ(x) − E x∼P1 ϕ(x) H k + sup h∈H h C+1 , E x∼P1 ϕ(x) − E x∼P1 ϕ(x) H k ≤ sup h∈H C+1 c=1 M hc (h c ) H k · E x∼P1 ϕ(x) − E x∼P2 ϕ(x) H k + sup h∈H h C+1 H k · E x∼P1 ϕ(x) − E x∼P2 ϕ(x) H k ≤ sup h∈H C+1 c=1 h c ∞ · h c H k MMD H k (P 1 , P 2 ) + sup h∈H h C+1 H k MMD H k (P 1 , P 2 ), here we use Lemma 1 ≤ (C + 1)M m + M MMD H k (P 1 , P 2 ). Note that d H (P 1 , P 2 ) = sup h,h∈H E x∼P1 [(h(x) −h(x)) 2 ] − E x∼P2 [(h(x) −h(x)) 2 ] . we have d H (P 1 , P 2 ) ≤ LMMD H k (P 1 , P 2 ),</formula><p>where L = max{4M m(C + 1), (C + 1)M m + M }.</p><p>Proof of the main result: Theorem 4. Let H k be an RKHS with a universal kernel. Suppose that (y, y ) = y − y 2 2 is the squared loss, and consider the hypothesis set H = H * ∪ F, where</p><formula xml:id="formula_110">H * = {[h 1 , ..., h C+1 ] T : h c ∈ H k , h c H k ≤ M, h c ∞ ≤ m},</formula><p>and F is a set which only contains a constant vector-value function [0, ..., 0, ..., 1] T ∈ R (C+1) , here M, m &gt; 0 is a constant. Given the source domain and target domain P X s Y s and P X t Y t , if for any h ∈ H, (h(x), y) is measurable respect to P X s Y s and P X t Y t , then</p><formula xml:id="formula_111">R t (h) 1 − π t C+1 ≤ R s (h) + LMMD H k (P X s , P X t |Y s ) + ∆ o + LMMD H k (P X s , P X t |Y s ) + Λ, where Λ = min h∈H R s (h) + R t * (h), L = max{4M m(C + 1), (C + 1)M m + M } and open set difference ∆ o = R t u,C+1 (h) 1−π t C+1 − R s u,C+1 (h)</formula><p>. Proof for Theorem 4. Step 1. Givenh ∈ H, using the triangle inequality of L 2 norm and Proposition 2, we have</p><formula xml:id="formula_112">(1 − π t C+1 )R t * (h) = X ×Y s h(x) − y 2 2 dP X t Y t (x, y) ≤ X ×Y s h (x) − y 2 2 dP X t Y t (x, y) + X ×Y s h(x) −h(x) 2 2 dP X t Y t (x, y) = (1 − π t C+1 )R t * (h) + (1 − π t C+1 ) X h(x) −h(x) 2 2 dP X t |Y s (x) = (1 − π t C+1 )R t * (h) + (1 − π t C+1 )E P X t |Y s h −h 2 2 .<label>(32)</label></formula><p>Note that</p><formula xml:id="formula_113">E P X t |Y s (h,h) = E P X t |Y s h −h 2 2 ≤ E P X s h −h 2 2 + d H (P X s , P X t |Y s ) = E P X s (h,h) + d H (P X s , P X t |Y s ), we obtain (1 − π t C+1 )R t * (h) ≤ (1 − π t C+1 )R t * (h) + (1 − π t C+1 )E P X s (h,h) + (1 − π t C+1 )d H (P X s , P X t |Y s )<label>(33)</label></formula><p>By Fubini Theorem and triangle inequality for L 2 norm:</p><formula xml:id="formula_114">E P X s (h,h) = X h(x) −h(x) 2 2 dP X s (x) = X ×Y s h(x) −h(x) 2 2 dP X s Y s (x, y) ≤ X ×Y s h (x) − y 2 2 dP X s Y s (x, y) + X ×Y s h(x) − y 2 2 dP X s Y s (x, y) = R s (h) + R s (h).<label>(34)</label></formula><p>Step 2. We claim that π t</p><formula xml:id="formula_115">C+1 R t C+1 (h) ≤ (1 − π t C+1 )d H (P X t |Y s , P X s ) + R t u,C+1 (h) − (1 − π t C+1 )R t u,C+1 (h). Firstly, R t u,C+1 (h) = X (h(x), y C+1 )dP X t (x) = π t C+1 X (h(x), y C+1 )dP X t |y C+1 (x) + (1 − π t C+1 ) X (h(x), y C+1 )dP t X t |Y s (x).<label>(35)</label></formula><p>We note that</p><formula xml:id="formula_116">R t C+1 (h) = X (h(x), y C+1 )dP X t |y C+1 (x).<label>(36)</label></formula><p>Therefore, according to <ref type="bibr" target="#b34">(35)</ref> and <ref type="bibr" target="#b35">(36)</ref>, we have</p><formula xml:id="formula_117">R t u,C+1 (h) − (1 − π t C+1 )R s u,C+1 (h) =π t C+1 R t C+1 (h) + (1 − π t C+1 ) X (h(x), y C+1 )dP X t |Y s (x) − (1 − π t C+1 ) X (h(x), y C+1 )dP X s (x).<label>(37)</label></formula><p>According to the definition of discrepancy distance and the condition of the constant vector value function g := y C+1 ∈ H, we have</p><formula xml:id="formula_118">X (h(x), y C+1 )dP X t |Y s (x) − X (h(x), y C+1 )dP X s (x) ≤ d H (P X t |Y s , P X s ).<label>(38)</label></formula><p>Combining <ref type="formula" target="#formula_16">(37)</ref> and <ref type="bibr" target="#b37">(38)</ref>, we show that</p><formula xml:id="formula_119">π t C+1 R t C+1 (h) ≤(1 − π t C+1 )d H (P X t |Y s , P X s ) + R t u,C+1 (h) − (1 − π t C+1 )R t u,C+1 (h).<label>(39)</label></formula><p>Step 3. Using Proposition 1, Lemma 3, inequalities (33), <ref type="bibr" target="#b33">(34)</ref>, <ref type="bibr" target="#b38">(39)</ref> and inequality</p><formula xml:id="formula_120">√ a + b ≤ √ a + √ b (a, b ≥ 0), we get R t (h) ≤ 1 − π t C+1 R t * (h) + π t C+1 R t C+1 (h) Using prposition 1 and √ a + b ≤ √ a + √ b ≤ 1 − π t C+1 R t * (h) + E P X s (h,h) + d H (P X s , P X t |Y s ) + π t C+1 R t C+1 (h)</formula><p>Above inequality uses <ref type="bibr" target="#b32">(33)</ref> and</p><formula xml:id="formula_121">√ a + b ≤ √ a + √ b ≤ 1 − π t C+1 R s (h) + R t * (h) + R s (h) + L(1 − π t C+1 )MMD H k (P X s , P X t |Y s ) + π t C+1 R t C+1 (h)</formula><p>Above inequality uses (34) Lemma 3 and</p><formula xml:id="formula_122">√ a + b ≤ √ a + √ b ≤ 1 − π t C+1 R s (h) + R t * (h) + R s (h) + L(1 − π t C+1 )MMD H k (P X s , P X t |Y s ) + √ ∆, Using (39)<label>(40)</label></formula><p>where</p><formula xml:id="formula_123">∆ = (1 − π t C+1 )d H (P X t |Y s , P X s ) + R t u,C+1 (h) − (1 − π t C+1 )R t u,C+1 (h). Consider ∆ with Lemma 3, ∆ 1 − π t C+1 ≤ ∆ o + LMMD H k (P X s , P X t |Y s ).<label>(41)</label></formula><p>From inequalities <ref type="formula" target="#formula_7">(40)</ref>, <ref type="formula" target="#formula_4">(41)</ref>, we have, for anyh ∈ H,</p><formula xml:id="formula_124">R t (h) 1 − π t C+1 ≤ R s (h) + LMMD H k (P X s , P X t |Y s ) + ∆ o + LMMD H (P X s , P X t |Y s ) + R t * (h) + R s (h),</formula><p>Hence, we obtain the result:</p><formula xml:id="formula_125">R t (h) 1 − π t C+1 ≤ R s (h) + LMMD H k (P X s , P X t |Y s ) + ∆ o + LMMD H (P X s , P X t |Y s ) + miñ h∈H R t * (h) + R s (h).</formula><p>IV. APPENDIX D: LEARNING BOUND BASED ON EMPIRICAL VERSION Although Theorem 1 and Theorem 4 give bounds for our problem, it is hard to evaluate because the bounds are expressed based on random variables. An empirical version of those bounds is needed to evaluate the values. Our idea for deriving these bounds is based on the Rademacher complexity and the Natarajan dimension <ref type="bibr">[2]</ref> , which measure the richness of a class of real-valued functions. However, before introducing the Rademacher complexity and Natarajan dimension, we need to introduce the empirical risks.</p><formula xml:id="formula_126">1. R s (h): Given samples S = {(x s 1 , y s 1 ), ..., (x s n s , y s n s )} ∼ P X s Y s i.i.d, let P X s Y s be the empirical distribution respect to S. Then R s (h) := X ×Y s (h(x), y)d P X s Y s (x, y) = 1 n s n s i=1 (h(x s i ), y s i ).<label>(42)</label></formula><p>2. R s u,C+1 (h): Given samples S X = {x s 1 , ..., x s n s } ∼ P X s i.i.d, let P X s be the empirical distribution respect to S X . Then</p><formula xml:id="formula_127">R s u,C+1 (h) := X (h(x), y C+1 )d P X s (x) = 1 n s n s i=1 (h(x s i ), y C+1 ).<label>(43)</label></formula><p>3. R t u,C+1 (h): Given samples T X = {x t 1 , ..., x t n t } ∼ P X t i.i.d, let P X t be the empirical distribution respect to T X . Then</p><formula xml:id="formula_128">R t u,C+1 (h) := X (h(x), y C+1 )d P X t (x) = 1 n t n t i=1 (h(x t i ), y C+1 ).<label>(44)</label></formula><p>We then introduce the definition of Rademacher complexity.</p><p>Definition 3 (Rademacher Complexity). Let F be a class of real-valued functions defined in a space Z. Given sample S = {z 1 , ..., z n } ∈ Z, then the Empirical Rademacher Complexity of F with respect to the sample S is</p><formula xml:id="formula_129">S (F) = E σ [sup f ∈F 1 n n i=1 σ i f (z i )],<label>(45)</label></formula><p>where σ = (σ 1 , ..., σ n ) are Rademacher variables, with σ i s independent uniform random variables taking values in −1, +1. where l ≤ B. Then for a distribution Q on space Z, samples S = {z 1 , ..., z n } ∼ Q i.i.d, we have with probability of at least 1 − δ &gt; 0, for all f ∈ F:</p><formula xml:id="formula_130">R(f ) − R(f ) ≤ 2E S∼Q n S (F) + B 2 log(2/δ) n ,<label>(46)</label></formula><p>where R(f ) := Z l(f (z), z)dQ(z) and R(f ) := 1 n n i=1 l(f (z i ), z i ). Using the same technique of Lemma 4, we can also have: with probability of at least 1 − δ &gt; 0, for all f ∈ F:</p><formula xml:id="formula_131">R(f ) − R(f ) ≤ 2E S∼Q n S (F) + B 2 log(2/δ) n ,</formula><p>Next we provide a proof for Corollary 1.1. We firstly define the Natarajan dimension <ref type="bibr">[2]</ref>, which is a generalization of the VC dimension to classes of multi-class predictors.</p><p>Definition 4 (Shattering <ref type="bibr">[2]</ref>). Given a feature space X , we say that a set U ⊂ X is shattered by H if there exist two functions h 0 , h 1 : U → Y t , such that • For every x ∈ U , h 0 (x) = h 1 (x).</p><p>• For every V ⊂ U , there exists a function h ∈ H such that ∀x ∈ V, h(x) = h 0 (x) and ∀x ∈ U \V, h(x) = h 1 (x).</p><p>Hence, we can define the Natarajan dimension as follows:</p><p>Definition 5 (Natarajan Dimension <ref type="bibr">[2]</ref>). The Natarajan dimension of H, denoted Ndim(H), is the maximal size of a shattered set U ⊂ X .</p><p>It is not hard to see that in the case that there are exactly two classes, Ndim(H) = VCdim(H). Therefore, the Natarajan dimension generalizes the VC dimension. Corollary 1.1 Given a symmetric loss function satisfying the triangle inequality and bounded by B, and a hypothesis H ⊂ {h : X → Y t }, with conditions: 1) g = y C+1 ∈ H and 2) the Natarajan dimension of H is d, if a random labeled samples of size n s is generated by P X s Y s -i.i.d and a random unlabeled samples of size n t is generated by P X t -i.i.d, then for any h ∈ H and δ ∈ (0, 1) with probability at least 1 − 3δ, we have</p><formula xml:id="formula_132">R t (h) 1 − π t C+1 ≤ R s (h) + d H (P X t |Y s , P X s ) + ∆ o + Λ + 4B</formula><p>8d log n s + 16d log(C + 1) + 2 log 2/δ n s + 2B 8d log n t + 16d log(C + 1) + 2 log 2/δ</p><formula xml:id="formula_133">(1 − π t C+1 ) 2 n t , where Λ = min h∈H R s (h) + R t * (h) and empirical open set difference ∆ o = R t u,C+1 (h) 1−π t C+1 − R s u,C+1 (h) . Proof.</formula><p>Step 1. We prove that with probability of at least 1 − δ &gt; 0, for all h ∈ H:  <ref type="bibr">[2]</ref>, we obtain the following bound:</p><formula xml:id="formula_134">R s (h) ≤ R s (h) +</formula><formula xml:id="formula_135">1 n s E σ [sup a∈A n s i=1 σ i a i ] ≤ B 2d log n s + 4d log(C + 1) n s ,</formula><p>where a i is the i-th coordinate value of a and σ = (σ 1 , ..., σ n ) are Rademacher variables, with σ i s independent uniform random variables taking values in −1, +1.</p><formula xml:id="formula_136">Let F be { (h(x), y) : h ∈ H, (x, y) ∈ X × Y s }, then S (F) = 1 n s E σ [sup a∈A n s i=1 σ i a i ] ≤ B 2d log n s + 4d log(C + 1) n s .</formula><p>Following Lemma 4, we have with probability of at least δ &gt; 0, for all h ∈ H:</p><formula xml:id="formula_137">R s (h) ≤ R s (h)+2B</formula><p>2d log n s + 4d log(C + 1) n s +B 2 log(2/δ) n s ≤ R s (h)+2B 8d log n s + 16d log(C + 1) + 2 log(2/δ) n s .</p><p>Step 2. We prove that with probability of at least 1 − δ &gt; 0, for all h ∈ H:  <ref type="bibr">[2]</ref>, we obtain the following bound:</p><formula xml:id="formula_138">− R s u,</formula><formula xml:id="formula_139">1 n s E σ [sup a∈A n s i=1 σ i a i ] ≤ 2d log n s + 4d log(C + 1) n s ,</formula><p>where a i is the i-th coordinate value of a and σ = (σ 1 , ..., σ n ) are Rademacher variables, with σ i s independent uniform random variables taking values in −1, +1.</p><formula xml:id="formula_140">Let F be { (h(x), y C+1 ) : h ∈ H, x ∈ X }, then S X (F) = 1 n s E σ [sup a∈A n s i=1 σ i a i ] ≤ 2d log n s + 4d log(C + 1) n s .</formula><p>Following Lemma 4, we have with probability of at least δ &gt; 0, for all h ∈ H:</p><formula xml:id="formula_141">− R s u,C+1 (h) ≤ −R s u,C+1 (h) + 2B</formula><p>8d log n s + 16d log(C + 1) + 2 log(2/δ) n s .</p><p>Step 3. We prove that with probability of at least 1 − δ &gt; 0, for all h ∈ H:</p><formula xml:id="formula_142">R t u,C+1 (h) ≤ R t u,C+1 (h) + 2B 8d log n t + 16d log(C + 1) + 2 log(2/δ) n t .</formula><p>Let the source samples be T X = {x t 1 , ..., x t n t }. Recall that the Natarajan lemma (Lemma 29.4 of <ref type="bibr">[2]</ref>) tells us that if Ndim(H) is d, then</p><formula xml:id="formula_143">|{h(x t 1 ), ..., h(x t n t ) : h ∈ H}| ≤ (n t ) d (C + 1) 2d . Denote A = {( (h(x t 1 )</formula><p>, y C+1 ), ..., (h(x t n t )), y C+1 ) : h ∈ H}. This clearly implies that |A| ≤ |{h(x t 1 ), ..., h(x t n t ) : h ∈ H}| ≤ (n t ) d (C + 1) 2d . Combining this with Lemma 26.8 of <ref type="bibr">[2]</ref>, we obtain the following bound:</p><formula xml:id="formula_144">1 n t E σ [sup a∈A n t i=1 σ i a i ] ≤ 2d log n t + 4d log(C + 1) n t ,</formula><p>where a i is the i-th coordinate value of a and σ = (σ 1 , ..., σ n ) are Rademacher variables, with σ i s independent uniform random variables taking values in −1, +1.</p><formula xml:id="formula_145">Let F be { (h(x), y C+1 ) : h ∈ H, x ∈ X }, then T X (F) = 1 n t E σ [sup a∈A n t i=1</formula><p>σ i a i ] ≤ 2d log n t + 4d log(C + 1) n t .</p><p>Following Lemma 4, we have with probability of at least δ &gt; 0, for all h ∈ H:</p><formula xml:id="formula_146">R t u,C+1 (h) ≤ R t u,C+1 (h) + 2B</formula><p>8d log n t + 16d log(C + 1) + 2 log(2/δ) n t .</p><p>Combining Steps 1, 2, 3 with Theorem 1, we obtain the result.</p><p>Lastly, we provide an empirical analysis for Theorem 4. Theorem 5. Let H k be an RKHS with a universal kernel. Suppose that (y, y ) = y − y 2 2 is the squared loss, and consider the hypothesis set H = H * ∪ F, where</p><formula xml:id="formula_147">H * = {[h 1 , ..., h C+1 ] T : h c ∈ H k , h c H k ≤ M, h c ∞ ≤ m},</formula><p>and F is a set which only contains a constant vector-value function [0, ..., 0, ..., 1] T ∈ R (C+1) , here M, m &gt; 0 is a constant. If a random labeled labeled samples S of size n s is generated by P X s Y s i.i.d and a random unlabeled samples T of size n t is generated by P X t i.i.d, then for any h ∈ H, δ ∈ (0, 1) with probability at least 1 − 3δ, we have </p><formula xml:id="formula_148">R t (h) 1 − π t C+1 ≤ R s (h) + LMMD H k (P X t |Y s , P X s ) + ∆ o + LMMD H k (P X t |Y</formula><p>where B is (1 + m) 2 + Cm 2 , which is the upper bound of (h(x), y), here h ∈ H, y ∈ Y t Then n s S (F)</p><formula xml:id="formula_150">= E σ [sup F n s i=1 σ i (h(x s i ), y s i )] = E σ [sup F n s i=1 C+1 j=1 σ i h j (x s i ) − y s i j 2 ] ≤ C+1 j=1 E σ [ sup h∈H n s i=1 σ i h j (x s i ) − y s i j 2 ],<label>(48)</label></formula><p>where y s i j is the value of jth coordinate for ith source sample's label y s i .</p><p>We study E σ [sup h∈H</p><formula xml:id="formula_151">n s i=1 σ i h j (x s i ) − y s σ i h j (x s i ) ≤ 2(1 + m)E σ sup h∈H * 0 ∪{1} n s i=1 σ i h(x s i ) = 2(1 + m)E σ sup h∈H * 0 n s i=1 σ i h(x s i ) ,<label>(49)</label></formula><p>According to inequalities <ref type="formula" target="#formula_7">(48)</ref> and <ref type="formula" target="#formula_7">(49)</ref>, we have</p><formula xml:id="formula_152">S (F) ≤ 2(1 + m)(1 + C) S X (H * 0 ),<label>(50)</label></formula><p>which implies that for δ ∈ (0, 1) with probability at least 1 − δ, </p><p>2.Consider R t u,C+1 (h). According to Lemma 4, we have that for δ ∈ (0, 1) with probability at least 1 − δ, R t u,C+1 (h) ≤ R t u,C+1 (h) + 2 T X (F) + 4B 2 log(4/δ) n t ,</p><p>where F is a functional space</p><formula xml:id="formula_155">F = { (h(x), y C+1 ) : h ∈ H, x ∈ X }<label>(53)</label></formula><p>here y C+1 is the one-hot vector [0, ..., 0, ..., 0, 1] T ∈ R C+1 . Repeating the process shown for the inequality (49), we have</p><formula xml:id="formula_156">T X (F) ≤ (2Cm + 2(1 + m)) T X (H * 0 ),<label>(54)</label></formula><p>which implies that for δ ∈ (0, 1) with probability at least 1 − δ, R t u,C+1 (h) ≤ R t u,C+1 (h) + 4 ((C + 1)m + 1) T X (H * 0 ) + 4B 2 log(4/δ) n t ,</p><p>3.Consider R s u,C+1 (h). Repeating the process shown for the inequality (55), we obtain for δ ∈ (0, 1) with probability at least 1 − δ, − R s u,C+1 (h) ≤ −R s u,C+1 (h) + 4 ((C + 1)m + 1) S X (H * 0 ) + 4B 2 log(4/δ) n s</p><p>Incorporating the above three inequalities (49), <ref type="bibr" target="#b54">(55)</ref> and <ref type="formula" target="#formula_8">(56)</ref>, we have the result.</p><p>Then, we show that manifold regularization matters via conducting more ablation studies. Though <ref type="figure" target="#fig_7">Fig. 4</ref> (b) has shown that there is no significant change in performance when the coefficient ρ of manifold regularization is set in range 0 to 1, we have conducted more experiments and found that the manifold regularization does make a positive impact on many UOSDA tasks.</p><p>We have conducted the ablation study on datasets Office-31 (AlexNet,VGG16, VGG19, ResNet50) and ImageCLEF-DA (VGG16, VGG19, ResNet50). We show the results in Tables V-VI (D w/o M means DAOD (ρ = 0) without the manifold regularization and DAOD means DAOD (ρ = 1) with the manifold regularization ).</p><p>DAOD with the manifold regularization has achieved better performance Acc(OS) than DAOD without manifold regularization on most tasks (55 out of 60). The average classification accuracy of DAOD with the manifold regularization on 60 tasks is 75.3%, gaining a performance improvement of 1.04% compared to DAOD without the manifold regularization. The average classification accuracy of DAOD with the manifold regularization on datasets ImageCLEF-DA is 66.77%, gaining a performance improvement of 1.07% compared to DAOD without the manifold regularization. The average classification accuracy of DAOD with the manifold regularization on datasets Office-31 is 88.15%, gaining a performance improvement of 1.00% compared to DAOD without the manifold regularization.</p><p>Though the contribution of manifold regularization is not significant (improving around 1.00%), the manifold regularization can make a positive impact on many UOSDA tasks.   Similar to the open set recognition <ref type="bibr">[5]</ref>, <ref type="bibr">[6]</ref>, we define openness O as the number of unknown classes the number of all classes .</p><p>The above formula estimates the level of openness. O = 0 represents a completely closed problem and larger values denote more open problems.</p><p>In our experiments, we only tested the special cases O ≈ 0.5, 0.6 and 0.7. To verify that DAOD is robust to different levels of openness, we conducted experiments on the Office-Home datasets with openness ranging from 0.10 to nearly 0.85. We took classes from 1 to 10 as the known classes and the classes from 11 to 11 + i (i = 0, 1, 2, 3, 4) as the unknown classes. The openness O therefore ranged from 0.10 to 0.30. We also took classes from 11 to 65 − 10 * i or 20 (i = 0, 1, 2, 3, 4) as unknown classes. In this setting, the openness O ranged from 0.50 to 0.85.</p><p>To show that DAOD is robust to openness change, we used the same parameters for all openness values. Due to space limitations, the average results are reported in <ref type="figure" target="#fig_0">Fig. 1</ref>. Compared with the best baseline algorithm OSBP, DAOD performed steadily and achieved the best performances for almost all values of openness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Time Complexity</head><p>We empirically checked the time complexity of DAOD and compared it with the top two baselines ATI and JGSA on different tasks. The environment was an Intel Core i7 − 7700HQ CPU with 32.0 GB memory and all algorithms relied on the same input features. Based on papers <ref type="bibr">[7]</ref>, <ref type="bibr">[8]</ref>, we implement JGSA and DAOD for 10 iterations, and ATI for 5 iterations. Note that the time complexity of deep algorithm OSBP is not comparable with DAOD since it requires many backpropagations. The results in <ref type="table" target="#tab_0">Table VII</ref> reveal that, beyond its superiority in classification accuracy, DAOD also has a comparable running time to the two best baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Zhen Fang, Jie Lu, Feng Liu, Junyu Xuan and Guangquan Zhang are with the Centre for Artificial Intelligence, Faulty of Engineering and Information Technology, University of Technology Sydney, NSW, 2007, Australia, e-mail: Zhen.Fang@student.uts.edu.au, { Jie.Lu; Feng.Liu; Junyu.Xuan; Guangquan.Zhang}@uts.edu.au. Unsupervised open set domain adaptation problem (UOSDA), where the target domain contains "unknown" classes that are not contained in the label set of the source domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Aim of UOSDA. (a) The original source and target samples are given. (b) UCSDA algorithm matches the source and target samples, leading to negative transfer. Because the unknown target samples interfere with distribution matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 )</head><label>5</label><figDesc>When α ranged between 0.2 to 1.2 and δ was chosen from [0.05, 0.2], the performance Acc(OS) of DAOD δ was superior to the best baseline. 6) Although α is not the main factor influencing the performance of DAOD, we compare figures (α &lt; 1.0) with figures (α ≥ 1.0) and find that a smaller α achieves slightly better performance than a larger α. In general, we select α from [0.2, 0.4] and δ from [0.05, 0.25].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>The horizontal axis is the difference in the open set parameters δ = α − γ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Guangquan 2 II 22 SUMMARY</head><label>222</label><figDesc>Zhang is a Professor and Director of the Decision Systems and e-Service Intelligent (DeSI) Research Laboratory, Faculty of Engineering and Information Technology, University of Technology Sydney, Australia. He received his PhD in applied mathematics from Curtin University of Technology, Australia, in 2001. His research interests include fuzzy machine learning, fuzzy optimization, and machine learning and data analytics. He has authored four monographs, five textbooks, and 350 papers including 160 refereed international journal papers. Dr. Zhang has won seven Australian Research Council (ARC) Discovery Project grants and many other research grants. He was awarded an ARC QEII Fellowship in 2005. He has served as a member of the editorial boards of several international journals, as a guest editor of eight special issues for IEEE Transactions and other international journals, and has cochaired several international conferences and work-shops in the area of fuzzy decision-making and knowledge engineering. Theoretical Bound and Algorithm CONTENTS I Appendix A: Definition, Notations and Proof for Theorem 1 experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 V-B Openness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 V-C Time Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 References Some important definitions and notations are restated in Appendix A. The proof for Theorem 2 is provided in Appendix B. The learning bound based on MMD distance (Theorem 4) is contained in Appendix C. In Appendix D, Corollary 1.1 is proven in Appendix D along with an empirical proof of the MMD learning bound (Theorem 5). The experiments are represented in Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F 2 )Definition 1 .</head><label>21</label><figDesc>. B ⊗ C denotes the smallest σ-algebra containing all cylinder sets {B × C : B ∈ B, C ∈ C } and B ⊗ D denotes the smallest σ-algebra containing all cylinder sets {B × D : B ∈ B, D ∈ D}.6. The source domain and target domain are defined as follows. The source domain and target domain are joint distributions P (X s , Y s ) and P (X t , Y t ), where X s , X t and Y s , Y t are the random variables defined (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>7 .</head><label>7</label><figDesc>The marginal distributions P X s and P X t are defined as follows: For any measurable set B ∈ B, P X s (B) := P (ω ∈ Ω : X s (ω) ∈ B), P X t (B) := P (ω ∈ Ω : X t (ω) ∈ B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Lemma 4 .</head><label>4</label><figDesc>(Theorem 26.5 in [2].) Given a space Z, a function l : R × Z → R + and a hypothesis set H ⊂ {f : Z → R}, let F := l • H = {l(f (z), z) : f ∈ H},</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 ,</head><label>4</label><figDesc>s , P X s ) + Λ+ 4 (1 + m)(1 + C) S X (H * 0 where Λ = min h∈H R s (h) + R t * (h), empirical open set difference ∆ o = R t u,C+1 (h) 1−π t C+1 −R s u,C+1 (h), L = max{8M m(C + 1), 2(C + 1)M m + 2M }, S X is unlabeled source samples and H * 0 = {h ∈ H k : h H k ≤ M, h ∞ ≤ m}. Proof for Theorem 5. This proof is divided into three parts. 1.Consider R s (h). Let F := { (h(x), y) : h ∈ H, (x, y) ∈ X × Y s }.According to Lemma 4, we get that for δ ∈ (0, 1) with probability at least 1 − δ, R s (h) ≤ R s (h) + 2 S (F) + 4B 2 log(4/δ) n s ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>R</head><label></label><figDesc>s (h) ≤ R s (h) + 4(1 + m)(C + 1) S (H * 0 ) + 4B 2 log(4/δ) n s ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 1 :</head><label>1</label><figDesc>Accuracy (OS) w.r.t. different openness levels in the target domain B. Openness</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Introduction of datasets.</figDesc><table><row><cell>Dataset Office-31 Office-Home PIE</cell><cell>Type Object Object Face</cell><cell>#Sample 4,110 15,500 1,1554</cell><cell>#Feature 4,096 2,048 1,024</cell><cell>#Class 31 65 68</cell><cell>Domain A,W,D Ar,Cl,Pr,Rw P1,...,P5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Acc(OS*) and Acc(OS) (%) on Office-31, Office-Home and PIE Datasets.</figDesc><table><row><cell cols="2">Dataset A→W 56.0 OSNN OS* OS 54.0 A→D 75.4 71.9 D→A 62.6 60.3 D→W 93.0 88.1 W→A 58.6 56.8 W→D 99.3 93.3 Average 74.2 70.7</cell><cell cols="2">TCA OS* 54.8 54.8 OS 68.0 67.1 53.4 52.7 84.6 80.9 56.1 55.6 97.8 94.8 69.2 68.5</cell><cell cols="2">JDA OS* 63.0 64.8 OS 70.1 70.6 60.4 60.7 98.4 94.7 62.5 62.6 99.3 96.1 75.6 74.9</cell><cell cols="2">JGSA OS* OS 75.7 75.2 74.8 73.3 62.4 61.5 98.0 93.2 64.0 62.9 100.0 94.4 79.2 76.7</cell><cell>ATI OS* 70.6 85.9 68.3 95.8 64.0 97.8 80.4</cell><cell>OS 69.7 84.0 67.6 94.1 62.8 94.5 78.8</cell><cell cols="2">OSBP OS* OS 69.1 70.1 76.4 76.6 62.3 62.5 94.6 98.9 82.2 82.3 96.8 96.9 80.2 80.4</cell><cell cols="2">DAOD OS* OS 84.2 84.2 89.8 88.5 71.8 72.6 98.0 96.0 72.9 74.2 97.5 96.3 85.7 85.3</cell></row><row><cell>Ar→Pr 39.4 Ar→Cl 32.1 Ar→Rw 56.6 Cl→Ar 32.3 Cl→Pr 39.1 Cl→Rw 46.9 Rw→Ar 51.4 Rw→Cl 38.0 Rw→Pr 59.2 Pr→Ar 38.5 Pr→Cl 35.0 Pr→Rw 59.6 Average 44.0</cell><cell>40.6 33.7 57.0 34.0 40.3 47.7 52.1 39.2 59.2 39.7 36.3 59.7 45.0</cell><cell>37.7 24.4 55.7 31.3 34.8 41.4 49.4 34.9 57.3 33.2 35.8 58.3 41.2</cell><cell>37.9 24.1 55.3 32.1 34.8 41.2 49.2 34.1 56.5 33.4 36.1 57.5 41.0</cell><cell>59.7 39.1 67.5 41.9 49.1 59.7 55.8 44.1 68.0 48.4 41.2 70.4 53.8</cell><cell>59.0 39.6 66.4 42.1 48.9 59.1 55.1 43.9 68.2 48.0 41.1 68.9 53.4</cell><cell>64.1 45.9 74.1 43.8 55.8 62.8 56.9 48.7 66.5 55.8 44.1 73.5 57.7</cell><cell>63.3 46.0 72.8 44.5 55.8 62.5 56.4 48.6 65.3 55.5 44.4 72.3 57.3</cell><cell>70.4 54.2 78.1 59.1 68.3 75.3 70.8 55.4 79.4 62.6 54.1 81.1 67.4</cell><cell>68.6 53.1 77.3 57.8 66.7 74.3 70.0 55.2 78.3 61.2 53.9 79.9 66.4</cell><cell>69.2 53.3 79.1 58.2 72.4 72.3 68.2 59.2 80.8 61.0 56.9 83.9 67.9</cell><cell>68.4 53.1 78.0 57.9 71.6 71.4 66.5 57.8 78.6 59.6 55.7 82.1 66.7</cell><cell>72.6 55.3 78.2 59.1 70.8 77.8 71.3 58.4 81.8 66.7 60.0 84.1 69.6</cell><cell>71.8 55.4 77.6 59.2 70.1 77.0 70.5 57.8 80.6 65.8 59.1 82.2 68.9</cell></row><row><cell>P1→P2 32.1 P1→P3 46.5 P1→P4 60.1 P1→P5 22.9 P2→P1 35.6 P2→P3 61.5 P2→P4 71.0 P2→P5 28.5 P3→P1 43.3 P3→P2 53.5 P3→P4 64.9 P3→P5 34.6 P4→P1 56.5 P4→P2 78.1 P4→P3 78.3 P4→P5 43.1 P5→P1 23.2 P5→P2 26.5 P5→P3 31.0 P5→P4 37.2 Average 46.4</cell><cell>34.3 48.3 61.2 26.1 37.9 62.5 71.4 31.2 45.2 54.8 65.4 37.0 57.7 78.0 78.3 44.8 25.7 28.4 32.7 38.9 48.0</cell><cell>20.6 20.2 30.7 10.6 25.4 38.8 49.3 20.4 20.1 37.3 34.6 12.7 24.8 64.0 33.8 17.1 11.6 18.3 12.3 19.4 26.2</cell><cell>21.4 20.3 30.5 11.5 25.5 38.3 48.5 20.7 20.4 36.5 34.2 13.0 24.6 62.1 33.3 17.7 12.8 18.3 13.3 20.0 26.1</cell><cell>42.1 50.0 62.3 28.3 47.9 62.9 71.6 37.3 51.1 64.2 68.5 39.2 64.2 75.2 81.5 52.1 29.6 31.0 33.1 49.7 52.1</cell><cell>41.3 49.1 61.2 28.2 47.3 61.4 69.6 37.1 50.6 62.5 66.6 39.0 62.4 72.4 78.9 50.9 30.2 31.1 32.9 49.1 51.1</cell><cell>55.4 54.4 63.2 35.8 68.5 62.5 78.6 49.0 66.9 66.9 75.6 42.5 75.8 78.3 81.3 65.8 46.4 44.0 55.4 63.8 61.5</cell><cell>54.4 53.5 61.8 35.7 67.2 61.3 76.9 48.0 65.5 65.2 73.8 41.8 73.9 76.1 79.1 64.4 45.9 43.6 54.6 62.7 60.3</cell><cell>44.0 56.3 67.9 45.4 59.5 56.3 77.1 36.7 68.4 55.0 74.0 47.1 66.8 78.1 61.7 48.5 23.5 36.7 41.9 58.6 55.2</cell><cell>41.9 53.6 64.6 43.3 56.7 53.6 73.5 34.9 66.9 52.4 70.5 44.8 63.7 74.4 58.7 46.2 30.2 34.9 39.9 55.8 53.0</cell><cell>66.6 69.1 80.0 50.2 54.2 63.5 81.3 44.2 61.0 64.6 76.9 46.7 68.7 85.0 67.6 63.8 66.6 35.8 46.3 53.5 62.2</cell><cell>64.2 66.4 76.2 49.1 52.9 61.5 87.6 41.2 61.3 64.1 74.7 46.3 67.2 82.2 66.9 59.9 64.2 35.4 45.1 52.2 61.0</cell><cell>57.3 53.1 85.2 47.3 69.7 71.7 91.2 49.8 68.3 70.4 87.1 53.3 87.1 84.8 80.0 61.3 60.6 34.8 44.4 70.3 66.4</cell><cell>56.5 52.2 82.4 46.1 68.1 69.9 88.2 49.4 66.6 68.5 83.9 52.3 84.4 82.4 77.6 59.9 59.2 35.0 44.6 68.6 64.8</cell></row><row><cell>All avg 50.0</cell><cell>50.6</cell><cell>37.7</cell><cell>37.5</cell><cell>56.3</cell><cell>55.6</cell><cell>63.1</cell><cell>61.9</cell><cell>63.0</cell><cell>61.3</cell><cell>66.8</cell><cell>65.9</cell><cell>70.4</cell><cell>69.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Jie Lu (F'18) is a Distinguished Professor and the Director of the Centre for Artificial Intelligence at the University of Technology Sydney, Australia. She received her PhD degree from Curtin University of Technology, Australia, in 2000. Her main research interests arein the areas of fuzzy transfer learning, concept drift, decision support systems, and recommender systems. She is an IEEE fellow, IFSA fellow and Australian Laureate fellow. She has published six research books and over 450 papers in refereed journals and conference proceedings; has won over 20 ARC Laureate, ARC Discovery Projects, government and industry projects. She serves as Editor-In-Chief for Knowledge-Based Systems (Elsevier) and Editor-In-Chief for International journal of computational intelligence systems. She has delivered over 25 keynote speeches at international conferences and chaired 15 international conferences. She has received various awards such as the UTS Medal for Research and Teaching Integration (2010), the UTS Medal for Research Excellence (2019), the Computer Journal Wilkes Award (2018), the IEEE Transactions on Fuzzy Systems Outstanding Paper Award (2019), and the Australian Most Innovative Engineer Award (2019). Liu is a Doctoral candidate in Centre for Artificial intelligence, Faculty of Engineering and Information Technology, University of Technology Sydney, Australia. He received an M.Sc. degree in probability and statistics and a B.Sc. degree in pure mathematics from the School of Mathematics and Statistics, Lanzhou University, China, in 2015 and 2013, respectively. His research interests include domain adaptation and two-sample test. He has served as a senior program committee member for ECAI and program committee members for NeurIPS, ICML, IJCAI, CIKM, FUZZ-IEEE, IJCNN and ISKE. He also serves as reviewers for TPAMI, TNNLS, TFS and TCYB. He has received the UTS-FEIT HDR Research Excellence Award (2019), Best Student Paper Award of FUZZ-IEEE (2019) and UTS Research Publication Award (2018). Xuan is working as a Postdoctoral Research Fellow in the Faculty of Engineering and IT at the University of Technology Sydney in Australia. His main research interests include Machine Learning, Bayesian Nonparametric Learning, Text Mining, and Web Mining. He has published almost 40 papers in high-quality journals and conferences, including Artificial Intelligence, Machine Learning, IEEE TNNLS, ACM Computing Surveys, IEEE TKDE, ACM TOIS, and IEEE TCYB, etc.</figDesc><table><row><cell>Feng Junyu</cell></row><row><cell>Zhen Fang received his M.Sc. degree in pure math-ematics from the School of Mathematical Sciences Xiamen University, Xiamen, China, in 2017. He is working toward a PhD degree with the Faculty of Engineering and Information Technology, University of Technology Sydney, Australia. His research inter-ests include transfer learning and domain adaptation. He is a Member of the Decision Systems and e-Service Intelligence (DeSI) Research Laboratory, CAI, University of Technology Sydney.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Notations and their descriptions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II :</head><label>II</label><figDesc>Notations and their descriptions.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>S S X T X S X,c</cell><cell>source samples source samples without labels unlabeled target samples</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>This clearly implies that |A| ≤ |{h(x s 1 ), ..., h(x s n s ) : h ∈ H}| ≤ (n s ) d (C + 1) 2d . Combining this with Lemma 26.8 of</figDesc><table><row><cell>2B</cell><cell>8d log n s + 16d log(C + 1) + 2 log(2/δ) n s</cell><cell>.</cell></row><row><cell cols="3">Let the source samples be S = {(x s 1 , y s 1 ), ..., (x s n s , y s n s )}. Recall that the Natarajan lemma (Lemma 29.4 of [2]) tells us that if Ndim(H) is d, then |{h(x s 1 ), ..., h(x s n s ) : h ∈ H}| ≤ (n s ) d (C + 1) 2d . Denote A = {( (h(x s 1 ), y s 1 ), ..., (h(x s n s ), y s n</cell></row></table><note>s ) : h ∈ H}.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>C+1 ), ..., (h(x s n s ), y C+1 )) : h ∈ H}. This clearly implies that |A| ≤ |{h(x s 1 ), ..., h(x s n s ) : h ∈ H}| ≤ (n s ) d (C + 1) 2d . Combining this with Lemma 26.8 of</figDesc><table><row><cell>Denote A = {( (h(x s 1 ), y</cell><cell></cell><cell></cell></row><row><cell>C+1 (h) ≤ −R s u,C+1 (h) + 2B</cell><cell>8d log n s + 16d log(C + 1) + 2 log(2/δ) n s</cell><cell>.</cell></row></table><note>Let the source samples be S X = {x s 1 , ..., x s ns }. Recall that the Natarajan lemma (Lemma 29.4 of [2]) tells us that if Ndim(H) is d, then |{h(x s 1 ), ..., h(x s ns ) : h ∈ H}| ≤ (n s ) d (C + 1) 2d .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Ablation Study on manifold regularization: Acc(OS) (%) on ImageCLEF-DA datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Ablation Study on manifold regularization: Acc(OS) (%) on Office-31 datasets.</figDesc><table><row><cell>AlexNet D w/o M DAOD VGG16 D w/o M DAOD VGG19 D w/o M DAOD ResNet50 D/M DAOD</cell><cell>A→W 82.6 84.2 A→W 86.8 88.5 A→W 89.5 89.2 A→W 96.2 96.4</cell><cell>A→D 89.6 88.5 A→D 90.1 90.5 A→D 88.9 90.5 A→D 93.9 94.9</cell><cell>W→A 72.2 72.6 W→A 73.1 75.2 W→A 74.9 75.4 W→A 82.0 82.3</cell><cell>W→D 95.9 96.0 W→D 95.5 97.6 W→D 97.2 98.6 W→D 94.9 96.2</cell><cell>D→A 73.6 74.2 D→A 71.2 74.2 D→A 73.5 75.6 D→A 84.6 85.0</cell><cell>D→W 93.2 96.3 D→W 96.7 97.7 D→W 97.7 98.6 D→W 97.1 97.3</cell><cell>AVE 84.5 85.3 AVE 85.6 87.3 AVE 87.0 88.0 AVE 91.5 92.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VII :</head><label>VII</label><figDesc>Running Times of JGSA, ATI and DAOD.</figDesc><table><row><cell>Task A → W Ar → Cl P1 → P2</cell><cell>#Sample × #Feature 1,326×4,096 5,454×2,048 2,609×1,024</cell><cell>JGSA 171.2s 98.3s 42.5s</cell><cell>ATI 55.3s 77.3s 68.5s</cell><cell>DAOD 32.5s 83.8s 15.0s</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The work presented in this paper was supported by the Australian Research Council (ARC) under DP170101632 and FL190100149. We also wish to thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>However, the bound provided by <ref type="bibr">[1]</ref> only considers scalar valued functions not the scoring functions (vector-valuedd functions). The problem here, however, is based on scoring functions, thus we propose a new version of this bound: </p><p>and F is a set that only contains a constant vector-value function [0, ..., 0, ..., 1] T ∈ R (C+1) , here M, m &gt; 0 is a constant. Let P 1 and P 2 be two distributions over X . Then the following inequality holds:</p><p>where L = max{4M m(C + 1), (C + 1)M m + M }.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<title level="m">Dataset Shift in Machine Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transfer learning using computational intelligence: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Behbood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transfer learning via dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="677" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="601" to="608" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysis of kernel mean matching under covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="607" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature analysis of marginalized stacked denoising autoenconder for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1321" to="1334" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Prediction reweighting for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1682" to="1695" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised knowledge transfer using similarity embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="946" to="950" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with sphere retracting transformation,&quot; in IJCNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised heterogeneous domain adaptation via shared fuzzy equivalence relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3555" to="3568" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Heterogeneous domain adaptation: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
			<publisher>Early Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fuzzy regression transfer learning in takagi-sugeno fuzzy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pedrycz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Behbood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1795" to="1807" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Open set domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="156" to="171" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Syn2real: A new benchmark for synthetic-to-real visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1806.09755</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">To transfer or not to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning Factorized Representations for open-set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Separate to adapt: Open set domain adaptation via progressive separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving open set domain adaptation using image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1258" to="1263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="137" to="144" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain adaption via feature selection on explicit feature map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1180" to="1190" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Butterfly: A panacea for all difficulties in wildly unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS LTS Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptation regularization: A general framework for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1076" to="1089" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint geometrical and statistical alignment for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5150" to="5158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1205" to="1213" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4058" to="4065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation methods in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Micheals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Face Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="551" to="574" />
		</imprint>
	</monogr>
	<note>2nd Edition</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nearest neighbors distance ratio open-set classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Mendes-Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Oliveira Werneck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Pazinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>De Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A B</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Da Silva Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="359" to="386" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Probability models for open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2317" to="2324" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards open-set face recognition using hashing functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Vareto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Oliveira Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Joint Conference on Biometrics</title>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-01" />
			<biblScope unit="page" from="634" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-class open set recognition using probability of inclusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="393" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7404" to="7413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>COLT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large margin transductive transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Quanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1327" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transfer learning with graph co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1805" to="1818" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation: A multi-task learning-based method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation by covariance matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2724" to="2739" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Heterogeneous domain adaptation using manifold alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<editor>IJCAI, T. Walsh</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1541" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Natarajan</surname></persName>
		</author>
		<title level="m">Machine Learning: A Theoretical Approach</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1106" to="1114" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The CMU pose, illumination, and expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1615" to="1618" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Universal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2724" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">In alphabetical order, we used the first 25 classes as known classes and classes 26-65 as the unknown classes. Following the standard protocol and for fair comparison with the other algorithms, we extracted feature vectors from ResNet-50, VGG16, VGG19. The results of ResNet-50 are listed in Table IV of the main text. ImageCLEF-DA [4] is a benchmark dataset for ImageCLEF 2014 domain adaptation challenge, which aims to classify 12 categories shared in three datasets: Caltech-256 (C)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Appendix E: Experiments A ; Ar → Pr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">.</forename><surname>Rw → Ar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; C → B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C → I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C → P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">More experiments We conduct experiments on different features extracted from different deep frameworks: VGG16, VGG19 and ResNet-50. Office-Home [3] consists of 4 different domains: Artistic (Ar)</title>
		<meeting><address><addrLine>Bing</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>(B) and PascalVOC 2012 (P). We used the first 8 classes as known classes and classes 9-12 as the unknown classes. By considering each as a domain. we build 12 transfer tasks</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Following the standard protocol and for a fair comparison with the other algorithms, we extracted feature vectors from ResNet-50, VGG16 and VGG19</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P → I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The results are shown in Tables III, IV. TABLE III: Acc(OS) (%) on Office-Home datasets using different features</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<title level="m">TABLE IV: Acc(OS) (%) on ImageCLEF-DA datasets using different features. VGG16 B→C B→I B→P I→B I→C I→P C→B C→I C→P P→B P→C P→I AVE JGSA 59</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B→c B→i B→p I→b I→c I→p C→b C→i C→p P→b P→c P→i</forename><surname>Vgg16</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ave</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Understanding machine learning: From theory to algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Toward open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Rezende Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Separate to adapt: Open set domain adaptation via progressive separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Joint geometrical and statistical alignment for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5150" to="5158" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
