<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MMDENSELSTM: AN EFFICIENT COMBINATION OF CONVOLUTIONAL AND RECURRENT NEURAL NETWORKS FOR AUDIO SOURCE SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation</orgName>
								<address>
									<addrLine>Minato-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabarun</forename><surname>Goswami</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sony India Software Centre</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sony Corporation</orgName>
								<address>
									<addrLine>Minato-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MMDENSELSTM: AN EFFICIENT COMBINATION OF CONVOLUTIONAL AND RECURRENT NEURAL NETWORKS FOR AUDIO SOURCE SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-convolution</term>
					<term>recurrent</term>
					<term>DenseNet</term>
					<term>LSTM</term>
					<term>audio source separation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have become an indispensable technique for audio source separation (ASS). It was recently reported that a variant of CNN architecture called MM-DenseNet was successfully employed to solve the ASS problem of estimating source amplitudes, and state-of-the-art results were obtained for DSD100 dataset. To further enhance MMDenseNet, here we propose a novel architecture that integrates long short-term memory (LSTM) in multiple scales with skip connections to efficiently model long-term structures within an audio context. The experimental results show that the proposed method outperforms MMDenseNet, LSTM and a blend of the two networks. The number of parameters and processing time of the proposed model are significantly less than those for simple blending. Furthermore, the proposed method yields better results than those obtained using ideal binary masks for a singing voice separation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Audio source separation (ASS) has recently been intensively studied. Various approaches have been introduced such as local Gaussian modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, non-negative factorization <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, kernel additive modeling <ref type="bibr" target="#b5">[6]</ref> and their combinations <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. Recently, deep neural network (DNN) based ASS methods have shown a significant improvement over conventional methods. In <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, a standard feedforward fully connected network (FNN) was used to estimate source spectra. A common way to exploit temporal contexts is to concatenate multiple frames as the input. However, the number of frames that can be used is limited in practice to avoid the explosion of the model size. In <ref type="bibr" target="#b11">[12]</ref>, long short-term memory (LSTM), a type of recurrent neural network (RNN), was used to model longer contexts. However, the model size tends to become excessively large and the training becomes slow owing to the full connection between the layers and the gate mechanism in an LSTM cell. Recently, convolutional neural networks (CNNs) have been successfully applied to audio modeling of spectrograms <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, although CNNs were originally intro-duced to address the transition-invariant property of images. A CNN significantly reduces the number of parameters and improves generalization by sharing parameters to model local patterns in the input. However, a standard CNN requires considerable depth to cover long contexts, making training difficult. To address this problem, a multi-scale structure was used to adapt a CNN to solve the ASS problem in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, where convolutional layers were applied on multiple scales obtained by downsampling feature maps, and low-resolution feature maps were progressively upsampled to recover the original resolution. Another problem in applying a two dimensional convolution to a spectrogram is the biased distribution of the local structure in the spectrogram. Unlike an image, a spectrogram has different local structures depending on the frequency bands. Complete sharing of convolutional kernels over the entire frequency range may reduce modeling flexibility. In <ref type="bibr" target="#b17">[18]</ref>, we proposed a multi-band structure where each band was linked to a single CNN dedicated to particular frequency bands along with a full-band CNN. The novel CNN architecture called DenseNet was extended to the multiscale and multi-band structure called MMDenseNet, which outperformed an LSTM system and achieved a state-of-theart performance for the DSD100 dataset <ref type="bibr" target="#b18">[19]</ref>. Although it has been suggested that CNNs often work better than RNNs even for sequential data modeling <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, RNNs can also benefit from multi-scale and multi-band modeling because they make it easier to capture long-term dependencies and can save parameters by omitting redundant connections between different bands.</p><p>Moreover, the blending of two systems improves the performance even when one system consistently performs better than the other <ref type="bibr" target="#b11">[12]</ref>. Motivated by these observations, here we propose a novel network architecture called MMDenseL-STM. This combines LSTM and DenseNet in multiple scales and multiple bands, improving separation quality while retaining a small model size. There have been several attempts to combine CNN and RNN architectures. In <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, convolutional layers and LSTM layers were connected serially . Shi et al. proposed convolutional LSTM for the spatio-temporal sequence modeling of rainfall prediction <ref type="bibr" target="#b22">[23]</ref>, where matrix multiplications in LSTM cells were replaced with convolutions. In contrast to these methods, in which convolution  and LSTM operate at a single scale, we show that combining them at multiple low scales increases the performance and efficiency. Moreover, we systematically compare several architectures to search for the optimal strategy to combine DenseNet and LSTM. Experimental results show that the proposed method outperforms current state-of-the-art methods for the DSD100 and MUSDB18 datasets. Furthermore, MM-DenseLSTM even outperforms an ideal binary mask (IBM), which is usually considered as an upper baseline, when we train the networks with a larger dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MULTI-SCALE MULTI-BAND DENSELSTM</head><p>In this section, we first summarize multi-scale multi-band DenseNet (MMDenseNet) as our base network architecture. Then, we introduce strategies to combine dense block and LSTM at multiple scales and multiple bands. Finally, we discuss the architecture of MMDenseLSTM in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">MMDenseNet</head><p>Among the various CNN architectures, DenseNet shows excellent performance in image recognition tasks <ref type="bibr" target="#b23">[24]</ref>. The basic idea of DenseNet is to improve the information flow between layers by concatenating all preceding layers as,</p><formula xml:id="formula_0">x l = H l ([x l−1 , x l−2 , . . . , x 0 ])</formula><p>, where x l and [. . .] denote the output of the lth layer and the concatenation operation, respectively. H l (·) is a nonlinear transformation consisting of batch normalization (BN) followed by ReLU and convolution with k feature maps. Such dense connectivity enables all layers to receive the gradient directly and also reuse features computed in preceding layers. To cover the long context required for ASS, multi-scale DenseNet (MDenseNet) applies a dense block at multiple scales by progressively downsampling its output and then progressively upsampling the output to recover the original resolution, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>. Here, s is the scale index, i.e., the feature maps at scale s are downsampled s − 1 times and have 2 2(s−1) times lower resolution than the original feature maps. To allow forward and backward signal flow without passing though lower-resolution blocks, an inter-block skip connection, which directly connects two dense blocks of the same scale, is also introduced. In contrast to an image, in an audio spectrogram, different patterns occur in different frequency bands, although a certain amount of translation of patterns exists for a relatively small pitch shift. Therefore, limiting the band that shares the kernels is suitable for efficiently capturing local patterns. MMDenseNet addresses this problem by splitting the input into multiple bands and applying band-dedicated MDenseNet to each band. MMDenseNet has demonstrated state-of-theart performance for the DSD100 dataset with about 16 times fewer parameters than the LSTM model, which obtained the best score in SiSEC 2016 <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Combining LSTM with MMDenseNet</head><p>Uhlich et al. have shown that blending two systems gives better performance even when one system consistently outperforms the other <ref type="bibr" target="#b11">[12]</ref>. The improvement tends to be more significant when two very different architectures are blended such as a CNN and RNN, rather than the same architectures with different parameters. However, the blending of architectures increases the model size and computational cost in an additive manner, which is often undesirable when deploying the systems. Therefore, we propose combining the dense block and LSTM block in a unified architecture. The LSTM block consists of a 1 × 1 convolution that reduces the number of feature maps to 1, followed by a bi-directional LSTM layer, which treats the feature map as sequential data along the time axis, and finally a feedforward linear layer that transforms back the input frequency dimension f s from the number of LSTM units m s . We consider three configurations with different combinations of the dense and LSTM blocks as shown in <ref type="figure">Fig. 2</ref>. The Sa and Sb configurations place the LSTM block after and before the dense block, respectively, while the dense block and LSTM block are placed in parallel and concatenated in the P configuration. We focus on the use of the Sa configuration since a CNN is effective at modeling the local structure and the LSTM block benefits from local pattern modeling as it covers the entire frequency at once. This claim will be empirically validated in Sec. 3.2.</p><p>Naively inserting LSTM blocks at every scale greatly increases the model size. This is mostly due to the full connection between the input and output units of the LSTM block in the scale s = 1. To address this problem, we propose the insertion of only a small number of LSTM blocks in the upsampling path for low scales (s &gt; 1). This makes it easier for LSTM blocks to capture the global structure of the input with a much smaller number of parameters. On the other hand, a CNN is advantageous for modeling fine local structures; thus placing only dense block at s = 1 is suitable. The multi-band structure is also beneficial for LSTM blocks since the compression from the input frequency dimension f s to m s LSTM units is relaxed or it even allows the dimension (f s &lt; m s ) to be increased while using fewer LSTM units, increasing the modeling capabilities as discussed in <ref type="bibr" target="#b24">[25]</ref>. The entire proposed architecture is illustrated in <ref type="figure">Fig. 3</ref>. To capture the pattern that spans the bands, MDenseLSTM for the full band is also built in parallel along with the band dedicated MDenseLSTM. The outputs of the MDenseLSTMs are concatenated and integrated by the final dense block, as MM-DenseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Architectural details</head><p>Details of the proposed network architecture for ASS are described in <ref type="table" target="#tab_1">Table 1</ref>. We split the input into three bands at 4.1kHz and 11kHz. The LSTM blocks are only placed at bottleneck blocks and at some blocks at s = 2 in the upsampling path, which greatly reduces the model size. The final dense block has three layers with growth rate k = 12. The effective context size of the architecture is 356 frames. Note that MM-DenseLSTM can be applied to an input of arbitrary length since it consists of convolution and LSTM layers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSE increase in parameters[%]</head><p>increase in parameters MSE <ref type="figure">Fig. 4</ref>. Effect of LSTM block at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Setup</head><p>We evaluated the proposed method on the DSD100 and MUSDB18 datasets, prepared for SiSEC 2016 <ref type="bibr" target="#b18">[19]</ref> and SiSEC 2018 <ref type="bibr" target="#b25">[26]</ref>, respectively. MUSDB18 has 100 and 50 songs while DSD100 has 50 songs each in the Dev and Test sets. In both datasets, a mixture and its four sources, bass, drums, other and vocals, recorded in stereo format at 44.1kHz, are available for each song. Short-time Fourier transform magnitude frames of the mixture, windowed at 4096 samples with 75% overlap, with data augmentation <ref type="bibr" target="#b11">[12]</ref> were used as inputs. The networks were trained to estimate the source spectrogram by minimizing the mean square error with the Adam optimizer. For the evaluation on MUSDB18, we used the museval package <ref type="bibr" target="#b25">[26]</ref>, while we used the BSSEval v3 toolbox <ref type="bibr" target="#b26">[27]</ref> for the evaluation on DSD100 for a fair comparison with previously reported results. The SDR values are the median of the average SDR of each song.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture validation</head><p>In this section we validate the proposed architecture for the singing voice separation task on MUSDB18.</p><p>Combination structure The SDR values obtained by the Sa-, Sband Ptype MMDenseLSTMs are tabulated in <ref type="table">Table  2</ref>. These results validate our claim (Sec. 2.2) that the Sa </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM insertion scale</head><p>The efficiency of inserting the LSTM block at lower scales was validated by comparing seven MMDenseLSTMs with a single 64 unit LSTM layer inserted at different scales in band 1 (all other LSTM layers in <ref type="table" target="#tab_1">Table 1</ref> are omitted). <ref type="figure">Figure 4</ref> shows the percentage increase in the number of parameters compared with that of the base architecture and the mean square error (MSE) values for the seven networks. It is evident that inserting LSTM layers at low scales in the up-scaling path gives the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of dense and LSTM layers</head><p>We further compared the l2 norms of the feature maps ( <ref type="figure" target="#fig_3">Fig.5</ref>) in the LSTM block d4 of band 1. It can be seen that the norm of the LSTM feature map is similar to the highest norm among the dense feature maps. Even though some dense feature maps have low norms, we confirmed that they tend to learn sparse local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with state-of-the-art methods</head><p>We next compared the proposed method with five state-ofthe-art methods, DeepNMF <ref type="bibr" target="#b3">[4]</ref>, NUG <ref type="bibr" target="#b9">[10]</ref>, BLSTM <ref type="bibr" target="#b11">[12]</ref>,  BLEND <ref type="bibr" target="#b11">[12]</ref> and MMDenseNet [18] on DSD100. The task was to separate the four sources and accompaniment, which is the residual of the vocal extraction, from the mixture. Here, the multichannel Wiener filter was applied to MMDenseL-STM outputs as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. <ref type="table">Table 3</ref> shows that the proposed method improves SDRs by an average of 0.2dB compared with MMDenseNet, showing that the MMDenseLSTM architecture further improves the performance for most sources.</p><p>To further improve the capability of music source separation and utilize the full modeling capability of MMDenseL-STM, we next trained models with the MUSDB dev set and an internal dataset comprising 800 songs resulting in a 14 times larger than the DSD100 dev set. The proposed method was compared with BLSTM <ref type="bibr" target="#b11">[12]</ref>, MMDenseNet <ref type="bibr" target="#b17">[18]</ref> and a blend of these two systems (BLEND2) as in <ref type="bibr" target="#b11">[12]</ref>. All baseline networks were trained with the same training set, namely 900 songs. For a fair comparison with MMDenseNet, we configured it with the same base architecture as in <ref type="table" target="#tab_1">Table  1</ref>, with an extra layer in the dense blocks, corresponding to the LSTM block in our proposed method. We also included the IBM as an upper baseline since it uses oracle separation. <ref type="table" target="#tab_3">Table 4</ref> shows the result of this experiment. We obtained average improvements of 0.43dB over MMDenseNet and 0.41dB over BLEND2, achieving state-of-the-art results in SiSEC2018 <ref type="bibr" target="#b25">[26]</ref>. The proposed method even outperformed the IBM for accompaniment. <ref type="table" target="#tab_3">Table 4</ref> also shows that MM-DenseLSTM can efficiently utilize the sequence modeling capability of LSTMs in conjunction with MMDenseNet, having 24 times fewer parameters than the naive combination of BLSTM and MMDenseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We proposed an efficient way to combine DenseNet and LSTM to improve ASS performance. The proposed MM-DenseLSTM achieves state-of-the-art results on DSD100 and MUSDB18 datasets. MMDenseLSTM outperforms a naive combination of BSLTM and MMDenseNet despite having much fewer parameters, and even outperforms an IBM for a singing voice separation task when the networks were trained with 900 songs. The improvement over MMDenseNet is less for bass, which will be further investigated in future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>MDenseNet architecture. Multi-scale dense blocks are connected though down-or upsampling layer or through block skip connections. The figure shows the case s = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Configurations with different combinations of dense and LSTM blocks. LSTM blocks are inserted at some of the scales MMDenseLSTM architecture. Outputs of MDenseL-STM dedicated to different frequency band including the full band are concatenated and the final dense block integrates features from these bands to create the final output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Average l2 norm of feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The proposed architecture. All dense blocks are equipped with 3×3 kernels with growth rate k. l and m s denote the number of layer and LSTM units of LSTM block, respectively. ds denotes scale s in the downsampling path while us is that in the upsampling path.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison of MMDenseLSTM configurations. Comparison of SDR on DSD100 dataset.</figDesc><table><row><cell>type</cell><cell>Sa</cell><cell>Sb</cell><cell>P</cell><cell></cell><cell></cell></row><row><cell cols="4">SDR 2.83 2.31 2.47</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>SDR in dB</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="5">Bass Drums Other Vocals Acco.</cell></row><row><cell>DeepNMF [4]</cell><cell>1.88</cell><cell>2.11</cell><cell>2.64</cell><cell>2.75</cell><cell>8.90</cell></row><row><cell>NUG [10]</cell><cell>2.72</cell><cell>3.89</cell><cell>3.18</cell><cell>4.55</cell><cell>10.29</cell></row><row><cell>BLSTM [12]</cell><cell>2.89</cell><cell>4.00</cell><cell>3.24</cell><cell>4.86</cell><cell>11.26</cell></row><row><cell>BLEND [12]</cell><cell>2.98</cell><cell>4.13</cell><cell>3.52</cell><cell>5.23</cell><cell>11.70</cell></row><row><cell cols="2">MMDenseNet [18] 3.91</cell><cell>5.37</cell><cell>3.81</cell><cell>6.00</cell><cell>12.10</cell></row><row><cell cols="2">MMDenseLSTM 3.73</cell><cell>5.46</cell><cell>4.33</cell><cell>6.31</cell><cell>12.73</cell></row><row><cell cols="6">configuration performs the best because the LSTM layer can</cell></row><row><cell cols="6">efficiently model the global modulations utilizing the local</cell></row><row><cell cols="6">features extracted by the dense layers at this scale. Hence-</cell></row><row><cell cols="5">forth, all experiments use the Sa configuration.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of SDR on MUSDB18 dataset.</figDesc><table><row><cell></cell><cell>#params</cell><cell></cell><cell></cell><cell>SDR in dB</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="6">[×10 6 ] Bass Drums Other Vocals Acco.</cell></row><row><cell>IBM</cell><cell>-</cell><cell>5.30</cell><cell>6.87</cell><cell>6.42</cell><cell>7.50</cell><cell>10.83</cell></row><row><cell>BLSTM [12]</cell><cell>30.03</cell><cell>3.99</cell><cell>5.28</cell><cell>4.06</cell><cell>3.43</cell><cell>14.51</cell></row><row><cell>MMDenseNet [18]</cell><cell>0.33</cell><cell>5.19</cell><cell>6.27</cell><cell>4.64</cell><cell>3.87</cell><cell>15.41</cell></row><row><cell>BLEND2</cell><cell>30.36</cell><cell>4.72</cell><cell>6.25</cell><cell>4.75</cell><cell>4.33</cell><cell>16.04</cell></row><row><cell>MMDenseLSTM</cell><cell>1.22</cell><cell>5.19</cell><cell>6.62</cell><cell>4.93</cell><cell>4.94</cell><cell>16.40</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Underdetermined reverberant audio source separation using a full-rank spatial covariance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Q K</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech &amp; Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1830" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PRO-JET -spatial audio separation using projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Badeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cauchy nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep NMF for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6670</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multichannel blind source separation based on non-negative tensor factorization in wavenumber domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Saruwatari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel additive models for source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4298" to="4310" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multichannel nonnegative matrix factorization in convolutive mixtures for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ozerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="563" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable audio separation with light kernel additive modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="76" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Projectionbased demixing of spatial audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1560" to="1572" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>IEEE/ACM Trans. Audio</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multichannel music separation with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Nugraha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EUSIPCO</title>
		<meeting>EUSIPCO</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural network based instrument extraction from music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2135" to="2139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Very deep multilingual convolutional neural networks for LVCSR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4955" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fully convolutional deep auditory model for musical chord recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Korzeniowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MLSP</title>
		<meeting>MLSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aenet: Learning deep audio features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="513" to="524" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Singing voice separation with deep U-Net convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IS-MIR</title>
		<meeting>IS-MIR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-scale multi-band DenseNets for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WAS-PAA</title>
		<meeting>WAS-PAA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The 2016 Signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rivet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fontecave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LVA/ICA, 2017</title>
		<meeting>LVA/ICA, 2017</meeting>
		<imprint>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional Sequence Modeling Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional-recurrent neural networks for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan Tashev Chin-Hui Lee Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuayb</forename><surname>Zarar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2401" to="2405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">W</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc LVA/ICA</title>
		<meeting>LVA/ICA</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
