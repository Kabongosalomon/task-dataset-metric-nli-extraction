<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Product-based Neural Networks for User Response Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
							<email>hcai@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
							<email>kren@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>wnzhang@apex.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
							<email>ying.wen@cs.ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>j.wang@cs.ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Product-based Neural Networks for User Response Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfield categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Learning and predicting user response now plays a crucial role in many personalization tasks in information retrieval (IR), such as recommender systems, web search and online advertising. The goal of user response prediction is to estimate the probability that the user will provide a predefined positive response, e.g. clicks, purchases etc., in a given context <ref type="bibr" target="#b0">[1]</ref>. This predicted probability indicates the user's interest on the specific item such as a news article, a commercial item or an advertising post, which influences the subsequent decision making such as document ranking <ref type="bibr" target="#b1">[2]</ref> and ad bidding <ref type="bibr" target="#b2">[3]</ref>.</p><p>The data collection in these IR tasks is mostly in a multifield categorical form, for example, [Weekday=Tuesday, Gender=Male, City=London], which is normally transformed into high-dimensional sparse binary features via onehot encoding <ref type="bibr" target="#b3">[4]</ref>. For example, the three field vectors with one-hot encoding are concatenated as Many machine learning models, including linear logistic regression <ref type="bibr" target="#b4">[5]</ref>, non-linear gradient boosting decision trees <ref type="bibr" target="#b3">[4]</ref> and factorization machines <ref type="bibr" target="#b5">[6]</ref>, have been proposed to work on such high-dimensional sparse binary features and produce high quality user response predictions. However, these models highly depend on feature engineering in order to capture highorder latent patterns <ref type="bibr" target="#b6">[7]</ref>.</p><p>Recently, deep neural networks (DNNs) <ref type="bibr" target="#b7">[8]</ref> have shown great capability in classification and regression tasks, including computer vision <ref type="bibr" target="#b8">[9]</ref>, speech recognition <ref type="bibr" target="#b9">[10]</ref> and natural language processing <ref type="bibr" target="#b10">[11]</ref>. It is promising to adopt DNNs in user response prediction since DNNs could automatically learn more expressive feature representations and deliver better prediction performance. In order to improve the multi-field categorical data interaction, <ref type="bibr" target="#b11">[12]</ref> presented an embedding methodology based on pre-training of a factorization machine. Based on the concatenated embedding vectors, multi-layer perceptrons (MLPs) were built to explore feature interactions. However, the quality of embedding initialization is largely limited by the factorization machine. More importantly, the "add" operations of the perceptron layer might not be useful to explore the interactions of categorical data in multiple fields. Previous work <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref> has shown that local dependencies between features from different fields can be effectively explored by feature vector "product" operations instead of "add" operations.</p><p>To utilize the learning ability of neural networks and mine the latent patterns of data in a more effective way than MLPs, in this paper we propose Product-based Neural Network (PNN) which (i) starts from an embedding layer without pretraining as used in <ref type="bibr" target="#b11">[12]</ref>, and (ii) builds a product layer based on the embedded feature vectors to model the inter-field feature interactions, and (iii) further distills the high-order feature patterns with fully connected MLPs. We present two types of PNNs, with inner and outer product operations in the product layer, to efficiently model the interactive patterns.</p><p>We take CTR estimation in online advertising as the working example to explore the learning ability of our PNN model. The extensive experimental results on two large-scale realworld datasets demonstrate the consistent superiority of our model over state-of-the-art user response prediction models <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref> on various metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The response prediction problem is normally formulated as a binary classification problem with prediction likelihood or cross entropy as the training objective <ref type="bibr" target="#b13">[14]</ref>. Area under ROC Curve (AUC) and Relative Information Gain (RIG) are common evaluation metrics for response prediction accuracy <ref type="bibr" target="#b14">[15]</ref>. From the modeling perspective, linear logistic regression (LR) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[16]</ref> and non-linear gradient boosting decision trees (GBDT) <ref type="bibr" target="#b3">[4]</ref> and factorization machines (FM) <ref type="bibr" target="#b5">[6]</ref> are widely used in industrial applications. However, these models are limited in mining high-order latent patterns or learning quality feature representations.</p><p>Deep learning is able to explore high-order latent patterns as well as generalizing expressive data representations <ref type="bibr" target="#b10">[11]</ref>. The input data of DNNs are usually dense real vectors, while the solution of multi-field categorical data has not been well studied. Factorization-machine supported neural networks (FNN) was proposed in <ref type="bibr" target="#b11">[12]</ref> to learn embedding vectors of categorical data via pre-trained FM. Convolutional Click Prediction Model (CCPM) was proposed in <ref type="bibr" target="#b12">[13]</ref> to predict ad click by convolutional neural networks (CNN). However, in CCPM the convolutions are only performed on the neighbor fields in a certain alignment, which fails to model the full interactions among non-neighbor features. Recurrent neural networks (RNN) was leveraged to model the user queries as a series of user context to predict the ad click behavior <ref type="bibr" target="#b16">[17]</ref>. Product unit neural network (PUNN) <ref type="bibr" target="#b17">[18]</ref> was proposed to build high-order combinations of the inputs. However, neither can PUNN learn local dependencies, nor produce bounded outputs to fit the response rate.</p><p>In this paper, we demonstrate the way our PNN models learn local dependencies and high-order feature interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP LEARNING FOR CTR ESTIMATION</head><p>We take CTR estimation in online advertising <ref type="bibr" target="#b13">[14]</ref> as a working example to formulate our model and explore the performance on various metrics. The task is to build a prediction model to estimate the probability of a user clicking a specific ad in a given context.</p><p>Each data sample consists of multiple fields of categorical data such as user information (City, Hour, etc.), publisher information (Domain, Ad slot, etc.) and ad information (Ad creative ID, Campaign ID, etc.) <ref type="bibr" target="#b18">[19]</ref>. All the information is represented as a multi-field categorical feature vector, where each field (e.g. City) is one-hot encoded as discussed in Section I. Such a field-wise one-hot encoding representation results in curse of dimensionality and enormous sparsity <ref type="bibr" target="#b11">[12]</ref>. Besides, there exist local dependencies and hierarchical structures among fields <ref type="bibr" target="#b0">[1]</ref>.</p><p>Thus we are seeking a DNN model to capture high-order latent patterns in multi-field categorical data. And we come up with the idea of product layers to explore feature interactions automatically. In FM, feature interaction is defined as the inner product of two feature vectors <ref type="bibr" target="#b19">[20]</ref>.</p><p>The proposed deep learning model is named as Productbased Neural Network (PNN). In this section, we present PNN model in detail and discuss two variants of this model, namely Inner Product-based Neural Network (IPNN), which has an inner product layer, and Outer Product-based Neural Network (OPNN) which uses an outer product expression. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Product-based Neural Network</head><p>The architecture of the PNN model is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. From a top-down perspective, the output of PNN is a real numberŷ ∈ (0, 1) as the predicted CTR:</p><formula xml:id="formula_0">y = σ(W 3 l 2 + b 3 ),<label>(1)</label></formula><p>where W 3 ∈ R 1×D2 and b 3 ∈ R are the parameters of the output layer, l 2 ∈ R D2 is the output of the second hidden layer, and σ(x) is the sigmoid activation function: σ(x) = 1/(1 + e −x ). And we use D i to represent the dimension of the i-th hidden layer. The output l 2 of the second hidden layer is constructed as</p><formula xml:id="formula_1">l 2 = relu(W 2 l 1 + b 2 ),<label>(2)</label></formula><p>where l 1 ∈ R D1 is the output of the first hidden layer. The rectified linear unit (relu), defined as relu(x) = max(0, x), is chosen as the activation function for hidden layer output since it has outstanding performance and efficient computation. The first hidden layer is fully connected with the product layer. The inputs to it consist of linear signals l z and quadratic signals l p . With respect to l z and l p inputs, separately, the formulation of l 1 is:</p><formula xml:id="formula_2">l 1 = relu(l z + l p + b 1 ),<label>(3)</label></formula><p>where all l z , l p and the bias vector b 1 ∈ R D1 . Then, let us define the operation of tensor inner product:</p><formula xml:id="formula_3">A B i,j A i,j B i,j ,<label>(4)</label></formula><p>where firstly element-wise multiplication is applied to A, B, then the multiplication result is summed up to a scalar. After that, l z and l p are calculated through z and p, respectively:</p><formula xml:id="formula_4">l z = l 1 z , l 2 z , . . . , l n z , . . . , l D1 z , l n z = W n z z<label>(5)</label></formula><p>l p = l 1 p , l 2 p , . . . , l n p , . . . , l D1 p , l n p = W n p p where W n z and W n p are the weights in the product layer, and their shapes are determined by z and p respectively. By introducing a "1" constant signal, the product layer can not only generate the quadratic signals p, but also maintaining the linear signals z, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. More specifically,</p><formula xml:id="formula_5">z = z 1 , z 2 , . . . , z N f 1 , f 2 , . . . , f N , (6) p = {p i,j }, i = 1...N, j = 1...N,<label>(7)</label></formula><p>where f i ∈ R M is the embedding vector for field i. p i,j = g(f i , f j ) defines the pairwise feature interaction. Our PNN model can have different implementations by designing different operation for g. In this paper, we propose two variants of PNN, namely IPNN and OPNN, as will be discussed later.</p><p>The embedding vector f i of field i, is the output of the embedding layer:</p><formula xml:id="formula_6">f i = W i 0 x[start i : end i ],<label>(8)</label></formula><p>where x is the input feature vector containing multiple fields, and x[start i : end i ] represents the one-hot encoded vector for field i. W 0 represents the parameters of the embedding layer, and W i 0 ∈ R M ×(endi−starti+1) is fully connected with field i. Finally, supervised training is applied to minimize the log loss, which is a widely used objective function capturing divergence between two probability distributions:</p><formula xml:id="formula_7">L(y,ŷ) = −y logŷ − (1 − y) log(1 −ŷ),<label>(9)</label></formula><p>where y is the ground truth (1 for click, 0 for non-click), and y is the predicted CTR of our model as in Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inner Product-based Neural Network</head><p>In this section, we demonstrate the Inner Product-based Neural Network (IPNN). In IPNN, we firstly define the pairwise feature interaction as vector inner product :</p><formula xml:id="formula_8">g(f i , f j ) = f i , f j .</formula><p>With the constant signal "1", the linear information z is preserved as:</p><formula xml:id="formula_9">l n z = W n z z = N i=1 M j=1 (W n z ) i,j z i,j .<label>(10)</label></formula><p>As for the quadratic information p, the pairwise inner product terms of g(f i , f j ) form a square matrix p ∈ R N ×N . Recalling the definition of l p in Eq. (5), l n p = N i=1 N j=1 (W n p ) i,j p i,j and the commutative law in vector inner product, p and W n p should be symmetric. Such pairwise connection expands the capacity of the neural network, but also enormously increases the complexity. In this case, the formulation of l 1 , described in Eq.  <ref type="bibr" target="#b19">[20]</ref>, we come up with the idea of matrix factorization to reduce complexity.</p><p>By introducing the assumption that W n p = θ n θ nT , where θ n ∈ R N , we can simplify l 1 's formulation as:</p><formula xml:id="formula_10">W n p p = N i=1 N j=1 θ n i θ n j f i , f j = N i=1 δ n i , N i=1 δ n i<label>(11)</label></formula><p>where, for convenience, we use δ n i ∈ R M to denote a feature vector f i weighted by θ n i , i.e. δ n i = θ n i f i . And we also have δ n = δ n 1 , δ n 2 , . . . , δ n i , . . . , δ n N ∈ R N ×M . With the first order decomposition on n-th single node, we give the l p complete form:</p><formula xml:id="formula_11">l p = i δ 1 i , . . . , i δ n i , . . . , i δ D1 i .<label>(12)</label></formula><p>By reduction of l p in Eq. <ref type="formula" target="#formula_0">(12)</ref>, the space complexity of l 1 becomes O(D 1 M N ), and the time complexity is also O(D 1 M N ). In general, l 1 complexity is reduced from quadratic to linear with respect to N . This well-formed equation makes reusable for some intermediate results. Moreover, matrix operations are easily accelerated in practice with GPUs.</p><p>More generally, we discuss K-order decomposition of W n p at the end of this section. We should point out that W n p = θ n θ T n is only the first order decomposition with a strong assumption. The general matrix decomposition method can be derived that:</p><formula xml:id="formula_12">W n p p = N i=1 N j=1 θ i n , θ j n f i , f j .<label>(13)</label></formula><p>In this case, θ i n ∈ R K . This general decomposition is more expressive with weaker assumptions, but also leading to K times model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Outer Product-based Neural Network</head><p>Vector inner product takes a pair of vectors as input and outputs a scalar. Different from that, vector outer product takes a pair of vectors and produces a matrix. IPNN defines feature interaction by vector inner product, while in this section, we discuss the Outer Product-based Neural Network (OPNN).</p><p>The only difference between IPNN and OPNN is the quadratic term p. In OPNN, we define feature interaction as g(f i , f j ) = f i f T j . Thus for every element in p, p i,j ∈ R M ×M is a square matrix.</p><p>For calculating l 1 , the space complexity is O(D 1 M 2 N 2 ) , and the time complexity is also O(D 1 M 2 N 2 ). Recall that D 1 and M are the hyper-parameters of the network architecture, and N is the number of the input fields, this implementation is expensive in practice. To reduce the complexity, we propose the idea of superposition.</p><p>By element-wise superposition, we can reduce the complexity by a large step. Specifically, we re-define p formulation as</p><formula xml:id="formula_13">p = N i=1 N j=1 f i f T j = f Σ (f Σ ) T , f Σ = N i=1 f i ,<label>(14)</label></formula><p>where p ∈ R M ×M becomes symmetric, thus W n p should also be symmetric. Recall Eq. (5) that W p ∈ R D1×M ×M . In this case, the space complexity of l 1 becomes O(D 1 M (M + N )), and the time complexity is also O(D 1 M (M + N )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussions</head><p>Compared with FNN <ref type="bibr" target="#b11">[12]</ref>, PNN has a product layer. If removing l p part of the product layer, PNN is identical to FNN. With the inner product operator, PNN is quite similar with FM <ref type="bibr" target="#b19">[20]</ref>: if there is no hidden layer and the output layer is simply summing up with uniform weight, PNN is identical to FM. Inspired by Net2Net <ref type="bibr" target="#b20">[21]</ref>, we can firstly train a part of PNN (e.g., the FNN or FM part) as the initialization, and then start to let the back propagation go over the whole net. The resulted PNN should at least be as good as FNN or FM.</p><p>In general, PNN uses product layers to explore feature interactions. Vector products can be viewed as a series of addition/multiplication operations. Inner product and outer product are just two implementations. In fact, we can define more general or complicated product layers, gaining PNN better capability in exploration of feature interactions.</p><p>Analogous to electronic circuit, addition acts like "OR" gate while multiplication acting like "AND" gate, and the product layer seems to learn rules other than features. Reviewing the scenario of computer vision, while pixels in images are real-world raw features, categorical data in web applications are artificial features with high levels and rich meanings. Logic is a powerful tool in dealing with concepts, domains and relationships. Thus we believe that introducing product operations in neural networks will improve networks' ability for modeling multi-field categorical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we present our experiments in detail, including datasets, data processing, experimental setup, model comparison, and the corresponding analysis <ref type="bibr" target="#b0">1</ref> . In our experiments, PNN models outperform major state-of-the-art models in the CTR estimation task on two real-world datasets.</p><p>A. Datasets 1) Criteo: Criteo 1TB click log 2 is a famous ad tech industry benchmarking dataset. We select 7 consecutive days of samples for training, and the next 1 day for evaluation. Because of the enormous data volume and high bias, we apply negative down-sampling on this dataset. Define the down-sampling ratio as w, the predicted CTR as p, the recalibrated CTR q should be q = p/(p + 1−p w ) <ref type="bibr" target="#b3">[4]</ref>. After down-sampling and feature mapping, we get a dataset, which comprises 79.38M instances with 1.64M feature dimensions.</p><p>2) iPinYou: The iPinYou dataset 3 is another real-world dataset for ad click logs over 10 days. After one-hot encoding, we get a dataset containing 19.50M instances with 937.67K input dimensions. We keep the original train/test splitting scheme, where for each advertiser the last 3-day data are used as the test dataset while the rest as the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Comparison</head><p>We compare 7 models in our experiments, which are implemented with TensorFlow 4 , and trained with Stochastic Gradient Descent (SGD).</p><p>LR: LR is the most widely used linear model in industrial applications <ref type="bibr" target="#b21">[22]</ref>. It is easy to implement and fast to train, however, unable to capture non-linear information.</p><p>FM: FM has many successful applications in recommender systems and user response prediction tasks <ref type="bibr" target="#b19">[20]</ref>. FM explores feature interactions, which is effective on sparse data.</p><p>FNN: FNN is proposed in <ref type="bibr" target="#b11">[12]</ref>, being able to capture highorder latent patterns of multi-field categorical data.</p><p>CCPM: CCPM is a convolutional model for click prediction <ref type="bibr" target="#b12">[13]</ref>. This model learns local-global features efficiently. However, CCPM highly relies on feature alignment, and is lack of interpretation. IPNN: PNN with inner product layer III-B. OPNN: PNN with outer product layer III-C. PNN*: This model has a product layer, which is a concatenation of inner product and outer product.</p><p>Additionally, in order to prevent over-fitting, the popular L2 regularization term is added to the loss function L(y,ŷ) when training LR and FM. And we also employ dropout as a regularization method to prevent over-fitting when training neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>Four evaluation metrics are tested in our experiments. The two major metrics are:</p><p>AUC: Area under ROC curve is a widely used metric in evaluating classification problems. Besides, some work validates AUC as a good measurement in CTR estimation <ref type="bibr" target="#b14">[15]</ref>.</p><p>RIG: Relative Information Gain, RIG = 1 − N E, where NE is the Normalized Cross Entropy <ref type="bibr" target="#b3">[4]</ref>.</p><p>Besides, we also employ Log Loss (Eq. (9)) and root mean square error (RMSE) as our additional evaluation metrics. <ref type="table" target="#tab_0">Table I</ref> and II show the overall performance on Criteo and iPinYou datasets, respectively. In FM, we employ 10order factorization and correspondingly, we employ 10-order embedding in network models. CCPM has 1 embedding layer, 2 convolution layers (with max pooling) and 1 hidden layer (5 layers in total). FNN has 1 embedding layer and 3 hidden layers (4 layers in total). Every PNN has 1 embedding layer, 1 product layer and 3 hidden layers (5 layers in total). The impact of network depth will be discussed later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Comparison</head><p>The LR and FM models are trained with L2 norm regularization, while FNN, CCPM and PNNs are trained with dropout. By default, we set dropout rate at 0.5 on network hidden layers, which is proved effective in <ref type="figure">Figure 2</ref>. Further discussions about the network architecture will be provided in Section IV-E.   Firstly, we focus on the AUC performance. The overall results in <ref type="table" target="#tab_0">Table I</ref> and II illustrate that (i) FM outperforms LR, demonstrating the effectiveness of feature interactions; (ii) Neural networks outperform LR and FM, which validates the importance of high-order latent patterns; (iii) PNNs perform the best on both Criteo and iPinYou datasets. As for log loss, RMSE and RIG, the results are similar.</p><p>We also conduct t-test between our proposed PNNs and the other compared models. <ref type="table" target="#tab_0">Table III</ref> shows the calculated p-values under log loss metric on both datasets. The results verify that our models significantly improve the performance of user response prediction against the baseline models.</p><p>We also find that PNN*, which is the combination of IPNN and OPNN, has no obvious advantages over IPNN and OPNN on AUC performance. We consider that IPNN and OPNN are sufficient to capture the feature interactions in multi-field categorical data. <ref type="figure" target="#fig_3">Figure 3</ref> shows the AUC performance with respect to the training iterations on iPinYou dataset. We find that network  models converge more quickly than LR and FM. We also observe that our two proposed PNNs have better convergence than other network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study on Network Architecture</head><p>In this section, we discuss the impact of neural network architecture. For IPNN and OPNN, we take three hyperparameters (or settings) into consideration: (i) embedding layer size, (ii) network depth and (iii) activation function. Since CCPM shares few similarities with other neural networks and PNN* is just a combination of IPNN and OPNN, we only compare FNN, IPNN and OPNN in this section.</p><p>1) Embedding Layer: The embedding layer is to convert sparse binary inputs to dense real-value vectors. Take word embedding as an example <ref type="bibr" target="#b10">[11]</ref>, an embedding vector contains the information of the word and its context, and indicates the relationships between words.</p><p>We take the idea of embedding layer from <ref type="bibr" target="#b11">[12]</ref>. In this paper, the latent vectors learned by FM are explained as node representations, and the authors use a pre-trained FM to initialize the embedding layers in FNN. Thus the factorization order of FM keeps consistent with the embedding order.</p><p>The input units are fully connected with the embedding layer within each field. We compare different orders, like 2, 10, 50 and 100. However, when the order grows larger, it is harder to fit the parameters in memory, and the models are much easier to over-fit. In our experiments, we take 10-order embedding in neural networks.</p><p>2) Network Depth: We also explore the impact of network depth by adjusting the number of hidden layers in FNN and PNNs. We compare different number of hidden layers: 1, 3, 5 and 7. <ref type="figure">Figure 4</ref> shows the performance as network depth grows. Generally speaking, the networks with 3 hidden layers have better generalization on the test set.</p><p>For convenience, we call convolution layers and product layers as representation layers. These layers can capture complex feature patterns using fewer parameters, thus are efficient in training, and generalize better on the test set.   3) Activation Function: We compare three mainstream activation functions: sigmoid(x) = 1 1+e −x , tanh(x) = 1−e −2x 1+e −2x , and relu(x) = max(0, x). Compared with the sigmoidal family, relu function has the advantages of sparsity and efficient gradient, which is possible to gain more benefits on multi-field categorical data. <ref type="figure" target="#fig_5">Figure 5</ref> compares these activation functions on FNN, IPNN and OPNN. From this figure, we find that tanh has better performance than sigmoid. This is supported by <ref type="bibr" target="#b11">[12]</ref>. Besides tanh, we find relu function also has good performance. Possible reasons include: (i) Sparse activation, nodes with negative outputs will not be activated; (ii) Efficient gradient propagation, no vanishing gradient problem or exploding effect; (iii) Efficient computation, only comparison, addition and multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we proposed a deep neural network model with novel architecture, namely Product-based Neural Network, to improve the prediction performance of DNN working on categorical data. And we chose CTR estimation as our working example. By exploration of feature interactions, PNN is promising to learn high-order latent patterns on multi-field categorical data. We designed two types of PNN: IPNN based on inner product and OPNN based on outer product. We also discussed solutions to reduce complexity, making PNN efficient and scalable. Our experimental results demonstrated that PNN outperformed the other state-of-the-art models in 4 metrics on 2 datasets. To sum up, we obtain the following conclusions: (i) By investigating feature interactions, PNN gains better capacity on multi-field categorical data. (ii) Being both efficient and effective, PNN outperforms major stateof-the-art models. (iii) Analogous to "AND"/"OR" gates, the product/add operations in PNN provide a potential strategy for data representation, more specifically, rule representation.</p><p>In the future work, we will explore PNN with more general and complicated product layers. Besides, we are interested in explaining and visualizing the feature vectors learned by our models. We will investigate their properties, and further apply these node representations to other tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Product-based Neural Network Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(3), has the space complexity of O(D 1 N (M + N )), and the time complexity of O(N 2 (D 1 + M )), where D 1 and M are the hyper-parameters about network architecture, N is the number of input fields. Inspired by FM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Learning Curves on the iPinYou Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>AUC Comparison over Various Activation Functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Overall Performance on the Criteo Dataset.</figDesc><table><row><cell>Model</cell><cell>AUC</cell><cell>Log Loss</cell><cell>RMSE</cell><cell>RIG</cell></row><row><cell>LR</cell><cell>71.48%</cell><cell>0.1334</cell><cell>9.362e-4</cell><cell>6.680e-2</cell></row><row><cell>FM</cell><cell>72.20%</cell><cell>0.1324</cell><cell>9.284e-4</cell><cell>7.436e-2</cell></row><row><cell>FNN</cell><cell>75.66%</cell><cell>0.1283</cell><cell>9.030e-4</cell><cell>1.024e-1</cell></row><row><cell>CCPM</cell><cell>76.71%</cell><cell>0.1269</cell><cell>8.938e-4</cell><cell>1.124e-1</cell></row><row><cell>IPNN</cell><cell>77.79%</cell><cell>0.1252</cell><cell>8.803e-4</cell><cell>1.243e-1</cell></row><row><cell>OPNN</cell><cell>77.54%</cell><cell>0.1257</cell><cell>8.846e-4</cell><cell>1.211e-1</cell></row><row><cell>PNN*</cell><cell>77.00%</cell><cell>0.1270</cell><cell>8.988e-4</cell><cell>1.118e-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Overall Performance on the iPinYou Dataset.</figDesc><table><row><cell>Model</cell><cell>AUC</cell><cell>Log Loss</cell><cell>RMSE</cell><cell>RIG</cell></row><row><cell>LR</cell><cell>73.43%</cell><cell>5.581e-3</cell><cell>5.350e-07</cell><cell>7.353e-2</cell></row><row><cell>FM</cell><cell>75.52%</cell><cell>5.504e-3</cell><cell>5.343e-07</cell><cell>8.635e-2</cell></row><row><cell>FNN</cell><cell>76.19%</cell><cell>5.443e-3</cell><cell>5.285e-07</cell><cell>9.635e-2</cell></row><row><cell>CCPM</cell><cell>76.38%</cell><cell>5.522e-3</cell><cell>5.343e-07</cell><cell>8.335e-2</cell></row><row><cell>IPNN</cell><cell>79.14%</cell><cell>5.195e-3</cell><cell>4.851e-07</cell><cell>1.376e-1</cell></row><row><cell>OPNN</cell><cell>81.74%</cell><cell>5.211e-3</cell><cell>5.293e-07</cell><cell>1.349e-1</cell></row><row><cell>PNN*</cell><cell>76.61%</cell><cell>4.975e-3</cell><cell>4.819e-07</cell><cell>1.740e-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>P-values under the Log Loss Metric.</figDesc><table><row><cell>Model</cell><cell></cell><cell>LR</cell><cell>FM</cell><cell>FNN</cell><cell>CCPM</cell></row><row><cell>IPNN</cell><cell></cell><cell>&lt; 10 −6</cell><cell>&lt; 10 −6</cell><cell>&lt; 10 −6</cell><cell>&lt; 10 −6</cell></row><row><cell>OPNN</cell><cell></cell><cell>&lt; 10 −6</cell><cell>&lt; 10 −5</cell><cell>&lt; 10 −6</cell><cell>&lt; 10 −6</cell></row><row><cell></cell><cell>78.0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>77.5%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Criteo AUC</cell><cell>76.0% 76.5% 77.0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>75.5%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>75.0%</cell><cell cols="3">Dropout 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</cell></row><row><cell cols="6">Fig. 2: AUC Comparison of Dropout (OPNN).</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We release the repeatable experiment code on GitHub: https://github.com/ Atomu2014/product-nets<ref type="bibr" target="#b1">2</ref> Criteo terabyte dataset download link: http://labs.criteo.com/downloads/ download-terabyte-click-logs/.<ref type="bibr" target="#b2">3</ref> iPinYou dataset download link: http://data.computational-advertising.org. We only use the data from season 2 and 3 because of the same data schema.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">TensorFlow: https://www.tensorflow.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Response prediction using collaborative filtering with hierarchies and side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>Chitrapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimizing web search using web click-through data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal real-time bidding for display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimating conversion rate in display advertising from past erformance data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Orten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasdan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="768" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Factorization machines with follow-the-regularized-leader for ctr prediction in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-P</forename><surname>Ta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE BigData. IEEE</publisher>
			<biblScope unit="page" from="2889" to="2891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bid landscape forecasting in online ad exchange marketplace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Speech recognition with deep recurrent neural networks,&quot; in ICASSP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep learning over multi-field categorical data: A case study on user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A convolutional click prediction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1743" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Predicting clicks: estimating the click-through rate for new ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>WWW. ACM</publisher>
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft&apos;s bing search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Borchert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">User response learning for directly optimizing campaign performance in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sequential click prediction for sponsored search with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5772</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training product unit neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ismail</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.7073</idno>
		<title level="m">Real-time bidding benchmarking with ipinyou dataset</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
