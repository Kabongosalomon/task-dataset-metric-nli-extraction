<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Ensemble-Distillation for Semantic Segmentation Based Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Hao</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Wun</forename><surname>Cheng</surname></persName>
							<email>bobcheng15@gapp.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yi</forename><surname>Lee</surname></persName>
							<email>cylee@gapp.nthu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elsa</forename><surname>Lab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing Hua University</orgName>
								<address>
									<settlement>Hsinchu</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Ensemble-Distillation for Semantic Segmentation Based Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent researches on unsupervised domain adaptation (UDA) have demonstrated that end-to-end ensemble learning frameworks serve as a compelling option for UDA tasks. Nevertheless, these end-to-end ensemble learning methods often lack flexibility as any modification to the ensemble requires retraining of their frameworks. To address this problem, we propose a flexible ensemble-distillation framework for performing semantic segmentation based UDA, allowing any arbitrary composition of the members in the ensemble while still maintaining its superior performance. To achieve such flexibility, our framework is designed to be robust against the output inconsistency and the performance variation of the members within the ensemble. To examine the effectiveness and the robustness of our method, we perform an extensive set of experiments on both GTA5→Cityscapes and SYNTHIA→Cityscapes benchmarks to quantitatively inspect the improvements achievable by our method. We further provide detailed analyses to validate that our design choices are practical and beneficial. The experimental evidence validates that the proposed method indeed offer superior performance, robustness and flexibility in semantic segmentation based UDA tasks against contemporary baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past few years, semantic segmentation has been attracting the attention of computer vision researchers. Many supervised semantic segmentation methods have been proposed and achieved remarkable performance <ref type="bibr">[1]</ref><ref type="bibr">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr">[4]</ref><ref type="bibr">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr">[7]</ref><ref type="bibr">[8]</ref><ref type="bibr">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Typically, those supervised semantic segmentation methods require abundant labeled training data, which are usually expensive to annotate and are commonly unavailable in most real-world scenarios. To resolve this problem, semantic segmentation based unsupervised domain adaptation (UDA) methods  have been introduced to bridge different domains. These semantic segmentation based UDA models learn to generalize to a target domain by training with the annotated data from a source domain and the unlabeled data from a target domain. Among these works, the authors   <ref type="figure" target="#fig_12">Figure 1</ref>: An illustrative example of (1) the inconsistency issue and (2) the performance variation issue mentioned in Section 1. For <ref type="bibr">(1)</ref>, the models t1, ..., tn are trained with ADA methods, which produce relatively low-certainty outputs. In contrast, the model tn+1 is trained with self-training method and outputs high-certainty predictions. After averaging, the ensemble predicts class A as its final prediction instead of the majority consensus, i.e., class C. For <ref type="bibr">(2)</ref>, the models t1, ..., tn are assumed to be high-performing members, while tn+1 is an under-performing one. After averaging, the high-certainty predictions from tn+1 could dominate the output of the ensemble, and cause the overall performance to degrade.</p><p>in <ref type="bibr">[14-25, 31, 37-39]</ref> resorted to adversarial domain adaptation (ADA) methods, through which the domain discrepancy is minimized by using their adversarial training schemes. Another branch of works has opted for self-training frameworks <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>, which aim to improve the stability of their models during deployment by minimizing the entropy of the models' predictions in a target domain. These ADA and self-training methods have demonstrated how a single model is able to learn to generalize to an annotation-less target domain. However, they only learn from a single distribution, leaving space for further improvements. Recently, in light of the potential benefits of combining multiple UDA models, a number of works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> have attempted to borrow the concepts from ensemble learning. These works demonstrated how a group of UDA models can be trained simultaneously in an end-to-end fashion to learn different distributions of semantic information, and meanwhile transferring the knowledge to a compact student model. Despite their successes in bridging the domain gaps with multiple learners, these ensemble learning methods often lack flexibility as any modification to the teacher ensemble requires complete retraining of the whole framework.</p><p>To address such a problem, the concept of ensembledistillation <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b49">[49]</ref><ref type="bibr" target="#b50">[50]</ref> can be leveraged since its focus is on designing an effective distillation process instead of a costly end-to-end ensemble learning framework. Typically, these ensemble distillation frameworks view the members in an ensemble as probabilistic models, and transfer the knowledge using expected certainty outputs. Nonetheless, the robustness of these methods is not guaranteed as they do not carefully take into account the followings: <ref type="bibr">(1)</ref> the inconsistency in the scale of the output certainty values among the members in an ensemble (abbreviated as the 'inconsistency issue' hereafter), and (2) the performance variations across the members in an ensemble (abbreviated as the 'performance variation issue' hereafter). An example of these two issues is illustrated in <ref type="figure" target="#fig_12">Fig. 1</ref>. For the former, since each teacher model in the ensemble can be trained independently using different methods (e.g., ADA, self-training, data augmentation, or compound usage of them), the scale of the output certainty values may not be consistent across the ensemble. This may result in a situation that few members' decisions with high certainty values dominate the entire ensemble's output. As a result, the outputs from the members in an ensemble should be treated in an equal manner, as the inconsistency in their certainty values may come from their different training objectives instead of the real data distribution in the target domain. For the latter, since the performance (either per-class or average accuracy) of each teacher model in the ensemble may vary substantially, few underperforming members in the ensemble may cause the quality of the combined prediction to degrade significantly. This problem is especially severe under the context of UDA, since the ground truth labels in the target domain are unavailable, and the performance of the ensemble in the target domain is actually unknown. The above observations suggest that an effective mechanism is necessary to deal with these two issues and prevent them from influencing the quality of the combined predictions.</p><p>Being aware of these problems, we introduce a novel ensemble-distillation framework to avoid the aforementioned pitfalls. First, to tackle the certainty inconsistency issue, we introduce an output unification method in the framework to reduce the impact of the inconsistent scales of the certainty outputs. Next, we embrace a new category of fusion function in our framework, named channel-wise fusion, to resolve the performance variation issue. Moreover, we design a method to determine the fusion policy of the proposed channel-wise fusion function to further enhance its effectiveness. To validate our designs, we evaluate the proposed framework with two commonlyadopted metrics, GTA5 <ref type="bibr" target="#b51">[51]</ref>→Cityscapes <ref type="bibr" target="#b52">[52]</ref> and SYN-THIA <ref type="bibr" target="#b53">[53]</ref>→Cityscapes, to demonstrate the effectiveness and robustness of our framework against a number of baselines. The contributions are summarized as follows:</p><p>• We introduce a flexible UDA ensemble-distillation framework which is robust against the inconsistency in the scale of the output certainty values and the performance variations among the members in an ensemble. • We propose a new category of fusion function, called channel-wise fusion, along with a fusion policy selection strategy as well as a conflict resolving mechanism to enhance its effectiveness. • We evaluate our framework under various configurations, and demonstrate that it is able to outperform the baselines in terms of its robustness and effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Unsupervised Domain Adaptation: A number of methods have been proposed to bridge the discrepancy between different domains. One branch of these works adopted ADA frameworks to learn representations of their target domains <ref type="bibr">[14-25, 31, 37-39]</ref>. These approaches typically employ a generator and a discriminator trained against each other to minimize the domain gap, and have shown significant improvements over those trained directly in the source domains. Another line of works has turned their attention to self-training and data augmentation measures to tackle UDA problems. For those works utilizing self-training, the concentration was mainly on preventing overfitting by using regularization <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> or class-balancing <ref type="bibr" target="#b26">[27]</ref> when minimizing the uncertainty in their target domains. The authors in <ref type="bibr" target="#b29">[30]</ref> extended the concept of self-training and proposed a data augmentation technique. Their proposed method fine-tunes a model with mixed labels generated by combining ground truth annotations from a source domain and pseudo labels from a target domain. Recent researchers employed ensemble learning frameworks to resolve UDA problems <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. The authors in <ref type="bibr" target="#b40">[41]</ref> proposed an end-to-end ensemble framework to solve UDA classification problems. The authors in <ref type="bibr" target="#b39">[40]</ref> extended the idea of ensemble learning and proposed a joint learning ensemble framework to solve person reidentification UDA problems. These works showed how the ensemble learning frameworks can be integrated into UDA.</p><p>Pseudo Labeling: Pseudo labeling is a self-training method originally proposed to improve the performance of classification networks <ref type="bibr" target="#b54">[54]</ref>, and is usually accomplished by minimizing the entropy of a model's predictions on unseen data. Pseudo labeling enables a better decision boundary to be achieved as the certainty of a model's prediction increases <ref type="bibr" target="#b54">[54]</ref>. This concept has been further extended to the field of semantic segmentation, and has gained success by incorporating the information of unlabeled data. Since selftraining via pseudo labeling and UDA share many similar characteristics in terms of their problem formulations, it has recently been used to solve UDA problems <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>.</p><p>Ensemble-Distillation Method: Ensemble-distillation is an extension of knowledge distillation. The authors in <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b45">46]</ref> studied how the knowledge of a teacher model ensemble can be transferred to a student by training it with the soft predictions of the ensemble. They adopted averaging operation for combining the predictions and used KL-divergence as the loss function to transfer the knowledge. The authors in <ref type="bibr" target="#b50">[50]</ref> aimed at resolving the diversity collapse issue in the ensemble-distillation problem. They argued that the averaging operation harms the diversity of the models in an ensemble and proposed to use a prior network <ref type="bibr" target="#b55">[55]</ref> to estimate the distributions of their output uncertainties. These works have demonstrated their effectiveness under supervised training settings. However, the existing ensemble-distillation methods are not designed to handle unsupervised tasks, and are susceptible to the issues introduced in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head><p>Problem Definition: For semantic segmentation based UDA problems, a model has access to the image-label pairs, x src , y src , from a source domain dataset D src , but only the images x tgt from a target domain dataset D tgt . The training objective is to train the model such that its predictions can best estimate the ground truth labels y tgt in the target domain. In other words, the mean intersection-over-union (mIoU) between the predictions of the model and y tgt should be maximized. In the problem formulation concerned by this paper, a pretrained model ensemble T is given, where each member in T is separately trained using any arbitrary semantic segmentation based UDA method. The goal is to develop an ensemble-distillation strategy that can effectively integrate the knowledge from T and distill it into a single student model, in a way that the mIoU of the student's predictions for the instances in D tgt is maximized.</p><p>Previous Ensemble-Distillation Method: In this section, we explain how the concepts of the previous ensembledistillation works <ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b49">[49]</ref> can be borrowed to perform semantic segmentation based UDA ensemble-distillation tasks. Typically, these works view T as a set of probabilistic models, and complete the ensemble-distillation process through minimizing the negative log-likelihood loss L KL between the expected outputs from the ensemble and the student model, as depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>   these methods under the settings of semantic segmentation based UDA can be formulated as:</p><formula xml:id="formula_0">L KL = − p∈I c∈Cs (p,c) log(r (p,c) ),<label>(1)</label></formula><p>where I is a set of pixels in an image, C is a given set of semantic classes. r (p,c) ∈ R |I|×|C| is the student's certainty output, ands (p,c) ∈ R |I|×|C| represents the expected probabilistic prediction for class c ∈ C at pixel p ∈ I. A common way to captures is through averaging, expressed as follows:</p><formula xml:id="formula_1">s (p,c) = 1 |T | t∈Tŝ (p,c,t) ,<label>(2)</label></formula><p>whereŝ (p,c,t) ∈ R |I|×|C|×|T | is the probabilistic prediction from t ∈ T for class c ∈ C at pixel p ∈ I on target instances. As a result, the knowledge of the teacher ensemble can be transferred through minimizing L KL in a target domain.</p><p>Pitfalls: As discussed in Section 1, directly adopting previous methods to solve semantic segmentation based UDA problems is problematic because of the inconsistency issue and the performance variation issue. For the former, the scale ofŝ may vary across the models in T under our problem formulation. This suggests that a direct operation, such as the averaging operation in Eq. (2), is inappropriate as a feŵ s with high certainty values may dominate the ensemble's output decision. For the performance variation issue, the perclass or the average performance of each member in T can vary substantially under our problem formulation. Since the fusion function formulated in Eq. (2) fuses the predictions of all teacher models, the under-performing members in T can influence the quality of the fused results. Therefore, the adoption of such a fusion function is inappropriate as it may be sensitive to the performance variations within T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>To address the aforementioned problems, we introduce a new ensemble-distillation framework, and illustrate it in <ref type="figure" target="#fig_1">Fig. 2</ref> (a). The main difference between the proposed method and the previous ones lies in two aspects: Output Unification and Fusion Function. In Sections 4.1 and 4.2, we walk through the designs of the output unification operation  as well as the fusion function, and discuss how they may contribute to resolving the aforementioned issues. Finally, in Section 4.3, we formulate and summarize the proposed ensemble-distillation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Output Unification</head><p>To resolve the inconsistency issue, we argue that the soft predictionsŝ in Eq. (2) should be unified first in the target domain, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. This additional unification operation ensures that the raw output certainty values from the models in T do not directly influence the subsequent fusion results. To accomplish this, we unify the soft predictions by converting them to pseudo labels, so as to make them all bear the same scale, i.e., representing the final decisions of the models in T . The unification operation is formulated as:</p><formula xml:id="formula_2">y (p,c,t) =    1, if c = arg max c∈C {ŝ (p,c,t) } 0, otherwise ,<label>(3)</label></formula><p>whereŷ (p,c,t) is the unified output prediction from t ∈ T for class c ∈ C at pixel p ∈ I in the target domain. This operation ensures that the subsequent fusion function can operate on items with a consistent scale, and thus eliminates the impact of the inconsistency in the original certainty outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fusion Function</head><p>We next move on to focus on investigating a fusion function that can take advantage of the unified predictions to achieve robustness against the performance variations issue. We compare two categories of fusion functions: pixel-wise fusion and channel-wise fusion. The former is a direct conversion from Eq. (2) and is used as our baseline method. The latter is the proposed method and is adopted to address the performance variation issue. Both pixel-wise fusion and channel-wise fusion are mapping functions f : I → C 0 that assign a class label c ∈ C 0 for the pixel p ∈ I in the fusion output based onŷ (p,c,t) , where C 0 := C ∪ {c 0 } is a set that includes all c ∈ C as well as the unlabeled symbol c 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pixel-Wise Fusion</head><p>Pixel-wise fusion (f P ixel ) adopts a statistical view on T , and is designed to capture the average behavior of the ensemble. As depicted in <ref type="figure" target="#fig_3">Fig. 3</ref>, pixel-wise fusion treats each pixel in a semantic segmentation map as the basic unit of the fusion operation. Specifically, the fused result of each pixel is determined by taking majority voting among the   predictions from T , and is implemented as the following:</p><formula xml:id="formula_3">f P ixel (p) = arg max c∈C t∈Tŷ (p,c,t) ,<label>(4)</label></formula><p>whereŷ is the unified output generated according to Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Channel-Wise Fusion</head><p>Based on a different perspective, channel-wise fusion (f Channel ) treats each class channel as the basis for fusion, as depicted in <ref type="figure" target="#fig_3">Fig. 3</ref>. Instead of fusing the outputs of all t ∈ T , channel-wise fusion relies on a fusion policy π : C → T , which is a mapping function for recombining the unified outputs from different teacher models. More specifically, for each class c ∈ C, the fusion policy π selects that class channel from the unified outputŷ (p,c,t) of a teacher model t ∈ T , as illustrated in the example shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. Given such a π, the channel-wise fusion function is formulated as:</p><formula xml:id="formula_4">f Channel (p) =      (i) ε, if p ∈ A π o (ii) c, if p ∈ A π c \ A π o , (iii) c 0 , otherwise<label>(5)</label></formula><p>where (i) is the condition that p is labeled by multiple teachers, (ii) is the condition that p labeled as a certain class by a single teacher, and (iii) is the condition that p is unlabeled, as illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>. In Eq. (5), ε denotes a class label to be assigned in scenario (i), c is a class in C in scenario (ii), and c 0 denotes the unlabeled symbol in scenario (iii).</p><formula xml:id="formula_5">A π c := {p | p ∈ I, π(c) = t,ŷ (p,c,t) = 1}</formula><p>is a set of pixels comprisingŷ (p,c,t) for a given class c generated by a teacher t selected according to the given fusion policy π. Since different A π c may be produced by different t ∈ T for different c ∈ C, the pixels they cover are not necessarily mutually exclusive. Therefore, A π o is defined to represent the set of pixels labeled by multiple teachers, expressed as follows:</p><formula xml:id="formula_6">A π o = c1 =c2, c1,c2∈C (A π c1 ∩ A π c2 ).<label>(6)</label></formula><p>Training Testing High-quality pseudo labels</p><p>Low-quality pseudo labels Uncertain Certain <ref type="figure">Figure 6</ref>: An illustration of how the quality of a teacher model's pseudo labels can affect the output certainty values of the student model. If the student model is trained with high-quality pseudo labels (i.e., pseudo labels with high IoU's w.r.t. ytgt), it can learn a mapping from input features to the segmentation mask effectively, and generates high-certainty predictions. In contrast, if the student model is trained with low-quality pseudo labels (i.e., pseudo labels with low IoU's w.r.t ytgt) that are mismatched with the input features, the student's output certainty values are likely to degrade.</p><p>The mechanism that assigns the value of ε for all p ∈ A π o is referred as the conflict-resolving mechanism. In this work, we employ a spatially-aware conflict-resolving mechanism that assigns a class label for each pixel in A π o using majority voting on a kernel. The size of the kernel is denoted as κ, and the set of pixels covered by the kernel centered at p is referred to as B κ p . The mechanism is formulated as follows:</p><formula xml:id="formula_7">ε = arg max c∈C π p |B κ p ∩ A π c |.<label>(7)</label></formula><p>where C π p := {c | c ∈ C; p ∈ A π c } represents a set of class(es) assigned to a pixel p under A π c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Theoretical Properties of Channel-Wise Fusion</head><p>Let Φ (c,t) andΦ (c,π(c)) be the per-class IoU's w.r.t. y tgt for the pseudo labels generated by t ∈ T and the fused pseudo labels generated by f Channel , respectively. Channelwise fusion conforms to the following properties: Proposition 1. Consider an arbitrary fusion policy π. Given a constant α ∈ (0, 1) and classes c 1 , ..., c n ∈ C. If Φ (ci,t) ≥ α, ∀i ∈ {1, ..., n}, ∀t ∈ T and |A π o | = 0, we have:</p><formula xml:id="formula_8">mIoU = 1 |C| c∈CΦ (c,π(c)) ≥ nα |C| .<label>(8)</label></formula><p>Proposition 2. Consider an optimal fusion policy π * (c) = arg max t∈T {Φ (c,t) }. Assume |A π * o | = 0, we have:</p><formula xml:id="formula_9">mIoU = 1 |C| c∈CΦ (c,π * (c)) ≥ 1 |C| c∈C Φ (c,t) , ∀t ∈ T . (9)</formula><p>For the detailed elaborations and proofs with regard to these properties, please refer to the supplementary materials. Proposition 1 states the condition when the mIoU lower bound can be ensured. On the other hand, Proposition 2 describes how the effectiveness of channel-wise fusion can be maximized. In the next section, we discuss how an effective π can be determined.  <ref type="table">RD  SW  BD  WL  FE  PE  LT  SN  VG  TN  SY  PN  RR  CR  TK  BS  TN</ref> MR BE SYN <ref type="figure">Figure 7</ref>: The cosine similarity between the per-class IoU of each t ∈ T and the per-class certainty values of the student evaluated on the training set of Cityscapes, i.e., the cosine similarity between Φ (c,t) in Proposition 2 and ρ (c,t) in Eq. (10). The experimental results reveal that the two variables are positively correlated to each other, as the cosine similarity values for all c ∈ C are greater than 0 and close to 1. For the detailed settings, please refer to Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTA5</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cosine Similarity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Certainty-Aware Policy Selection Strategy</head><p>Since the fusion policy π determines which teacher model is allowed to involve in f Channel , choosing an appropriate π is therefore crucial to the robustness of our framework. In order to achieve this objective, a suitable measure is necessary for evaluating the quality of each teacher's predictions in the target domain without using any target domain ground truth. The experimental clue illustrated and explained in <ref type="figure">Fig. 6</ref> and 7 offers an empirical manner for the above purpose. From <ref type="figure">Fig. 6</ref>, it is observed that the quality of the unified outputŝ y (p,c,t) , i.e., the pseudo labels, from t ∈ T are positively correlated to the output certainty values of distilled student model. This correlation suggests that any low-qualityŷ (p,c,t) , which might be generated by some under-performing teacher models, can confuse the student model and causes its output certainty values to degrade. This correlation between the quality of the pseudo labels from t ∈ T and the student's output certainty values thus shed light on the development of the measure for approximating a teacher model's performance without using target domain ground truth. In practice, an offline fusion policy selection strategy is adopted. Our framework first performs knowledge distillation on all t ∈ T and transfers their knowledge to |T | identical student models using the unified outputs. Then, their output certainty values are measured to obtain the approximated performance of their corresponding teacher models. Finally, for each c ∈ C, a t ∈ T that maximizes the student model's output certainty values is selected. The fusion policy π is written as follows:</p><formula xml:id="formula_10">π(c) = arg max t∈T {ρ (c,t) },<label>(10)</label></formula><p>where ρ (c,t) ∈ R |C|×|T | refers to the average certainty outputs of class c ∈ C from the student model trained with the unified outputs generated by teacher t ∈ T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Proposed Framework</head><p>Based on the formulations of output unification and fusion function, the loss function L CE for performing the ensembledistillation in our framework is defined as follows:</p><formula xml:id="formula_11">L CE = − p∈I c∈Cỹ (p,c) log(r (p,c) ),<label>(11)</label></formula><p>whereỹ (p,c) ∈ {0, 1} |I|×|C| is the fused results, defined as:</p><formula xml:id="formula_12">y (p,c) = 1, if c = f Channel (p) 0, otherwise .<label>(12)</label></formula><p>The pseudo code of the ensemble-distillation method in our framework is summarized in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Baselines and Evaluation Methods: In this work, we evaluate and compare the experimental results in terms of the effectiveness and the robustness. To examine the effectiveness of the proposed framework, we compare our method against two ensemble-distillation schemes and a number of UDA baselines. The ensemble-distillation schemes include EnD <ref type="bibr" target="#b42">[43]</ref> and its recent revision EnD 2 <ref type="bibr" target="#b50">[50]</ref>. The semantic segmentation based UDA baselines cover APODA <ref type="bibr" target="#b19">[20]</ref>, PatchAlign <ref type="bibr" target="#b20">[21]</ref>, AdvEnt <ref type="bibr" target="#b21">[22]</ref>, FDA-MBT <ref type="bibr" target="#b35">[36]</ref>, PIT <ref type="bibr" target="#b34">[35]</ref>, CBST <ref type="bibr" target="#b26">[27]</ref>, MRKLD <ref type="bibr" target="#b27">[28]</ref>, R-MRNet <ref type="bibr" target="#b28">[29]</ref>, and DACS <ref type="bibr" target="#b29">[30]</ref>.</p><p>To examine the robustness of our framework, we select the members for T based on two criteria: (1) each member in T is trained with different UDA methods; and (2) there exists large per-class and average performance variations among the members in T . According to these criteria, we select DACS <ref type="bibr" target="#b29">[30]</ref> (data augmentation), R-MRNet <ref type="bibr" target="#b28">[29]</ref> (adversarial training), MRKLD <ref type="bibr" target="#b27">[28]</ref> (self-training), and CBST <ref type="bibr" target="#b26">[27]</ref> (self-training) to form T in our experiments. We evaluate the proposed framework and the baselines on two commonly adopted benchmarks: GTA5 <ref type="bibr" target="#b51">[51]</ref>→Cityscapes <ref type="bibr" target="#b52">[52]</ref> and SYNTHIA <ref type="bibr" target="#b53">[53]</ref>→Cityscapes. For the former, the models have access to 24,966 image-label pairs from the training set of GTA5, and 2,975 images from the training set of Cityscapes. We evaluate the student model's per-class IoU's of the 19 semantic classes as well as the its mIoU's on the validation set of Cityscapes. For the latter, the models have access to 9,400 image-label pairs from the training set of SYNTHIA, and 2,975 images from the training set of Cityscapes. In a similar fashion, we evaluate the student model's per-class IoU's of 13 and 16 semantic classes as well as its mIoU's on the validation set of Cityscapes.</p><p>Implementation Details: For the student model, we adopt Deeplabv3+ <ref type="bibr" target="#b11">[12]</ref> architecture with DRN-D-54 <ref type="bibr">[1]</ref> as our backbone, which is trained using SGD with a learning rate initialized to 2.5 × 10 −4 and decreased with a factor of 0.9. The weight decay is set to 5 × 10 −3 , the momentum is set to 0.9, and the batch size is set to 10 for 100K iterations with early stopping. The value of κ in f Channel is set to 13. During the process of certainty-aware policy selection strategy, 500 images from the training set of Cityscapes are used for measuring ρ (c,t) , while the other 2475 images are used for training the student. The student model is pretrained in the source domain, and fine-tuned with (x src , y src ) and (x tgt ,ỹ) during the distillation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>In this section, we present a number of experiments to validate the design of our framework. First, we compare our framework against a number of baselines and demonstrate its superior performance. Next, according to the experimental results, we show that the output unification operation can provide robustness against the inconsistency issue. Then, we present another experiment, in which under-performing members are added to T to create performance variations, to demonstrate the robustness of our framework with f Channel against the performance variation issue. In addition, we perform experiments to validate the effectiveness of the fusion policy selection strategy and analyze how the value of κ in the conflict resolving mechanism can impact the performance. Finally, we explore the flexibility of our framework by adding additional teacher models in an iterative manner, and show that our framework is able to evolve with time. <ref type="table">Table 1</ref> first demonstrates the quantitative results of the proposed framework against a number of baselines mentioned in Section 5 on the two benchmarks: GTA5→Cityscapes and SYNTHIA→Cityscapes. It is observed that the student models trained under our proposed framework with f Channel (i.e., 'Ours (Channel)') is able to outperform the previous ensemble-distillation baselines, i.e., EnD <ref type="bibr" target="#b42">[43]</ref> and EnD 2 <ref type="bibr" target="#b50">[50]</ref>, by a margin of 6.64% mIoU and 6.02% mIoU on GTA5→Cityscapes, and 6.41% mIoU and 4.57% mIoU on SYNTHIA→Cityscapes, respectively. In addition, it is also observed that the student model trained with the proposed framework with f Channel (i.e., 'Ours (Channel)') is able to outperform that with the baseline f P ixel (i.e., 'Ours (Pixel)') by a margin of 4.72% mIoU on GTA5→Cityscapes, and 4.52% mIoU on SYNTHIA→Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Quantitative Results on the Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Robustness of the Proposed Framework</head><p>In this section, we validate the robustness of the proposed framework against the two issues discussed in Section 1. First, to verify that the proposed output unification operation can provide robustness against the inconsistency issue, we leverage the insights from an experiment conducted on T , with the members of T bearing substantial output certainty scale variations, as shown in <ref type="figure" target="#fig_7">Fig. 8</ref>. The results from <ref type="table">Table 1</ref> reveal that the performance of the student models trained with 'Ours (Pixel)', which is basically the ensembledistillation baseline EnD <ref type="bibr" target="#b42">[43]</ref> equipped with the proposed output unification method, is able to outperform EnD by a noticeable margin on both benchmarks. This implies that the adoption of the output unification method is able to provide robustness against the inconsistency issue. Nevertheless, as discussed in Section 4.2, 'Ours (Pixel)' may still be vulnerable to the performance variations of the members in T .  <ref type="table">Table 1</ref>: The quantitative results evaluated on the GTA5→Cityscapes and SYNTHIA→Cityscapes benchmarks. The numbers presented in the middle and the last two columns correspond to per-class IoUs, mIoU, and mIoU*, respectively. mIoU* represents the mean IoU over all the semantic classes excluding those with superscript *, and is adopted by a few baseline methods in their original papers. The models used in our semantic segmentation based UDA model ensemble T are highlighted in blue. The setting 'Source Only' indicates that the student model is trained only with the source domain ground truth annotations. The evaluation results of EnD <ref type="bibr" target="#b42">[43]</ref> and EnD 2 <ref type="bibr" target="#b50">[50]</ref> are obtained from our self-implemented models, while those of the remaining baselines are directly obtained from their original papers.  <ref type="bibr" target="#b28">[29]</ref> MRKLD <ref type="bibr" target="#b27">[28]</ref> CBST <ref type="bibr" target="#b26">[27]</ref> 3e8 6e8 9e8 0 0.8 0.6   <ref type="figure">Figure 9</ref>: The performance comparison of our framework with f Channel ('Ours (Channel)'), our framework with f P ixel ('Ours (Pixel)'), and the baseline EnD and EnD 2 methods, with under-performing members added to T . In this experiment, the members in T are evaluated on the GTA5→Cityscapes benchmark.</p><p>To inspect the robustness of the proposed framework with channel-wise fusion ('Ours (Channel)') against the performance variation issue, we next look into an experi-ment that introduces performance variations by adding underperforming members into T . Specifically, we use the models trained using only source domain instances, i.e., 'Source Only' in <ref type="table">Table 1</ref>, as the under-performing members. The analysis is presented in <ref type="figure">Fig. 9</ref>. It is observed that the performance of the students trained with 'Ours (Pixel)', EnD, and EnD 2 all degrade when the number of under-performing members increases. In contrast, the students trained using 'Ours (Channel)' is able to maintain their performance despite the inclusion of the under-performing members in T . These results demonstrate the significance of preventing clueless incorporation of information from all t ∈ T , and highlight the effectiveness of f Channel in providing robustness against the unfavorable performance variation issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis on Channel-Wise Fusion</head><p>Fusion Policy: To examine the effectiveness of π selected according to the strategy described in Section 4.2.4, we design two additional fusion policies, π rnd and π tgt , for comparison purposes. π rnd designates a t ∈ T for each c ∈ C randomly, while π tgt carries out this designation greedily according to the oracle performance of the models in T in the target domain (i.e., π * described in Proposition 2). As shown in <ref type="table" target="#tab_8">Table 3</ref>, the mIoU of the fused pseudo labels generated based on the π selected according to the proposed strategy is 7.09% higher than that with π rnd . It is also observed that the mIoU of the fused pseudo labels generated     based on this π is very close to the mIoU obtained from π tgt . These pieces of evidence validate that the proposed policy selection strategy is able to generate a fusion policy π that is very close to the optimal policy π * without leveraging any sort of unavailable target domain ground truth annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict-Resolving Mechanism:</head><p>To investigate the effectiveness of the conflict-resolving mechanism described in Section 4.2 and Eq. <ref type="formula" target="#formula_7">(7)</ref>, we perform parameter analysis on f Channel with different choices of κ. In this analysis, the fusion policy adopted is π rnd to ensure that results are independent of the design of π. <ref type="figure" target="#fig_9">Fig. 10</ref> shows the visualized results of the fused pseudo labels with different κ. <ref type="table" target="#tab_6">Table 2</ref> reports the mIoU gained for various κ with respect to the condition κ = 1. It is observed that pixel predictions in the overlapped area can be better determined if a moderate collection of predictions from the neighboring pixels are taken into account. However, if κ becomes too large, an excessive amount of unrelated semantic information is included in the conflict resolving mechanism, and degrades the quality of the fused pseudo labels. <ref type="table" target="#tab_8">Table 3</ref> shows that different policies, e.g., π and π tgt , can also benefit from the conflict-resolving mechanism, thus justifying it from a different perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Flexibility of the Proposed Framework</head><p>In comparison to end-to-end UDA ensemble learning methods, our framework is more flexible as the proposed framework can operate on any arbitrary compositions of T . Such flexibility enables our framework to evolve with time as (1) a model trained with any newly developed UDA method can be integrated into our framework, and (2) the student models can be added back to T and further improve  <ref type="figure" target="#fig_12">Figure 11</ref>: The performance of the student models trained with the proposed framework using f Channel with different compositions of T . The experiment is performed on GTA5→Cityscapes. For settings 'S4', 'S5' and 'S6', the student models trained under the previous settings, i.e., 'S3', 'S4', and 'S5', are added back to T . the overall performance. To demonstrate these two merits of our framework, we evaluate our framework with different composition of T , as shown in <ref type="figure" target="#fig_12">Fig. 11</ref>. It is observed that the student models trained under the settings 'S1', 'S2', and 'S3', which simulate the addition of the members trained with newly developed UDA methods, can outperform their corresponding teacher models in T (whose mIoU's are reported in <ref type="table">Table 1</ref>). This implies that our framework offers the potential to evolve with time. In addition, 'S4', 'S5', and 'S6' in <ref type="figure" target="#fig_12">Fig. 11</ref> showcase that the performance of our framework can be further improved if the students are added back to T , indicating that our framework can be applied in an iterative manner to produce better results. These results highlight the flexibility of our framework as any UDA methods can be incorporated and potentially enhance its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we presented a flexible ensemble-distillation framework to address the common pitfalls, i.e., the lack of robustness, of previous methods. We incorporated an output unification operation into the proposed framework to ensure that the fused outputs of the ensemble are free of the influence from the certainty inconsistency among the models in the ensemble. In addition, to tackle the performance variation issue, we proposed a channel-wise fusion function that is robust against this issue. As our framework is able to integrate different types of UDA methods while maintaining its robustness, it therefore pioneers a new direction for future semantic segmentation based UDA researches.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1 Background Material</head><p>In this section, we walk through the background material of the previous semantic segmentation based unsupervised domain adaptation (UDA) methods. We first offer an overview of the concepts behind the adversarial domain adaptation (ADA) methods in Section S1.1. Then, we review the pseudo labeling strategy in Section S1.2. Some commonly used symbols are summarized in <ref type="table" target="#tab_11">Table S1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.1 The Concepts Behind the Adversarial Domain Adaptation Methods</head><p>For the semantic segmentation based UDA problem considered in this paper, the models are granted accesses to the image-label pairs x src ∈ R |I|×3 , y src ∈ {0, 1} |I|×|C| from a source domain dataset D src , and the images x tgt ∈ R |I|×3 from a target domain dataset D tgt , where I is the set of pixels in an image, and C is a given set of semantic classes. The goal is to train a model G θ parameterized by θ, from which the semantic segmentation predictions can best estimate the target domain ground truth y tgt . For example, in AdaptSegNet <ref type="bibr">[1]</ref>, a generator G θ : R |I|×3 → R |I|×|C| is trained against a discriminator D θ : R |I|×|C| → R 2 using an adversarial training scheme for minimizing the domain gap. The training objective of D θ is to distinguish whether the semantic segmentation outputs from G θ belong to the source domain or not. In contrast, the training objectives of G θ is to confuse the discriminator D θ with its predictions. Their loss functions L G , L D are defined as follows, respectively:</p><formula xml:id="formula_13">L G = − p∈I log D 0 (ŝ (p,c) tgt ),<label>(S1)</label></formula><formula xml:id="formula_14">L D = − p∈I (1 − z) log D 0 (ŝ (p,c) tgt ) + z log D 1 (ŝ (p,c) src ),<label>(S2)</label></formula><p>where c ∈ C denotes a class, p ∈ I denotes a pixel in an image,ŝ (p,c)</p><formula xml:id="formula_15">src = G θ (x src ) ∈ R |I|×|C| is the softmax output of G θ for x src , andŝ (p,c) tgt = G θ (x tgt ) ∈ R |I|×|C|</formula><p>is the softmax output of G θ for x tgt . D 0 , D 1 denote the first and second output channels of D θ , which represent the certainty of D θ on whether the input is drawn from D tgt or D src , respectively. The binary indicator z is either zero or one to indicate that the samples are drawn from the target or the source domains, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.2 Pseudo Labeling Strategy</head><p>Pseudo labeling was pioneered in <ref type="bibr">[2]</ref> for improving the performance of classification tasks. For the semantic segmentation based UDA problem, pseudo labeling is a common measure used in the fine-tuning phase by several self-training methods. During the fine-tuning phase, a model is trained to minimize the loss between the pseudo labels (ŷ tgt ) and the predictions of the model on target domain instances (x tgt ). These pseudo labels are generated by taking the arg max operation over the softmax predictionsŝ tgt of the model, which can be formulated as the following equation:</p><formula xml:id="formula_16">y (p,c) tgt =    1, if c = arg max c∈C {ŝ (p,c) tgt } 0, otherwise ,<label>(S3)</label></formula><p>whereŝ (p,c) tgt = m θ (x tgt ) ∈ R |I|×|C| is the softmax output from a segmentation model m θ : R |I|×3 → R |I|×|C| , which is the model parameterized by θ to be fine-tuned in the target domain. By reducing the cross-entropy loss between the predictionsŝ tgt and the one-hot pseudo labelsŷ tgt , the decision boundaries of the model m θ are adjusted to lie in low-density regions <ref type="bibr">[2]</ref>. This additional fine-tuning stage encourages the model m θ to produce high-certainty predictions, and enhances its stability in deployment time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Theoretical Properties of Channel-Wise Fusion</head><p>In this section, we provide detailed descriptions of the theoretical properties of the proposed channel-wise fusion function (i.e., f Channel ). We first define the evaluation metric for semantic segmentation maps, i.e., mIoU, in Section S2.1. Next, we elaborate on the differences between f Channel and f P ixel in Section S2.2. Then, we discuss how the conflict-resolving mechanism can influence the effectiveness of f Channel in Section S2.3. Finally, in Section S2.4, we investigate the properties of the proposed f Channel under the condition that |A π o | = 0, and derive the proofs for Proposition 1 and Proposition 2 mentioned in the main manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.1 Mean Intersection Over Union</head><p>In this section, we provide the definition and detailed explanation of the commonly used evaluation metric mIoU for semantic segmentation maps. Given a segmentation map A ∈ 2 I × C with |C| different class channels, its mIoU with respect to the ground truth is represented as the following:</p><formula xml:id="formula_17">1 |C| c∈C Φ(A c ),<label>(S4)</label></formula><p>where Φ : 2 I → R is the IoU function that calculates the per-class IoU of a segmentation map, and A c := {p | p ∈ I and the predicted label of p is c} ∈ 2 I is the segmentation map of a class c ∈ C. The IoU with respect to the ground truth of a class is calculated by dividing the overlapped regions between the predicted segmentation and the ground truth, by the union of them. Therefore, given the ground truth segmentation map A gt c := {p | p ∈ I and the ground truth label of p is c} ∈ 2 I of a class c, Φ(A c ) can be represented as the following:</p><formula xml:id="formula_18">Φ(A c ) = |A gt c ∩ A c | |A gt c ∪ A c | . (S5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.2 Differences between Channel-Wise Fusion and Pixel-Wise Fusion</head><p>Channel-wise fusion (f Channel ) differs from pixel-wise fusion (f P ixel ) in that the mIoU's of the fused pseudo labels from f Channel are dependent on two additional factors: (1) the fusion policy π, and (2) the conflict-resolving mechanism that assigns the value of ε. For (1), since the fusion policy π : C → T is a mapping function that assigns each c ∈ C to a teacher model t ∈ T , there may exist |T | |C| possible mappings for π. For (2), since the conflict-resolving mechanism assigns a class label ε ∈ C π p ∪ {c 0 } to each of the pixels in A π o given a π, there may exist |C π p ∪ {c 0 }| |A π o | possible fusion outcomes. In order to examine how these factors can impact the mIoU's of the fused pseudo labels generated by f Channel , in the following section, we analyze the scenarios when the effectiveness of f Channel is maximized and when it is minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.3 Influences of the Conflict-Resolving Mechanism</head><p>The conflict-resolving mechanism is a method that assigns a class label for ε ∈ C π p ∪ {c 0 }. Based on the definition of IoU and the formulation of f Channel , the IoU's of the fused pseudo labels generated using f Channel w.r.t. y tgt for a given class c ∈ C and an arbitrary fusion policy π are maximized when the following conditions are met. An illustration of these conditions is plotted in <ref type="figure" target="#fig_12">Fig. S1 (a)</ref>.</p><p>• Condition a.1: The conflict-resolving mechanism assigns class label c to the pixels under the area A π o1,c := A π o,c ∩ A gt c , where A π o,c := A π o ∩ A π c is the overlapped area of class c and the other class(es). Under such conditions, the IoU w.r.t. the target domain ground truth (y tgt ) for class c is given by:</p><formula xml:id="formula_19">Φ * (c,π(c)) := |A gt c ∩ (A π c \ A π o2,c )| |A gt c ∪ (A π c \ A π o2,c )| = |A gt c ∩ A π c | |A gt c ∪ (A π c \ A π o2,c )| . (S6) Eq. (S6) suggests thatΦ * (c,π(c)) ≥ Φ (c,π(c)) , ∀c ∈ C, where Φ (c,π(c)) := Φ(A π c ) = |A gt c ∩A π c | |A gt c ∪A π</formula><p>c | is the IoU w.r.t. y tgt for class c before applying the conflict resolving mechanism.</p><p>In contrast, the IoU's of the fused pseudo labels generated using f Channel w.r.t. y tgt for a given class c ∈ C and an arbitrary fusion policy π are minimized when the following conditions are met. An illustration of these conditions is plotted in <ref type="figure" target="#fig_12">Fig. S1 (b)</ref>.</p><p>• Condition b.1: The conflict-resolving mechanism assigns the class label c ∈ C π p ∪ {c 0 }, c = c to pixels under the area A π o1,c . • Condition b.2: The conflict-resolving mechanism assigns the class label c to pixels under the area A π o2,c .</p><p>Under such conditions, the IoU w.r.t. the target domain ground truth (y tgt ) for class c is given by:</p><formula xml:id="formula_20">Φ (c,π(c)) := |A gt c ∩ (A π c \ A π o1,c )| |A gt c ∪ (A π c \ A π o1,c )| .<label>(S7)</label></formula><p>Based on the definition in Eq. (S7), the inequalityΦ (c,π(c)) ≤ Φ (c,π(c)) , ∀c ∈ C holds.</p><p>Proposition S1. ∀c ∈ C,Φ * (c,π(c)) = Φ (c,π(c)) =Φ (c,π(c)) if and only if |A π o | = 0.</p><p>Proof. (⇒) ∀c ∈ C,Φ * (c,π(c)) = Φ (c,π(c)) =Φ (c,π(c)) , the following equality holds:  <ref type="figure" target="#fig_1">Figure S2</ref>: An illustration of the counter example described in Proposition 2. Each column in the figure represents a segmentation map with three pixels. The notations 't1', 't2', 't3' represent three different teacher models in T , and 'c1', 'c2', 'c3' represent the class labels in C. The small stripes with three digits indicate the softmax outputs for those classes.</p><formula xml:id="formula_21">∀c ∈ C,Φ * (c,π(c)) = |A gt c ∩ A π c | |A gt c ∪ (A π c \ A π o2,c )| = |A gt c ∩ (A π c \ A π o1,c )| |A gt c ∪ (A π c \ A π o1,c )| =Φ (c,π(c)) ⇒ ∀c ∈ C, |A gt c ∩ A π c | |A gt c ∪ (A π c \ A π o1,c )| = |A gt c ∩ (A π c \ A π o1,c )| |A gt c ∪ (A π c \ A π o2,c )|</formula><p>In the illustrated example, the IoU's of class 'c1' for the models 't1','t2','t3' are all greater than a positive constant α (e.g., 0.3). However, after fusion, the mIoU's of the fused results generated using averaging or f P ixel are equal to zero. On the contrary, the mIoU of the fused results generated by f Channel is greater than nα |C| (e.g., 1×0.3 3 ), when a constant fusion policy π(c) = t3, ∀c ∈ {c1, c2, c3} is adopted.</p><p>Based on the definition of A π o1,c and A π o2,c , the above equation can be re-formulated as follows:</p><formula xml:id="formula_22">∀c ∈ C, |A gt c ∩ A π c | (|A gt c ∪ A π c | − |A π o1,c |) = (|A gt c ∩ A π c | − |A π o1,c |) (|A gt c ∪ A π c | − |A π o2,c |) ⇒ ∀c ∈ C, |A π o1,c | |A π o2,c | = |A gt c ∩ A π c | |A π o2,c | + |A π o1,c | (|A gt c ∪ A π c | − |A gt c ∩ A π c |) Since |A gt c ∩ A π c | &gt; |A π o1,c | and (|A gt c ∪ A π c | − |A gt c ∩ A π c |) &gt; 0, |A π o1,c | = |A π o2,c | = 0, ∀c ∈ C. This implies |A π o | = 0, as A π o = c∈C A π o,c = c∈C (A π o1,c ∪ A π o2,c ).</formula><p>(⇐) If |A π o | = 0, then ∀c ∈ C, |A π o1,c | = |A π o2,c | = 0. This implies the following:</p><formula xml:id="formula_23">∀c ∈ C,Φ * (c,π(c)) = |A gt c ∩ A π c | |A gt c ∪ (A π c \ A π o2,c )| = |A gt c ∩ A π c | |A gt c ∪ A π c | = |A gt c ∩ (A π c \ A π o1,c )| |A gt c ∪ (A π c \ A π o1,c )| =Φ (c,π(c)) .</formula><p>Therefore, the equalityΦ * (c,π(c)) = Φ (c,π(c)) =Φ (c,π(c)) , ∀c ∈ C holds. This also implies that, under such a condition, the IoUΦ (c,π(c)) of the fused results achieved by f Channel is solely determined by the fusion policy π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.4 Proofs for the Propositions in the Main Manuscript</head><p>In this section, we provide proofs for the two propositions in Section 4.2.3 of the main manuscript based on the discussions in Section S2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1.</head><p>Consider an arbitrary fusion policy π. Given a constant α ∈ (0, 1) and classes c 1 , ..., c n ∈ C. If Φ (ci,t) ≥ α, ∀i ∈ {1, ..., n}, ∀t ∈ T and |A π o | = 0, we have:</p><formula xml:id="formula_24">mIoU = 1 |C| c∈CΦ (c,π(c)) ≥ nα |C| . (S8)</formula><p>Proof. As discussed in Proposition S1, given an arbitrary fusion policy π, if |A π o | = 0, then the IoUΦ of the fused results achieved by f Channel is solely determined by π sinceΦ * (c,π(c)) = Φ (c,π(c)) =Φ (c,π(c)) . Therefore,Φ (c,π(c)) = Φ (c,π(c)) holds for all c ∈ C. If Φ (ci,t) ≥ α, i ∈ {1, ..., n}, ∀t ∈ T , the mIoU of the fused results according to Eq. (S4) and the definition of π can be expressed as the following:</p><formula xml:id="formula_25">1 |C| c∈CΦ (c,π(c)) = 1 |C| c∈C Φ (c,π(c)) ≥ 1 |C| (nα + c∈C\{c1,...,cn} Φ (c,π(c)) ) ≥ nα |C| .<label>(S9)</label></formula><p>As a result, the mIoU achieved by f Channel with any π is ensured to be greater than or equal to nα |C| . On the other hand, the mIoU's achieved by either averaging or f P ixel are not guaranteed to be greater than nα |C| under the same condition (i.e., Φ (ci,t) ≥ α, i ∈ {1, ..., n}, ∀t ∈ T ). As demonstrated in the counter example in <ref type="figure" target="#fig_1">Fig. S2</ref>, the IoU's of class 'c 1 ' for every teacher model 't 1 ', 't 2 ', 't 3 ' are greater than a constant α ∈ (0, 1). However, the mIoU's of the fused results generated by averaging and f P ixel are below nα |C| .</p><p>Proposition 2. Consider an optimal fusion policy π * (c) = arg max t∈T {Φ (c,t) }. Assume |A π * o | = 0, we have:</p><formula xml:id="formula_26">mIoU = 1 |C| c∈CΦ (c,π * (c)) ≥ 1 |C| c∈C Φ (c,t) , ∀t ∈ T .<label>(S10)</label></formula><p>Proof. As discussed in Proposition S1, given an arbitrary fusion policy π, if |A π o | = 0, then the IoUΦ of the fused results achieved by f Channel is solely determined by π sinceΦ * (c,π(c)) = Φ (c,π(c)) =Φ (c,π(c)) . Therefore,Φ (c,π(c)) = Φ (c,π(c)) holds for all c ∈ C. Under such a condition, the optimal IoU's for every class can be reached by following a policy π * (c) = arg max t∈T {Φ (c,t) }. Such a policy is a greedy one that selects t ∈ T to maximize the target domain per-class IoU's Φ (c,t) w.r.t. y tgt for all c ∈ C. This suggests that the inequality Eq. (S10) holds for t ∈ T , since:  <ref type="table" target="#tab_6">Table S2</ref>: A summary of the hyperparameters used in the proposed method and the baseline methods.</p><formula xml:id="formula_27">1 |C| c∈CΦ (c,π * (c)) = 1 |C| c∈C Φ (c,π * (c)) ≥ 1 |C| c∈C Φ (c,t) , ∀t ∈ T .<label>(S11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 A Detailed Training Guide for Reproduction</head><p>In this section, we provide a detailed training guide for reproducing our work. In Section S3.1, we offer the pseudo code as well as the link to the source code for training the proposed framework. Then, in Section S3.2, we summarize the hyper-parameters for training the proposed framework and the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3.1 Pseudo Code and Source Code</head><p>The pseudo code for training the proposed framework is presented in Algorithm S1. For more details about the source codes, please refer to the GitHub repository: https://github.com/Chao-Chen-Hao/ Rethinking-EnD-SegUDA.</p><p>Algorithm S1 The Proposed Ensemble-Distillation Method Initialize the weights of a student model m θ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Sample x tgt from D train tgt , and generate the fused pseudo labelsỹ (p,c) using f Channel with the constant policy ∀c ∈ C, π const (c) = t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Train m θ with the loss in Eq. (9) in the manuscript. <ref type="bibr">8:</ref> Evaluate the average per-class output certainty values ρ (c,t) of m θ with instances in D val tgt . 9: end for // Ensemble-Distillation 10: Initialize the weights of a student model m θ . 11: Sample x tgt from D tgt , and generate the fused pseudo labelsỹ (p,c) using f Channel with π selected based on Eq. (8) in the manuscript. 12: Train m θ with the loss in Eq. (9) in the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3.2 Detailed Hyper-Parameter Settings</head><p>The detailed hyperparameters for training each of the teacher models in T , EnD <ref type="bibr">[7]</ref>, EnD 2 <ref type="bibr">[8]</ref>, and the proposed framework are summarized in <ref type="table" target="#tab_6">Table S2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4 Additional Experimental Results</head><p>In this section, we report the additional experimental results and provide discussions on them. We first demonstrate the performance of the proposed framework under different backbone settings in Section S4.1. Next, we showcase the reproducibility and the stability of the proposed framework in Section S4.2. Finally, we present some additional visualized results of our framework in Section S4.3. <ref type="table" target="#tab_8">Table S3</ref> compares the performance of our framework using different backbone architectures in the student model. The first, second, and third columns correspond to the backbone architectures, the number of trainable parameters, and the average inference speed (denoted as IS), respectively. The column 'Before Distillation' denotes the mIoU of the fused pseudo labels generated by f Channel . The column 'After Distillation' refers to the student model's performance after being trained with the fused pseudo labels. As suggested in <ref type="bibr">[</ref>  <ref type="table" target="#tab_8">Table S3</ref>: A comparison of the performance of the proposed framework using different backbone architectures (ResNet-101, DRN-D-54, and MobileNetV2) in the student model. The numerical results are evaluated on the GTA5→Cityscapes benchmark. The inference speed is derived based on the average over 500 inferences. 'IS' denotes the inference speed evaluated on an NVIDIA GTX TITAN V GPU. 'mIoU (train)' refers to the mIoU evaluated on the training set of Cityscapes, which includes 2975 instances. 'mIoU (val)' represents the mIoU evaluated on the validation set of Cityscapes, which includes 500 instances. The column 'Before Distillation' refers to the mIoU of the fused pseudo labels generated by f Channel , while 'After Distillation' represents the mIoU of the student's predictions. 'Oracle' refers to the experimental setting that the student is trained directly with ytgt in the training set of Cityscapes and evaluated on the validation set of Cityscapes.  <ref type="table">Table S4</ref>: Validation of the stability and the reproducibility for the proposed framework on the GTA5 → Cityscapes and the SYNTHIA → Cityscapes benchmarks. The middle columns and the last column report the per-class IoU's and the mIoU's, respectively. Different rows correspond to different backbone configurations. Each of the numerical results are obtained from five models trained with different initial random seeds without early-stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4.1 A Comparison of the Backbone of the Student Model</head><p>the distillation process typically requires a larger backbone to fully learn the knowledge from the teachers. However, adopting a larger backbone contradicts the core idea of ensemble-distillation, as the objective is to reduce the model size so that the computational cost at deployment time is affordable. Therefore, in our experiments, a stronger backbone 'Deeplabv3+ (DRN-D-54)' is adopted, as its number of parameters is comparable with 'Deeplabv2 (ResNet-101)' adopted by the members in T , while performing predictions with better effectiveness. Under such a setting, it is observed that the student model is able to effectively approximate the fused pseudo labels, as mIoU's (train) only degrade slightly (0.85%) after distillation. <ref type="table">Table S4</ref> demonstrates the reproducibility and the stability of the proposed ensemble-distillation framework. Each row in the table corresponds to a backbone configuration. Each of the numerical results is obtained from five models trained with different initial random seeds. From <ref type="table">Table S4</ref>, it is observed that both the per-class IoU's and the mIoU's show only slight fluctuations in terms of their variances, indicating that the proposed method is relatively stable and thus is reproducible. <ref type="figure" target="#fig_3">Fig. S3</ref> shows a few additional visualized results that qualitatively demonstrate the effectiveness of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4.2 The Reproducibility and the Stability of the Proposed Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4.3 Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Ground truth EnD EnD Ours (Pixel) Ours (Channel) 2 <ref type="figure" target="#fig_3">Figure S3</ref>: The visualized results evaluated on the validation set of Cityscapes. These figures are presented for qualitatively comparing the student models trained by EnD <ref type="bibr">[7]</ref>, EnD 2 <ref type="bibr">[8]</ref>, as well as those trained by the proposed framework with pixel-wise fusion (i.e., Ours (Pixel)) and channel-wise fusion (i.e., Ours (Channel)).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A comparison between (a) the proposed ensembledistillation framework and (b) the baseline framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A illustration of the pixel-wise and channel-wise fusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>An illustrative example of the three scenarios in Eq. (5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>An illustrative example of π used in f Channel .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>The distribution of the pixel output certainty values from the models in T , which are trained on GTA5→Cityscapes, with their output certainty values normalized to the range [0, 1] using softmax operation and evaluated on the training set of Cityscapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>+0.15 +0.37 +0.73 +0.31 +0.09</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>The visualized results of the pseudo labels generated by the proposed channel-wise fusion with different choices of κ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>1 ,c 2 ∈C (A π c 1 ∩ A π c 2 )</head><label>122</label><figDesc>{p | p ∈ I, π(c) = t,ŷ (p,c,t) = 1} is the pseudo label of class c selected according to policy π. A π o A π o := c 1 =c 2 , c is the overlapped area between A π c . := A π o ∩ A π c is the overlapped area of a class c. ΦThe function that calculates the IoU of a segmentation map with its ground truth annotation.ΦThe IoU of the fused results generated using f Channel w.r.t. y tgt .c 0 An unlabeled symbol.ε A class label to be assigned in A π o under the formulation of f Channel . C π p C π p := {c | c ∈ C; p ∈ A π c } is the set of class(es) that collects the class label(s) in p ∈ A π c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 ( 2 Figure S1 :</head><label>12S1</label><figDesc>a) The scenario when the IoU is maximized. (b) The scenario when the IoU is minimized.2   1 An illustration of the scenarios under which the IoU's of the fused pseudo label generated using f Channel can be maximized or minimized.• Condition a.2: The conflict-resolving mechanism assigns class label c ∈ C π p ∪ {c 0 }, c = c to the pixels under the area A π o2,c := A π o,c \ A gt c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>1 :</head><label>1</label><figDesc>Input: Ensemble T , and dataset D tgt 2: Output: Student model m θ // Certainty-Aware Policy Selection Strategy 3: Split D tgt into D train tgt and D val tgt 4: for t ∈ T do 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>The mIoU gains of f Channel with π rnd for different κ.</figDesc><table><row><cell>= 5</cell><cell>= 13</cell><cell>= 21 Ground Truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>The mIoU's of the fused pseudo labels generated by channel-wise fusion f Channel under different fusion policies w.r.t.</figDesc><table><row><cell>|A π o |</cell></row></table><note>the ground truth annotations of the training set of Cityscapes. π rnd , π, and π tgt refer to the fusion policies mentioned in Section 6.3.'Ratio' refers to the average proportion of the overlapped area in an image (i.e.,|I| ). The 'mIoU gains' represents the mIoU gains from the adoption of the proposed conflict-resolving mechanism.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table of Contents</head><label>of</label><figDesc>The Concepts Behind the Adversarial Domain Adaptation Methods . . . . . . . . . . . . . 14 S1.2 Pseudo Labeling Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 Mean Intersection Over Union . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 S2.2 Differences between Channel-Wise Fusion and Pixel-Wise Fusion . . . . . . . . . . . . . . 15 S2.3 Influences of the Conflict-Resolving Mechanism . . . . . . . . . . . . . . . . . . . . . . . . 15 S2.4 Proofs for the Propositions in the Main Manuscript . . . . . . . . . . . . . . . . . . . . . . 17 S3 A Detailed Training Guide for Reproduction 20 S3.1 Pseudo Code and Source Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 S3.2 Detailed Hyper-Parameter Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Comparison of the Backbone of the Student Model . . . . . . . . . . . . . . . . . . . . . 20 S4.2 The Reproducibility and the Stability of the Proposed Framework . . . . . . . . . . . . . . 22 S4.3 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22</figDesc><table><row><cell>S1 Background Material S1.1 S2 Theoretical Properties of Channel-Wise Fusion S2.1 S4 Additional Experimental Results Definition c A class. C The set of all c. t A teacher model. T The set of all t. p A pixel in an image. I The set of all p. x src Source domain image. y src Source domain ground truth. x tgt Target domain image. y tgt Target domain ground truth. D src Source domain dataset. D tgt Target domain dataset. f P ixel Pixel-wise fusion. f Channel Channel-wise fusion. S4.1 A Symbol π A fusion policy.</cell><cell>14 15 20</cell></row></table><note>A A segmentation map.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S1 :</head><label>S1</label><figDesc>List of commonly-used symbols.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Cityscapes Model (Backbone) Road SideW Build Wall Fence Pole Light Sign Veg Terrain Sky Person Rider Car Truck Bus Train Motor Bike mIOU Deeplabv2 92.89 55.61 84.42 41.09 36.53 26.16 37.39 46.14 82.82 44.68 81.96 56.27 32.94 83.27 54.82 46.59 0.00 34.27 50.72 52.07 .15 0.10 0.75 0.45 0.11 0.15 0.10 0.07 0.34 0.30 0.21 0.38 0.10 1.25 0.46 0.00 0.46 0.54 0.24 Deeplabv3+ 93.32 59.17 86.20 33.58 37.85 37.45 43.67 52.36 86.34 43.54 86.34 62.81 34.53 86.72 46.07 45.81 0.00 32.00 53.74 53.63 .05 0.17 1.19 1.21 0.32 0.43 0.79 0.12 1.11 0.45 0.26 0.42 0.86 2.02 1.18 0.00 3.60 3.43 0.45 Deeplabv3+ 94.50 61.58 87.91 35.87 39.68 40.74 48.90 55.13 88.20 48.93 88.57 67.06 38.78 89.26 55.00 50.48 0.02 40.03 54.91 57.13 .55 0.15 0.85 0.89 0.35 0.67 0.44 0.05 0.47 0.39 0.53 1.12 0.20 2.74 1.25 0.06 0.95 1.20 0.28 SYNTHIA → Cityscapes Model (Backbone) Road SideW Build Wall Fence Pole Light Sign Veg Terrain Sky Person Rider Car Truck Bus Train Motor Bike mIOU Deeplabv2 87.83 43.42 81.17 18.85 3.69 26.07 27.65 34.05 80.78</figDesc><table><row><cell cols="3">(ResNet-101) 0.10 1(MobileNetV2) ± ± 0.06 1(DRN-D-54) ± 0.22 1-± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ±</cell><cell>± ± ± 82.60 54.82 18.78 83.63 ± ± ± ± ± ± ± ± ±</cell><cell>± ± ± -</cell><cell cols="2">± ± ± 46.09 -± ± ±</cell><cell>± ± ± 20.08 49.05 47.41 ± ± ± ± ± ±</cell></row><row><cell>(ResNet-101)</cell><cell>± 0.04 0.31 0.11 0.37 0.29 0.10 0.86 0.27 0.10 ± ± ± ± ± ± ± ±</cell><cell>--</cell><cell>± 0.19 0.33 0.16 0.16 ± ± ±</cell><cell>--</cell><cell>± 1.38</cell><cell>--</cell><cell>± 0.64 0.21 0.15 ± ±</cell></row><row><cell>Deeplabv3+</cell><cell>88.72 46.91 82.90 18.68 3.89 34.4 29.61 36.93 84.13</cell><cell>-</cell><cell>88.25 60.18 19.35 87.01</cell><cell>-</cell><cell cols="2">49.01 -</cell><cell>16.0 52.30 49.89</cell></row><row><cell>(MobileNetV2)</cell><cell>± 0.18 0.35 0.16 0.53 0.16 0.24 1.18 0.15 0.17 ± ± ± ± ± ± ± ±</cell><cell>--</cell><cell>± 0.13 0.24 0.23 0.24 ± ± ±</cell><cell>--</cell><cell>± 1.67</cell><cell>--</cell><cell>± 2.66 0.15 0.26 ± ±</cell></row><row><cell>Deeplabv3+</cell><cell>88.64 47.04 83.59 19.43 3.03 36.11 32.15 37.87 84.39</cell><cell>-</cell><cell>87.56 63.35 21.12 87.94</cell><cell>-</cell><cell cols="2">52.58 -</cell><cell>21.93 53.76 51.28</cell></row><row><cell>(DRN-D-54)</cell><cell>± 0.19 0.36 0.08 0.39 0.31 0.14 2.57 0.29 0.35 ± ± ± ± ± ± ± ±</cell><cell>--</cell><cell>± 0.44 0.41 0.58 0.20 ± ± ±</cell><cell>--</cell><cell>± 1.10</cell><cell>--</cell><cell>± 1.97 0.80 0.13 ± ±</cell></row></table><note>GTA5 →</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<imprint>
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">FCNs in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Significanceaware information bottleneck for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="6778" to="6787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="2477" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="518" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An adversarial perturbation oriented domain adaptation approach for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Thirty-Fourth AAAI Conf. Artificial Intelligence (AAAI)</title>
		<meeting>the Thirty-Fourth AAAI Conf. Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020-02" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="6810" to="6818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C. Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="1992" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised scene adaptation with memory regularization in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Artificial Intelligence (IJCAI)</title>
		<meeting>Int. Joint Conf. Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-ensembling with ganbased data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via classbalanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Vijaya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-020-01395-y.2</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DACS: Domain adaptation via cross-domain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conf. on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03756</idno>
		<title level="m">Spigan: Privileged adversarial learning from simulation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dada: Depth-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7364" to="7373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-domain semantic segmentation via domain-invariant interactive relation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FDA: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Crdoco: Pixel-level domain transfer with cross-domain consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ssf-dan: Separated semantic feature based domain adaptation network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="982" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised multi-target domain adaptation through knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Nguyen-Meidine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-A</forename><surname>Blais-Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conf. on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1339" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th ACM SIGKDD Int. Conf. Knowledge Discovery and Data mining (KDD-06)</title>
		<meeting>12th ACM SIGKDD Int. Conf. Knowledge Discovery and Data mining (KDD-06)</meeting>
		<imprint>
			<date type="published" when="2006-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="4794" to="4802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Le Thanh Nguyen-Meidine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Antoine</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blais-Morin</surname></persName>
		</author>
		<title level="m">Joint progressive knowledge distillation and unsupervised domain adaptation. Int. Joint Conf. on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Knowledge distillation for semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orbes-Arteainst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sørensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Modat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pai</surname></persName>
		</author>
		<idno>OR 2.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Context-Aware Operating Theaters and Machine Learning in Clinical Neuroimaging</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="68" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2604" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ensemble distribution distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mlodozeniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7047" to="7058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Vijaya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision (ECCV)</title>
		<meeting>European Conf. Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="5982" to="5991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-020-01395-y</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">DACS: Domain adaptation via cross-domain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conf. on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Ensemble distribution distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mlodozeniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR</title>
		<meeting>Int. Conf. Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
