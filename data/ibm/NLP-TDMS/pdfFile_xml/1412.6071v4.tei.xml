<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fractional Max-Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-05-13">May 13, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
							<email>b.graham@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of Statistics</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<postCode>CV4 7AL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fractional Max-Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-05-13">May 13, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional networks almost always incorporate some form of spatial pooling, and very often it is α × α max-pooling with α = 2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor α. The amazing by-product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of maxpooling where α is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state of the art for CIFAR-100 without even using dropout.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Convolutional neural networks</head><p>Convolutional networks are used to solve image recognition problems. They can be built by combining two types of layers:</p><p>• Layers of convolutional filters.</p><p>• Some form of spatial pooling, such as max-pooling.</p><p>Research focused on improving the convolutional layers has lead to a wealth of techniques such as dropout <ref type="bibr" target="#b9">[10]</ref>, DropConnect <ref type="bibr" target="#b11">[12]</ref>, deep networks with many small filters <ref type="bibr" target="#b1">[2]</ref>, large input layer filters for detecting texture <ref type="bibr" target="#b4">[5]</ref>, and deeply supervised networks <ref type="bibr" target="#b5">[6]</ref>.</p><p>By comparison, the humble pooling operation has been slightly neglected. For a long time 2 × 2 max-pooling (MP2 has been the default choice for building convolutional networks. There are many reasons for the popularity of MP2pooling: it is fast, it quickly reduces the size of the hidden layers, and it encodes a degree of invariance with respect to translations and elastic distortions. However, the disjoint nature of the pooling regions can limit generalization. Additionally, as MP2-pooling reduces the size of the hidden layers so quickly, stacks of back-to-back convolutional layers are needed to build really deep networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. Two methods that have been proposed to overcome this problems are:</p><p>• Using 3 × 3 pooling regions overlapping with stride 2 <ref type="bibr" target="#b4">[5]</ref>.</p><p>• Stochastic pooling, where the act of picking the maximum value in each pooling region is replaced by a form of size-biased sampling <ref type="bibr" target="#b12">[13]</ref>.</p><p>However, both these techniques still reduce the size of the hidden layers by a factor of two. It seems natural to ask if spatial-pooling can usefully be applied in a gentler manner. If pooling was to only reduce the size of the hidden layers by a factor of √ 2, then we could use twice as many layers of pooling. Each layer of pooling is an opportunity to view the input image at a different scale. Viewing images at the 'right' scale should make it easier to recognize the tell-tale features that identify an object as belonging to a particular class.</p><p>The focus of this paper is thus a particular form of max-pooling that we call fractional max-pooling (FMP). The idea of FMP is to reduce the spatial size of the image by a factor of α with 1 &lt; α &lt; 2. Like stochastic pooling, FMP introduces a degree of randomness to the pooling process. However, unlike stochastic-pooling, the randomness is related to the choice of pooling regions, not the way pooling is performed inside each of the pooling regions.</p><p>In Section 2 we give a formal description of fractional max-pooling. Briefly, there are three choices that affect the way FMP is implemented:</p><p>• The pooling fraction α which determines the ratio between the spatial sizes of the input and the output of the pooling layer. Regular 2 × 2 max-pooling corresponds to the special case α = 2.</p><p>• The pooling regions can either be chosen in a random or a pseudorandom fashion. There seems to be a trade off between the use of randomness in FMP and the use of dropout and/or training data augmentation. Random-FMP seems to work better on its own; however, when combined with 'too much' dropout or training data augmentation, underfitting can occur.</p><p>• The pooling regions can be either disjoint or overlapping. Disjoint regions are easier to picture, but we find that overlapping regions work better.</p><p>In Section 3 we describe how our convolutional networks were designed and trained. In Section 4 we give results for the MNIST digits, the CIFAR-10 and CIFAR-100 datasets of small pictures, handwritten Assamese characters and the CASIA-OLHWDB1.1 dataset of handwritten Chinese characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Fractional max-pooling</head><p>Each convolutional filter of a CNN produces a matrix of hidden variables. The size of this matrix is often reduced using some form of pooling. Max-pooling is a procedure that takes an N in × N in input matrix and returns a smaller output matrix, say N out × N out . This is achieved by dividing the N in × N in square into N 2 out pooling regions (P i,j ):</p><formula xml:id="formula_0">P i,j ⊂ {1, 2, . . . , N in } 2 for each (i, j) ∈ {1, . . . , N out } 2 ,</formula><p>and then setting</p><formula xml:id="formula_1">Output i,j = max (k,l)∈Pi,j Input k,l . For regular 2 × 2 max-pooling, N in = 2N out and P i,j = {2i − 1, 2i} × {2j − 1, 2j}.</formula><p>In <ref type="bibr" target="#b4">[5]</ref>, max-pooling is applied with overlapping 3 × 3 pooling regions so N in = 2N out + 1 and the P i,j are 3 × 3 squares, tiled with stride 2. In both cases, N in /N out ≈ 2 so the spatial size of any interesting features in the input image halve in size with each pooling layer. In contrast, if we take N in /N out ≈ n √ 2 then the rate of decay of the spatial size of interesting features is n times slower. For clarity we will now focus on the case N in /N out ∈ (1, 2) as we are primarily interested in accuracy; if speed is an overbearing concern then FMP could be applied with N in /N out ∈ (2, 3).</p><p>Given a particular pair of values (N in , N out ) we need a way to choose pooling regions (P i,j ). We will consider two type of arrangements, overlapping squares and disjoint collections of rectangles. In <ref type="figure">Figure 1</ref> we show a number of different ways of dividing up a 36 × 36 square grid into disjoint rectangles. Pictures two, three and six in <ref type="figure">Figure 1</ref> can also be used to define an arrangement of overlapping 2 × 2 squares: take the top left hand corner of each rectangle in the picture to be the top left hand corner of one of the squares.</p><p>To give a formal description of how to generate pooling regions, let (a i ) Nout i=0 and (b i ) Nout i=0 be two increasing sequence of integers starting at 1, ending with N in , and with increments all equal to one or two (i.e. a i+1 − a i ∈ {1, 2}). The regions can then be defined by either</p><formula xml:id="formula_2">P = [a i−1 , a i − 1] × [b j−1 , b j − 1] or P i,j = [a i−1 , a i ] × [b j−1 , b j ].<label>(1)</label></formula><p>We call the two cases disjoint and overlapping, respectively. We have tried two different approaches for generating the integer sequence: using random sequences of numbers and also using pseudorandom sequences. We will say that the sequences are random if the increments are obtained by taking a random permutation of an appropriate number of ones and twos. We will say that the sequences are pseudorandom if they take the form</p><formula xml:id="formula_3">a i = ceiling(α(i + u)), α ∈ (1, 2), with some u ∈ (0, 1).</formula><p>Below are some patterns of increments corresponding to the case N in = 25, N out = 18. The increments on the left were generated 'randomly', and the increments on the right come from pseudorandom sequences:</p><p>211112112211112122 112112121121211212 111222121121112121 212112121121121211 121122112111211212 211211212112121121</p><p>Although both types of sequences are irregular, the pseudorandom sequences generate much more stable pooling regions than the random ones. To show the effect of randomizing the pooling regions, see <ref type="figure" target="#fig_1">Figure 2</ref>. We have taken a picture, and we have iteratively used disjoint random pooling regions to reduce the size of the image (taking averages in each pooling region). The result is that the scaled down images show elastic distortion. In contrast, if we use pseudorandom pooling regions, the resulting image is simply a faithfully scaled down version of the original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>The networks are trainined using an implementation of a sparse convolutional network <ref type="bibr" target="#b2">[3]</ref>. What this means in practice is that we can specify a convolutional network in terms of a sequence of layers, e.g.</p><formula xml:id="formula_4">10C2 − F M P √ 2 − 20C2 − F M P √ 2 − 30C2 − F M P √ 2 − 40C2 − 50C1 − output.</formula><p>The spatial size of the input layer is obtained by working from right to left: each C2 convolution increases the spatial size by one, and FMP √ 2 layers increase the spatial size by a factor of √ 2, rounded to the nearest integer; see <ref type="figure" target="#fig_2">Figure 3</ref>. The input layer will typically be larger than the input images-padding with zeros is automatically added as needed. Fractional max-pooling could also easily be implemented for regular convolutional neural network software packages.</p><p>For simplicity, all the networks we use have a linearly increasing number of filters per convolutional layer. We can therefore describe the above network using the shorthand form</p><formula xml:id="formula_5">(10nC2 − F M P √ 2) 3 − C2 − C1 − output,</formula><p>10n indicates that the number of filters in the n-th convolutional layer is 10n, and the subscript 3 indicates three pairs of alternating C2/FMP layers. When we use dropout, we use an increasing amount of dropout the deeper we go into the network; we apply 0% dropout in the first hidden layer, and increase linearly to 50% dropout in the final hidden layer. We use leaky rectified linear units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model averaging</head><p>Each time we apply an FMP network, either for training or testing purposes, we use different random or pseudorandom sequences to generate the pooling regions. An FMP network can therefore be thought of as an ensemble of similar networks, with each different pooling-region configuration defining a different member of the ensemble. This is similar to dropout <ref type="bibr" target="#b9">[10]</ref>; the different values the dropout mask can take define an ensemble of related networks. As with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Without training set augmentation or dropout</head><p>To compare the different kinds of fractional max-pooling, we trained FMP networks on the MNIST 1 set of digits and the CIFAR-100 dataset of small pictures <ref type="bibr" target="#b3">[4]</ref>. For MNIST we used a small FMP network:</p><p>input layer size 36 × 36 :</p><formula xml:id="formula_6">(32nC2 − F M P √ 2) 6 − C2 − C1 − output,</formula><p>and for CIFAR-100 we used a larger network:</p><p>input layer size 94 × 94 :</p><formula xml:id="formula_7">(64nC2 − F M P 3 √ 2) 12 − C2 − C1 − output.</formula><p>Without using training data augmentation, state-of-the-art test errors for these two datasets are 0.39% and 34.57%, respectively <ref type="bibr" target="#b5">[6]</ref>. Results for the FMP networks are in <ref type="table">Table 1</ref>. Using model averaging with multiplicity twelve, we find that random overlapping FMP does best for both datasets. For CIFAR-100, the improvement over method using regular max-pooling is quite substantial. To give an idea about network complexity, the CIFAR-100 networks have 12 million weights, and were trained for 250 repetitions of the training data (18 hours on a GeForce GTX 780). We experimented with changing the number of hidden units per layer for CIFAR-100 with random overlapping pooling:</p><p>• Using '16nC2' (0.8M weights) gave test errors of 42.07% / 34.87%.</p><p>• Using '32nC2' (3.2M weights) gave test errors of 35.09% / 29.66%.</p><p>• Using '96nC2' (27M weights) combined with dropout and a slower rate of learning rate decay gave test errors of 27.62% / 23.82%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Assamese handwriting</head><p>To compare the effect of training data augmentation when using FMP pooling versus MP2 pooling, we used the The Online Handwritten Assamese Characters Dataset <ref type="bibr" target="#b0">[1]</ref>. It contains 45 samples for each of 183 Indo-Aryan characters. 'Online' means that each pen stroke is represented as a sequence of (x, y) coordinates. We used the first 36 handwriting samples as the training set, and the remaining 9 samples for a test set. The characters were scaled to fit in a box of size 64 × 64. We trained a network with six layers of 2 × 2 max pooling,</p><formula xml:id="formula_8">32nC3 − M P 2 − (C2 − M P 2) 5 − C2 − output</formula><p>and an FMP network using 10 layers of random overlapping FMP √ 2 pooling,</p><formula xml:id="formula_9">(32nC2 − F M P √ 2) 10 − C2 − C1 − output.</formula><p>We trained the networks without dropout, and either</p><p>• no training data augmentation,</p><p>• with the characters shifted by adding random translations, or</p><p>• with affine transformations, using a randomized mix of translations, rotations, stretching, and shearing operations.  See <ref type="table" target="#tab_2">Table 2</ref>. In a sense, max-pooling and training data augmentation are two different ways of encoding our apriori knowledge that the meaning of handwriting is generally invariant under certain kinds of minor distortions. Interestingly, the FMP network without data augmentation does better than the MP2 network with training data augmentation, suggesting that FMP is a better way of encoding that information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Online Chinese handwriting</head><p>The CASIA-OLHWDB1.1 database contains online handwriting samples of the 3755 isolated GBK level-1 Chinese characters <ref type="bibr" target="#b7">[8]</ref>. There are approximately 240 training characters, and 60 test characters, per class. A test error of 5.61% is achieved using 4 levels of MP2 pooling <ref type="bibr" target="#b1">[2]</ref>. We used the representation for online characters described in <ref type="bibr" target="#b2">[3]</ref>; the characters were drawn with size 64×64 and additional features measuring the direction of the pen are added to produce an array of size 64 × 64 × 9. Using 6 layers of 2 × 2 max-pooling, dropout and affine training data augmentation resulted in a 3.82% test error <ref type="bibr" target="#b2">[3]</ref>. Replacing max-pooling with pseudorandom overlapping FMP:</p><formula xml:id="formula_10">(64nC2 − F M P √ 2) 7 − (C2 − M P 2 − C1) 2 − C2 − C1 − output</formula><p>results in test errors of 3.26% (1 test) and 2.97% (12 tests).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CIFAR-10 with dropout and training data augmentation</head><p>For CIFAR-10 we used dropout and extended the training set using affine transformations: a randomized mix of translations, rotations, reflections, stretching, and shearing operations. We also added random shifts to the pictures in RGB colorspace. For a final 10 training epochs, we trained the network without the affine transformations. For comparison, human performance on CIFAR-10 is estimated to be 6% 2 . A recent Kaggle competition relating to CIFAR-10 was won with a test error of 4.47% 3 using the same training data augmentation scheme, and architecture (300nC2 − 300nC2 − M P 2) 5 − C2 − C1 − output. we obtained test errors of 4.50% (1 test), 3.67% (12 tests) and 3.47% (100 tests).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have trained convolutional networks with fractional max-pooling on a number of popular datasets and found substantial improvements in performance.</p><p>Overlapping FMP seems to be better than disjoint FMP. Pseudorandom pooling regions seem to do better than random pooling regions when training data augmentation is used. It is possible that random pooling might regain the upperhand if we fine-tuned the amount of dropout used. Looking again at the distortions created by random pooling in <ref type="figure" target="#fig_1">Figure 2</ref>, note that the distortion is 'decomposable' into an x-axis distortion and a y-axis distortion. It might be interesting to explore pooling regions that cannot be written using equation 1, as they might encode more general kinds of distortion into the resulting convolutional networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 : 2 .</head><label>12</label><figDesc>Left to right: A 36 × 36 square grid; disjoint pseudorandom FMP regions with α ∈ { 3 and disjoint random FMP regions for α = √ For α ∈ (1, 2) the rectangles have sides of length 1 or 2. For α ∈ (2, 3) the rectangles have sides of length 2 or 3. [Please zoom in if the images appear blurred.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Top left, 'Kodak True Color' parrots at a resolution of 384 × 256. The other five images are one-eighth of the resolution as a result of 6 layers of average pooling using disjoint random FMP √ 2-pooling regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Layer sizes for a tiny FMP √ 2 network. The fractions 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The effect of repeat testing for a single MNIST trained FMP network. dropout, model averaging for FMP networks can help improve performance. If you classify the same test image a number of times, you may get a number of different predictions. Using majority voting after classifying each test image a number of times can substantially improve accuracy; seeFigure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 3 √ 2 ) 12 −</head><label>3212</label><figDesc>http://karpathy.ca/myblog/?p=160 3 https://www.kaggle.com/c/cifar-10/ Using a pseudorandom overlapping pooling FMP network (160nC2 − F M P C2 − C1 − output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Assamese % test error with different type of data augmentation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://yann.lecun.com/exdb/mnist/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Spatially-sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeply-Supervised Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network in network. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CASIA online and offline Chinese handwriting databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting>11th International Conference on Document Analysis and Recognition (ICDAR)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regularization of Neural Networks using DropConnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR W&amp;CP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Stochastic Pooling for Regularization of Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
