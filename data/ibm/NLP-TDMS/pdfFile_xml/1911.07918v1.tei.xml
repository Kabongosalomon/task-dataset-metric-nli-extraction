<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Document Classification with Multi-Sense Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Nokhiz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshit</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
						</author>
						<title level="a" type="main">Improving Document Classification with Multi-Sense Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficient representation of text documents is an important building block in many NLP tasks. Research on long text categorization has shown that simple weighted averaging of word vectors for sentence representation often outperforms more sophisticated neural models. Recently proposed Sparse Composite Document Vector (SCDV) [32] extends this approach from sentences to documents using soft clustering over word vectors. However, SCDV disregards the multi-sense nature of words, and it also suffers from the curse of higher dimensionality. In this work, we address these shortcomings and propose SCDV-MS. SCDV-MS utilizes multi-sense word embeddings and learns a lower dimensional manifold. Through extensive experiments on multiple real-world datasets, we show that SCDV-MS embeddings outperform previous state-of-the-art embeddings on multi-class and multi-label text categorization tasks. Furthermore, SCDV-MS embeddings are more efficient than SCDV in terms of time and space complexity on textual classification tasks. We have released SCDV-MS source code with the paper. 6 4 IIT Guwahati, email: harshitgupta@iitg.ac.in 5 IISC Bangalore, email:</p><p>7 Fuzzy clusters wv in K clusters; 8 Each word wi and cluster c k , obtain P(c k |wi) ; 9 SP(c|wi) = make-sparse( P(c|wi));</p><p>3.3 Word Topic Vectors (Algo 1: 9 -16) Similar to SCDV, for each word wi ∈ V , we created K different word-cluster vectors of d dimensions ( wcv ik ) by weighting the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed word embeddings such as word2vec <ref type="bibr" target="#b34">[35]</ref> are effective in capturing the semantic meanings of the words by representing them in a lower-dimensional continuous space. A smooth inverse frequency-based word vector averaging technique for sentence embeddings was proposed by <ref type="bibr" target="#b3">[4]</ref>. However, because the final representation is in the same space as the word vectors, these methods are only capable of capturing the meaning of a single sentence. Thus, embedding a large text document in a dense, low-dimensional space is a challenging task. <ref type="bibr" target="#b31">[32]</ref> attempted to resolve this problem by proposing a clusteringbased word vector averaging approach (SCDV) for embedding larger text documents. SCDV embeds each document by cluster-based averaging, thus representing each document in a more representative space than the original vectors. This model combines the word embedding models with a latent topic model where the topic space is learned efficiently using a soft clustering technique over embeddings. The final document vectors are also made sparse to reduce time and space complexities in several downstream tasks.</p><p>SCDV has many shortcomings: applying thresholding-based sparsity on the document representations can be unreliable since it is highly sensitive to the number of documents in the corpus. SCDV does not utilize the word contexts to disambiguate word sense for ppt@iisc.ac.in 6 https://github.com/vgupta123/SCDV-MS learning sense-aware document representations. Ignoring the multisense nature of the words during the representation leads to cluster ambiguity. SCDV neglects the negative additive effect of common words such as 'and', 'the', etc. during final document representations. Lastly, the documents represented by SCDV suffer from the curse of high dimensionality and cannot be utilized for deep learning applications which require low-dimensional continuous representations. To overcome these challenges, we proposed a novel document representation technique, namely SCDV-MS. Our proposed SCDV-MS mitigated the above shortcomings by the following contributions.</p><p>1. To overcome the problem of cluster ambiguity SCDV-MS replaced the single sense word vector representations with multisense context-sensitive word representations to resolve word sense disambiguation. 2. SCDV-MS removed the noise in the final representation by applying a threshold-based sparsity directly on fuzzy word cluster assignments instead of the document representations. Sparser representations result in better performance, lower time and space complexities. 3. To overcome the noisy negative additive effect of common words such as 'and', 'the', etc. SCDV-MS learned and used Doc2VecCinitialized <ref type="bibr" target="#b8">[9]</ref> robust word vectors to zero out common and high frequent words. 4. Lastly, we showed that the sparse word-topic vectors can be projected into a non-linear local neighborhood preserving a manifold to learn continuous distributed representations much more effectively and efficiently than SCDV which proves to be useful for deep learning application.</p><p>Overall, we show that: disambiguating the multiple senses of words based on their context words (adjacent words) can lead to better document representations. Sparsity in representations is helpful for effective and efficient lower-dimensional manifold representation learning. Representation noise at words' level has a significant impact on the final downstream tasks.</p><p>In section 1, we provided a brief introduction to the problem statement. In section 2 we discuss the related work in document representations. In section 3, we describe the proposed algorithm and then discuss the proposed modifications compared to SCDV in section 4. We move on to experiments in section 5, followed by conclusions in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For document representation, averaging of word-vectors with an unweighted scheme was proposed by <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34]</ref>. <ref type="bibr" target="#b43">[44]</ref> extended the previous simple averaging model by tf-idf weighting of word vectors to form document vectors. <ref type="bibr" target="#b25">[26]</ref> proposed paragraph models (PV-DM and PV-DBOW) similar to word vector models (CBoW and SGNS) by treating each paragraph as a pseudoword. <ref type="bibr" target="#b45">[46]</ref> used a parse tree to train a Recursive Neural Network (RNN) with supervision. In addition, several neural network models such as seq2seq models: Recurrent Neural Networks (RNN) <ref type="bibr" target="#b32">[33]</ref>, Long Short Term Memory Networks (LSTM) <ref type="bibr" target="#b15">[16]</ref> and a hierarchical model: Convolutional Neural Networks (CNN) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21]</ref> were proposed to capture syntax while representing documents. <ref type="bibr" target="#b48">[49]</ref> used supervised learning over the Paraphrase Dataset (PPDB) to learn Paraphrastic Sentence embeddings (PSL). Later, <ref type="bibr" target="#b47">[48]</ref> also propose an optimization of word embeddings based on a neural network and a cosine similarity measure.</p><p>Several models such as WTM <ref type="bibr" target="#b14">[15]</ref>, TWE <ref type="bibr" target="#b29">[30]</ref>, NTSG <ref type="bibr" target="#b29">[30]</ref>, LTSG <ref type="bibr" target="#b24">[25]</ref>, w2v-LDA <ref type="bibr" target="#b38">[39]</ref>, TV+MeanWV <ref type="bibr" target="#b27">[28]</ref>, Topic2Vec <ref type="bibr" target="#b39">[40]</ref>, Gaussian-LDA <ref type="bibr" target="#b10">[11]</ref>, Lda2vec <ref type="bibr" target="#b37">[38]</ref>, ETM <ref type="bibr" target="#b13">[14]</ref>, D-ETM <ref type="bibr" target="#b12">[13]</ref> and MvTM <ref type="bibr" target="#b28">[29]</ref> combine topic modeling <ref type="bibr" target="#b7">[8]</ref> with word vectors to learn better word and sentence representations. <ref type="bibr" target="#b23">[24]</ref> cast the distributional hypothesis to a sentence level by proposing skip-thought document vectors. Recently, two deep contextual word embeddings namely ELMo <ref type="bibr" target="#b40">[41]</ref> and BERT <ref type="bibr" target="#b11">[12]</ref> were proposed. These contextual embeddings perform as state of the art on multiple NLP tasks since they are very effective in capturing the surrounding context. Interestingly, <ref type="bibr" target="#b26">[27]</ref> checks the effect of using multi-sense embeddings on various NLP tasks. However, our goal is different and aim at effectively using multi-sense words embeddings to learn better document representations. A hard clustering-based averaging of word vectors was proposed by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3]</ref> to form document vectors. <ref type="bibr" target="#b17">[18]</ref> extended the approach with a better partitioning technique and tried it on other natural language tasks. <ref type="bibr" target="#b31">[32]</ref> further improved the state-of-the-art SCDV by using fuzzy clustering and tf-idf weighted word averaging. Their method outperformed earlier models on several NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Algorithm SCDV-MS</head><p>In this section, we will describe our new proposed algorithm 1 in details. The algorithm is similar to SCDV <ref type="bibr" target="#b31">[32]</ref>, but with important modifications and has three main components as described below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Sense Disambiguation (Algo 1: 1 -6):</head><p>We employed the widely-used AdaGram <ref type="bibr" target="#b5">[6]</ref> algorithm to disambiguate the multi-sense words in our corpora. We chose AdaGram because it's a nonparametric Bayesian extension of Skip-gram which automatically learns the counts of the senses of the multi-sense words and their representations. <ref type="bibr" target="#b6">7</ref> We first trained the AdaGram algorithm on the training corpora. <ref type="bibr" target="#b7">8</ref> We used the trained model to annotate the words with the corresponding word senses in all train-test examples. We then trained the Doc2vecC algorithm on an annotated corpus to obtain the final multi-sense word vectors. Lastly, we obtained the idf values of words of the vocabulary which we will use as a means for weighting the rare words (Lines 4-6 Algo 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Vector Clustering (Algo 1: 6 -9)</head><p>Similar to the SCDV approach, we clustered the word embeddings using Gaussian Mixture Models (GMM), which is a soft clustering technique, and obtained the word-cluster assignments probabilities P(c k |wi). Additionally, we made use of the fact that GMMs have <ref type="bibr" target="#b6">7</ref> One could also replace AdaGram with <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b4">[5]</ref>   an irrelevant noisy tail and made the cluster probability assignment P(c|wi) manually sparse by zeroing the values of P(c k |wi). Retaining only the top l maximum P(c k |wi) and zeroing the rest K − l values results in a sparse word-cluster assignment vector SP(c|wi) for each word. Here, K represents the total number of clusters and l is the sparsity constant (l &lt;&lt; K). One can use different values of l for each word (wi) depending on the values of P(c k |wi) . However, in our experiments we did not observe significant performance difference when l is varied with respect to the words.</p><formula xml:id="formula_0">SP(c k |wi) = P(c k |wi) if k ∈ {k| arg max l k P(c k |wi)} 0 otherwise (1)</formula><p>arg max l k P(c k |wi) outputs indices of the top l maximum assignments.</p><p>word's embedding with sparse probability distribution for the k th cluster, i.e., SP (c k |wi). Next, we concatenated the word-cluster vectors ( wcv ik ) which are K word-topic vectors in total, into a K × d embedding vector. We then weighted it with inverse document frequency (idf) of wi to obtain word-topic vectors ( wtvi). For all words appearing in a given document Dn, we computed the average of the corresponding projected lower dimensional word-topic vectors wtvi to obtain the document vector dvD n . Furthermore, one can optionally project the ( wtvi) into a lower dimensional continuous representation called reduced word topic vectors, ( rwtvi), using manifold learning algorithms, namely Random Projection <ref type="bibr" target="#b1">[2]</ref>, PCA <ref type="bibr" target="#b0">[1]</ref> and Denoising Autoencoders <ref type="bibr" target="#b46">[47]</ref>. We can then use them instead of ( wtvi) for document representation. We call this reduction-based representation method as R-SCDV-MS. Refer to section 4.1 on manifold learning for more details.</p><formula xml:id="formula_1">wcv ik = wvi × SP(c k |wi) (2) wtvi = idf(wi) × ⊕ K k=1 wcv ik (3) rwtvi = manifold-proj( wtvi)<label>(4)</label></formula><p>is the concatenation and manifold-proj is the manifold learning algorithm we utilized. <ref type="figure" target="#fig_3">Figures 2 and 3</ref> show the flow-chart of high level flow of our proposed SCDV-MS embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion on Proposed Modifications</head><p>In this section, we will describe the modifications applied to the SCDV embeddings in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Word Representation: Single Sense vs Multi Sense</head><p>SCDV-MS used a multi-sense approach instead of single sense word embeddings because SCDV does not disambiguate the senses of the words based on the context words used in the documents. SCDV-MS performed an automatic word sense disambiguation using multisense word embeddings according to the context determined by the neighboring words to resolve cluster ambiguity for polysemous words. <ref type="table" target="#tab_0">Table 1</ref> shows examples of multi-sense words along with their fitting context and the prominent words of the assigned clusters.  <ref type="figure" target="#fig_2">Figure 1</ref> shows the effect of sense disambiguation on fuzzy GMM clustering on the 20NewsGroup dataset. The same word is assigned to different clusters depending on its context which helps in resolving the word cluster ambiguities, e.g., without sense disambiguation, the word 'Subject' belongs to cluster 13 with probability 0.25 and cluster 50 with probability 0.65. But after sense disambiguation we acquire two word embeddings of the word 'Subject', i.e., 'Subject#1' and 'Subject#2'. 'Subject#1' belongs to cluster 50 with probability of 0.9 and 'Subject#2' belongs to cluster 13 with probability 0.8. So depending on the context in which word 'Subject' is used, the algorithm assigns 'Subject' to a single cluster based on its sense; thus word cluster ambiguity is resolved. We observe similar disambiguation effects for other polysemous words in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Thresholding Word Cluster Assignments</head><p>In SCDV, the thresholding is applied to the final document vectors. However, applying the hard thresholding in an earlier stage in word cluster assignments (P (ci|w)) results in better heavy tail noise removal and yields more robust representations. Thus, SCDV-MS applied the hard thresholding directly on the word cluster assignments instead of the final document representations. Furthermore, applying sparsity over vocabulary words with fewer dimensions (O(V K)) instead of millions of documents (O(N Kd)) results in higher efficiency (N d &gt;&gt; V ). Here, N is the number documents, V is vocabulary, K is number of clusters, and d is word vector dimensions. Empirically, on 20NewGroups we observe that about 98% of entries in word-cluster assignments (P (ci|w)) for all words are close to 0 (&lt; 0.01). For each word on average, the probability of cluster assignment (P (ci|w)) for 58 cluster assignments out of 60 (variance of 1.56) is less than 0.01. Thus, applying thresholding at word-cluster assignment level is reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Doc2VecC vs SGNS Representations</head><p>In the SCDV approach, the SGNS word vectors represent the common words (mainly stop words) as non-zero vectors. This makes the clusters redundant and generates a heavy tail noise. SCDV-MS addressed this issue by using Doc2VecC <ref type="bibr" target="#b8">[9]</ref> which introduces corruption while doing context addition in word embeddings to help in learning robust word vector representations. In this approach, the common words of the corpus are forcefully learned as zeroed vectors. We observed that using Doc2VecC trained word vectors results in non-redundant diverse clusters. Thus, using Doc2VecC trained word vectors not only improves the performance but also reduces the feature size. There is no running time overhead for Doc2VecC compared to the SGNS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Low Dimensional Manifold Learning</head><p>SCDV <ref type="bibr" target="#b31">[32]</ref> represents documents as high dimensional sparse vectors. The SCDV approach showed that such vectors are useful for linear classification models. However, being useful for many downstream applications (especially the ones using deep learning models) requires a continuous low dimensional document representation similar to word vectors. To overcome this issue, SCDV-MS projected the sparse word-topic vectors into a lower-dimensional manifold which preserves the local neighborhood using simple techniques such as random projection. Furthermore, the manifold learning is applied over word vocabularies instead of millions of documents, which is more efficient. Dimensionality reduction for SCDV-MS is roughly (O( N V )) (where N is the number of documents and V is the size of vocabulary) faster than SCDV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>Document embeddings obtained using SCDV-MS can be used as direct features for downstream supervised tasks. We experimented with   text classification tasks (see <ref type="table" target="#tab_1">Table 2</ref>) to show the effectiveness of our embeddings. We evaluated the following questions through our experiments. Q4. Can effective lower dimensional manifold be learned from the sparse high dimensional word topic vectors?</p><p>Baselines: We considered the following baselines: Bag-of-Words (BoW) <ref type="bibr" target="#b18">[19]</ref>, Bag of Word Vector (BoWV) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b8">9</ref> Sparse Composite Document Vectors (SCDV) <ref type="bibr" target="#b31">[32]</ref>, 10 paragraph vectors <ref type="bibr" target="#b25">[26]</ref>, pmeans <ref type="bibr" target="#b42">[43]</ref>, ELMo <ref type="bibr" target="#b41">[42]</ref>, Topical word embeddings (TWE-1) <ref type="bibr" target="#b30">[31]</ref>, Neural Tensor Skip-Gram Model (NTSG-1 to NTSG-3) <ref type="bibr" target="#b29">[30]</ref>, tf-idf weighted average word-vector <ref type="bibr" target="#b43">[44]</ref> and weighted Bag of Concepts (weight-BoC) <ref type="bibr" target="#b21">[22]</ref>, and BERT <ref type="bibr" target="#b11">[12]</ref>. In BoC we built topic-document vectors by counting the member words in each topic. For BERT, we reported the results on the unsupervised pre-trained (pr) model because of a fair comparison to our approach which is also unsupervised. In Doc2VecC <ref type="bibr" target="#b8">[9]</ref> averaging and training the vectors was done jointly with corruption. Also, in SIF <ref type="bibr" target="#b3">[4]</ref> we used the inverse frequency weights for weighting while averaging word vectors, and finally removed the common components from the average. The results of our proposed embeddings is represented by SCDV-MS in <ref type="table" target="#tab_5">Tables 5 and   9</ref> https://bit.ly/2X0XfBH 10 https://bit.ly/36NxGZh</p><p>3. We also compared our representation with topic modeling-based embedding methods, described in the related work.</p><p>The Experimental Setting: We learned the word embeddings with Skip-Gram Negative Sampling (SGNS) using commonly used parameters, e.g., negative sample size of 10, minimum word frequency of 20, and the window size of 10. We ensure for usual data cleansing like stop word removal, lemmetization and stemming for all the baselines. In addition, we used simple models such as LinearSVM for multi-class classification and Logistic regression with a OneVs-Rest setting for the multi-label classification tasks so that we can directly compare our results with the previous approaches which uses the same classifiers. Similar to SCDV, to tune the hyperparameters, we employed a 5-fold cross-validation on the F1 score. We also used the Doc2VecC model <ref type="bibr" target="#b8">[9]</ref> to initialize the word embeddings on the annotated corpora for performance improvement. To ensure a fair comparison with SCDV, we fixed the same number of clusters to 60 and used full covariance for GMM clustering for all experiments based on our best empirical results with cross-validation. We tuned the hard threshold sparsity constant l from range {3, 5, 7} with crossvalidation to select the best hyper-parameter for making the word cluster assignments sparse. Moreover, we used AdaGram <ref type="bibr" target="#b5">[6]</ref> for disambiguating the sense of multi-sense words using a neighborhood of 5 context words on both sides, so that the window size is 10. We first ranked our words based on their tf-idf scores; we then selected a practicable number (top 5000 words) as candidates for the polysemic words. Next, we selected the true polysemic words by applying Ada-Gram on the candidates. <ref type="bibr" target="#b10">11</ref> The best parameter settings were used to generate baselines results. We used 200 dimensions for the tf-idf weighted word-vector model, 400 for the paragraph vector model, 80 topics and 400 dimensional vectors for TWE/NTSG/LTSG, and 60 topics and 200 dimensional word vectors for SCDV and BOWV. We reported the average of 5 runs. Our results were robust across multiple runs with a variance of O(10 −6 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Text Classification Results</head><p>We evaluated the classifier's performance on multi-class classification using several metrics such as accuracy, macro-averaging precision, recall, and macro F1-score. <ref type="table" target="#tab_2">Table 3</ref> shows a comparison with multiple state-of-the-art document representations (the first 7 except BERT/ELMo are clustering-based, the next 11 topic-word embeddings based, the next 6 are simple averaging or topic modeling methods) on the 20NewsGroup dataset. We also reported the results (micro F1) on the 20 classes of 20NewsGroup in <ref type="table" target="#tab_4">Table 4</ref>. Furthermore, we evaluated the multi-label classification performance using several metrics such as Precision@K, nDCG@k <ref type="bibr" target="#b6">[7]</ref>, Coverage error, Label ranking average precision score (LRAPS) <ref type="bibr" target="#b11">12</ref> and macro F1-score. <ref type="table" target="#tab_5">Table 5</ref> shows the results on the Reuters dataset. Datasets: We evaluated our approach by running multi-class classification experiments on the 20NewsGroup dataset, <ref type="bibr" target="#b12">13</ref> and multilabel classification experiments on the Reuters-21578 dataset. 14 For more details on dataset statistics refer to <ref type="table" target="#tab_1">Table 2</ref>. We used script for datasets preprocessing. <ref type="bibr" target="#b14">15</ref> Results and Analysis. We observed that our modified embeddings (SCDV-MS) with Doc2VecC-initialized word vectors, direct thresholding on word cluster assignments, and multi-sense disambiguation using AdaGram outperforms all earlier embeddings on both the 20NewsGroup and the Reuters datasets. From class-wise results in <ref type="table" target="#tab_4">Table 4</ref>, we notice a consistent performance improvement where we are outperforming SCDV in 18 out of 20 classes. It should be noted that the improvement on Reuters is not as great as the 20NewsGroup dataset due to the fact that the number of unique polysemic words in Reuters (250) is significantly fewer than 20NewsGroup (1000); thus each word is assigned to only one cluster. Therefore, the use of AdaGram for sense disambiguation and the sparsity operation over <ref type="bibr" target="#b11">12</ref>              the word-cluster assignments does not improve the performance by a large margin. We verified this claim in the ablation study below. We can conclude that our modifications yield notable improvements if the dataset has more multi-sense words.</p><p>Ablation Study: To understand the contributions of each of the three modifications, we compared five different versions of our embeddings. In the first version, we ablated the sparsity of the wordcluster assignments and applied sparsity directly on the document vectors similar to SCDV while keeping the Doc2VecC multi-sense word embeddings and the sense annotated corpus intact. In the second version, we ablated the Doc2VecC embeddings with normal SGNS embedding while keeping the word topic vector sparsity, and the sense annotated corpus intact. In the third version, we ablated multi-sense embeddings and the annotation of the corpus while keeping the Doc2VecC word training and the word topic vector sparsity intact. We also compared our results with an all ablation approach, i.e., the SCDV baseline and a none ablation approach, i.e., our new embeddings in SCDV-MS. <ref type="table" target="#tab_6">Table 6</ref> shows the results obtained with ablation on 20NewsGroup and Reuters datasets. We obtained the best performance with the none ablation approach, i.e., SCDV-MS. Thus, we can conclude that all three modifications is needed to yield the best performance. Multi-sense is the most pivotal improvement for 20newsgroup since by ablating it we observed the lowest performance out of ablating each of the three modifications. Whereas on the Reuters dataset, multi-sense was the least important because of fewer multi-sense words. On Reuters, the noise removal at word level representations was the most important.</p><p>Comparison with Contextual Embeddings: SCDV-MS is a lot simpler than unsupervised contextual embeddings like BERT (pr) and ELMo, but it outperformed them. We presume that SCDV-MS's concentration on capturing semantics (local and global) in sparse high dimension representations instead of capturing both semantics and syntax in single lower dimensional continuous representations (what BERT does) is the reason behind our method's superior performance. Because understanding syntax is not as influential as semantics in our classification and similarity tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">wtv's Lower Dimensional Manifold Learning</head><p>We tried three popular lower-dimensional manifold learning algorithms to reduce high dimensional sparse word topic vectors to lowerdimensional word embeddings, namely Random Projection <ref type="bibr" target="#b1">[2]</ref>, PCA (Subspace) <ref type="bibr" target="#b0">[1]</ref>, and Autoencoders <ref type="bibr" target="#b46">[47]</ref>. For PCA (Subspace), we observed that not all subspaces (200 dimensions) of word topic vectors have a complete full rank (200). Most of the subspaces were of ranks much smaller than 200 (rank ≤ 200). Therefore, we applied PCA on each of 200 dimension subspaces separately and concatenated the subspace reduced vectors. We refer to <ref type="table" target="#tab_0">Table 10</ref> as the criteria for PCA-based subspace rank reduction. For Auto-encoders, we used the standard architecture for reducing the word topic vectors from 12000 to 2000 dimensions, through a intermediate layer of 4000 dimensions (see <ref type="figure" target="#fig_8">Figure 7</ref>). We used the mean squared error minimization, tanh non-linear activation, and Adam optimization routine for training the autoencoders. We used the initial learning rate of 0.001, β1 = 0.9 and β2 = 0.999 for Adam optimization routine. Results: <ref type="table" target="#tab_7">Table 7</ref> shows the performance of the dimensionality reduction techniques such as Random Projection, PCA (Subspace), and Autoencoders on the 20NewsGroup dataset. We observed that autoencoders outperform other reduction methods because they easily fit any non-linear function. Also, we observed only a small decrease of 1% in the performance after a reduction to 2000 dimensions with most methods. This decrease is associated to loss due to data compression and can be explained by information theory. However, word topic vectors can be efficiently projected into a lower-dimensional manifold without a significant loss in the performance. We compared the percentage loss in performance (F1-Score) %RL = Orignal−Reduced Orignal on text classification with a dimension-reduced wtv through random projection for both SCDV and SCDV-MS on 20newsgroup. In <ref type="figure" target="#fig_5">Figures 4</ref>, 5 and 6 we observe that the %RL loss in SCDV-MS's F1-Score is distinctively less compared to SCDV for all reduction methods. Furthermore, we observed that the reduction time for SCDV-MS was shorter than SCDV, particularly for random projection, because SCDV-MS wtv are sparser than SCDV wtv. We also tried a direct reduction of final document representations which yielded a poor performance and took a longer reduction time for both embeddings. Overall, reducing SCDV-MS wtv is much more effective than reducing SCDV wtv or document vectors. Similar observations for a multi-label classification task on the Reuters dataset, see <ref type="table" target="#tab_0">Table 11</ref>.</p><p>Application to Deep Learning: One significance of our reduction of wtv is that they can be used as direct word embeddings in popular deep learning architectures such as CNNs on downstream classification tasks. We used the same architecture (see the supplementary material, <ref type="figure" target="#fig_2">Figure 8 16</ref> ) for both embeddings. Employing CNN, our results outperformed the original word embeddings of the same dimension for the 20NewsGroup classification task, shown in <ref type="table" target="#tab_8">Table 8</ref>. <ref type="table" target="#tab_9">Table 9</ref> illustrates empirical results for time and space complexities on SCDV (12000 dimensions), SCDV-MS (12000 dimensions) and reduced R-SCDV-MS (2000 dimensions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Time and Space Complexity</head><p>Feature Formation Time: Due to the direct thresholding of word cluster assignments in SCDV-MS, the word topic vectors ( wtv) are extremely sparse. SCDV-MS wtv has only 2% of active attributes (98% sparse), whereas SCDV wtv has 99% of active attributes (only 1% sparse). Therefore, we can use an efficient sparse operation (sparse addition and multiplication) over sparse vectors to speedup feature formation. We observed that by adding sparsity we can reduce the feature formation time by a significant factor of 43.</p><p>Overall Prediction Time: Overall, due to enhanced feature formation and reduced wtv loading time, SCDV-MS will predict faster compared to the original SCDV. However, we observed an insignificant difference in the prediction time and model size as SCDV sparsifies the final document vectors (both equally sparse). Furthermore, after reducing the SCDV-MS wtv to 2000-dense dimension features using auto-encoders, we observed a distinctive reduction of 8.5 times in the prediction time. One can directly store the reduced wtv for the complete vocabulary instead of the reduction model. Refer to <ref type="table" target="#tab_0">Table  12</ref> for SCDV-MS wtv reduction timing. Furthermore, one can directly also reduce the words appearing in the documents i.e. use a real-time reduction model during prediction. Vector Space Complexity, Training Time, and Model Size: We only require 0.1 of the original space to store sparse wtv. We achieved these improvements despite having 1.63 times of the size of the original vocabulary due to multi-sense word embeddings. The projected wtv is 3 times larger than the wtv of SCDV-MS; however, it is 3.7 times smaller than the wtv in SCDV. SCDV is marginally (1.1 times) sparser than SCDV-MS due to manual thresholding of document vectors. SCDV-MS also has a slightly faster training time compared to the original SCDV. We also observed that our training model on reduced vectors (R-SCDV-MS) is 6 times smaller than SCDV, and the training process of our classifier is 1.25 times faster than SCDV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed several novel modifications to overcome the shortcomings of SCDV, a state-of-the-art document embedding method. Our proposed SCDV-MS, outperformed the previous embedding (including SCDV and BERT (pr)) on the downstream tasks of text classification. Overall, we have shown that disambiguating multi-sense words based on context words (adjacent words) can lead to better document representations. Sparsity in representation is helpful for effective and efficient lower-dimensional manifold representation learning. Representation noise in words level can have a significant impact on the downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>8 https://github.com/sbos/AdaGram.jl</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :1 18 dvD n = 0; 19 for word wi in Dn do 20 dvD n += wtvw i ;</head><label>1181920</label><figDesc>SCDV-MS Data: Documents Dn, n = 1, . . . , N Result: Document vectors dvD n , n = 1, . . . , N / * Word Sense Disambiguation * / Use adagram for word sense disambiguation; 2 Annotate each word with a sense according to the neighboring context words; 3 Obtain word vectors ( wvi) on annotated corpus using Doc2VecC; 4 for each word wi ∈ V do 5 obtain idf values, idf(wi), i = 1..|V | ; / * |V | is the vocabulary * each word wi ∈ V do 11 for each cluster c k do 12 wcv ik = wvi × SP(c k |wi); 13 end 14 wtvi = idf(wi) × K k=1 wcv ik ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Effect of sense disambiguation on word cluster assignment probabilities</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Flowchart representing modified wtv computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Flowchart representing final document vector computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Percentage loss in F1-Score (%RL) after Random Projection-based wtv dimensionality reduction on 20NewsGroup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Percentage loss in F1-Score (%RL) after Autoencoder-based wtv dimensionality reduction on 20NewsGroup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Percentage loss in F1-Score (%RL) after PCA (Subspace)-based wtv dimensionality reduction on 20NewsGroup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>The AutoEncoder architecture (12000 − 4000 − 2000 − 4000 − 12000) used to reduce the word topic vectors from 12000 dimensions to 2000 dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of multi-sense words along with their context words and the corresponding prominent cluster wordsMulti-Sense Words Sentence (Context Words) Prominent Cluster Words SubjectThe math subject is a nightmare for many students physics, chemistry, math, science In anxiety, he sent an email without a subject mail, letter, email, gmail After promotion, he went to Maldives for spring break vacation, holiday, trip, spring BreakBreaking government websites is common for hackers encryption, cipher, security, privacy Use break to stop growing recursion loops if, elseif, endif, loop, continue UnitThe S.I. unit of distance is meter calculation, distance, mass, length Multimeter shows a unit of 5V electronics, KWH, digital, signal InterestHis interest lies in astrophysics information, enthusiasm, question Banks interest rates are controlled by RBI bank, market, finance, investment</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Text classification datasets overview.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell cols="2">#Classes #train / #test</cell></row><row><cell>Multi-class</cell><cell>20NewsGroup</cell><cell>20</cell><cell>11K / 8K</cell></row><row><cell cols="2">Multi-label Reuters-21578</cell><cell>444</cell><cell>13K / 6K</cell></row></table><note>Q1. Does disambiguating word-cluster assignments using multi-sense embedding improve classification performance? Q2. Does hard thresholding over word-cluster assignments improve performance, space and time complexities? Q3. Is representational noise reduction using Doc2Vec initialization effective?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance on multi-class classification. Values in bold show the best performance using the SCDV-MS embeddings.</figDesc><table><row><cell>Model</cell><cell cols="4">Accuracy Precision Recall F-measure</cell></row><row><cell>SCDV-MS</cell><cell>86.19</cell><cell>86.20</cell><cell>86.18</cell><cell>86.16</cell></row><row><cell>R-SCDV-MS</cell><cell>84.9</cell><cell>84.9</cell><cell>84.9</cell><cell>84.9</cell></row><row><cell>BERT (pr)[12]</cell><cell>84.9</cell><cell>84.9</cell><cell>85.0</cell><cell>85.0</cell></row><row><cell>SCDV [32]</cell><cell>84.6</cell><cell>84.6</cell><cell>84.5</cell><cell>84.6</cell></row><row><cell>RandBin</cell><cell>83.9</cell><cell>83.99</cell><cell>83 .9</cell><cell>83.76</cell></row><row><cell>BoWV [17]</cell><cell>81.6</cell><cell>81.1</cell><cell>81.1</cell><cell>80.9</cell></row><row><cell>pmeans [43]</cell><cell>81.9</cell><cell>81.9</cell><cell>81.9</cell><cell>81.5</cell></row><row><cell>Doc2VecC [9]</cell><cell>84.0</cell><cell>84.1</cell><cell>84.1</cell><cell>84.0</cell></row><row><cell>BoE [20]</cell><cell>83.1</cell><cell>83.1</cell><cell>83.1</cell><cell>83.1</cell></row><row><cell>NTSG-2 [30]</cell><cell>82.5</cell><cell>83.7</cell><cell>82.8</cell><cell>82.4</cell></row><row><cell>LTSG [25]</cell><cell>82.8</cell><cell>82.4</cell><cell>81.8</cell><cell>81.8</cell></row><row><cell>WTM [15]</cell><cell>80.9</cell><cell>80.3</cell><cell>80.3</cell><cell>80.0</cell></row><row><cell>ELMo [42]</cell><cell>74.1</cell><cell>74.0</cell><cell>74.1</cell><cell>73.9</cell></row><row><cell>w2v-LDA [39]</cell><cell>77.7</cell><cell>77.4</cell><cell>77.2</cell><cell>76.9</cell></row><row><cell>TV+MeanWV [28]</cell><cell>72.2</cell><cell>71.8</cell><cell>71.5</cell><cell>71.6</cell></row><row><cell>MvTM [29]</cell><cell>72.2</cell><cell>71.8</cell><cell>71.5</cell><cell>71.6</cell></row><row><cell>TWE-1 [31]</cell><cell>81.5</cell><cell>81.2</cell><cell>80.6</cell><cell>80.6</cell></row><row><cell>lda2Vec [38]</cell><cell>81.3</cell><cell>81.4</cell><cell>80.4</cell><cell>80.5</cell></row><row><cell>lda [8]</cell><cell>72.2</cell><cell>70.8</cell><cell>70.7</cell><cell>70.0</cell></row><row><cell>weight-AvgVec [44]</cell><cell>81.9</cell><cell>81.7</cell><cell>81.9</cell><cell>81.7</cell></row><row><cell>BoW [44]</cell><cell>79.7</cell><cell>79.5</cell><cell>79.0</cell><cell>79.0</cell></row><row><cell>weight-BOC [44]</cell><cell>71.8</cell><cell>71.3</cell><cell>71.8</cell><cell>71.4</cell></row><row><cell>PV-DBoW [26]</cell><cell>75.4</cell><cell>74.9</cell><cell>74.3</cell><cell>74.3</cell></row><row><cell>PV-DM [26]</cell><cell>72.4</cell><cell>72.1</cell><cell>71.5</cell><cell>71.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Class-wise F1-Score on the 20newsgroup dataset with different document representations.</figDesc><table><row><cell>Class Name</cell><cell cols="3">SCDV SCDV-MS R-SCDV-MS</cell></row><row><cell>alt.atheism</cell><cell>80.14</cell><cell>81.35</cell><cell>80.39</cell></row><row><cell>comp.graphics</cell><cell>78.99</cell><cell>76.84</cell><cell>76.95</cell></row><row><cell cols="2">comp.os.ms-windows.misc 75.65</cell><cell>77.65</cell><cell>78.28</cell></row><row><cell cols="2">comp.sys.ibm.pc.hardware 72.08</cell><cell>73.43</cell><cell>68.38</cell></row><row><cell cols="2">comp.sys.mac.hardware 82.15</cell><cell>86.82</cell><cell>80.16</cell></row><row><cell>comp.windows.x</cell><cell>81.8</cell><cell>82.97</cell><cell>83.27</cell></row><row><cell>misc.forsale</cell><cell>82.8</cell><cell>85.13</cell><cell>84.99</cell></row><row><cell>rec.autos</cell><cell>89.06</cell><cell>92.53</cell><cell>91.77</cell></row><row><cell>rec.motorcycles</cell><cell>94.27</cell><cell>96.11</cell><cell>94.27</cell></row><row><cell>rec.sport.baseball</cell><cell>93.57</cell><cell>96.47</cell><cell>93.68</cell></row><row><cell>rec.sport.hockey</cell><cell>97.27</cell><cell>96.78</cell><cell>96.41</cell></row><row><cell>sci.crypt</cell><cell>93.1</cell><cell>92.82</cell><cell>93.5</cell></row><row><cell>sci.electronics</cell><cell>77.38</cell><cell>77.45</cell><cell>74.25</cell></row><row><cell>sci.med</cell><cell>88.58</cell><cell>92.30</cell><cell>91.57</cell></row><row><cell>sci.space</cell><cell>90.33</cell><cell>91.40</cell><cell>90.71</cell></row><row><cell>soc.religion.christian</cell><cell>89.56</cell><cell>89.97</cell><cell>89.76</cell></row><row><cell>talk.politics.guns</cell><cell>80.69</cell><cell>84.18</cell><cell>83.05</cell></row><row><cell>talk.politics.mideast</cell><cell>95.96</cell><cell>95.95</cell><cell>96.1</cell></row><row><cell>talk.politics.misc</cell><cell>69.33</cell><cell>73.49</cell><cell>73.67</cell></row><row><cell>talk.religion.misc</cell><cell>65.53</cell><cell>65.54</cell><cell>60.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance on various metrics for multi-label classification on the Reuters dataset. Values in bold show the best performance using the SCDV-MS algorithm.</figDesc><table><row><cell>Model</cell><cell>Prec</cell><cell>Prec</cell><cell>nDCG</cell><cell>Cover.</cell><cell cols="2">LRAPS F1</cell></row><row><cell></cell><cell>@1</cell><cell>@5</cell><cell>@5</cell><cell>Error</cell><cell></cell><cell>Score</cell></row><row><cell cols="5">SCDV-MS 95.06 37.56 50.20 5.87</cell><cell cols="2">94.21 82.71</cell></row><row><cell cols="5">R-SCDV-MS 93.56 37.00 49.47 6.74</cell><cell cols="2">92.96 81.94</cell></row><row><cell>BERT (pr)</cell><cell>93.8</cell><cell>37</cell><cell>49.6</cell><cell>6.3</cell><cell>93.1</cell><cell>81.9</cell></row><row><cell>SCDV</cell><cell cols="3">94.00 37.05 49.6</cell><cell>6.65</cell><cell cols="2">93.34 81.77</cell></row><row><cell cols="5">Doc2VecC 93.45 36.86 49.28 6.83</cell><cell cols="2">92.66 81.29</cell></row><row><cell>pmeans</cell><cell cols="4">93.29 36.65 48.95 7.66</cell><cell cols="2">91.72 77.81</cell></row><row><cell>BoWV</cell><cell cols="4">92.90 36.14 48.55 8.16</cell><cell cols="2">91.46 79.16</cell></row><row><cell>TWE-1</cell><cell cols="4">90.91 35.49 47.54 8.16</cell><cell cols="2">91.46 79.16</cell></row><row><cell>PV-DM</cell><cell cols="6">87.54 33.24 44.21 13.15 86.21 70.24</cell></row><row><cell cols="7">PV-DBoW 88.78 34.51 46.42 11.28 87.43 73.68</cell></row><row><cell cols="5">tfidf AvgVec 89.33 35.04 46.83 9.42</cell><cell cols="2">87.90 71.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation Study reporting F1 scores. In ±x, x is the variance across several runs.</figDesc><table><row><cell cols="2">Ablation (w/o) 20NewsGroup</cell><cell>Reuters</cell></row><row><cell>Sparsity</cell><cell cols="2">85.28 ± 0.002 82.17 ± 0.001</cell></row><row><cell>Doc2VecC</cell><cell cols="2">85.41 ± 0.001 82.08 ± 0.002</cell></row><row><cell>MultiSense</cell><cell cols="2">85.16 ± 0.001 82.43 ± 0.001</cell></row><row><cell>All</cell><cell cols="2">84.61 ± 0.004 81.77 ± 0.003</cell></row><row><cell>None</cell><cell cols="2">86.16 ± 0.002 82.71 ± 0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Performance in terms of accuracy with various dimensionality reduction methods on the 20NewsGroup dataset. Similar results were acquired for precision, recall, and F1 score.</figDesc><table><row><cell>Dimension</cell><cell>Random</cell><cell>PCA</cell><cell>Autoencoder</cell></row><row><cell></cell><cell>Projection</cell><cell>(SubSpace)</cell><cell></cell></row><row><cell>200</cell><cell>78.97</cell><cell>80.62</cell><cell>81.44</cell></row><row><cell>500</cell><cell>82.19</cell><cell>83.14</cell><cell>83.83</cell></row><row><cell>1000</cell><cell>83.75</cell><cell>83.80</cell><cell>84.31</cell></row><row><cell>2000</cell><cell>84.47</cell><cell>84.34</cell><cell>84.80</cell></row><row><cell>3000</cell><cell>84.94</cell><cell>84.86</cell><cell>84.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Performance of Convolutional Neural Network (CNN) for multi-class text classification on 20NewsGroup with the original word embeddings (200 dimensions) and the reduced word topic vectors (2000 dimensions). In ±x, x is the variance across several runs.</figDesc><table><row><cell>Embedding</cell><cell>Dimen</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-Score</cell></row><row><cell>Word2Vec</cell><cell>200</cell><cell cols="2">82.07 ± 0.003 82.22 ± 0.005</cell><cell>81.9 ± 0.004</cell><cell>81.9 ± 0.005</cell></row><row><cell>Reduce Word Topic Vector</cell><cell>200</cell><cell cols="4">82.13 ± 0.002 82.36 ± 0.003 82.06 + ± 0.003 82.05 ± 0.003</cell></row><row><cell>Word2Vec</cell><cell>2000</cell><cell cols="2">82.31 ± 0.004 82.87 ± 0.005</cell><cell>82.31 ± 0.004</cell><cell>82.38 ± 0.005</cell></row><row><cell>Reduce Word Topic Vector</cell><cell>2000</cell><cell cols="2">82.85 ± 0.001 83.18 ± 0.004</cell><cell>82.64 ± 0.002</cell><cell>82.68 ± 0.003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Time and Space complexity analysis of embedding methods. Bold values represent the best results.</figDesc><table><row><cell>Method</cell><cell>Vocab wtv</cell><cell>wtv</cell><cell>dv</cell><cell>Cluster</cell><cell>Feature</cell><cell>Predict</cell><cell>Training</cell><cell>Model</cell><cell>wtv</cell></row><row><cell></cell><cell>Dim</cell><cell>Sparsity(%)</cell><cell>Sparsity(%)</cell><cell>(sec)</cell><cell>(µ sec)</cell><cell>(µ sec)</cell><cell>(min)</cell><cell>Size (KB)</cell><cell>Space(MB)</cell></row><row><cell>SCDV</cell><cell>15591 12000</cell><cell>1</cell><cell>81</cell><cell>242</cell><cell>2.56</cell><cell>119</cell><cell>82</cell><cell>1900</cell><cell>748</cell></row><row><cell cols="2">SCDV-MS 25466 12000</cell><cell>98</cell><cell>74</cell><cell>569</cell><cell>0.06</cell><cell>111</cell><cell>79</cell><cell>1900</cell><cell>71</cell></row><row><cell cols="2">R-SCDV-MS 25466 2000</cell><cell>0</cell><cell>0</cell><cell>576</cell><cell>0.86</cell><cell>14</cell><cell>66</cell><cell>333</cell><cell>203</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>PCA based subspace rank reduction criteria. 10 reduce to 10 else the original rank 1000 rank &gt; 20 reduce to 15 else the original rank 2000 rank &gt; 100 reduce to 30 else the original rank 3000 rank &gt; 100 reduce to 50 else the original rank</figDesc><table><row><cell>Red-Dim</cell><cell>Subspace Rank Reduction Criteria</cell></row><row><cell>500</cell><cell>rank &gt;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Performance on reduced word topic vectors using several reduction techniques on various dimensions for multi-label classification -the Reuters dataset.</figDesc><table><row><cell cols="4">Method Dimen Prec@1 Prec nDCG Cover LRAP F1</cell></row><row><cell></cell><cell></cell><cell cols="2">nDCG @5 @5 Error</cell><cell>Score</cell></row><row><cell></cell><cell>500</cell><cell cols="2">92.14 36.53 48.74 8.02 91.34 79.96</cell></row><row><cell>Auto</cell><cell cols="3">1000 92.95 36.82 49.17 7.14 92.32 81.05</cell></row><row><cell cols="4">Encoder 2000 93.39 36.95 49.39 6.87 92.75 81.65</cell></row><row><cell></cell><cell cols="2">3000 93.56</cell><cell>37 49.47 6.74 92.96 81.94</cell></row><row><cell></cell><cell>500</cell><cell cols="2">91.98 36.26 48.47 7.41</cell><cell>91 79.03</cell></row><row><cell cols="4">Random 1000 92.59 36.62 48.91 6.98 91.84 80.39</cell></row><row><cell cols="4">Projection 2000 93.39 36.83 49.26 6.95 92.59 81.12</cell></row><row><cell></cell><cell cols="3">3000 93.32 36.91 49.33 6.78 92.75 81.39</cell></row><row><cell></cell><cell>500</cell><cell cols="2">90.73 36.03 48.06 8.40 90.11 78.06</cell></row><row><cell>PCA</cell><cell cols="3">1000 92.15 36.55 48.78 7.44 91.48 80</cell></row><row><cell cols="4">(SubSpace) 2000 92.95 36.87 49.22 6.86 92.30 81.2</cell></row><row><cell></cell><cell cols="3">3000 93.38 36.97 49.4 6.69 92.8 81.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Time complexity for dimensionality reduction of word topic vectors to a 2000-dimension dense representation using various reduction techniques on 20NewsGroup</figDesc><table><row><cell>Method</cell><cell>Random</cell><cell>PCA</cell><cell>Autoencoder</cell></row><row><cell></cell><cell>Projection</cell><cell>(SubSpace)</cell><cell></cell></row><row><cell>Time (sec)</cell><cell>35</cell><cell>66</cell><cell>608</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://bit.ly/2Jv6wxX</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">https://bit.ly/33473wk</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Principal component analysis&apos;, Wiley interdisciplinary reviews: computational statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynne</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Database-friendly random projections: Johnsonlindenstrauss with binary coins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and System Sciences</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vector of locally aggregated embeddings for text representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple but toughto-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Probabilistic fasttext for multi-sense word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02901</idno>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Breaking sticks and ambiguities with adaptive skip-gram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kondrashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse local embeddings for extreme multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient vector representation for documents through corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A mixture model for learning multi-sense word embeddings</title>
		<editor>Dat Quoc Nguyen Dai Quoc Nguyen, Ashutosh Modi, Stefan Thater, and Manfred Pinkal</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gaussian lda for topic models with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 53th Annual Meeting of the Association of Computational Linguistic (ACL)</title>
		<meeting>The 53th Annual Meeting of the Association of Computational Linguistic (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Association of Computational Linguistic</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05545</idno>
		<title level="m">The dynamic embedded topic model</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Topic modeling in embedding spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04907</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving distributed word representation and topic model by word-topic mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghua</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangwang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 8th Asian Conference on Machine Learning</title>
		<meeting>The 8th Asian Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Product classification in e-commerce using distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Karnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashendra</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradhuman</forename><surname>Jhala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised document representation using partition word-vectors averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar Saw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praneeth</forename><surname>Netrapalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distributional structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag-ofembeddings for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the ACL</title>
		<meeting>the 52nd Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bag-of-concepts: Comprehending document representation through clustering words in distributed representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjoong</forename><surname>Han Kyul Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungzoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Neurocomputing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<title level="m">Skip thought vectors</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ltsg: Latent topical skip-gram for mutually learning topic model and vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarvan</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Hankz Hankui Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07117</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01070</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative topic embedding: a continuous representation of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 54th Annual Meeting of the ACL (ACL). Association of Computational Linguistic</title>
		<meeting>The 54th Annual Meeting of the ACL (ACL). Association of Computational Linguistic</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Integrating topic modeling with word embeddings by mixtures of vmfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changchun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning contextsensitive word embeddings with neural tensor skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2014, the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>IJCAI 2014, the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Proceedings of AAAI 2015</title>
		<meeting>AAAI 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse composite document vectors using soft clustering over distributional representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Mekala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargavi</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Karnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scdv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jančernockỳ</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Composition in distributional models of semantics&apos;, Cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mixing dirichlet topic models and word embeddings to make lda2vec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moody</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02019</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Topic2vec: learning distributed representations of topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Asian Language Processing (IALP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Concatenated p-mean word embeddings as universal cross-lingual sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01400</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Words are not equal: Graded weighting model for building composite document vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranjal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitabha</forename><surname>Mukerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICON-2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Tensor product variable binding and the representation of symbolic structures in connectionist systems&apos;, Artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings&apos;, International Conference of Learning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
