<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Aesthetic Quality Assessment with Semantic Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueying</forename><surname>Kao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
						</author>
						<title level="a" type="main">Deep Aesthetic Quality Assessment with Semantic Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human beings often assess the aesthetic quality of an image coupled with the identification of the image's semantic content. This paper addresses the correlation issue between automatic aesthetic quality assessment and semantic recognition. We cast the assessment problem as the main task among a multitask deep model, and argue that semantic recognition task offers the key to address this problem. Based on convolutional neural networks, we employ a single and simple multi-task framework to efficiently utilize the supervision of aesthetic and semantic labels. A correlation item between these two tasks is further introduced to the framework by incorporating the inter-task relationship learning. This item not only provides some useful insight about the correlation but also improves assessment accuracy of the aesthetic task. Particularly, an effective strategy is developed to keep a balance between the two tasks, which facilitates to optimize the parameters of the framework. Extensive experiments on the challenging AVA dataset and Photo.net dataset validate the importance of semantic recognition in aesthetic quality assessment, and demonstrate that multi-task deep models can discover an effective aesthetic representation to achieve state-ofthe-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Aesthetic image analysis has attracted increasing attention in computer vision community <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. It is related to the high-level perception of visual aesthetics. Machine learning models for visual aesthetic quality assessment have shown to be useful in many applications, e.g., image retrieval, photo management, image editing, and photography <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Since visual aesthetics is a subjective attribute, automatically assessing aesthetic quality of images is still challenging. Many data-driven approaches <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> have been proposed to address this issue. These methods often learn from the aesthetic quality of images that are labeled by humans. Most of these methods aim to discover a meaningful and better aesthetic representation, and often formulate the representation learning as a single and standalone classification task.</p><p>Handcrafted features are earlier attempts. They are based on the intuitions of how people perceive the aesthetic quality of images or photographic rules. These features include color <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>, the rule of thirds <ref type="bibr" target="#b12">[13]</ref>, simplicity <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and composition <ref type="bibr" target="#b14">[15]</ref>. Later, generic image descriptors such as bag-of-visual-words (BOV) <ref type="bibr" target="#b22">[23]</ref> and fisher vectors (FV) <ref type="bibr" target="#b23">[24]</ref> are used to assess aesthetic quality. They are shown to outperform the traditional handcrafted features <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Recently, deep convolutional neural networks (CNNs) <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> have been applied to aesthetic quality assessment <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Nevertheless, these computational approaches provide either accurate or interpretable results <ref type="bibr" target="#b3">[4]</ref>.</p><p>For human beings, aesthetic quality assessment is always coupled with the identification of semantic content of images <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. It is difficult for humans to treat aesthetic quality assessment as an isolate and independent task. When humans assess the aesthetic quality of an image, they first understand what they are assessing. That is, they have known the sematic information of this image. Seen from <ref type="figure" target="#fig_0">Fig. 1</ref>, we can recognize the semantic content from these images at a glance and assess the aesthetic quality quickly. Hence it is reasonable to assume that, assessing aesthetic quality and semantic recognition are correlated tasks for machine learning. However, the relationship between semantic recognition and automatically assessing visual aesthetic quality has not been fully explored. This paper addresses the correlation issue between automatic aesthetic quality assessment and semantic recognition. We employ multi-task convolutional neural network to explore the potential correlation. Multi-task learning can learn multiple related tasks in parallel with shared knowledge. It has been demonstrated that this approach can boost some or all of the tasks <ref type="bibr" target="#b34">[35]</ref>. Our goal is to utilize semantic recognition in the joint objective function to improve the aesthetic quality assessment, our main task. However, there is still a typical challenge in the multi-task learning for our multi-task problem. That is, the aesthetic task and semantic task face the different learning difficulties. The main reason is that the semantic recognition is much easier than aesthetics assessment. The semantic content is much objective, while the aesthetic attributes are subjective. Thus, different from the strategies of treating all tasks equally and early stopping <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> we present a strategy to keep the effect of both tasks balanced in the joint objective function.</p><p>In addition, to discover the relationships between aesthetic and semantic tasks automatically and to better exploit the intertask relatedness for more effective feature learning, we model the task relationship and impose it in the objective function. To some extent, it can explain the factors in aesthetic quality assessment and make our results more interpretable. Thus, to investigate how to make full use of semantic information and  how semantic information influence aesthetic task, our multitask framework considers the strategy of keeping the effect of two tasks balanced and the relationship learning between semantic and aesthetic tasks.</p><p>In the evaluation, the most challenging large-scale AVA dataset <ref type="bibr" target="#b24">[25]</ref> is used to verify the effectiveness of semantic information for aesthetic feature learning and investigate the correlation among aesthetic and semantic content recognitions. The experiments show that our results significantly outperform the state-of-the-art results <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> for aesthetic quality assessment on AVA dataset. Furthermore, it is demonstrated that the learned representation with our multi-task framework can be transferred for the dataset (here we use Photo.net dataset <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>) with only aesthetic labels and other semantic representation (such as from Imagenet) can also be used for aesthetic representation learning.</p><p>Our contributions lie in three-fold:</p><p>• Instead of taking visual aesthetic quality assessment as an isolated task, we propose to exploit the semantic recognition to jointly assess the aesthetic quality with a single multi-task convolutional neural network (MTCNN). It is a novel attempt to learn aesthetic features with the help of a related task, i.e. semantic recognition. • We propose to automatically learn the correlations between the aesthetic and semantic tasks by simultaneously modeling the inter-task relationship and controlling the parameters' complexity of each task in our multi-task framework. It can explain the factors in aesthetic quality assessment and makes our results more interpretable. • Facing the different learning difficulties between the two tasks, we present a strategy to keep the effect of both tasks balanced in the joint objective function. The proposed method outperforms the state-of-the-art methods on the challenging AVA dataset and Photo.net dataset.</p><p>The rest of this paper is organized as follows: we summarize related work in Section II, describe our method in detail in Section III, present the experiments in Section IV, and conclude the paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Since our work is related to the aesthetic quality assessment and multi-task learning, we will mainly review work related to the two parts in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Aesthetic quality assessment</head><p>Most previous works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> on aesthetic quality assessment focus on the challenging problem of designing appropriate features. Typically, handcrafted features are proposed based on the intuitions about human perception of the aesthetic quality of images or photographic rules. For example, Datta et al. <ref type="bibr" target="#b12">[13]</ref> design certain visual features such as colorfulness, the rule of thirds, and low depth of field indicators, to discriminate between aesthetically pleasing and displeasing images. Dhar et al. <ref type="bibr" target="#b14">[15]</ref> extract some high level attributes including compositional, content, and sky-illumination attributes, which are characteristically used by humans to describe images. Luo et al. <ref type="bibr" target="#b37">[38]</ref> and Tang et al. <ref type="bibr" target="#b2">[3]</ref> consider that photos may have different aesthetic criteria in mind for different type of images and design visual features in different ways according to the variety of photo content. In <ref type="bibr" target="#b15">[16]</ref>, generic image descriptors are used to assess aesthetic quality, which are shown to outperform the traditional handcrafted features.</p><p>Despite the success of handcrafted features and generic image descriptors, CNNs have been applied to aesthetic quality assessment <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> and obtain the state-of-theart performance. CNNs learn aesthetic features automatically. However, they extract features by treating aesthetic quality assessment as an independent problem. The network in <ref type="bibr" target="#b28">[29]</ref>, RDCNN, hopes to leverage the idea of multi-task learning with the style attributes to help determine the aesthetic quality of images. Unfortunately, due to many missing labels for style attributes, they can not jointly perform aesthetics categorization and style classification in a neural network, and just concatenate the features of the aesthetics and style by using transfer learning. Our work is also related to CNNs for aesthetics classification. In contrast, firstly, we exploit semantic information to assist in learning aesthetic representation with a multi-task learning framework. We can jointly learn aesthetics categorization and semantic recognition with a single multitask network, which is different from RDCNN <ref type="bibr" target="#b28">[29]</ref>. Secondly, our multi-task CNN considers the strategy of keeping the effect of two tasks balanced and the relationship learning between semantic and aesthetic tasks. Finally, images are labeled with semantic information much easier than style attributes in real world. This is because only professional photographer and photography amateurs are familiar with all the style attributes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-task learning</head><p>Multi-task learning aims to boost the generalization performance by learning multiple related tasks simultaneously <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b40">[41]</ref>. It does this by learning tasks in parallel while using a shared representation <ref type="bibr" target="#b34">[35]</ref>. Deep neural network can learn features jointly under multiple objectives and it is the earliest models for multi-task learning. Multi-task learning based on deep neural network has been applied to many computer vision problems <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b41">[42]</ref>. However, there are many strategies for sharing knowledge and learning process for different problems. For example, Zhang et al. <ref type="bibr" target="#b42">[43]</ref> share parameters in all layers and learn the common features for all tasks, while Liu et al. <ref type="bibr" target="#b43">[44]</ref> just sharing in some bottom layers and learn respective representation in some top layers for each task. Yim et al. <ref type="bibr" target="#b35">[36]</ref> treat all tasks equally important. In contrast, early stopping strategy is used in some related tasks <ref type="bibr" target="#b36">[37]</ref>, due to different learning difficulties and convergence rates in different tasks. In our problem, because semantic recognition task is much easier than aesthetic quality assessment, common features of our two tasks are learned simultaneously and an effective strategy of keeping effect of all the tasks balanced in the joint objective function is used. In addition, the task relationships can be learned from the data automatically in the conventional methods <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Inspired by this, we consider the relationship learning in our multi-task neural networks to explore the relationships between the aesthetic and semantic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we propose to exploit the semantic information to help identify the aesthetic quality of images, assuming that they are considered as the related attributes <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>Here the aesthetic quality assessment is our main task and the semantic content recognition is the aided task. Our problem is firstly formulated as a multi-task convolutional neural network (MTCNN) model without learning task relationships automatically from data. Then we develop a multi-task relationship learning convolutional neural network (MTRLCNN) model by adding the task relationship learning in the objective function to discover the correlation between aesthetic task and semantic tasks. An example of MTCNN architectures is illustrated in <ref type="figure">Fig. 2</ref>. Furthermore, we explore and adapt different network structures to our problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-Task Probabilistic Framework</head><p>Our problem can be interpreted as a probabilistic model. Using the probabilistic formulation, various deep networks can solve our problem by optimizing the model parameters that maximize the posterior probability. Then, Bayesian analysis is leveraged to predict most likely aesthetic quality and semantic attributes of given images.</p><p>Assuming a training dataset with a total of N samples, which are associated with C aesthetic classes and M semantic attributes. Considering each image has only one aesthetic class and multiple semantic attributes in real world, each image is represented as (x n , y n , z n ), n = 1, 2, ..., N . Here x n represents the n-th image sample, y n = c, c = 0, ..., C − 1 is the aesthetic label and z n = [z 1 n , ..., z m n , ..., z M n ] T is the semantic label for the n-th image sample. If the n-th image sample has the m-th semantic attribute, the m-th semantic label is set as z m n = 1, otherwise z m n = 0. Therefore a given dataset is denoted as (X, Y, Z) = {(x n , y n , z n ), n ∈ {1, 2, ..., N }}. For our MTCNNs (our MTCNN #1 is shown in <ref type="figure">Fig. 2</ref>), Θ denotes the common parameters in some bottom layers to learn features for all tasks, and W = [W a , W s ] indicates the specific parameters for associated tasks. W a and W s represent the parameters for aesthetic quality assessment and semantic recognition respectively. Each column in W a or W s corresponds to a subtask. The goal is to find the optimal or sub-optimal parameters Θ, W, λ by maximizing the following posterior probabilitŷ</p><formula xml:id="formula_0">Θ,Ŵ ,λ = argmax Θ,W,λ p(Θ, W, λ|X, Y, Z),<label>(1)</label></formula><p>where λ is the weight coefficient of the semantic recognition task in the joint learning process. Based on the Bayesian theorem, we have</p><formula xml:id="formula_1">p(Θ, W, λ|X, Y, Z) = p(X, Y, Z|Θ, W, λ)p(Θ, W, λ) p(X, Y, Z) ∝ p(X, Y, Z|Θ, W, λ)p(Θ, W, λ),<label>(2)</label></formula><p>where p(X, Y, Z|Θ, W, λ) is the conditional probability, and p(Θ, W, λ) is the prior probability.</p><p>Then Eqn. (1) takes the form</p><formula xml:id="formula_2">Θ,Ŵ ,λ ∝ argmax Θ,W,λ p(Y |X, Θ, W a )p(Z|X, Θ, W s , λ)p(Θ)p(W )p(λ).<label>(3)</label></formula><p>Each term in Eqn. (3) is defined as:</p><p>1) The conditional probability p(Y |X, Θ, W a ) corresponds to the task of aesthetic quality assessment. Here assessing aesthetic quality is interpreted as a classification problem and modeled as a multinomial logistic regression similar to traditional classification problems <ref type="bibr" target="#b26">[27]</ref>. The conditional probability p(Y |X, Θ, W a ) can be formulated as</p><formula xml:id="formula_3">p(Y |X, Θ, W a ) = N n=1 C c=1 1{y n = c}p(y n = c|x n , Θ, W a ),<label>(4)</label></formula><p>where 1{·} is the indicator function, it has two values, 1{a true statement} = 1, and 1{a f alse statement} = 0. p(y n = c|x n , Θ, W a ) is calculated by the softmax function</p><formula xml:id="formula_4">p(y n = c|x n , Θ, W a ) = exp(W c a T (Θ T x n )) C l=1 exp(W l a T (Θ T x n )) .<label>(5)</label></formula><p>2) The conditional probability p(Z|X, Θ, W s , λ) corresponds to the semantic recognition. Since each element of the semantic label of a given image is binary: z m n ∈ {0, 1}, each semantic attribute recognition can be interpreted as a logistic regression. Hence the conditional probability p(Z|X, Θ, W s , λ) can be</p><formula xml:id="formula_5">p(Z|X, Θ, W s , λ) = N n=1 M m=1 (p(z m n = 1|x n , Θ, W m s ) z m n (1 − p(z m n = 1|x n , Θ, W m s )) 1−z m n ) λ ,<label>(6)</label></formula><p>where p(z m n = 1|x n , Θ, W m s ) is calculated by a sigmoid function σ(x) = 1/(1 + exp(−x)).</p><p>3) The prior probability p(Θ) corresponds to the network parameters for common features. The parameters Θ can be initialized as a standard normal distribution like previous network <ref type="bibr" target="#b26">[27]</ref>.</p><formula xml:id="formula_6">p(Θ) = K k=1 p(θ k ) = K k=1 N (0, I),</formula><p>where 0 is a zero matrix and I is an identity matrix. 4) Similar to Θ, the parameters W for specific tasks can also be initialized as a standard normal distribution. Thus, the prior probability can be p(W ) = p(W a )p(W s ) = N a (0, I)N s (0, I).</p><p>5) λ is used to control the influence of semantic recognition task in the final objective function. The prior probability p(λ) is implemented by defining λ obeying a normal distribution, p(λ) = N(µ, σ 2 ).</p><p>Then Eqns. <ref type="formula" target="#formula_3">(4)</ref>, <ref type="formula" target="#formula_4">(5)</ref> and <ref type="formula" target="#formula_5">(6)</ref> are substituted into Eqn. <ref type="formula" target="#formula_2">(3)</ref>, negative log function is taken for Eqn. (3), and the constant terms are omitted. As a result, the objective function can be</p><formula xml:id="formula_7">argmin Θ,W,λ {− N n=1 C c=1 1{y n = c}log exp(W c a T (Θ T x n )) C l=1 exp(W l a T (Θ T x n )) − λ N n=1 M m=1 (z m n logσ(W m s T (Θ T x n )) + (1 − z m n )(1− logσ(W m s T (Θ T x n )))) + Θ T Θ + W T W + (λ − µ) 2 }.<label>(7)</label></formula><p>B. Multi-Task Relationship Learning Probabilistic Framework</p><p>To automatically learn the relationships between aesthetic and semantic tasks and to better exploit the inter-task relatedness for aesthetic feature learning, we model the relationships between tasks as a covariance matrix Ω and add it to our above multi-task framework. The new framework is called Multi-Task Relationship Learning (MTRL) framework. In the MTRL framework, the goal is to find the optimal or suboptimal parameters Θ, W, λ, Ω by maximizing the following posterior probabilitŷ</p><formula xml:id="formula_8">Θ,Ŵ ,λ = argmax Θ,W,λ p(Θ, W, λ, Ω|X, Y, Z),<label>(8)</label></formula><p>Based on the Bayesian theorem, Eqn. (8) takes the form</p><formula xml:id="formula_9">Θ,Ŵ ,λ ∝ argmax Θ,W,λ p(Y |X, Θ, W a )p(Z|X, Θ, W s , λ)· p(W |Ω)p(Θ)p(W )p(λ).<label>(9)</label></formula><p>The conditional probability p(Y |X, Θ, W a ), the conditional probability p(Z|X, Θ, W s , λ), the prior probability p(Θ), the prior probability p(W ) and the prior probability p(λ) are same to the above definition in Section III-A. For the prior on the W , we consider two terms p(W ) and p(W |Ω). The prior probability p(W ) is to model the each column of W as a standard normal distribution for each task and can separately penalize the complexity of the each column of W . The p(W |Ω) is to model the structure of W between tasks by using a matrix-variate normal distribution <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b47">[48]</ref>. So we have</p><formula xml:id="formula_10">p(W |Ω) = M N (0, I ⊗ Ω) = exp(− 1 2 tr(I −1 W Ω −1 W T )) (2π) d(M +C)/2 |I| (M +C)/2 |Ω| d/2 ,<label>(10)</label></formula><p>where d is the dimension of the common representation of all the tasks, such as the dimension of layer 7 in <ref type="figure">Fig. 2</ref>. The new objective function can be</p><formula xml:id="formula_11">argmin Θ,W,λ {− N n=1 C c=1 1{y n = c}log exp(W c a T (Θ T x n )) C l=1 exp(W l a T (Θ T x n )) − λ N n=1 M m=1 (z m n logσ(W m s T (Θ T x n )) + (1 − z m n )(1− logσ(W m s T (Θ T x n )))) + Θ T Θ + W T W + (λ − µ) 2 + tr(W Ω −1 W T )}, s.t. Ω ≥ 0, tr(Ω) = 1.<label>(11)</label></formula><p>Where the constraint tr(Ω) = 1 is the same as in <ref type="bibr" target="#b44">[45]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization Procedure</head><p>The multi-task objective function in Eqn. <ref type="formula" target="#formula_7">(7)</ref> and <ref type="formula" target="#formula_0">(11)</ref> can be optimized by a network through stochastic gradient descent (SGD) <ref type="bibr" target="#b26">[27]</ref>. Here a specific CNN is applied to search optima for the parameters Θ, W, λ, Ω. One architecture of our MTCNNs is shown in <ref type="figure">Fig. 2</ref>. For the optimization procedure of MTCNNs, firstly, all tasks share knowledge in bottom layers. Then specific features are learned for each task in top layers. Finally, the combination of the softmax loss function for aesthetic quality prediction (the first term in Eqn. <ref type="bibr" target="#b6">(7)</ref>) and the cross entropy loss function for semantic recognition (the second term in Eqn. <ref type="bibr" target="#b6">(7)</ref>) are employed to update the parameters of the network jointly by back propagation. For the MTRLCNN, we adopt an alternate optimization procedure <ref type="bibr" target="#b44">[45]</ref> to minimize the objective function in Eqn. <ref type="bibr" target="#b10">(11)</ref> for the parameters Θ, W, Ω. Firstly, we update Θ, W by back propagation like the MTCNN with fixed Ω. Then fix Θ, W and optimize the Ω, Ω = (W T W ) 1/2 tr(W T W ) 1/2 ) . We repeat this procedure until convergence.</p><p>Traditionally, multiple tasks are treated equally important in back propagation of multi-task learning <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> assuming that they can reach best performance roughly at the same time. However, different tasks may have different learning difficulties and convergence rates. Caruana <ref type="bibr" target="#b34">[35]</ref> propose to control the effect of different tasks by adjusting the learning weight on each output task. He also put forward some strategies for this problem, such as early-stopping. Early stopping strategy has been used to some works <ref type="bibr" target="#b36">[37]</ref> and good performance is achieved. Nevertheless, this strategy is not suited to our problem. This is because the extra task (i.e., semantic recognition task) is much easier, and often converges more quickly than the main task (i.e., aesthetic quality assessment). Our experimental results (details in <ref type="table" target="#tab_3">Table I and Section IV)</ref> show that, if the convergent semantic recognition task is early stopped, the training loss of the aesthetic task will do not drop obviously and converge in a low rate. We think that it is mainly because the aesthetic is subjective and needs the help of semantic task in entire training process. Hence, we present a simple strategy to keep the effect of all tasks balanced in back propagation. Because the softmax loss function only considers the value corresponding ground truth label for each example. In our problem, λ = 1/M is fixed in the objective function in the entire training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Architectures Implementation Exploration</head><p>To implement the multi-task model, we investigate several multi-task network architectures to utilize semantic information for visual aesthetic quality assessment. Take the MTCNN as an example and adapt the networks to our problem, then apply suited network architecture to our MTRLCNN. These networks are explained in <ref type="figure" target="#fig_1">Fig. 3</ref>. The supervision of aesthetic and semantic labels can be in the same or different layers in the network. Here we propose and explore three basic network architectures and an enhanced network. For all networks, the input is a 227 × 227 × 3 patch randomly extracted from a resized image 256 × 256 × 3 as previous work <ref type="bibr" target="#b28">[29]</ref>.</p><p>MTCNN #1: Since our goal is to discover the effective features for aesthetic assessment with the help of semantic information, a simple idea is to learn all parameters for aesthetic representations with aesthetic and semantic supervision in a network until the last layers. MTCNN #1 implements this idea. The architecture of MTCNN #1 (in <ref type="figure" target="#fig_1">Fig. 3)</ref> is detailed in <ref type="figure">Fig. 2</ref>. The network contains four convolutional layers and two fully-connected layers with parameters Θ for common feature learning. Then the network is split into two branches, the two last layers for two specific tasks. Thus the parameters W = [W a , W s ] from layer 6 to layer 7 for each task are learned separately. Then, the softmax loss function is adopted for aesthetic quality prediction, and the cross entropy loss function for semantic recognition. The combination of the two loss functions is employed to jointly update the parameters of the network. MTCNN #2: To explore different structures for aesthetic features learning, we introduce MTCNN #2 (shown in <ref type="figure" target="#fig_1">Fig. 3</ref>) to allow some top layers to learn aesthetic representations independently without semantic supervision. Similar to MTCNN #1, the network #2 contains four convolutional layers with parameters Θ for common feature learning. Then the network is split into two branches earlier than MTCNN #1 for two specific tasks. Different from the architecture #1, layers 5, 6 and 7 in the network #2 learn parameters W = [W a , W s ] separately for the two tasks. The loss functions are also the same as the architecture #1.</p><p>MTCNN #3: Since CNNs can learn hierarchical features, we consider the low-level features of a network for our main task in the MTCNN #3 (shown in <ref type="figure" target="#fig_1">Fig. 3</ref>). In this network, four convolutional layers and three fully-connected layers are designed for semantic recognition, while two convolutional layers and two fully-connected layers for aesthetic quality assessment. The two tasks share knowledge Θ in the two convolutional layers. The other layers are used to learn specific parameters W = [W a , W s ] for each task. The loss functions are also the same as the architecture #1.</p><p>Enhanced MTCNN: To further explore the effective aesthetic features, we propose an enhanced MTCNN by combining MTCNN #1 and MTCNN #3. That is, we add extra aesthetic supervision in the first two layers in MTCNN #1. Shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the common parameters Θ 1 in the first and second convolutional layers are learned for three tasks, the common parameters Θ 2 in other two convolutional layers and two fully-connected layers are learned for two tasks, and specific parameters W = [W a , W a , W s ] are learned separately in top layers. Our goal is to enhance the supervision of aesthetic labels in the first and second convolutional layers under the premise of ensuring the influence of semantic information in all network. Here we denote Θ = [Θ 1 , Θ 2 ]. The objective function in Eqn. <ref type="bibr" target="#b6">(7)</ref> is transformed to</p><formula xml:id="formula_12">argmin Θ,W,λ {− N n=1 C c=1 1{y n = c}log exp(W c a T (Θ T x n )) C l=1 exp(W l a T (Θ T x n )) − N n=1 C c=1 1{y n = c}log exp(W c a T (Θ T 1 x n )) C l=1 exp(W l a T (Θ T 1 x n )) − λ N n=1 M m=1 (z m n logσ(W m s T (Θ T x n )) + (1 − z m n )(1− logσ(W m s T (Θ T x n )))) + Θ T Θ + W T W + (λ − µ) 2 },<label>(12)</label></formula><p>where the first term in Eqn. (12) is our main task, and the second term is the added task. We fix λ = 2/M based on our strategy for the enhanced MTCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Transfer learning with semantic information</head><p>Semantic content recognition has been studied for many years in computer vision, such as object recognition, object detection, image classification and semantic segmentation <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Recently, deep learning methods have achieved great succuss on the semantic recognition, especially the image classification on Imagenet <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. The Imagenet <ref type="bibr" target="#b52">[53]</ref> dataset contains rich semantic information and can be utilized to further help aesthetic representation learning. Thus we transfer the semantic representation learned from the network pretrained on Imagenet to aesthetic quality assessment. A trained model on a dataset can be transferred to another dataset for a similar or different task <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Specifically, our multi-task architecture from Layer 1 to Layer 6 in <ref type="figure">Fig. 2</ref> is replaced with AlexNet <ref type="bibr" target="#b26">[27]</ref>, VGG Net <ref type="bibr" target="#b50">[51]</ref> or ResNet <ref type="bibr" target="#b51">[52]</ref>. It is shown MTCNN #1 performs best in the three basic MTCNNs from <ref type="table" target="#tab_3">Table II</ref>. We initialize the networks with models pretrained on Imagenet and finetune it with the training data labeled with aesthetic labels and semantic labels.</p><p>In addition, another meaningful direction is how to exploit the massive dataset of visual semantic understanding for the limited dataset with only aesthetic labels for aesthetic assessment. To transfer the learned representation with both aesthetic and semantic supervision to the dataset with only aesthetic labels, we initialize the networks with pretrained multi-task models and finetune it with the training data labeled with only aesthetic labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate the proposed method on the challenging large-scale AVA dataset and Photo.net dataset. Experimental results show that the benefits of semantic information and the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>AVA dataset: The AVA dataset <ref type="bibr" target="#b24">[25]</ref> is one of the most large-scale and challenging dataset for visual aesthetic quality assessment. It contains more than 255,000 images gathered from www.dpchallenge.com. Each image has about 200 voters to assess the aesthetic score from one to ten. In addition, each image contains 0, 1 or 2 semantic tags (attributes). We select 185,751 images used in this paper based on the following rules. 1) More than 3000 images are available for each tag; 2) each image contains at least one tag. Eventually 29 semantic tags are chosen and the number of images for each tag is listed in <ref type="figure" target="#fig_2">Fig. 4</ref>. From the 185,751 images, 20,000 images are randomly selected as the testing set similar to <ref type="bibr" target="#b28">[29]</ref>, and the rest 165,751 images as the training set. For aesthetic labels, we follow the experimental setup as <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>, the training set is divided into two classes: high quality and low quality images. We designate the images with an average score larger than 5 + δ as high quality images, those with an average score smaller than 5 − δ as low quality images. Images with an average score between 5 + δ and 5 − δ are discarded. We set δ to 0 and 1 respectively for the training set to obtain the ground truth labels. There are 165,751 images in the training set when δ = 0 and 38,994 images in the training set when δ = 1. We set δ to 0 for the testing set regardless of the value of δ for the training set. For semantic labels, each image is labeled as a 29-dim binary vector. Photo.net dataset 1 : The Photo.net dataset <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>    with only aesthetic labels. It contains 20,278 images collected from www.photo.net. Each image is rated by at least 10 users to assess the aesthetic quality from one to seven. Due to some missing images in the dataset, we collect 17,232 images in all. From the overall images, 3000 images are randomly selected as the testing set, and the rest 15,232 images as the training set. For the ground truth labels, we follow <ref type="bibr" target="#b12">[13]</ref> and choose the average score 5.0 as median aesthetic ratings. The images with an average score larger than 5+δ are designated as high quality images, those with an average score smaller than 5 − δ as low quality images. We set δ to 0 in the experiment. Aesthetic quality assessment with δ = 0 is more challenging than that with δ &gt; 0 <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluating the Effectiveness of Keeping Balance Strategy</head><p>In the objective function, λ is used to control the contributions from semantic information. To validate our strategy of keeping the influence of two tasks balanced, we implement our MTCNN #1 with our strategy λ = 1/M (here λ = 1/29) and we also compare the experimental results of MTCNN #1 with λ = 0, λ = 2/29, λ = 1 and early stopping strategy (shown in <ref type="table" target="#tab_3">Table I</ref>). By comparing the results with or without the supervision of semantic labels, the MTCNN #1 with λ = 0 performs better than that with λ = 0. This indicates the supervision is effective. What's more, the results shown in <ref type="table" target="#tab_3">Table I</ref> demonstrate that our strategy λ = 1/29 performs best on both values of δ. When λ = 1/29, the aesthetic and semantic tasks have same effect on the process of back propagation. Therefore the effectiveness of our strategy is verified.</p><p>To further demonstrate the effectiveness of our MTCNN with our strategy, we also analyze the accuracy on each semantic tag using MTCNN #1 with different setting of λ in <ref type="figure" target="#fig_3">Fig. 5</ref>. As shown, our MTCNN #1 with λ = 1/29 performs best on overall images and most semantic tags. We also observe that different results are achieved on various semantic tags with the same method, and different improvements with MTCNNs are also different on various semantic tags. For example, the semantic tags "Family" and "Snapshot" obtain an great improvement with different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluating the impact of network architectures</head><p>To evaluate the impact of network architectures, we analyze the results with the three basic MTCNNs with λ = 1/M and enhanced MTCNN with λ = 2/M (shown in <ref type="table" target="#tab_3">Table II</ref>). We can see that our enhanced MTCNN for the main task performs best. For the enhanced MTCNN, under the premise of ensuring the effect of semantic information in the whole network, we enhance the aesthetic supervision in the two bottom layers. Experimental results also show that MTCNN #1 performs best in the three basic MTCNNs. Comparing the MTCNN #1 and MTCNN #2, we can see that late splitting obtains better performance for aesthetic quality assessment and semantic information is helpful for aesthetic representation learning. This also demonstrates that the more supervision semantic labels makes on the aesthetic feature learning, the better performance our MTCNN achieves. It also reveals that the low-level features of MTCNN #3 can still perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluating the Benefits of Semantic Information</head><p>To evaluate our MTCNNs with the help of semantic information for aesthetic classification, we compare our results of four MTCNNs with those of our single task CNN (STCNN,   <ref type="figure">Fig. 9</ref>. Correlation in any two subtasks of aesthetic quality classification and semantic recognition learned by MTRLCNN #1 with δ = 0.</p><p>MTCNN #1, λ = 0) on the AVA dataset with both values of δ. Shown in <ref type="table" target="#tab_3">Table II and Table IV)</ref>, all the four MTCNNs perform better than our STCNN especially when δ = 0. Aesthetic quality classification with δ = 0 is more challenging than that with δ = 1 <ref type="bibr" target="#b24">[25]</ref>. These results demonstrate the effectiveness of semantic information. Furthermore, we also train a separate model for each semantic labels to assess aesthetic quality. Due to different number of images for different semantic labels, we only train four CNNs separately for "Landscape", "Nature", "Still Life" and "Black and White". The four labels have the most number of images in 29 labels. Here we call the CNNs trained separately for the four semantic labels "respective CNN". For example, the respective CNN for "Landscape" is trained only with "Landscape" images for aesthetic categorization. <ref type="figure" target="#fig_4">Figure 6</ref> shows the results with different methods for aesthetic classification on "Landscape", "Nature", "Still Life" and "Black and White" separately with both value of δ. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, all the MTCNNs outperform the respective CNN on each semantic labels, which also demonstrates the effectiveness of semantic information for representation learning. Moreover, MTCNNs don't need to know the semantic labels of the testing images, while the respective CNNs have to know the semantic labels.</p><p>To qualitatively demonstrate the benefits of our MTCNN with semantic information, we show learned filters in the first convolutional layer with a STCNN for aesthetic task only and our MCTNN #1 with both δ = 0 and δ = 1 in <ref type="figure">Fig. 7</ref>. Compared to the filters learned without semantic information, the filters with semantic information are smoother, cleaner and more understandable. The proposed MTCNN can learn more color and high frequency edge information than STCNN. These differences can also be observed from the examples of test images correctly classified by MTCNN but misclassified by STCNN in <ref type="figure">Fig. 8</ref>. The high quality images often have more vivid color and clearer edge than low quality images. Most of the low quality images in <ref type="figure">Fig. 8</ref> are blurred and dull. This indicates that the supervision of semantic labels for aesthetic feature learning is very beneficial, and aesthetic and semantic tasks are related to some extent.</p><p>To exploit the semantic information in the Imagenet, we select the late splitting multi-task network (such as MTCNN #1) and replace the MTCNN #1 architecture from Layer 1 to Layer 6 in <ref type="figure">Fig. 2</ref> with AlexNet <ref type="bibr" target="#b26">[27]</ref>, VGG Net <ref type="bibr" target="#b50">[51]</ref> or ResNet <ref type="bibr" target="#b51">[52]</ref> respectively. That is because that the MTCNN #1 performs best in the three basic MTCNNs. The networks are initialized with models pretrained on Imagenet and finetuned with the training data labeled with aesthetic labels and semantic labels. <ref type="table" target="#tab_3">Table III</ref> shows the results of the three MTCNN networks (AlexNet FT, VGG Net FT and ResNet FT) with finetuning. It demonstrates the effectiveness of semantic information in Imagenet dataset. By comparing among three pre-trained networks, especially the ResNet <ref type="bibr" target="#b51">[52]</ref>, the deeper network learns more semantic representation and performs better for aesthetic quality assessment by transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Inter Tasks Correlation Analysis</head><p>To further demonstrate the effectiveness of semantic information and investigate how semantic information influence aesthetic task again, we analyze the correlation between the two tasks. Since each column vector of task-specific matrix W = [W a , W s ] in the network corresponds to the parameters of a subtask, we use the learned covariance matrix Ω and calculate the correlation coefficient between any two subtasks <ref type="bibr" target="#b55">[56]</ref>. Shown in layer 7 of <ref type="figure">Fig. 2</ref> in our problem, the aesthetic classification task has two subtasks: high aesthetic and low aesthetic, the semantic recognition task has 29 subtasks. <ref type="figure">Figure 9</ref> presents the correlation between the aesthetic subtasks and sematnic subtasks learned by MTRLCNN #1 with δ = 0, which also verifies that semantic information is beneficial for aesthetic estimation. Seen from <ref type="figure">Fig. 9</ref>, a low aesthetic task has high negative correlation with a high aesthetic task. We can also see that the aesthetic tasks have high correlation with certain semantic attributes. For instance, the semantic tags "Snapshot" and "Candid" recognition has high positive correlation with the low aesthetic task. In real word, most of "Snapshot" and "Candid" images are usually regarded as low aesthetic quality images. While "Advertisement" and "Seascapes" recognition has positive correlation with the high aesthetic task. This accords with the knowledge that most of "Seascapes" and "Advertisement" images are usually taken as high aesthetic quality images. In addition, <ref type="figure">Fig. 9</ref> can also visualize the correlation in different semantic tag recognitions. We also present the results of networks with or without relationship learning for aesthetic quality assessment in <ref type="table" target="#tab_3">Table III</ref>, which validates the task relationship learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison with Other State-of-the-art Methods</head><p>To further validate our method with semantic information for aesthetic classification, we compare our results with those of the state-of-the-art methods in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> on the AVA dataset. Shown in <ref type="table" target="#tab_3">Table II and Table IV</ref>, all the multi-task models perform better than the method in <ref type="bibr" target="#b24">[25]</ref>, SCNN <ref type="bibr" target="#b28">[29]</ref>, and RDCNN <ref type="bibr" target="#b28">[29]</ref> in on both values of δ. The method in <ref type="bibr" target="#b24">[25]</ref> is the baseline of the AVA dataset and is implemented by extracting fisher vector (FV) descriptors <ref type="bibr" target="#b56">[57]</ref> on the top of SIFT <ref type="bibr" target="#b15">[16]</ref> information and SVM classifier <ref type="bibr" target="#b57">[58]</ref>. SCNN is a single-column CNN, and RDCNN is a double-column CNN with an aesthetic column and a pretrained style column. Our results of MTRLCNN with VGG net and ResNet finetuning outperform the state-of-the-art method <ref type="bibr" target="#b31">[32]</ref>. Thus, these results in <ref type="table" target="#tab_3">Table II and Table IV</ref> illustrate the effectiveness of our method with semantic recognition task.</p><p>Since the name list of 20,000 testing images used in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> are unavailable, the 20,000 images for testing in this paper maybe potentially different from the 20,000 testing images in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Thus, we performed 4 times with similar operation (20,000 images are randomly selected for testing at each time) for MTCNN #1 (λ = 1/29, δ = 0). The mean and variance (76.25%, 0.0066) are close to our 76.15%, which shows the robustness of our method. In addition, in this paper we selects 185,751 training images according to some rules, including the rule that all images need to have at least one semantic tag. It seems that the our training set is more clean than the 230,000 training images in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> and maybe helpful. To clarify how much benefit our method training with a "clean" set, we implement the baseline model (STCNN) trained on the full training set of 230,000 images. The accuracies on the same test set are 72.20% (δ = 0), 75.27% (δ = 1) and close to 72.19% (δ = 0), 75.15% (δ = 1) with a "clean" set. It seems that training with a "clean" set does not help the current method. This also demonstrates that our multi-task models with smaller training data can still outperform the state-of-the-art methods. Although our goal is to improve the performance of aesthetic quality assessment without considering the evaluation of semantic task, we also give the 64.89% Average Precision of MTCNN#1 (λ = 1/29, δ = 0) and 67.44% of MTRLCNN with ResNet FT (λ = 1/29, δ = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Evaluating the Transfer Learning for Photo.net Dataset</head><p>To utilize the semantic information for the dataset with only aesthetic labels, we transfer the learned representation with both aesthetic labels and semantic labels for the dataset with only aesthetic labels. In this paper, we exploit the learned representation with aesthetic and semantic labels from AVA dataset in MTCNN #1 and finetune it with Photo.net dataset with only aesthetic labels. We call this model as MTCNN #1 FT. To validate the effectiveness of transferred representation with semantic information, we finetune the pretrained STCNN model on AVA dataset with only aesthetic labels for Photo.net dataset (STCNN FT). Moreover, we also train a STCNN on Photo.net dataset without finetuning. Furthermore, we implement the GIST descriptors <ref type="bibr" target="#b58">[59]</ref> and FV on the top of SIFT with a SVM classifier (GIST SVM and FV SIFT SVM). <ref type="table" target="#tab_8">Table V</ref> shows the accuracy of these methods on Photo.net dataset. <ref type="figure" target="#fig_0">Fig. 10</ref> visualizes some testing images correctly classified by MTCNN #1 FT but incorrectly by STCNN FT in the Photo.net dataset. These reveal the effectiveness of transfer learning with semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we have employed the semantic information to help discover representations for aesthetic quality assessment by formulating an end-to-end multi-task deep learning framework. Aesthetic quality assessment has not been taken as an isolation problem. To make full use of the semantic information and investigate how semantic information influence aesthetic task, four MTCNNs have been explored to learn the aesthetic representation jointly with the supervision of aesthetic and semantic labels. At the same time, a strategy of keeping the effect of two tasks balanced is presented to optimize the parameters of our multi-task networks. In addition, task relationship learning is modeled in the multitask framework and the correlations in the two tasks have been learned to investigate the role of semantic recognition in aesthetic quality assessment. Experimental results have shown that our method performs better than the state-of-theart methods. It is demonstrated that the semantic information is beneficial to aesthetic feature learning and the high-level features in the network play an important role in aesthetic quality assessment.</p><p>Although the proposed multi-task framework results in state-of-the-art results on the challenging dataset, how to perform aesthetic quality assessment like a human brain is still an ongoing issue. Future work is to explore other possible solutions to efficiently utilize the aesthetic and semantic information in a brain-like way. Another possible trend is to discover more possible and potential factors to affect aesthetic quality assessment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example images with their aesthetic and semantic labels on AVA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Explored MTCNNs with different architectures. The details of MTCNN #1 are illustrated in Fig. 2. Color code used: purple = convolutional layer + max pooling, grey = convolutional layer, yellow = fully-connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The number of images for each semantic tag on AVA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Accuracy on each semantic tag using MTCNN #1 with different λ when δ = 0 and δ = 1 on the AVA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>The accuracy with different methods for aesthetic classification on "Landscape", "Nature", "Still Life" and "Black and White" separately with both δ = 0 and δ = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>29 Fig. 7 .Fig. 8 .</head><label>2978</label><figDesc>(a) δ= 0, STCNN (MTCNN, λ=0) (b) δ= 0, MTCNN, λ=1/29(c) δ= 1, STCNN (MTCNN, λ=0) (d) δ= 1, MTCNN, λ=1/Learned filters in the first convolutional layer with STCNN for aesthetic task only and MCTNN #1 for the two tasks with both δ = 0 and δ = 1. Example test images correctly classified by MTCNN but incorrectly by STCNN in the AVA dataset. The labels of the images on the first and second rows are high aesthetic quality, and the labels of the images on the third and fourth rows are low aesthetic quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Example test images correctly classified by MTCNN #1 FT but incorrectly by STCNN FT in the Photo.net dataset. The labels of the images on the first and second rows are high aesthetic quality, and the labels of the images on the third and fourth rows are low aesthetic quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1604.04970v3 [cs.CV] 21 Oct 2016</figDesc><table><row><cell>Aesthetic:</cell><cell>High</cell><cell>High</cell><cell>Low</cell><cell>Low</cell></row><row><cell cols="2">Semantic: Portraiture</cell><cell>Sky, Architecture</cell><cell>Food and Drink</cell><cell>Still Life, Nature</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I</head><label>I</label><figDesc>ACCURACY (%) OF OUR MTCNN #1 WITH DIFFERENT λ ON THE AVA</figDesc><table><row><cell></cell><cell></cell><cell cols="2">DATASET.</cell><cell></cell></row><row><cell cols="5">δ λ = 0 λ = 1/29 λ = 2/29 λ = 1 with early stopping</cell></row><row><cell>0 72.19</cell><cell>76.15</cell><cell>75.76</cell><cell>73.54</cell><cell>73.43</cell></row><row><cell>1 75.13</cell><cell>75.90</cell><cell>75.82</cell><cell>73.12</cell><cell>74.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II ACCURACY</head><label>II</label><figDesc>(%) OF FOUR MTCNNS ON THE AVA DATASET.</figDesc><table><row><cell cols="5">δ MTCNN #1 MTCNN #2 MTCNN #3 Enhanced MTCNN</cell></row><row><cell>0</cell><cell>76.15</cell><cell>75.91</cell><cell>75.92</cell><cell>76.58</cell></row><row><cell>1</cell><cell>75.90</cell><cell>75.81</cell><cell>75.37</cell><cell>76.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III ACCURACY</head><label>III</label><figDesc>(%) OF DIFFERENT NETWORK WITH OR WITHOUT RELATIONSHIP LEARNING ON THE AVA DATASET.</figDesc><table><row><cell cols="5">Architecture MTCNN #1 AlexNet FT VGG Net FT ResNet FT</cell></row><row><cell>MTCNN</cell><cell>76.15</cell><cell>76.70</cell><cell>77.73</cell><cell>78.56</cell></row><row><cell>MTRLCNN</cell><cell>76.56</cell><cell>77.35</cell><cell>78.46</cell><cell>79.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV ACCURACY</head><label>IV</label><figDesc>(%) OF DIFFERENT METHODS ON THE AVA DATASET.</figDesc><table><row><cell>δ</cell><cell>Our STCNN</cell><cell>MTCNN #1</cell><cell>MTRLCNN AlexNet FT</cell><cell>MTRLCNN VGG Net FT</cell><cell>MTRLCNN ResNet FT</cell><cell cols="4">[25] SCNN [29] RDCNN [29] DMA-Net [31]</cell><cell>MNA-CNN [32] (VGG Net FT)</cell></row><row><cell cols="2">0 72.19</cell><cell>76.15</cell><cell>77.35</cell><cell>78.46</cell><cell>79.08</cell><cell>66.7</cell><cell>71.20</cell><cell>74.46</cell><cell>75.41</cell><cell>77.4</cell></row><row><cell cols="2">1 75.13</cell><cell>75.90</cell><cell>76.80</cell><cell>77.41</cell><cell>77.71</cell><cell>67.0</cell><cell>68.63</cell><cell>73.70</cell><cell>-</cell><cell>76.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V</head><label>V</label><figDesc>ACCURACY (%) OF DIFFERENT METHODS ON THE PHOTO.NET DATASET.</figDesc><table><row><cell cols="6">δ GIST SVM FV SIFT SVM STCNN STCNN FT MTCNN #1 FT</cell></row><row><cell>0</cell><cell>59.90</cell><cell>60.80</cell><cell>61.00</cell><cell>62.10</cell><cell>65.20</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at http://ritendra.weebly.com/aesthetics-datasets.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithmic inferencing of aesthetics and emotion in natural images: An exposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="105" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovering beautiful attributes for aesthetic image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="246" to="266" />
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A reliable methodology to collect ground truth data of image aesthetic appeal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Siahaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1338" to="1350" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pictures we like are our image: Continuous mapping of favorite pictures into selfassessed and attributed personality traits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Segalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Content-based prediction of movie style, aesthetics, and affect: Data set and baseline experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sjöberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oittinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2085" to="2098" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Consensus analysis and modeling of visual aesthetic perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="272" to="285" />
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning the consensus on visual quality for next-generation image management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="533" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unified photo enhancement by discovering aesthetic communities from flickr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1124" to="1135" />
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Actively learning human gaze shifting paths for semantics-aware photo cropping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2235" to="2245" />
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Photo and video quality evaluation: Focusing on the subject</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="386" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Assessing the aesthetic quality of photographs using generic image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1784" to="1791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video aesthetic quality assessment by temporal integration of photo-and motion-based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beauty is here: Evaluating aesthetics in videos using multimodal features and free training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, 2013</title>
		<meeting>ACM Int. Conf. Multimedia, 2013</meeting>
		<imprint>
			<biblScope unit="page" from="369" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fusion of multichannel local and global structural cues for photo aesthetics evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1419" to="1429" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to predict the perceived visual quality of photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perception-guided multimodal feature fusion for photo aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aesthetic quality classification of photographs based on color harmony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on statistical learning in computer vision, ECCV</title>
		<meeting>Workshop on statistical learning in computer vision, ECCV</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="487" to="493" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ava: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning beautiful (and ugly) attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rapid: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual aesthetic quality assessment with a regression model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1583" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep multi-patch aggregation network for image style, aesthetics, and quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Composition-preserving deep photo aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">There is beauty in gist: An investigation of aesthetic perception in rapidly presented scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hayn-Leichsenring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wagemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="123" to="123" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The aesthetic experience with visual art at first glance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Locher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Investigations Into the Phenomenology and the Ontology of the Work of Art</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="75" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rotating your face using multi-task deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="676" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2206" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">What makes a professional video? a computational aesthetics approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1037" to="1049" />
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-task deep visualsemantic embedding for video thumbnail selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3707" to="3715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-task cnn model for attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Abdulnabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1949" to="1959" />
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-task convnet for blind face inpainting with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Biometrics</title>
		<meeting>International Conference on Biometrics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep model based transfer and multi-task learning for biological image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1475" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A convex formulation for learning task relationships in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Uncertain. Artif. Intell</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Online learning of multiple tasks and their relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="643" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-task gaussian process prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Nagar</surname></persName>
		</author>
		<title level="m">Matrix variate distributions</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sas for monte carlo studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felsovalyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Sivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Keenan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAS Institute</title>
		<imprint>
			<biblScope unit="page" from="87" to="89" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Libsvm: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
