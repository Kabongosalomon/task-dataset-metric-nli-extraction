<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoder Modulation for Indoor Depth Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senushkin</forename><surname>Dmitry</surname></persName>
							<email>d.senushkin@partner.samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romanov</forename><surname>Mikhail</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belikov</forename><surname>Ilia</surname></persName>
							<email>ilia.belikov@samsung.com</email>
							<affiliation key="aff2">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konushin</forename><surname>Anton</surname></persName>
							<email>a.konushin@samsung.com</email>
							<affiliation key="aff3">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patakin</forename><surname>Nikolay</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Samsung AI Center</orgName>
								<address>
									<settlement>Moscow</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Decoder Modulation for Indoor Depth Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth completion recovers a dense depth map from sensor measurements. Current methods are mostly tailored for very sparse depth measurements from LiDARs in outdoor settings, while for indoor scenes Time-of-Flight (ToF) or structured light sensors are mostly used. These sensors provide semi-dense maps, with dense measurements in some regions and almost empty in others. We propose a new model that takes into account the statistical difference between such regions. Our main contribution is a new decoder modulation branch added to the encoder-decoder architecture. The encoder extracts features from the concatenated RGB image and raw depth. Given the mask of missing values as input, the proposed modulation branch controls the decoding of a dense depth map from these features differently for different regions. This is implemented by modifying the spatial distribution of output signals inside the decoder via Spatially-Adaptive Denormalization (SPADE) blocks. Our second contribution is a novel training strategy that allows us to train on a semi-dense sensor data when the ground truth depth map is not available. Our model achieves the state of the art results on indoor Matterport3D dataset <ref type="bibr" target="#b3">[4]</ref>. Being designed for semi-dense input depth, our model is still competitive with LiDAR-oriented approaches on the KITTI dataset <ref type="bibr" target="#b42">[42]</ref>. Our training strategy significantly improves prediction quality with no dense ground truth available, as validated on the NYUv2 dataset <ref type="bibr" target="#b29">[29]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, depth sensors have become an essential component of many devices, from self-driving cars to smartphones. However, the quality of modern depth sensors is still far from perfect. LiDAR systems provide accurate but spatially sparse measurements while being quite expensive and large. Commodity-grade depth sensors based on the active stereo with structured light (e.g., Microsoft Kinect) or Time-of-Flight (e.g., Microsoft Kinect Azure or depth sensors in many smartphones) provide estimations that are relatively dense but less accurate and within a limited distance range. LiDAR-based sensors are widely used in outdoor environments, especially for self-driving cars, while the other sensors are mainly applicable in an indoor setting. Due to the rapid growth of the self-driving car industry, the majority of recent depth completion methods are mostly focused on outdoor depth completion for LiDAR data <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b7">8]</ref>, often overlooking other types of sensors and scenarios. Nevertheless, these sensors are an essential part of many modern devices (such as mobile phones, AR glasses, and others).</p><p>LiDAR-oriented methods mainly deal with sparse measurements. Applying these methods to depth data captured with semi-dense sensors as-is may be a suboptimal strategy. This kind of transfer requires additional heuristics such as sparse random sampling. The most popular approach <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">28]</ref> of training LiDAR-oriented methods on a semi-dense depth map proceeds as follows. First, the gaps in semi-dense depth maps are filled using simple interpolation methods such as bilateral filtering or the approach of <ref type="bibr" target="#b20">[20]</ref>. Then, some depth points are uniformly sampled from the resulting map. This heuristic approach is used due to the lack of LiDAR data for indoor environments, but such kind of preprocessing suggests that it may be better to use a model originally designed to operate with semi-dense data. Such an approach would take into account the features of semi-dense sensor data and would not require separate heuristics for transfer.</p><p>Inspired by these observations, we present a novel solution for the indoor depth completion from semi-dense depth maps guided by color images. Since sensor data may be present for 60% of pixels and more, we propose to use a single encoder for the joint RGBD signal. Taking into account the statistical differences between regions with and without depth measurements, we design a decoder modulation branch that takes a mask as input and modifies the distributions of activation maps in the decoder. This modulation mechanism is based on Spatially-Adaptive Denormalization (SPADE) blocks <ref type="bibr" target="#b32">[32]</ref>. Since there are few publicly available datasets with both sensor and dense ground truth depth, we additionally propose a special training strategy for depth completion models that emulates semi-dense sensors and does not require dense depth reconstruction.</p><p>As a result, we offer the following contributions:</p><p>• a novel network architecture for indoor depth completion with a decoder modulation branch;</p><p>• a novel training strategy that emulates semi-dense sensors and does not require dense depth reconstruction;</p><p>• large-scale experimental validation on real datasets including Matterport3D, ScanNet, NYUv2, and KITTI.</p><p>The paper is organized as follows. In Section 2, we review related work on depth estimation and dense image labeling. Section 3 presents our approach, including the new architecture and training strategies. Section 4 describes the experimental setup, Section 5 presents the results of our experiments, and Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we review works on several topics related to depth processing for images or works that have served as the original inspiration for our work. Namely, we cover depth estimation, depth completion, and semantic segmentation, a well-studied case of dense image labeling.</p><p>Depth Estimation. Methods for single view depth estimation based on deep neural networks have significantly evolved in recent years, by now rapidly approaching the accuracy of depth sensors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27]</ref>; some of these methods are able to run in real-time <ref type="bibr" target="#b46">[46]</ref> or even on embedded platforms <ref type="bibr" target="#b0">[1]</ref>. However, the acquisition of accurate ground truth depth maps is complicated due to certain limitations of existing depth sensors. To overcome these difficulties, various approaches focusing on data acquisition, data refinement, and the use of additional alternative data sources have been proposed <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b18">18]</ref>. We also note several recently developed weakly supervised and unsupervised approaches <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>Depth Completion. Pioneering works on depth completion adopted complicated heuristic algorithms for processing raw sensor data. These algorithms were based on compressed sensing <ref type="bibr" target="#b15">[15]</ref> or used a combined wavelet-contourlet dictionary <ref type="bibr" target="#b26">[26]</ref>. Uhrig et al. <ref type="bibr" target="#b42">[42]</ref> were the first to present a successful learnable depth completion method based on convolutional neural networks, developing special sparsityinvariant convolutions to handle sparse inputs. Learnable methods were further improved by image guidance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b39">39]</ref>. Tang et al. <ref type="bibr" target="#b41">[41]</ref> proposed an approach to train content-dependent and spatially-variant kernels for sparse depth features processing. Li et al. <ref type="bibr" target="#b22">[22]</ref> suggested a multiscale guided cascade hourglass architecture for depth completion. Chen et al. <ref type="bibr" target="#b6">[7]</ref> presented a 2D-3D fusion pipeline based on continuous convolutions. Apart from utilizing images, some recently proposed methods make use of surface normals <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50]</ref> and object boundaries <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b50">50]</ref>.</p><p>Most of the above-mentioned works focus on LiDARbased sparse depth completion in outdoor scenarios and report results on the well-known KITTI benchmark <ref type="bibr" target="#b42">[42]</ref>. There are only a few works that consider processing non-LiDAR semi-dense depth data obtained with Kinect sensors. Recently, Zhang et al. <ref type="bibr" target="#b50">[50]</ref> introduced Matterport3D, a large-scale RGBD dataset for indoor depth completion, and used it to showcase a custom depth completion method. This method implicitly exploits extra data by using pretrained networks for normal estimation and boundary detection, and the resulting normals and boundaries are used in global optimization. Overall, the complexity of this method strictly limits its practical usage. Huang et al. <ref type="bibr" target="#b16">[16]</ref> was the first to outperform the original results on this dataset. Similar to Zhang et al. <ref type="bibr" target="#b50">[50]</ref>, their results were achieved via a complicated multi-stage method that involved resourceintensive preprocessing. Although it does not rely on pretrained backbones, it uses a normal estimation network explicitly trained on external data. In this work, we propose a novel depth completion method that presents strong baseline results while being scalable and straightforward.</p><p>Semantic segmentation and dense labeling. Since depth completion or depth estimation can be formulated as a dense labeling problem, techniques and architectures that have proven to be effective for other dense labeling tasks might  blocks <ref type="bibr" target="#b31">[31]</ref>. The decoder modulation branch modifies the spatial distribution of output signals inside the decoder via SPADE blocks <ref type="bibr" target="#b32">[32]</ref>.</p><p>be useful for depth completion as well. Encoder-decoder architectures with skip connections originally developed for semantic segmentation <ref type="bibr" target="#b38">[38]</ref> have shown themselves to be capable of solving a wide range of tasks. Chen et al. <ref type="bibr" target="#b4">[5]</ref> proposed a powerful architecture based on atrous spatial pyramid pooling for semantic segmentation and improved it in further work <ref type="bibr" target="#b5">[6]</ref>. Other important approaches include the refinement network <ref type="bibr" target="#b25">[25]</ref> and the pyramid scene parsing network <ref type="bibr" target="#b51">[51]</ref>. At the same time, lightweight networks such as <ref type="bibr" target="#b30">[30]</ref> capable of running in a resource-constrained device in real-time can be of use in other deep learning-driven applications. Our depth completion network is based on the blocks proposed in <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b31">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach and methods</head><p>Architecture overview. The general structure of the proposed architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Our architecture design follows the standard encoder-decoder paradigm with a pretrained backbone network modified for 4D input. In our experiments, we use the EfficientNet family <ref type="bibr" target="#b40">[40]</ref> as a backbone. The decoder part is based on a lightweight RefineNet decoder <ref type="bibr" target="#b31">[31]</ref> combined with a custom modulation branch described below. The network takes an image, sensor depth, and a mask as input and outputs a completed depth map. No additional data is required.</p><p>Decoder Modulation Branch. To introduce the decoder modulation branch, let us take a closer look at the forward propagation path of the network. The backbone network generates feature maps from the input RGBD signal. The input signal initially has an inhomogeneous spatial distribution, since a part of the depth data is missing. The signal compression inside a backbone smoothes this inhomogeneity, which works well for small depth gaps. If the depth gaps are too large, the convolutions generate incorrect activations due to the domain shift between RGB and RGBD signals. Aiming to reduce this domain gap, we propose to learn spatially-dependent scale and bias for normalized feature maps inside the decoder part of the architecture. This procedure is called spatially-adaptive denormalization (SPADE) and was first introduced by Park et al. <ref type="bibr" target="#b32">[32]</ref>. Let f i n,c,y,x denote the activation maps of the ith layer of the decoder for a batch of N samples with shape</p><formula xml:id="formula_0">C i × H i × W i , and let m denote a modulation signal. The output value from SPADE g i n,c,y,x at location (n ∈ N, c ∈ C i , y ∈ H i , x ∈ W i ) is g i n,c,y,x = γ i n,c,y,x (m) f i n,c,y,x − µ i c σ i c + β i n,c,y,x (m), where µ i c = 1 NiWiHi n,x,y f i n,c,y,x is the sample mean and σ i c = 1 NiWiHi n,x,y (f i n,c,y,x − µ i c ) 2</formula><p>is the sample (biased) standard deviation, and γ i n,c,y,x and β i n,c,y,x are the spatially dependent scale and bias for batch normalization respectively. In our case, the modulation signal m is the input mask of missing depth values. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the decoder modulation branch in detail. This subnetwork consists of a simple mask encoder composed of convolutions with bias terms and activations and SPADE blocks that perform modulation. A bias term in the convolutions is necessary to avoid zero signals that can cover a significant part of the input mask.</p><p>Training strategy. Existing highly annotated large-scale indoor datasets do not always include both sensor depth data and ground truth depth data <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b29">29]</ref>, which might be an issue for the development of depth completion models. If the sensor or reconstructed depth is not available, we propose to use specially developed corruption techniques in order to obtain synthetic semi-dense sensor data.</p><p>Let t ∈ T be a target sample that we want to degrade. Our goal is to construct a function h : T → S that transforms a depth map from the target domain T to pseudosensor domain S. We assume that this procedure is samplespecific and can be factorized:</p><formula xml:id="formula_1">h(·) = z g (·|q) • z n (·) = z n (z g (·|q)),</formula><p>where q is the input RGB image. The term z g emulates a zero masking process guided by the image and z n is the zero masking caused by noise. The noise term z n represents a random spattering procedure uniformly distributed  over the entire image. The specific form of z g may vary. <ref type="figure">Fig. 4</ref> presents some possible approaches results. As shown in <ref type="figure">Figs. 4c and 4b</ref>, the most suitable variant for semi-dense depth sensor simulation appears to be the graph-based segmentation algorithm introduced by Felzenszwalb and Huttenlocher <ref type="bibr" target="#b12">[12]</ref>, thresholded by segment area. After obtaining pseudo-sensor data, we can perform a standard training procedure on it. Our corruption strategy <ref type="figure">(Fig. 4c</ref>) based on image segmentation significantly differs from widely-used sparse uniform sampling <ref type="figure">(Fig. 4f</ref>). Below we compare these two strategies numerically on the NYUv2 dataset <ref type="bibr" target="#b29">[29]</ref> using our model and additional open-source approaches from the KITTI dataset leaderboard <ref type="bibr" target="#b42">[42]</ref>.</p><p>Loss function. Recent works underline two primary families of losses that are conceptually different: pixel-wise and pairwise. Pixel-wise loss functions measure the mean per-pixel distance between prediction and target, while their pairwise counterparts express the error by comparing the relationships between pairs of pixels i, j in the output. The latter loss functions force the relationship between each pair of pixels in the prediction to be similar to that of the corresponding pair in the ground truth. In this work, we have experimented with several different single-term loss functions, including pair-wise and pixel-wise approaches in a logarithmic and actual domain (see supplementary mate-(a) RGB (b) Initial real sensor (c) Graph-based <ref type="bibr" target="#b12">[12]</ref> (d) Quickshift <ref type="bibr" target="#b45">[45]</ref> (e) Slic <ref type="bibr" target="#b1">[2]</ref> (f) Uniform <ref type="bibr" target="#b28">[28]</ref> Figure 4: Qualitative comparison of different sampling strategies based on classical image segmentation methods applied to a NYUv2 <ref type="bibr" target="#b29">[29]</ref> sample from the raw subset: 4b and 4a -original image and depth map respectively; 4cgraph-based segmentation <ref type="bibr" target="#b12">[12]</ref>, 4d -quickshift segmentation (Vedaldi et al. <ref type="bibr" target="#b45">[45]</ref>), 4e -SLIC segmentation (Achanta et al. <ref type="bibr" target="#b1">[2]</ref>). All methods produce an image partition, then we replace depth data with zeros in segments with area below a predefined threshold value. Graph-based segmentation demonstrates the best match to the original sensor map, producing similar artifacts (e.g. diffusion on the border of the table). rials for details). The logarithmic L 1 pair-wise loss function <ref type="bibr" target="#b37">[37]</ref> appears to be the most suitable for our network. It can be expressed as</p><formula xml:id="formula_2">L(y i , y * i ) = 1 |O| 2 i,j∈O log y i y j − log y * i y * j ,</formula><p>where O is the set of pixels where the ground truth depth is non-zero, i, j are pixel indices, y i , y * i are the predicted and target depth respectively. Following Eigen et al. <ref type="bibr" target="#b10">[11]</ref>, our model predicts log y i directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>Datasets. We perform comparative experiments on the following datasets: Matterport3D <ref type="bibr" target="#b3">[4]</ref>, ScanNet <ref type="bibr" target="#b9">[10]</ref>, NYUv2 <ref type="bibr" target="#b29">[29]</ref> and KITTI <ref type="bibr" target="#b42">[42]</ref>. Matterport3D includes real sensor data and ground truth depth data obtained from official reconstructed meshes. We use it as the primary target dataset. In order to investigate the generalization capabilities of the model, we perform validation of the models trained on the Matterport3D dataset directly on ScanNet. NYUv2 does not provide dense depth reconstruction for the entire dataset, so we evaluate our training strategy on this dataset. Although our approach is not intended to be applied to sparse depth sensors, we compare it with the best performing models on the KITTI dataset.   iRMSE and iMAE metrics on the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>In our experiments, we use the Adam <ref type="bibr" target="#b17">[17]</ref> optimizer with initial learning rate set to 10 −4 , β 1 = 0.9, β 2 = 0.999 and without weight decay. The pretrained EfficientNet-b4 <ref type="bibr" target="#b40">[40]</ref> backbone is used unless otherwise stated. Batch normalization is controlled by the modulation process, so we fine-tune its parameters during the first epoch only, and afterwards these parameters are fixed. The training process is performed end-to-end for 100 epochs on a single Nvidia Tesla P40 GPU. We implement all models in Python 3.7 using the PyTorch library <ref type="bibr" target="#b33">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Matterport3D. We begin by inferencing our indoor pipeline on the Matterport3D dataset. Since very few previous approaches have been tested and achieved good results on this dataset, we train some of the best performing open-source KITTI models <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b44">44]</ref> for a fair comparison.</p><p>Assuming that the original training pipeline of these models might be designed specifically for LiDAR data, we also perform a complementary training procedure in our training setup.</p><p>The results of this quantitative comparison are presented in <ref type="table" target="#tab_2">Table 1</ref>. Our training pipeline applied to KITTI models improves the results in terms of δ i , especially with smaller values of i, but leads to artifacts captured by RMSE values. The original training setup of these methods also does not show state of the art performance on Matterport3D (see <ref type="table" target="#tab_2">Table 1</ref>). We use the original training procedure for further experiments. These methods do not produce sharp edges (see <ref type="figure" target="#fig_3">Fig. 5</ref>) that are crucial for indoor applications. Zhang et al. <ref type="bibr" target="#b50">[50]</ref> and Huang et al. <ref type="bibr" target="#b16">[16]</ref> managed to address this problem and received less blurry results. Our model produces improved completed depth while being more accurate in terms of both RMSE and MAE. In <ref type="table" target="#tab_2">Table 1</ref>, we also present ablation experiments including different masking strategies.</p><p>A visual comparison is shown in <ref type="figure" target="#fig_3">Figure 5</ref>  <ref type="table">Table 3</ref>: NYUv2 TEST. Quantitative comparison of training setups for different models. Semi-dense sampling preserves more accurate information that leads to better results. Although our approach is not intended to be applied to sparse depth sensors, it demonstrates strong results in the sparse training setting in indoor environments. We do not use any densification scheme for target depth reconstruction. Pseudo-sensor data is directly sampled from real sensor data.</p><p>keeps the sensor data almost unchanged and sharp. Moreover, the geometric shapes of the interior layout and objects in the scene remain distinct.</p><p>ScanNet. In order to evaluate the generalization capability of our method, we conduct a cross-dataset evaluation.</p><p>Since the test split was not provided for depth completion on ScanNet, we use 20% of the scenes for testing. For the sake of data diversity, we split all frames into intervals of consecutive 20 frames and take some samples out of each interval. We take the image with the largest variance of Laplacian <ref type="bibr" target="#b34">[34]</ref> and the image with the largest file size (which indicates the level of details for a frame). We test the models trained on Matterport3D <ref type="bibr" target="#b3">[4]</ref> on this subset that was not seen by the models during the training process.</p><p>Quantitative results are presented in <ref type="table" target="#tab_3">Table 2</ref>, and a qualitative evaluation is shown in <ref type="figure">Fig. 6</ref>. Our method provides sharp depth maps and significantly improves δ 1.05 , δ 1.10 , SSIM, and MAE metrics.</p><p>NYUv2. Since this dataset provides both sensor and reconstruction depth data only for the test subset, we use it to verify our training strategy that does not require ground truth. We first cut off black borders <ref type="bibr" target="#b45">(45,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b40">40</ref>  Li et al. <ref type="bibr" target="#b21">[21]</ref> Gansbeke et al.</p><p>[44]</p><p>Ours <ref type="figure">Figure 8</ref>: Qualitative comparison with the state-of-art methods on the KITTI test set. Even though our model was designed for a different use case scenario, it is still comparable to the best performing KITTI models in an outdoor environment.</p><p>strategy with the widely used random uniform sampling approach <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b41">41]</ref>. Qualitative and quantitative results are presented in <ref type="figure">Fig. 7</ref> and <ref type="table">Table 3</ref>. Since the original semi-dense depth maps contain more accurate information, our training approach demonstrates significant improvements in all target metrics. The compared performance of models originally designed for sparse inputs is shown in <ref type="table">Table 3</ref>. Our model demonstrates strong results in this setup as well.</p><p>KITTI. In general, this dataset is out of our scope, since it consists of sparse LiDAR depth measurements. It is a hard case for our model, because the architecture includes a unified encoder for the joint RGBD signal, expecting segments filled with correct depth values. Previous work <ref type="bibr" target="#b21">[21]</ref> has demonstrated that it is a suboptimal design for a sparse depth completion model. Since LiDAR-based outdoor depth completion differs from our use-case scenario, we perform an additional search for the most suitable loss function. As a result, we have chosen the L 2 loss in the logarithmic domain (see Supplementary material for more details). As the LiDAR points at the top of an image are rare, input images were cropped to 256 × 1216 for both training and testing, following <ref type="bibr" target="#b41">[41]</ref>. A horizontal flip was used as data augmentation.</p><p>A quantitative comparison is shown in <ref type="table" target="#tab_7">Table 4</ref>. Being designed for semi-dense sensors, our approach demonstrates mid-level performance compared to the KITTI leaderboard. In general, our model produces accurate depth maps, even though there are some errors at the borders of the image.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RMSE MAE iRMSE iMAE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have proposed a new depth completion method for semi-dense depth sensor maps with auxiliary color images. Our main innovation is a novel decoder architecture that exploits statistical differences between mostly filled and mostly empty regions. It is implemented by an additional decoder modulation branch that takes a mask of missing values as input and adjusts the activation mask distribution in the decoder via SPADE blocks.</p><p>In experimental evaluation, our model has shown stateof-the-art results on the Matterport3D dataset with generalization to ScanNet, and even competitive performance on the KITTI dataset with sparse depth measurements. We have also proposed a new training strategy for datasets with raw sensor data and without reconstructed ground truth depth, which allows us to achieve strong results on the NYUv2 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Point clouds are reconstructed from depth maps predicted by our model (top row) and ground truth (bottom row) taken from the Matterport3D [4] test subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>High-level architecture of the proposed DM-LRN network. Pretrained EfficientNet [40] backbone encodes the input RGBD signal. Extracted features are fed into the lightweight RefineNet decoder [31] consisting of chained residual pooling (CRP) blocks and fusion (FUSE)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of the decoder modulation branch (3a). It consists of a simple encoder composed of two biased convolutions with activations and a series of SPADE blocks (3c). These blocks include the SPADE layer (3b) that performs modulation. We use LeakyReLU activations, as the modulation should be able to decrease the scale of a signal and move it in the negative direction as well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>RGB Sensor GT Gansbeke et al. Li et al. Huang et al. Ours Qualitative comparison with Gansbeke et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Matterport3D TEST. We use the results for Huang et al.<ref type="bibr" target="#b16">[16]</ref> and Zhang et al.<ref type="bibr" target="#b50">[50]</ref> reported in<ref type="bibr" target="#b16">[16]</ref>. Gansbeke et al.<ref type="bibr" target="#b44">[44]</ref> and Li et al.<ref type="bibr" target="#b21">[21]</ref> are trained on Matterport3D using their official implementations. Models labeled as "ours" are trained using our proposed pipeline. The two bottom rows represent models without the decoder modulation branch, with and without the mask on the input. RMSE and MAE are measured in meters.Evaluation metrics Following the standard evaluation protocol for indoor depth completion, we use root mean squared error (RMSE), mean absolute error (MAE), δ i , and SSIM. The δ i metric denotes the percentage of predicted pixels where the relative error is less than a threshold i. Specifically, we evaluate δ i for i equal to 1.05, 1.10, 1.25, 1.25 2 , and 1.25<ref type="bibr" target="#b2">3</ref> ; smaller values of i correspond to making the δ i metric more sensitive, while larger values reflect a more accurate prediction. RMSE and MAE directly mea-sure absolute depth accuracy. RMSE is more sensitive to outliers than MAE and is usually chosen as the main metric for ranking models. In general, our testing pipeline for indoor depth completion is similar to Huang et al.<ref type="bibr" target="#b16">[16]</ref>.<ref type="bibr" target="#b0">1</ref> . Following the KITTI leaderboard, we evaluate RMSE, MAE,</figDesc><table><row><cell>very sharp</cell></row></table><note>[44], Li et al. [21], Huang et al. [16] on Matterport3D test set. We train [44] and [21] on Matterport3D using the official code of the corresponding approaches, and results for [16] are based on the official pretrained model. Rows 2 and 4 represent zoomed-in fragments from rows 1 and 3, respectively. All images are created using color maps with the same value limits. Our model generates the completed depth map withOurs Figure 6: Qualitative comparison with Gansbeke et al. [44], Li et al. [21], Huang et al. [16] on ScanNet [10]. All models are trained on Matterport3D. The images are received using a unified color map.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>ScanNet TEST. Cross-dataset testing demonstrates the strong generalization capability of our method. All models are trained on Matterport3D. RMSE and MAE are measured in meters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Qualitative comparison with Gansbeke et al. [44], Li et al. [21], Huang et al. [16] on NYUv2<ref type="bibr" target="#b29">[29]</ref> test set. All models are trained using our semi-dense sampling strategy. The third and fourth raws present a hard example. rel ↓ δ 1.25 ↑ δ 1.25 2 ↑ δ 1.25 3 ↑ RMSE ↓ rel ↓ δ 1.25 ↑ δ 1.25 2 ↑ δ 1.25 3 ↑</figDesc><table><row><cell>RGB</cell><cell>Sensor</cell><cell>GT</cell><cell></cell><cell cols="2">Gansbeke et al.</cell><cell>Li et al. [21]</cell><cell cols="3">Huang et al. [16]</cell><cell>ours</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[44]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Figure 7: semi-dense</cell><cell></cell><cell></cell><cell></cell><cell cols="3">sparse (500 points)</cell><cell></cell></row><row><cell cols="2">RMSE ↓ Huang et al. [16] 0.271</cell><cell>0.016</cell><cell>98.1</cell><cell>99.1</cell><cell>99.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Gansbeke et al. [44]</cell><cell>0.260</cell><cell>0.017</cell><cell>97.9</cell><cell>99.3</cell><cell>99.7</cell><cell>0.344</cell><cell>0.042</cell><cell>96.1</cell><cell>98.5</cell><cell>99.5</cell></row><row><cell>Li et al. [21]</cell><cell>0.190</cell><cell>0.018</cell><cell>98.8</cell><cell>99.7</cell><cell>99.9</cell><cell>0.272</cell><cell>0.034</cell><cell>97.3</cell><cell>99.2</cell><cell>99.7</cell></row><row><cell>DM-LRN (ours)</cell><cell>0.205</cell><cell>0.014</cell><cell>98.8</cell><cell>99.6</cell><cell>99.9</cell><cell>0.263</cell><cell>0.035</cell><cell>97.5</cell><cell>99.3</cell><cell>99.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>. Our model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>pixels from the top, bottom, left, and right side, respectively) from the original 640 × 480 RGBD images. Then the images are bilinearly interpolated to 320 × 256 resolution. These preprocessed RGBD images are used for pseudo sensor data sampling. At test time, the original sensor and ground truth depth data are used. We compare our sampling</figDesc><table><row><cell cols="2">RGB</cell></row><row><cell>Tang et al.</cell><cell>[41]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>KITTI TEST. Quantitative comparison with top ranked KITTI models. All metrics are measured in millimeters.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The evaluation code is available on the official page https://github.com/patrickwu2/Depth-Completion. To keep a fair comparison, we opt for an evaluation procedure based on the official code.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A: Loss function ablation study.</p><p>Firstly, we investigate how the choice of the loss function affects the performance in different use cases. We search for an appropriate loss function among the popular single-term loss functions that include L 1 and L 2 penalties in depth and log-depth domains and their pairwise variations. L 1 loss family appears to be more efficient for indoor semi-dense depth completion. These functions provide a balance between RMSE and MAE and improve accurate δ-metrics. In other words, they produce clearer edges and boundaries. The pairwise log-L 1 appears to be the most suitable for Matterport3D. More details can be found in <ref type="table">Table 5</ref>.</p><p>The same search procedure performed on KITTI validation set reveals an advantage of L 2 -family. A good balance was achieved by using these penalties. Even though we choose the log-L 2 as the main penalty for outdoor LiDARoriented depth completion, it can be switched by its pairwise counterpart. A quantitative comparison is shown in <ref type="table">Table 6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Backbone Depth.</head><p>Backbone scalability is an advantage of our approach. In order to investigate the behavior of the model, we carried out additional experiments in which we tried Efficient-Nets of different sizes. The results are shown in <ref type="figure">Figure 9</ref>. The general trend is predictable: as the size of the network grows, the validation error drops. This is true for the model with and without modulation. The mask modulation consistently gives an improvement in the target metric with the  exception "B3" configuration that demonstrated an unexpected behavior, assumed to be a random outlier. In order to comply with practical applications, we did not try the configurations larger than "B4".  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://www.ambarella.com/technology/technology-overview.Accessed" />
	</analytic>
	<monogr>
		<title level="j">Ambarella cvflow technology overview</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06152</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Matterport3d: Learning from rgbd data in indoor environments. International Conference on 3D Vision (3DV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6602" to="6611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dense disparity maps from sparse disparity measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleinsteuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2126" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Indoor depth completion with boundary consistency and self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep architecture with cross guidance between single image and sparse lidar data for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="79801" to="79810" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="689" to="694" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multi-scale guided cascade hourglass network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggen</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchao</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multi-scale guided cascade hourglass network for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggen</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchao</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<idno>abs/1804.00607</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning for disparity estimation through feature constancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lqlzj</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2811" to="2820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth reconstruction from sparse samples: Representation, algorithm, and sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1983" to="1996" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient deep learning for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5695" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sparse-to-dense: Depth prediction from sparse depth samples and a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Real-time joint semantic segmentation and depth estimation using asymmetric annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanuja</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Spek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="7101" to="7107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lightweight refinenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">125</biblScope>
		</imprint>
		<respStmt>
			<orgName>Northumbria University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Diatom autofocusing in brightfield microscopy: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Luis Pech</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Cristobal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chamorro-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fernandez-Valdivia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="314" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Suw-learn: Joint supervised, unsupervised, weakly supervised deep learning for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Haoyu Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwon</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards general purpose and geometry preserving single-view depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Patatkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vorontsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597[cs.CV]).3</idno>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dfusenet: Deep fusion of rgb and sparse depth information for image guided dense depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning guided convolutional network for depth completion. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fei-Peng Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Sparsity invariant cnns. 2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th International Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sparse and noisy lidar completion with rgb guidance and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 16th International Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Quick shift and kernel methods for mode seeking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>David A. Forsyth, Philip H. S. Torr, and Andrew Zisserman</editor>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="705" to="718" />
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">FastDepth: Fast Monocular Depth Estimation on Embedded Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Wofk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sertac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Depth completion from sparse lidar data with depth-normal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dense depth posterior (ddp) from single image and sparse range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deep depth completion of a single RGB-D image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno>abs/1803.09326</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/1612.01105</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
