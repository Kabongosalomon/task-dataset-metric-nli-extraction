<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Can Help Pedestrian Detection?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">The Institute for Theoretical Computer Science (ITCS) Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Megvii Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Megvii Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What Can Help Pedestrian Detection?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aggregating extra features has been considered as an effective approach to boost traditional pedestrian detection methods. However, there is still a lack of studies on whether and how CNN-based pedestrian detectors can benefit from these extra features. The first contribution of this paper is exploring this issue by aggregating extra features into CNNbased pedestrian detection framework. Through extensive experiments, we evaluate the effects of different kinds of extra features quantitatively. Moreover, we propose a novel network architecture, namely HyperLearner, to jointly learn pedestrian detection as well as the given extra feature. By multi-task training, HyperLearner is able to utilize the information of given features and improve detection performance without extra inputs in inference. The experimental results on multiple pedestrian benchmarks validate the effectiveness of the proposed HyperLearner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pedestrian detection, as the first and most fundamental step in many real-world tasks, e.g., human behavior analysis, gait recognition, intelligent video surveillance and automatic driving, has attracted massive attention in the last decade <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref>. However, while great progress has been made by deep convolutional neural networks (CNNs) on general object detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref>, research in the realm of pedestrian detection remains not as cumulative considering two major challenges.</p><p>Firstly, compared to general objects, pedestrians are less * Equal contribution. † Work was done during their internships at Megvii Inc. discriminable from backgrounds. In other words, the discrimination relies more on the semantic contexts. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), usually appearing in low resolution (less than 20×40 pixels), pedestrians together with the cluttered background bring about hard negative samples, such as traffic signs, pillar boxes, and models in shopping windows, which have very similar apparent features with pedestrians. Without extra semantic contexts, detectors working with such low-resolution inputs are unable to discriminate between them, resulting in the decrease in recall and increase in false alarms. How to accurately locate each pedestrian is the second  challenge. <ref type="figure" target="#fig_0">Figure 1</ref>(b) is one showcase in practical applications where the pedestrians stand close in a crowded scene. As a result, detectors typically fail to locate each individual and hence produce a dozen of false positives due to inaccurate localization. This problem becomes even worse for CNN-based detectors since while convolution and pooling layers generate high-level semantic activation maps, they also blur the boundaries between closely-laid instances. An intuitive alternative to address the problem is to make use of extra low-level apparent features (e.g. edges), for the purpose of solving the localization drawbacks by providing detectors with detailed apparent information.</p><p>In addition, in many applications, detectors can also benefit from other information, like depth when the camera is equipped with a depth sensor, or temporal information when a video sequence is input. However, it is still unclear how these information can be utilized by detectors, especially CNN-based detectors.</p><p>Given the observations above, one question comes up naturally: what kind of extra features are effective and how they actually work to improve the CNN-based pedestrian detectors? In this paper, we aim to answer this question and explore the characteristics of different extra features in pedestrian detection task. This paper contributes to:</p><p>• Firstly, we integrate extra features as input channels into CNN-based detectors. To investigate three groups of channel features: apparent-to-semantic channels, temporal channels and depth channels, extensive experiments are carried out on the KITTI pedestrian dataset <ref type="bibr" target="#b25">[26]</ref>.</p><p>• Then, we experimentally analyze both advantages and disadvantages of different kinds of channel features. Specifically, we quantify the improvement brought by different channel features and provide insight into the error sources.</p><p>• Moreover, a novel network architecture, namely Hy-perLearner, is proposed to aggregate extra features in a multi-task learning manner. In HyperLearner, channel features are aggregated as supervision instead of extra inputs, and hence it is able to utilize the information of given features and improve detection performance while requiring no extra inputs in inference. We verify the effectiveness of HyperLearner on several pedestrian detection benchmarks and achieve state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Traditional pedestrian detectors, extended from Viola and Jones paradigm <ref type="bibr" target="#b26">[27]</ref>, such as ACF <ref type="bibr" target="#b8">[9]</ref>, LDCF <ref type="bibr" target="#b21">[22]</ref>, and Checkerboards <ref type="bibr" target="#b34">[35]</ref>, filter various Integral Channels Features (ICF) <ref type="bibr" target="#b9">[10]</ref> before feeding them into a boosted decision forest, predominating the field of pedestrian detection for years. Coupled with the prevalence of deep convolutional neural network, CNN-based models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2]</ref> have pushed pedestrian detection results to an unprecedented level. In <ref type="bibr" target="#b32">[33]</ref>, given region proposals generated by a Region Proposal Network (RPN), CNN features extracted by an RoI pooling layer <ref type="bibr" target="#b12">[13]</ref> are fed into a boosted forest; while in Cai et al. <ref type="bibr" target="#b1">[2]</ref>, a downstream neural network architecture is proposed to preform end-to-end detection.</p><p>Integrating channel features of different types has been proved to be useful in many decision-forest-based pedestrian detectors. Prior work by Park et al. <ref type="bibr" target="#b22">[23]</ref> embeds optical flow into a boosted decision forest to improve pedestrian detectors working on video clips. CCF <ref type="bibr" target="#b31">[32]</ref> uses the activation maps of a VGG-16 <ref type="bibr" target="#b24">[25]</ref> network pretrained on ImageNet <ref type="bibr" target="#b15">[16]</ref> as channel feature, while Costea and Nedevschi <ref type="bibr" target="#b7">[8]</ref> utilize the heatmap of semantic scene parsing, in which detectors benefit from the semantic information within a large receptive field. However, the problem whether and how CNN-based pedestrian detectors can benefit from extra features still exhibits a lack of study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Channel features for pedestrian detection</head><p>In this section, we empirically explore the performance boost when extra channel features are integrated into CNNbased detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Before delving into our experiments, we first describe the dataset, evaluation metrics and baseline detector we use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI dataset</head><p>We choose KITTI dataset <ref type="bibr" target="#b25">[26]</ref> for channel feature analysis considering its possession of pedestrians of various scales in numerous scenes, as well as the information of adjacent frames and stereo data. KITTI contains 7, 481 labeled images of resolution 1250×375 and another 7, 518 images for testing. The training set is further split into two independent set for training and validation following <ref type="bibr" target="#b4">[5]</ref>. The person class in KITTI is divided into two subclasses: pedestrian and cyclist, both evaluated under PAS-CAL criteria <ref type="bibr" target="#b11">[12]</ref>. KITTI contains three evaluation metrics: easy, moderate and hard, with difference in the min. bounding box height, max. occlusion level, etc. Standard evaluation follows moderate metric.</p><p>Faster R-CNN Our baseline detector is an implementation of Faster R-CNN <ref type="bibr" target="#b23">[24]</ref>, initialized with VGG-16 <ref type="bibr" target="#b24">[25]</ref> weights pretrained on ImageNet <ref type="bibr" target="#b15">[16]</ref>. It consists of two components: a fully convolutional Region Proposal Network (RPN) for proposal generation, and a downstream Fast R-CNN (FRCNN) detector taking regions with high foreground likelihood as input.</p><p>Since KITTI contains abounding small objects, we slightly modify the framework as <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b1">[2]</ref>. Specifically, we adjust the number of anchors from 3 scales and 3 ratios to 5 scales and 7 ratios; besides, all conv5 layers are removed to preserve an activation map of high resolution for both RPN and FRCNN.</p><p>We choose Faster R-CNN not only for its prevalence and state-of-the-art performance, but also generality: our observations should remain mostly effective when similar techniques are applied in other CNN-based pedestrian detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Introduction to channel features</head><p>In this section, we introduce the channel features we integrated into the CNN-based pedestrian detector. Based on the type of information they carry, the selected channel features for integration are divided into three groups: apparentto-semantic channels, temporal channels and depth channels. <ref type="figure" target="#fig_1">Figure 2</ref> provides a demonstration of all channels.</p><p>Apparent-to-semantic channels This group of channels includes ICF channel <ref type="bibr" target="#b9">[10]</ref>, edge channel, segmentation channel and heatmap channel. The information in these channels ranges from low-level apparent to high-level semantic.  The ICF channel is a handy-crafted feature channel composed of LUV color channels, gradient magnitude channel, and histogram of gradient (HOG) channels, which has been widely employed in the decision-forest-based detectors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34]</ref>. Containing only colors and gradients within a local patch, ICF channel represents the most low-level but detailed information of an image.</p><p>The edge channel is extracted from the second and third layers of HED network <ref type="bibr" target="#b30">[31]</ref>. Different with traditional edge detector such as Canny <ref type="bibr" target="#b2">[3]</ref>, the HED framework produces more semantically meaningful edge maps (see <ref type="figure" target="#fig_1">Figure 2</ref>). The edge channel is thus considered as a mid-level feature channel containing both detailed appearance as well as high-level semantics.</p><p>As in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>, a fully convolutional network (FCN) is trained on MS-COCO dataset <ref type="bibr" target="#b17">[18]</ref> to generate the semantic segmentation channel, where each pixel represents the probability of the category (e.g., person and street) it belongs to. The segmentation channel carries higher-level semantic information, while still perserving some detailed appearance features, i.e., the boundaries between objects of different categories. However, two closely-laid instances of same category cannot be distinguished from each other in the segmentation channel without contour of each instance.</p><p>Furthermore, to obtain a feature channel with only highlevel semantics, we blur the segmentation channel into the heatmap channel. By doing so, the clear boundaries between objects of different categories are also removed and only high-level information of categories remains.</p><p>Temporal channels The temporal features (e.g., optical flow <ref type="bibr" target="#b0">[1]</ref> and motion <ref type="bibr" target="#b28">[29]</ref>) have been proved to be benefi-cial to traditional pedestrian detectors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23]</ref> working on videos. To test their effectiveness in CNN-based framework, we extract optical flow channel as representative using temporally adjacent frames.</p><p>Depth channels With more and more depth sensors employed in intelligent systems such as robotics and automatic driving, the depth information available in these tasks becomes an alternative extra channel feature to boost detectors. Instead of using the sparse point clouds captured by laser radars, we turn to DispNet <ref type="bibr" target="#b20">[21]</ref> to reconstruct the disparity channel from stereo images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Integration techniques</head><p>We integrate channel features by creating a new shallow side branch alongside the VGG-16 main stream (see <ref type="figure" target="#fig_3">Figure 3</ref>). This side branch consists of several convolution layers (with kernel size 3, padding 1 and stride 1) and max pooling layers (with kernel size 2 and stride 2), outputing an 128-channel activation maps of 1 / 8 input size, which is further concatenated with activation map conv4 3. The concatenated activation map is fed into the RPN and FR-CNN to preform detection.</p><p>We experiment different compositions of the side branch: the number of convolution layers and the initial weights (i.e., a random gaussian kernel, or pretrained weights). The technique we employed to pretrain the side branch is to train a Faster R-CNN detector which completely relies on the side branch and intialize the side branch with the weights from this network.  <ref type="table" target="#tab_1">Table 1</ref>, all integration methods improve the baseline Faster R-CNN detector in KITTI validation set on both classes across all three metrics. Nevertheless, the model with two extra convolution layers outperforms the model with only one extra convolution layer. A pretrained side branch does not perform well when further assembled with the VGG-16 network. When probing the network, we find that the model with pretrained weights tend to "rely" more on the sidebranch, (i.e., activation map produced by side branch has much greater value than the main stream). Given the fact that the side branch was pretrained to perform detection independently, this inbalance may be a cause accounting for the performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summariesed in</head><p>Based on the analysis, we use two convolution layers with random Gaussian initialization in all future experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison and analysis</head><p>We conduct experiments on two input scales (1x and 2x). <ref type="table">Table 3</ref> summarizes the results. For a fair comparison, a controlled experiment in which the original image is used as input of the side branch is also included.</p><p>In general, models integrated with extra channel features show improvement over the baseline. The experiment using original image as extra input shows nonobvious improvement, which confirms that the performance gain is indeed attributed to channel feature integration. Among all channel features, ICF channel shows least contribution to the detection performance in both scales. We conjecture the reason is that in deep convolutional networks, CNN features are more discriminative than hand-crafted features like HOG.</p><p>Recall the two major challenges for pedestrian detection: hard negative samples and the individual localization. Through detailed analysis, we demonstrate how CNN-based detectors can benefit from extra channel features to overcome these problems.</p><p>1x experiments In 1x experiments, channels that carry more semantic information show better performance. As shown in <ref type="table">Table 3</ref>, detectors with segmentation channel and heatmap channel bring most significant improvement to the detector. In accord with our previous hypotheses, the detectors utilize the semantic context provided by extra channel features to discriminate pedestrian of low resolution from hard negative samples. <ref type="table" target="#tab_2">Table 2</ref> provides the recall comparison at certain precision rate (70%) between models with segmentation channel and the baseline model for pedestrians of different sizes. All pedestrians are divided into four groups based on their heights in pixel. Leading absolute 4% recall rate on average, the detector with segmentation channel performs significantly better in recall for small pedestrians (less than or equal to 80 pixel in height).</p><p>2x experiments In 2x experiments, model with only highlevel semantic information but no low-level apparent features (i.e. the heatmap channel) fails to produce consistent  <ref type="table">Table 3</ref>. Channel features comparison on KITTI validation set. We list improvement across all three KTTTI metrics as well as the average. *: Our reproduced Faster R-CNN with same parametrs as in <ref type="bibr" target="#b23">[24]</ref>. The baseline is a re-implementation of Faster RCNN pipeline, consisting of slight differences with the basic Faster RCNN (See Section 3.1).</p><p>improvement over the baseline model compared to the 1x experiments. Nonetheless, channel features with both highlevel semantic and low-level apparent information (edge channel and segmentation channel) outperforms other channels. A possible explanation for this is that when it comes to large input scale, low-level details (e.g., edge) will show greater importance in detection. To further explore this phenomenon, we randomly sampled 1 / 4 of images (about 800) from validation set and collected false positive statistics at 70% recall rate, as shown in <ref type="figure" target="#fig_5">Figure 4</ref>(a). While in <ref type="figure" target="#fig_5">Figure 4(b)</ref>, we also count top-200 false positives in the validation set and show the fractions of each error source. Not only inhibiting false positives across all categories at a high recall, edge channel also contributes significantly to the localization precision. Integrated with the edge channel, detector lowers localization error rate by absolute 9% and 7% compared with the baseline and the detector with heatmap channel respectively. This proves that channel features with low-level apparent features (e.g., boundaries between individuals and contours of objects) improve localization precision when the input image is of high resolution.</p><p>Besides, We witness noticeable improvement in 1x when optical flow is integrated into the detector. Park et al. <ref type="bibr" target="#b22">[23]</ref> also proved this effectiveness in decision-forest-based detectors with a detailed analysis. For the disparity channel, the results are very similar to the results of heatmap channel. To have an insight into this, we should notice that the relative value in a disparity map also serves as a "segmentationlike" channel (see <ref type="figure" target="#fig_1">Figure 2</ref>), while the absolute value has only limited effects compared to the deep convolutional features and the predefined anchors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Jointly learn the channel features</head><p>As observed above, integrating channel features into the network can boost our detector working on images of both low resolution and high resolution. With these channel fea-  tures, we can narrow most of the gap between resolutions without introducing heavy computational cost brought by enlarging the input image, and push state-of-the-art forward. However, a brute-force integration method is computationally expensive with respect to the basic Faster R-CNN, given that the input channel feature usually requires extra computational cost. While many of the channel features comes from neural networks (e.g., semantic segmentation and edge), it is natural to think of "teaching" our neuralnetwork both channel features generation and detection. In the following section, we propose a new network structure to address the issue in a multi-task learning manner, namely, HyperLearner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">HyperLearner</head><p>The HyperLearner framework is illustrated in <ref type="figure" target="#fig_6">Figure 5</ref>. As shown, our system consists of four components: the body network for activation map generation, a channel feature network (CFN), a region proposal network (RPN) and a Fast R-CNN (FRCNN) network for final detection task.</p><p>From the very left, the entire image is forwarded through multiple convolution layers to generate the hierarchical activation maps. We first aggregate activation maps and make them into a uniform space, namely aggregated activation map. Aggregating activation maps from multiple level has been proved to be useful and important in many computer vision tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref> for its ability to collect rich hierarchical representations. This aggregated map is then fed into the channel feature network (CFN). CFN is a feed-forward fully convolutional network (FCN) for channel feature prediction. Unlike Faster R-CNN, RPN and FRCNN do not only take the output of the last convolution layer (conv4 3) as input. Instead, the aggregated activation map is also fed into the RPN, as well as FRCNN. By sharing the same aggregated activation map, the RPN and FRCNN are able to benefit from the representations CFN learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregated activation map</head><p>The body network takes the raw image, of shape 3×H×W , as its input, and outputs several activation maps. In our experiments, the body network is a VGG-16 <ref type="bibr" target="#b24">[25]</ref> network (without conv5 1 to conv5 3) intialized with the weights pretrained on Im-ageNet <ref type="bibr" target="#b15">[16]</ref>. We extract the activation maps from layer conv1 2, conv2 2, conv3 3 and conv4 3. Due to the pooling layer in the network, these maps are of different size and number of channels. We add two convolution layers after each map and keep their numbers of output channels same (32 in all our experiments). The high-level maps are then upsampled to the same size as the first activation map. Finally, they are concatenated together to form the aggregated activation map.</p><p>Channel Feature Network (CFN) The CFN directly takes the aggregated activation map to generate the predicted channel feature map through a fully convolutional structure. This map is typically of the same shape as the raw image. For example, the predicted channel feature may be a semantic segmentation map of several categories, or an edge detection map like HED Network <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Region Proposal Network (RPN) and Fast-RCNN (FR-CNN)</head><p>We build the RPN and FRCNN using the same structure as proposed in <ref type="bibr" target="#b23">[24]</ref>. RPN and FRCNN now take both last convolutional activation map in the VGG16 network (conv4 3) and the aggregated activation map from the body network as the inputs. The proposals generated by RPN are then fed into FRCNN to perform final detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Details</head><p>Loss Function During the training phase, besides the raw image and groundtruth bounding boxes for standard Faster R-CNN framework, the HyperLearner also takes a channel feature map as its supervisor, which is typically generated by another CNN (e.g., semantic segmentation and edge). To address the channel feature learning, we introduce a new pixel-level loss. Denote the feature map predicted by the CFN as C x,y , and the supervisor map as S x,y . The loss is computed by</p><formula xml:id="formula_0">: 1 H × W (x,y) (S x,y , C x,y ),</formula><p>where H and W represents the size of the feature map and is a loss function for a single pixel. In binary probabilistic maps, like edge map, cross-entropy loss is used, given by: The final loss for the network is thus computed by:</p><formula xml:id="formula_1">(p, q) = β x,y − p log q − (1 − p) log(1 − q) ,</formula><formula xml:id="formula_2">L = L CFN + λ 1 L RPNcls + λ 2 L RPNbbox + λ 3 L FRCNNcls + λ 4 L FRCNNbbox</formula><p>where the last four component remains the same as Faster R-CNN <ref type="bibr" target="#b23">[24]</ref>. In all our experiments, we set all λ i = 1.</p><p>Multi-stage training The aggregated activation map acts as an important role in the framework, which must be carefully trained. We employs a pragmatic multi-stage training methods, making the whole training process splitted into four stages.</p><p>In the first stage, only CFN is optimized. In detail, we fix parameters of all pretrained convolution layers in the body network (conv1 1 to conv4 3), and drop all RPN and FRCNN layers to train the CFN. In the second stage, we fix the whole body network (including the convolution layers for aggregating activation maps) and CFN, and train only RPN. Then in the third stage, body network, CFN and RPN are all fixed; only FRCNN component is optimized. While in the final stage, all layers are jointly optimized.</p><p>Acrossing all stages, in the optimization of the FRCNN, we treat region proposals coordinates from RPN as fixed value and do not back-propagate the gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and results</head><p>The performance of HyperLearner is evaluated across multiple pedestrian datasets: KITTI <ref type="bibr" target="#b25">[26]</ref>, Caltech Pedestrian <ref type="bibr" target="#b10">[11]</ref>, and Cityscapes <ref type="bibr" target="#b5">[6]</ref>. The datasets we chose cover most of the popular ones in pedestrian detection task.</p><p>One may also notice that our body network an implementation of HyperNet proposed in <ref type="bibr" target="#b14">[15]</ref>. Thus, we implement a control experiment where the CFN is removed as a typical HyperNet setting. That is, the body network keeps its side branches for aggregated activation map, but it does not learn from any extra supervision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">KITTI Dataset</head><p>We evaluated the performance of HyperLearner with two kinds of feature supervision: edge and semantic segmentation. These two kinds of channel features have been proved to be effective when directly integrated into the Faster R-CNN framework (see Section 3.3). The results on the validation set of KITTI dataset is illustrated in the <ref type="table">Table 4</ref>.</p><p>In experiments on 1x scale, we notice great performance improvement when our HyperLearner is jointly learned from an edge detection network or a semantic segmentation network compared to the Faster R-CNN baseline and the HyperNet. The quantitative analysis is consistent with the experiments in Section 3.3 where we directly integrate them as an extra input into the network through a branch network.</p><p>In experiments on 2x scale, HyperLearner as well as Hy-perNet make clear improvement. Based on former analysis, when the input image is of high resolution, the introduction of channel features with low-level details could benefit the detector. In HyperNet setting, side branches of the body network act as an multi-level feature extractor, and therefore such kind of improvement is expected.</p><p>As a transfer learning application, HyperLearner successfully boost a CNN-based detector using features learned by other networks with different architecture and trained for other tasks. From another perspective, HyperLearner offers an alternative way to perform feature learning in such CNNs and showed noticeable improvement. Based on the results in <ref type="table">Table 4</ref> and 5, it is safe to conclude that HyperLearner actually utilizes the extra supervision from channel features to generate a better hyper-feature extractor, especially for the detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Cityscapes dataset</head><p>The Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>, is a large-scale dataset for semantic urban segmentation which contains a diverse set of stereo video recordings from 50 cities. It consists of 2, 975 training and 500 validation images with fine annotations, as well as another 20, 000 training images with coarse annotations. The experiments are conducted on the fineannotated images. Compared with former standard datasets, Cityscapes possesses meticulous detection labeling (pixellevel), as well as fine semantic segmentation labeling.</p><p>As mentioned, the Cityscapes dataset provides pixellevel semantic segmentation labeling, so instead of using segmentation model pretrained on MS-COCO dataset, we directly address the multi-task learning by employing pixellevel segmentation labels as supervisor (i.e., our Hyper-Learner jointly learns pedestrian detection and semantic segmentation). During training, we only use segmentation labels for "person". As shown in <ref type="table">Table 5</ref>, we also witness significant improvement over the Faster R-CNN baseline and HyperNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Caltech dataset</head><p>The Caltech dataset <ref type="bibr" target="#b10">[11]</ref> is also a commonly used dataset for pedestrian detection evaluation. It consists of 2.5 hours 30Hz VGA video recorded from a vehicle traversing the streets of Los Angeles, USA. Detection results are evaluated on a test set consisting of 4024 frames.</p><p>Zhang et al. <ref type="bibr" target="#b33">[34]</ref> conducted a detailed survey and provided a refined groundtruth labeling on Caltech dataset. Our experiments is completely based on this new labeling (both training and testing). HyperLearner achieves state-of-theart performance on the test set. <ref type="figure">Figure 7</ref> shows the detailed comparison of HyperLearner, the Faster R-CNN baseline and other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Summary</head><p>In this paper, we integrated channel features into CNN-based pedestrian detectors, specifically, ICF channel, edge channel, segmentation channel and heatmap channel (apparent-to-semantic channel); optical flow channel (temporal channel); disparity channel (depth channel). Our quantitative experiments show semantic channel features can help detectors discriminate hard positive samples and negative samples at low resolution, while apparent channel features inhibit false positives of backgrounds and improve localization accuracy at high resolution.</p><p>To address the issue of computational cost, we propose a novel framework, namely HyperLearner, to jointly learn channel features and pedestrian detection. HyperLearner is able to learn the representation of channel features while requiring no extra input in inference, and provides significant improvement on several datasets. From another point of view, HyperLearner offers an alternative way to perform feature learning in HyperNet-like CNNs in a transfer learning manner.  <ref type="table">Table 5</ref>. Results on Cityspcaes validation set. The speed column shows the time each model needed to perform detection on a single image. The speed is tested on single NVIDIA TITAN-X GPU. We use all segmentation polygons labeled "person" to generate bounding boxes for the pedestrian detection task. Following the standard in Caltech dataset <ref type="bibr" target="#b10">[11]</ref>, all persons with (pixel-level) occlusion greater than 0.5 or of height less than 50 pixels are ignored. Furthermore, all polygons labeled "cyclist" or "person group" are also ignored.  False positive per image <ref type="figure">Figure 7</ref>. Detection quality on Caltech test set (reasonable, MR N −2 (MR N −4 )), evaluated on the new annotations <ref type="bibr" target="#b33">[34]</ref>. We achieve state-of-the-art results on both evaluation metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Examples of true pedestrians and hard negative samples of low resolution. Without extra semantic contexts, it is difficult to discriminate between them, even for human eyes. (b) Example of pedestrians in crowded scenes, where CNN-based detectors fail to locate each individual without low-level apparent features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A demonstration of various channel features. Includes: apparent-to-semantic features, temporal features, depth features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>As described in Section 3.3, our Faster R-CNN for channel feature integration. The side branch takes channel features as input and generates channel feature representations before concatenated with conv4 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Top-200 false positives sources</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>False positive analysis for baseline, edge channel and heatmap channel at 2x scale. All false positives are categorized into four types: localization error, background classification error, cyclist classification error, and annotation error. Localization error is defined as non-matched detection bounding boxes which overlap with a groundtruth but iou &lt; 0.5, while background error has no overlap with any groundtruth box. Cyclist error happens when a bounding box match cyclist groundtruth. Annotation error occurs when detection "matches" a de facto groundtruth which, however, is not annotated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>The proposed HyperLearner, which consists of 4 components: body network, channel feature network (CFN), region proposal network (RPN) and Fast R-CNN (FRCNN). HyperLearner learns representations of channel features while requiring no extra input in inference. Refer to Section 4.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>where β is a weight function to balance the positive labels and negative labels.If S x,y &gt; 0.5, β = 1 − |S + |/|S|; otherwise, β = |S + |/|S|, where |S + | = 1[S x,y &gt; 0.5].For multi-class probabilistic maps, like segmentation map, cross-entropy loss is used. For other tasks, MSE loss is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Results of HyperLearner on Cityscapes validation set. The left column shows our detection result, while the right column demonstrate CFN's output learned from segmentation labeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1705.02757v1 [cs.CV] 8 May 2017</figDesc><table><row><cell>Original Image</cell><cell>Gradient ( ICF)</cell><cell>Edge</cell><cell>Segmentation Heatmap</cell><cell>Disparity</cell><cell>Optical Flow</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Apparent-to-Semantic Channels</cell><cell>Depth Channels</cell><cell>Temporal Channels</cell></row><row><cell></cell><cell>Low-level details</cell><cell></cell><cell>High-level semantics</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Detection improvement by integrating channel features on KITTI validation set. Model "O" is our baseline detector. "#Convs" means the number of convolution layers in the side branch. "Init. W." denotes initial weights for the side branch. The input images are not enlarged.</figDesc><table><row><cell></cell><cell cols="2">Model</cell><cell>Pedestrian</cell></row><row><cell></cell><cell cols="2">#Convs Init. W.</cell><cell>Mod Easy Hard</cell></row><row><cell>O</cell><cell>N/A</cell><cell>N/A</cell><cell>68.96 73.33 60.43</cell></row><row><cell>A</cell><cell>2</cell><cell>random</cell><cell>70.80 78.15 62.16</cell></row><row><cell>B</cell><cell>1</cell><cell>random</cell><cell>70.40 75.17 61.92</cell></row><row><cell>C</cell><cell>2</cell><cell cols="2">pretrained 69.92 77.33 61.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Recall comparison at 70% precision between baseline and segmentation channel at different pedestrian heights. The results are based on 1x scale.</figDesc><table><row><cell>Model</cell><cell cols="3">Recall (0, 80] (80, 160] (160, inf] all scales</cell></row><row><cell>Baseline</cell><cell>21.3% 87.6%</cell><cell>96.8%</cell><cell>70.0%</cell></row><row><cell cols="2">+Segmentation 35.6% 88.2%</cell><cell>96.8%</cell><cell>74.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Original img 68.63 76.61 60.45 -0.33 +3.28 +0.02 +0.99 71.33 76.72 62.17 +0.12 -1.01 -0.02 -0.30 + ICF 68.40 73.56 60.20 -0.56 +0.23 -0.23 -0.19 71.80 77.40 62.79 +0.59 -0.33 +0.60 +0.29 + Edge 69.49 76.28 60.89 +0.53 +2.95 +0.46 +1.31 72.34 78.32 63.28 +1.13 +0.59 +1.09 +0.94 + Segmentation 70.80 78.15 62.16 +1.84 +4.82 +1.73 +2.80 72.54 78.49 63.61 +1.33 +0.76 +1.42 +1.17 + Heatmap 70.33 78.03 61.75 +1.37 +4.70 +1.32 +2.46 71.39 77.64 62.34 +0.18 -0.09 +0.15 +0.08 + Disparity 70.03 77.74 61.48 +1.07 +4.41 +1.05 +2.18 71.72 77.52 62.47 +0.51 -0.21 +0.28 +0.19 + Optical Flow 69.39 77.07 60.79 +0.43 +3.74 +0.36 +1.51 71.13 76.85 62.24 -0.08 -0.88 +0.05 -0.25</figDesc><table><row><cell>Model</cell><cell cols="5">Pedestrian 1x Input Mod Easy Hard Mod Easy Hard Avg Improvement</cell><cell cols="5">Pedestrian 2x Input Mod Easy Hard Mod Easy Hard Avg Improvement</cell></row><row><cell cols="2">Fr-RCNN* [24] 59.29 64.53 53.01</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.05 76.00 62.08</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MS-CNN [2]</cell><cell>68.37 73.70 60.72</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.26 76.38 64.08</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Our Baseline</cell><cell>68.96 73.33 60.43</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.21 77.73 62.19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>RCNN* [24] 59.29 64.53 53.01 71.05 76.00 62.08 MS-CNN [2] 68.37 73.70 60.72 72.26 76.38 64.08 Baseline 69.80 74.37 61.20 71.73 77.84 62.30 HyperNet 69.72 76.91 61.10 72.23 77.96 63.43 +Segmentation 71.15 79.43 62.34 72.35 79.17 62.34 +Edge 71.25 78.43 62.15 72.51 78.51 63.24</figDesc><table><row><cell>Model</cell><cell>1x input Mod Easy Hard Mod Easy Hard 2x input</cell></row><row><cell cols="2">Fr-Table 4. Results on KITTI validation set, the model HyperNet</cell></row><row><cell cols="2">refers to the HyperLearner without CFN. Evaluation follows mod-</cell></row><row><cell cols="2">erate metric in KITTI.</cell></row><row><cell cols="2">*: Fr-RCNN follows setting as [24] while baseline model is Faster-</cell></row><row><cell cols="2">RCNN with slightly different parameters. See also Table 3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.30 250ms 86.67 -0.53 -0.22 Jointsegmap 140ms 77.22 250ms 87.67 +2.25 +0.78</figDesc><table><row><cell>Model</cell><cell cols="3">540p input Speed AP Speed AP 540p 720p 720p input Improvement</cell></row><row><cell>Baseline</cell><cell>130ms 74.97 240ms 86.89</cell><cell>-</cell><cell>-</cell></row><row><cell>HyperNet</cell><cell>140ms 74</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The computation of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Beauchemin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="466" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07155</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic channels for fast pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2360" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00600</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scaleaware fast r-cnn for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08160</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02325</idno>
		<title level="m">Ssd: Single shot multibox detector</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02134</idno>
		<title level="m">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Local decorrelation for improved detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1134</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring weak stabilization for motion feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G P L R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">New features and insights for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR), 2010 IEEE conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2009-British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="124" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04693</idno>
		<title level="m">Subcategoryaware convolutional neural networks for object proposals and detection</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
			<affiliation>
				<orgName type="collaboration">S</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu</surname></persName>
			<affiliation>
				<orgName type="collaboration">S</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Is faster rcnn doing well for pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07032</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1751" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
