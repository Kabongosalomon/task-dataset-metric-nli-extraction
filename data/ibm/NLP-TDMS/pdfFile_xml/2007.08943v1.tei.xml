<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HDNet: Human Depth Estimation for Multi-Person Camera-Space Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
							<email>jiahao@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim</forename><forename type="middle">Hee</forename><surname>Lee</surname></persName>
							<email>gimhee.lee@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HDNet: Human Depth Estimation for Multi-Person Camera-Space Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human Depth Estimation · Multi-person Pose Estimation · Camera Coordinate Space</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current works on multi-person 3D pose estimation mainly focus on the estimation of the 3D joint locations relative to the root joint and ignore the absolute locations of each pose. In this paper, we propose the Human Depth Estimation Network (HDNet), an end-to-end framework for absolute root joint localization in the camera coordinate space. Our HDNet first estimates the 2D human pose with heatmaps of the joints. These estimated heatmaps serve as attention masks for pooling features from image regions corresponding to the target person. A skeleton-based Graph Neural Network (GNN) is utilized to propagate features among joints. We formulate the target depth regression as a bin index estimation problem, which can be transformed with a soft-argmax operation from the classification output of our HDNet. We evaluate our HDNet on the root joint localization and root-relative 3D pose estimation tasks with two benchmark datasets, i.e., Human3.6M and MuPoTS-3D. The experimental results show that we outperform the previous state-ofthe-art consistently under multiple evaluation metrics. Our source code is available at: https://github.com/jiahaoLjh/HumanDepth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human pose estimation is one of the active research topics in the community of computer vision and artificial intelligence due to its importance in many applications such as camera surveillance, virtual/augmented reality, and humancomputer interaction, etc. Extensive research has been done for human pose estimation in both 2D image space and 3D Cartesian space, respectively. Great successes have been achieved in the single-person 2D/3D pose estimation tasks thanks to the rapid development of deep learning techniques and the emergence of large-scale human pose datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>. On the other hand, multi-person 2D/3D pose estimation tasks are more challenging due to the unknown number of persons in the scene. To mitigate this problem, the multi-person pose estimation task is typically tackled in a two-stage scheme that decouples the estimation of the number of persons and the pose of each person, i.e., the topdown <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref> or bottom-up <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>   <ref type="figure">Fig. 1</ref>. Top-down multi-person 3D pose estimation pipeline. Camera-space root joint coordinate is estimated for each detected person bounding box, followed by a rootrelative 3D pose estimation, to obtain the absolute 3D poses and locations.</p><p>multi-person 3D pose datasets such as MuPoTS-3D <ref type="bibr" target="#b19">[20]</ref> are created to faciliate the research of multi-person 3D pose estimation. However, most of the existing works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> focus on the estimation of 3D pose relative to the root joint of each person in the scene. Global absolute locations of the respective 3D poses with respect to the camera coordinate space are ignored. Estimating the absolute 3D location of each pose in an image is essential for understanding human-to-human interactions. Recently, Moon et al. <ref type="bibr" target="#b20">[21]</ref> propose a multi-stage pipeline for the task of multi-person 3D pose estimation in the camera coordinate space as shown in <ref type="figure">Figure 1</ref>. The pipeline adopts the top-down scheme which predicts a bounding box for each person in the first stage. This is followed by estimation of the absolute root joint location for the person in each bounding box. Finally, the global pose of each person is recovered by applying single-person 3D pose estimation to get the relative location of other joints with respect to the root joint. The root joint localization framework proposed in <ref type="bibr" target="#b20">[21]</ref> estimates the depth of root joint for each person based on the size of the bounding box. Despite showing promising results, the approach relies on the size of bounding box for root joint localization, and hence is not sufficiently effective due to two reasons: <ref type="bibr" target="#b0">(1)</ref> The compactness of bounding boxes varies from person to person and also between different object detectors. (2) Sizes of bounding boxes carries no direct information about the size of the particular person due to the variation of poses.</p><p>In this paper, we propose an end-to-end Human Depth Estimation Network (HDNet) to address the problems of root joint depth estimation and localization. We adopt the same top-down pipeline for the task of multi-person absolute 3D pose estimation. Our key observation is that we can estimate the depth of a person in a monocular image with considerably high accuracy by leveraging on the prior knowledge of the typical size of the human pose and body joints. Inspired by this observation, we propose to jointly learn the 2D human pose and the depth estimation tasks in our HDNet. More specifically, we utilize the heatmaps of the joints from the human pose estimation task as attention masks to achieve pose-aware feature pooling in each joint type. Subsequently, we put the pose-aware features of the joints into a skeleton-based Graph Neural Network (GNN), where information are effectively propagated among body joints to enhance depth estimation. Following a recent work on scene depth estimation <ref type="bibr" target="#b6">[7]</ref>, we formulate our depth estimation of the root joint as a classification task, where the target depths are discretized into a preset number of bins. We also adopt a soft-argmax operation on the bins predicted by our HDNet for faster convergence during training and better performance without losing precision compared to direct numerical depth regression. Our approach outperforms previous stateof-the-art <ref type="bibr" target="#b20">[21]</ref> on the task of root joint localization on two benchmark datasets, i.e., Human3.6M <ref type="bibr" target="#b11">[12]</ref> and MuPoTS-3D <ref type="bibr" target="#b19">[20]</ref> under multiple evaluation metrics. Experimental results also show that accurate root localization benefits the task of root-aligned 3D human pose estimation.</p><p>Our contributions in this work are:</p><p>-An end-to-end Human Depth Estimation Network (HDNet) is proposed to address the problem of root joint localization for multi-person 3D pose estimation in the camera-space. -Several key components are introduced in our framework: (1) pose heatmaps are used as attention masks for pose-aware feature pooling; (2) a skeletonbased GNN is designed for effective information propagation among the body joints; and (3) depth regression of the root joint is formulated as a classification task, where the classification output is transformed to the estimated depth with a soft-argmax operation to facilitate accurate depth estimation. -Quantitative and qualitative results show that our approach consistently outperforms the state-of-the-art on multiple benchmark datasets under various evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Human pose estimation has been an interesting yet challenging problem in computer vision. Early methods use a variety of hand-crafted features such as silhouette, shape, SIFT features, HOG for the task. Recently, with the power of deep neural networks and well-annotated large-scale human pose datasets, increasing learning-based approaches are proposed to tackle this challenging problem.</p><p>Single-person 2D pose estimation. Early works, such as Stacked Hourglass <ref type="bibr" target="#b22">[23]</ref>, Convolutional Pose Machines <ref type="bibr" target="#b29">[30]</ref>, etc., have been proposed to use deep convolutional neural networks as feature extractors for 2D pose estimation. Heatmaps of joints are the commonly used representation to indicate the presence of joints at spatial locations with Gaussian peaks. More recent works including RMPE <ref type="bibr" target="#b4">[5]</ref>, CFN <ref type="bibr" target="#b9">[10]</ref>, CPN <ref type="bibr" target="#b2">[3]</ref>, HRNet <ref type="bibr" target="#b27">[28]</ref>, etc., introduce various framework designs to improve the joint localization precision.</p><p>Single-person 3D pose estimation. Approaches for 3D pose estimation can be generally categorized into two groups. Direct end-to-end estimation of 3D pose from RGB images regresses both 2D joint locations and the z-axis rootrelative depth for each joint. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> extend the notion of heatmap to the 3D space, where estimation is performed in a volumetric space. Another group of approaches decouples the task into a two-stage pipeline. The 2D joint locations are first estimated, followed by a 2D-to-3D lifting. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref> utilize Multi-Layer Perceptron (MLP) to learn the mapping.</p><p>Multi-person 2D pose estimation. Top-down <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref> and bottomup <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> approaches have been proposed to estimate poses for multiple persons. Top-down approaches utilize a human object detector to localize the bounding box, followed by a single-person pose estimation pipeline with image patch cropped from the bounding box. Bottom-up approaches detect human joints in a person-agnostic way, followed by a grouping process to identify joints belonging to the same person. Top-down approaches usually estimate joint locations more precisely because bounding boxes of different sizes are scaled to the same size in the single-person estimation stage. However, top-down approaches tend to be more computationally expensive due to the redundancy in bounding box detections.</p><p>Multi-person 3D pose estimation. Several works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref> have been conducted on multi-person 3D pose estimation. Rogez et al. <ref type="bibr" target="#b25">[26]</ref> propose a LCR-Net which consists of localization, classification, and regression parts and estimates each detected human with a classified and refined anchor pose. Mehta et al. <ref type="bibr" target="#b19">[20]</ref> propose a bottom-up approach which estimates a specially designed occlusion-robust pose map and readout the 3D poses given 2D poses obtained with Part Affinity Fields <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr">Dabral et al. [4]</ref> propose to incorporate hourglass network into Mask R-CNN detection heads for better 2D pose localization, followed by a standard residual network to lift 2D poses to 3D. Zanfir et al. <ref type="bibr" target="#b30">[31]</ref> design a holistic multi-person sensing pipeline, i.e. MubyNet, to jointly address the problems of multi-person 2D/3D skeleton/shape-based pose estimation. However, these works only estimate and evaluate the 3D pose after root joint alignment and ignore the global location of each pose. Recently, Moon et al. <ref type="bibr" target="#b20">[21]</ref> propose a multi-stage pipeline for multi-person camera-space 3D pose estimation. The pipeline follows the top-down scheme and consists of a RootNet which localizes the root joint for each detected bounding box. We also adopt the top-down scheme pipeline and estimate the camera-space root joint location and 3D pose for each detected bounding box. To our best knowledge, <ref type="bibr" target="#b20">[21]</ref> and our work are the only two works that focus on the estimation and evaluation of multi-person root joint locations. Compared to <ref type="bibr" target="#b20">[21]</ref> which relies on the size of detected bounding box, we utilize the underlying features and design a humanspecific pose-based root joint depth estimation framework to significantly boost the root localization performance.</p><p>3 Our Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given a 2D image with an unknown number of persons, the task of cameraspace multi-person 3D pose estimation is to: (1) identify all person instances, (2) estimate the 3D pose with respect to the root joint, i.e., pelvis, for each person, and (3) localize each person by estimating the 3D coordinate of root joint in the camera coordinate space. Following the top-down approaches in the literature of multi-person pose estimation, we assume that the 2D human bounding boxes for each person in the input image are available from a generic object detector. Given the person instances and detected bounding boxes, we propose an end-to-end depth estimation framework to localize the root joint of each person in the camera coordinate space as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. The root joint localization is decoupled into two sub-tasks: (1) localization of the root joint image coordinate (u, v), and (2) estimation of the root joint depth Z in the camera frame, which is then used to back-project (u, v) to 3D space. We use an off-the-shelf single-person 3D pose estimator to estimate the 3D joint locations of each person with respect to the root joint. The final absolute 3D pose of each person in the camera coordinate system is obtained by the transformation of each joint location with the absolute location of the root joint.</p><p>The details of our proposed root joint localization framework are introduced in Section 3.2. The choices of specific object detector and single-person 3D pose estimator used in our experiments are given in the implementation details in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Root Localization Framework</head><p>Our framework for monocular image single/multi-person depth estimation is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The framework consists of a Feature Pyramid Network (FPN)based backbone, a heatmap-based human pose estimation branch, and a Graph Neural Network (GNN)-based depth estimation branch.</p><formula xml:id="formula_0">FPN Backbone C=256 1x1 conv 1x1 conv C=128 8x 4x 2x Multi-scale Feature Extraction (a) (b) Upscale Upscale C=128 C=64</formula><p>3x3 conv 3x3 conv Backbone Network. We choose FPN <ref type="bibr" target="#b14">[15]</ref> as our backbone network due to its capability of explicitly handling features of multiple scales in the form of feature pyramids. Hence, it is suitable for perceiving the scale of human body parts and consequently enhances depth estimation of the human pose in an image. The FPN network consists of a ResNet-50 <ref type="bibr" target="#b8">[9]</ref> with feature blocks of four different scales C 2 , C 3 , C 4 , C 5 (cyan layers in <ref type="figure" target="#fig_1">Figure 3</ref>(a)), where a reversed hierarchy of feature pyramid P 5 , P 4 , P 3 , P 2 is built upon (orange layers in <ref type="figure" target="#fig_1">Figure 3</ref>(a)).</p><p>Each of the four scales encodes hierarchical levels of feature representations, which are then passed through two consecutive convolutional layers as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b). An upsampling operation with corresponding upsample scale factor is applied between the two convolutional layers to ensure matching spatial resolution from the output of the four scales. Batch Normalization <ref type="bibr" target="#b10">[11]</ref> and ReLU operations are used after each convolution layer. Weights are not shared across scales. Blocks of all scales are then concatenated to form the final feature block F. Since we find that the downstream tasks of pose estimation and depth estimation are not collaboratively correlated, we split the multi-scale feature processing from the output feature pyramid P 5 , P 4 , P 3 , P 2 of the backbone into two parallel branches without shared weights as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We denote the features as F pose and F depth , respectively.</p><p>2D Pose Estimation Branch. We propose to use estimated 2D pose as a guide to aggregate information from useful feature regions to effectively distil information from the image and discard irrelevant areas such as the background. We first regress N J heatmapsĤ that correspond to the N J joints with a 1 × 1 convolution from feature block F pose . Each of the N J heatmaps are normalized across all spatial locations with a softmax operation. A direct read out of the coordinate from the local maximum limits the precision of the joint location estimation due to the low resolution of the output heatmap (4x downsample from input image in ResNet backbone). To circumvent this problem, we follow the idea of "soft-argmax" in <ref type="bibr" target="#b28">[29]</ref> and compute the "integral" version of estimated coordinate (û,v) for each joint j using the weighted sum of coordinates:</p><formula xml:id="formula_1">(û j ,v j ) = (W −1,H−1) (u,v)=(0,0)Ĥ (j) u,v · (u, v),<label>(1)</label></formula><p>where W and H are the width and height of output heatmap. The softmax operation guarantees that the weightsĤ u,v form a valid distribution which sum up to 1 over all spatial locations. To supervise the heatmap regression, we generate a ground truth heatmap H (j)GT for each joint j. A Gaussian peak is created around the ground truth joint location (u j , v j ) with a preset standard deviation that controls the compactness of the Gaussian peak. We use standard Mean Squared Error (MSE) as the heatmap regression loss and L1 loss for the pose after soft-argmax as follows:</p><formula xml:id="formula_2">L hm = 1 N J HW N J j (W −1,H−1) (u,v)=(0,0) H (j)GT u,v −Ĥ (j) u,v 2 ,<label>(2)</label></formula><formula xml:id="formula_3">L pose = 1 N J N J j u GT j −û j + v GT j −v j .<label>(3)</label></formula><p>To deal with multiple persons in the image, we focus on a target person by zeroing out the regions of the heatmap outside the bounding box of that person from the object detector.</p><p>Depth Estimation Branch. After we obtain the heatmaps, we use them as attention masks to guide the network into focusing on specific regions of the image related to the target person. More specifically, we only care about features from pixel locations that are close to the joints of the target person. The intuition behind our design choice is that joint locations contain more scale-related information than the larger yet less discriminative areas such as the whole upper body trunk. Attention-guided feature pooling is also adopted in other tasks such as action recognition <ref type="bibr" target="#b16">[17]</ref> and hand pose estimation <ref type="bibr" target="#b12">[13]</ref>. We compute the weighted sum feature vector d for each joint j from the feature block F depth as:</p><formula xml:id="formula_4">d (j) = (W −1,H−1) (u,v)=(0,0)Ĥ (j) u,v · F depth u,v .<label>(4)</label></formula><p>To effectively aggregate features corresponding to different joint types, we formulate a standard Graph Neural Network (GNN) where each node represents one joint type, e.g., elbow, knee, etc. The aggregated features d (j) for each joint type j is fed into the corresponding node X (j) in in the graph as input. Each layer of the GNN is defined as:</p><formula xml:id="formula_5">X (i) out = σ ã ii f self (X (i) in ; Θ self ) + j =iã ij f inter (X (j) in ; Θ inter ) .<label>(5)</label></formula><p>The feature of each input node X in undergoes the linear mappings f self (.) and f inter (.) that are parametrized by Θ self and Θ inter , respectively. The output of the node, i.e. X (i) out is computed from a weighted aggregation of f self (.) and f inter (.) of all other nodes. The weighting factorã ij is an element of the normalized adjacency matrixÃ ∈ R Nj ×Nj that controls the extent of influence of the nodes on each other. The original adjacency matrix is A ∈ {0, 1} Nj ×Nj ; an element a ij equals 1 if there is a skeletal link between joint i and j, e.g. left knee to left ankle, or otherwise 0.Ã ∈ R Nj ×Nj is obtained by applying L1-normalization on each row of A. The non-linearity function σ(.) is implemented with a Batch Normalization followed by a ReLU. We stack L GNN layers in total. After the last GNN layer, we merge the feature output from each node with an average pooling operation.</p><p>Target output formulation Inspired by the work <ref type="bibr" target="#b6">[7]</ref> for scene depth estimation, we formulate the depth estimation as a classification problem instead of directly regressing the numerical value of depth. We follow the practice in <ref type="bibr" target="#b6">[7]</ref> to discretize the log-depth space into a preset number of bins, N B . We compute:</p><formula xml:id="formula_6">b(d) = log d − log α log β − log α · (N B − 1),<label>(6)</label></formula><p>where b gives the bin index of the depth, and the depth d of a pose is assumed to be within the range [α, β]. Here . is the round-off to the nearest integer operator. To eliminate quantization errors, we assign non-zero values to two consecutive bins i and i + 1, where i ≤ b &lt; i + 1. This operation is similar to the weights in bi-linear interpolation. For example, the ground truth values of the bins are given by B = [0, 0, 0.6, 0.4, 0] for N B = 5 and b = 2.4. Consequently, B is a 1D heatmap that can achieve any level of precision with a sufficiently accurate categorical estimation on the bins. Since a different focal length of the camera affects the scale of a target person in the image, it is unrealistic to estimate the absolute depth from images taken by any arbitrary camera. To alleviate this problem, we normalize out the camera intrinsic parameters by replacing the target d withd = d/f , where f is the focal length of camera. We approximate withd = d/ f x · f y in our experiments since the focal lengths in x and y directions are usually very close. Finally, we add a fully connected layer after the pooled feature from the last GNN layer to regress the N B values of the binsB. Softmax operation is used to normalize the output into a valid distribution. We transformB back to the estimated depth d of the root joint by:</p><formula xml:id="formula_7">d = exp b N B − 1 · (log β − log α) + log α · f x · f y , whereb = N B −1 i=0B i · i. (7)</formula><p>Similar to the soft-argmax operation used to transform heatmaps to joint locations,b is the weighted sum of the bin indices with the estimated heatmapB. To supervise the learning of the depth estimation branch, we adopt cross-entropy loss on the estimated binsB and L1 loss onb as follows:</p><formula xml:id="formula_8">L bins = − N B −1 i=0 B GT i · logB i , and L idx = b GT −b .<label>(8)</label></formula><p>We train the whole framework with losses from the pose estimation and depth estimation branches:</p><formula xml:id="formula_9">L = λ hm L hm + λ pose L pose + λ bins L bins + λ idx L idx .<label>(9)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>Human3.6M dataset. Human3.6M dataset <ref type="bibr" target="#b11">[12]</ref> is currently the largest publicly available dataset for human 3D pose estimation. The dataset consists of 3.6 million video frames captured by MoCap system in a constrained indoor studio environment. 11 actors performing 15 activities are captured from 4 camera viewpoints. 3D ground truth poses in world coordinate system and camera extrinsic (rotation and translation with respect to world coordinate) and intrinsic parameters (focal length and principal point) are available. We follow previous works that five subjects (S1, S5, S6, S7, S8) are used in training and two subjects (S9 and S11) are used for evaluation. We use every 5th and 64th frames in each video for training and evaluation respectively. No extra 2D pose dataset is used to augment the training. We follow the metric Mean Root Position Error (MRPE) proposed in <ref type="bibr" target="#b20">[21]</ref> to evaluate the root localization accuracy. Specifically, we consider the Euclidean distance between the estimated and the ground truth 3D coordinate of the root joint.</p><p>MuCo-3DHP and MuPoTS-3D datasets. MuCo-3DHP and MuPoTS-3D are two datasets proposed by Mehta et al. <ref type="bibr" target="#b19">[20]</ref> to evaluate multi-person 3D pose estimation performance. The training set MuCo-3DHP is a composite dataset which merges randomly sampled 3D poses from single-person 3D human pose dataset MPI-INF-3DHP <ref type="bibr" target="#b18">[19]</ref> to form realistic multi-person scenes. The test set MuPoTS-3D is a markerless motion captured multi-person dataset including both indoor and outdoor scenes. We use the same set of MuCo-3DHP synthesized images from <ref type="bibr" target="#b20">[21]</ref> for a fair comparison. No extra 2D pose dataset is used to augment the training. For evaluation of multi-person root joint localization, we follow <ref type="bibr" target="#b20">[21]</ref> to report the average precision and recall of 3D root joint location under different thresholds. A root joint with a smaller distance to the matched ground truth root joint location than a threshold is considered a true positive estimation. We follow <ref type="bibr" target="#b20">[21]</ref> to report 3DPCK abs for evaluation of the root-aware 3D pose estimation, where 3DPCK (3D percentage of correct keypoints) for the estimated poses is evaluated without root alignment. 3DPCK treats an estimated joint as correct if it is within 15 cm distance from the matched ground truth joint. Although our framework does not focus on root-relative 3D pose estimation, we also report the root-aligned 3DPCK rel to show that accurate root localization also benefits the precision of 3D pose estimation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Following previous work <ref type="bibr" target="#b20">[21]</ref>, we use Mask R-CNN <ref type="bibr" target="#b7">[8]</ref> as our person detector due to its high performance and generalizability to in-the-wild images. For singleperson 3D pose estimation, we use the volumetric-based 3D pose estimator by <ref type="bibr" target="#b28">[29]</ref>. Instead of cropping out areas of interest using bounding boxes, we keep the original scale of image and crop out a fixed size patch centered around the bounding box, or the principal point if no bounding box is provided in the singleperson scenario. The cropped out image is then rescaled to 256×256 and used as input to our network. The output resolution of the heatmap is 64 × 64. We use 2 layers of GNN operations in the depth estimation branch. We set the standard deviation of the Gaussian peak in the ground truth heatmap to be 0.75, and the bin range of d/f to [α = 1.0, β = 8.0] for a reasonably sufficient range of the depth. We do not see much performance change when different number of bins N B are used. All results of the experiments shown in the paper are obtained with N B = 71. We set λ in Eq. 9 to balance the four loss terms to same order of magnitudes. For training, we use Adam optimizer <ref type="bibr" target="#b13">[14]</ref> with learning rate 1e-4 and batch size 16. We train the model for 200k steps and decay the learning rate with a factor of 0.8 at every 20k steps. The evaluation of each image takes around 7ms with our root joint localization HDNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Human3.6M</head><p>The root joint localization results on Human3.6M dataset are shown in <ref type="table" target="#tab_1">Table  1</ref>. The baselines reported in the top 3 rows follow a two-stage approach, where 2D pose <ref type="bibr" target="#b28">[29]</ref> and 3D pose <ref type="bibr" target="#b17">[18]</ref> are estimated separately, and an optimization process is adopted to obtain the global root joint location that minimizes the reprojection error. "w/o limb joints" refers to optimization using only head and body trunk joints. "with RANSAC" refers to randomly sampling the set of joints used for optimization with RANSAC. The baseline results are taken from the figures reported in <ref type="bibr" target="#b20">[21]</ref>. We also compare with the state-of-the-art approach <ref type="bibr" target="#b20">[21]</ref>. It can be seen from <ref type="table" target="#tab_1">Table 1</ref> that optimization-based methods can achieve reasonable results, but with limited accuracy due to the errors from both the 2D and 3D estimation stages. Our root joint localization framework achieves 69.9mm Method S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg depth estimation error in MRPE z with a 35% improvement over <ref type="bibr" target="#b20">[21]</ref>. Since our approach uses the original scale image without scaling to person bounding box size which limits the 2D (u, v) localization precision, we also adopt a stateof-the-art 2D pose estimator CPN <ref type="bibr" target="#b2">[3]</ref> within the person bounding box area to further refine the (u, v) localization. Our MRPE for root joint achieves an overall performance of 77.6mm which significantly outperforms the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on MuPoTS-3D</head><p>Root Joint Localization. To evaluate our root joint localization performance on the multi-person MuPoTS-3D dataset, we estimate the root joint 3D coordinate for each bounding box detected from the object detector. All root joint candidates are matched with the ground truth root joints, and only candidates with distance to the matched ground truth lesser than a threshold are considered as an accurate estimate. We then analyze the average precision and recall over the whole dataset under various settings of thresholds ranging from 25cm to 10cm. The results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Our method achieves much higher AP and AR consistently across all levels of thresholds compared to the state-ofthe-art approach <ref type="bibr" target="#b20">[21]</ref>.</p><p>Camera-space absolute 3D pose estimation. We also evaluate the cameraspace absolute 3D pose estimation performance with 3DPCK abs . 3DPCK abs compares the estimated 3D pose with the matched ground truth pose in the Method S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg camera coordinate space without root alignment, thus requires highly accurate root joint localization. We use the same 3D pose estimator <ref type="bibr" target="#b28">[29]</ref> as the state-ofthe-art root joint localization method <ref type="bibr" target="#b20">[21]</ref> for a fair comparison. Results in <ref type="table">Table  3</ref> show that our method consistently outperforms the state-of-the-art in most of the test sequences and achieves a 35.2% average 3DPCK (3.4% improvement).</p><p>The performance breakdown of all joint types is shown in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Root-relative 3D pose estimation. The state-of-the-art root-relative 3D pose estimator <ref type="bibr" target="#b28">[29]</ref> adopts a volumetric output representation and estimates the root-relative depth for each joint. Absolute root joint depth has to be available to recover the 3D pose through back-projection. We follow <ref type="bibr" target="#b20">[21]</ref> and use our estimated root depth to back-project the 3D pose and evaluate the root-relative 3D pose estimation accuracy with 3DPCK rel after root joint alignment. Results are shown in <ref type="table">Table 5</ref>, where our method outperforms the previous best performance by 1.2% average 3DPCK. This demonstrates that more accurate root localization also benefits the precise 3D pose estimation in volumetric-based approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>We conduct ablation studies to show how each component in our framework affects the root joint localization accuracy. We evaluate the depth estimation accuracy MRPE z on Human3.6M dataset and the root joint localization AP root 25 on MuPoTS-3D dataset with different variants of our framework in <ref type="table" target="#tab_6">Table 6</ref>. The state-of-the-art approach <ref type="bibr" target="#b20">[21]</ref> is also included for comparison.</p><p>-"Ours direct regression": Performance drop (by 24.6mm and 12.1%) with directly regressing target depth instead of performing classification over binning shows the effectiveness of formulating the depth estimation as a classification task. -"Ours shared feature branch": One single multi-scale feature branch is kept after FPN, which means F pose and F depth use the same feature representation. This setting causes performance to drop (by 2.1mm and 7.5%), and thus demonstrates that the features used for pose estimation and depth estimation are not highly correlated. -"Ours w/o GNN": We replace the GNN layers in our depth estimation branch with same number of fully-connected layers and observe a performance drop (by 3mm and 6.7%), showing the effectiveness of the graph neural network in propagating and refining the features extracted for different types of joints. -"Ours w/o HM pooling": We remove feature pooling with estimated heatmaps as mask in the depth estimation branch and instead apply a global average pooling to obtain a single feature vector. The GNN layers are replaced with fully-connected layers since we do not explicitly differentiate between different joint types. We observe a performance drop (by 1.9mm and 13.4%), which demonstrates the effectiveness of utilizing estimated pose as attention mask for useful feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussions</head><p>We analyze the root joint localization results on the challenging multi-person dataset MuPoTS-3D and observe several sources of large errors as shown in <ref type="figure">Fig</ref>  the person closer to the camera partially occludes the other person farther away <ref type="figure">(Figure 4(a)</ref>). Masking the heatmaps with bounding box cannot effectively remove undesired regions of information and consequently the depth estimation for both persons are affected. The problem of fine-grained target person segmentation will be of interest for future research. (2) Since monocular depth estimation relies on prior knowledge such as typical scale of human bodies, estimation tends to be erroneous when the size of target person is far away from the "average" size, e.g., the target is a child or a relatively short person <ref type="figure">(Figure 4(b)</ref>). Research on person 3D size estimation may complement our depth estimation task and improve the generalizability to persons of different sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we proposed the Human Depth Estimation Network (HDNet), an end-to-end framework to address the problem of accurate root joint localization for multi-person 3D absolute pose estimation. Our HDNet utilizes deep features and demonstrates the capability to precisely estimate depth of root joints. We designed a human-specific pose-based feature aggregation process in the HDNet to effectively pool features from regions of human body joints. Experimental results on multiple datasets showed that our framework significantly outperforms the state-of-the-art in both root joint localization and 3D pose estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Our HDNet architecture. The framework takes an image together with the bounding box of a target person as input. A Feature Pyramid Network backbone is used for general feature extraction followed by separated multi-scale feature extraction for the tasks of pose and depth estimation. Estimated heatmaps are used as attention masks to pool depth features. A Graph Neural Network is utilized to propagate and aggregate features for the target person depth estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(a) ResNet-based Feature Pyramid Network Backbone for general feature extraction. (b) Multi-scale feature extraction subnet architecture used for both Pose feature and Depth feature extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>- ure 4 : ( 1 )Fig. 4 .</head><label>414</label><figDesc>Bounding boxes for two persons tend to have overlapping areas when Typical errors in multi-person root localization. (a) Close and overlapping bounding box regions. (b) Different sizes of target persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results on MuPoTS-3D dataset. Columns are: (1) image with bounding boxes (2) left-front view (3) right-front view (4) top-down view</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>scheme. In recent years, large-scale arXiv:2007.08943v1 [cs.CV] 17 Jul 2020</figDesc><table><row><cell>3D Root Joint Localization</cell><cell>3D Pose Estimation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>MRPE results comparison with state-of-the-arts on the Human3.6M dataset. MRPEx, MRPEy, and MRPEz are the average errors in x, y, and z axes, respectively.</figDesc><table><row><cell>Method</cell><cell cols="4">MRPE MRPEx MRPEy MRPEz</cell></row><row><cell>Baseline</cell><cell>267.8</cell><cell>27.5</cell><cell>28.3</cell><cell>261.9</cell></row><row><cell cols="2">Baseline w/o limb joints 226.2</cell><cell>24.5</cell><cell>24.9</cell><cell>220.2</cell></row><row><cell cols="2">Baseline with RANSAC 213.1</cell><cell>24.3</cell><cell>24.3</cell><cell>207.1</cell></row><row><cell>RootNet [21]</cell><cell>120.0</cell><cell>23.3</cell><cell>23.0</cell><cell>108.1</cell></row><row><cell>Ours</cell><cell>77.6</cell><cell>15.6</cell><cell>13.6</cell><cell>69.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Root joint localization accuracy comparison in average precision and recall with state-of-the-arts on MuPoTS-3D dataset.</figDesc><table><row><cell>Method</cell><cell>AP root 25</cell><cell cols="2">AP root 20</cell><cell>AP root 15</cell><cell cols="2">AP root 10</cell><cell cols="2">AR root 25</cell><cell>AR root 20</cell><cell cols="2">AR root 15</cell><cell>AR root 10</cell></row><row><cell cols="2">RootNet [21] 31.0</cell><cell>21.5</cell><cell></cell><cell>10.2</cell><cell>2.3</cell><cell></cell><cell>55.2</cell><cell></cell><cell>45.3</cell><cell>31.4</cell><cell></cell><cell>15.2</cell></row><row><cell>Ours</cell><cell>39.4</cell><cell cols="2">28.0</cell><cell>14.6</cell><cell>4.1</cell><cell></cell><cell cols="2">59.8</cell><cell>50.0</cell><cell>35.9</cell><cell></cell><cell>19.1</cell></row><row><cell cols="13">Table 3. Sequence-wise 3DPCK abs comparison with state-of-the-arts on MuPoTS-3D</cell></row><row><cell cols="9">dataset. Accuracy is measured on matched ground-truths.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell><cell>S5</cell><cell cols="2">S6</cell><cell>S7</cell><cell>S8</cell><cell>S9</cell><cell>S10</cell><cell>-</cell></row><row><cell cols="13">RootNet [21] 59.5 45.3 51.4 46.2 53.0 27.4 23.7 26.4 39.1 23.6</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="12">21.4 22.7 58.3 27.5 37.3 12.2 49.2 40.8 53.1 43.9</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Joint-wise 3DPCK abs comparison with state-of-the-arts on MuPoTS-3D dataset. Accuracy is measured on matched ground-truths.</figDesc><table><row><cell>Method</cell><cell cols="11">Head Neck Shoulder Elbow Wrist Hip Knee Ankle Avg</cell></row><row><cell cols="3">RootNet [21] 37.6 35.6</cell><cell>34.0</cell><cell></cell><cell>34.1</cell><cell cols="3">30.7 30.6 31.3</cell><cell></cell><cell cols="2">25.3 31.8</cell></row><row><cell>Ours</cell><cell cols="2">38.3 37.8</cell><cell>36.2</cell><cell></cell><cell cols="7">37.4 34.0 34.9 36.4 29.2 35.2</cell></row><row><cell cols="12">Table 5. Sequence-wise 3DPCK rel comparison with state-of-the-arts on MuPoTS-3D</cell></row><row><cell cols="8">dataset. Accuracy is measured on matched ground-truths.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell><cell>S5</cell><cell>S6</cell><cell>S7</cell><cell>S8</cell><cell>S9</cell><cell>S10</cell><cell>-</cell></row><row><cell cols="11">Rogez et al. [26] 69.1 67.3 54.6 61.7 74.5 25.2 48.4 63.3 69.0 78.1</cell><cell>-</cell></row><row><cell cols="11">Mehta et al. [20] 81.0 65.3 64.6 63.9 75.0 30.3 65.1 61.1 64.1 83.9</cell><cell>-</cell></row><row><cell cols="11">Rogez et al. [27] 88.0 73.3 67.9 74.6 81.8 50.1 60.6 60.8 78.2 89.5</cell><cell>-</cell></row><row><cell>RootNet [21]</cell><cell cols="10">94.4 78.6 79.0 82.1 86.6 72.8 81.9 75.8 90.2 90.4</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="10">94.4 79.6 79.2 82.4 86.7 73.0 81.6 76.3 90.1 90.5</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Rogez et al.<ref type="bibr" target="#b25">[26]</ref> 53.8 52.2 60.5 60.9 59.1 70.5 76.0 70.0 77.1 81.4 62.4 Mehta et al. [20] 72.4 69.9 71.0 72.9 71.3 83.6 79.6 73.5 78.9 90.9 70.8 Rogez et al. [27] 70.8 74.4 72.8 64.5 74.2 84.9 85.2 78.4 75.8 74.4 74.0 RootNet [21] 79.4 79.9 75.3 81.0 81.1 90.7 89.6 83.1 81.7 77.3 82.5 Ours 77.9 79.2 78.3 85.5 81.1 91.0 88.5 85.1 83.4 90.5 83.7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Ablation studies on components of the framework. Depth error MRPEz (mm) on Human3.6M dataset and AP root 25 (%) on MuPoTS-3D dataset are measured.</figDesc><table><row><cell>Method</cell><cell cols="2">MRPEz(↓) AP root 25 (↑)</cell></row><row><cell>RootNet [21]</cell><cell>108.1</cell><cell>31.0</cell></row><row><cell>Ours direct regression</cell><cell>94.5</cell><cell>27.3</cell></row><row><cell>Ours shared feature branch</cell><cell>72.0</cell><cell>31.9</cell></row><row><cell>Ours w/o GNN</cell><cell>72.9</cell><cell>32.7</cell></row><row><cell>Ours w/o HM pooling</cell><cell>71.8</cell><cell>26.0</cell></row><row><cell>Ours (full)</cell><cell>69.9</cell><cell>39.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-06" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multi-person 3d human pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A coarse-fine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMAPI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hand pose estimation via latent 2.5 d heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breuel Juergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="118" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICCV. vol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<editor>3DV. IEEE</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<editor>3DV. IEEE</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, partbased, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classificationregression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3433" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="8410" to="8419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
