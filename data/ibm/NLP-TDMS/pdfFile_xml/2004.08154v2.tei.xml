<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detailed 2D-3D Joint Representation for Human-Object Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
							<email>yongluli@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
							<email>xinpengliu0907@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detailed 2D-3D Joint Representation for Human-Object Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-Object Interaction (HOI) detection lies at the core of action understanding. Besides 2D information such as human/object appearance and locations, 3D pose is also usually utilized in HOI learning since its viewindependence. However, rough 3D body joints just carry sparse body information and are not sufficient to understand complex interactions. Thus, we need detailed 3D body shape to go further. Meanwhile, the interacted object in 3D is also not fully studied in HOI learning. In light of these, we propose a detailed 2D-3D joint representation learning method. First, we utilize the singleview human body capture method to obtain detailed 3D body, face and hand shapes. Next, we estimate the 3D object location and size with reference to the 2D humanobject spatial configuration and object category priors. Finally, a joint learning framework and cross-modal consistency tasks are proposed to learn the joint HOI representation. To better evaluate the 2D ambiguity processing capacity of models, we propose a new benchmark named Ambiguous-HOI consisting of hard ambiguous images. Extensive experiments in large-scale HOI benchmark and Ambiguous-HOI show impressive effectiveness of our method. Code and data are available at https: //github.com/DirtyHarryLYL/DJ-RN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-Object Interaction (HOI) detection recently receives lots of attentions. It aims at locating the active human-object and inferring the action simultaneously. As a sub-task of visual relationship <ref type="bibr" target="#b36">[37]</ref>, it can facilitate activity understanding <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b54">55]</ref>, imitation learning <ref type="bibr" target="#b1">[2]</ref>, etc.</p><p>What do we need to understand HOI? The possible answers are human/object appearance, spatial configuration, context, pose, etc. Among them, human body information * Cewu Lu is the corresponding author, member of Qing Yuan Research Institute and MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China.  <ref type="figure">Figure 1</ref>. HOI detection based on 2D may have ambiguities under various viewpoints. HOI representation in 3D is more robust. Thus, we estimate the 3D detailed human body and interacted object location and size to represent the HOI in 3D. Then we learn a joint 2D-3D representation to combine multi-modal advantages.</p><p>often plays an important role, such as 2D pose <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b12">13]</ref> and 3D pose <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b38">39]</ref>. Because of the various viewpoints, 2D human pose <ref type="bibr" target="#b6">[7]</ref> or segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b14">15]</ref> often has ambiguities, e.g. same actions may have very different 2D appearances and poses. Although 3D pose is more robust, rough 3D body joints are not enough to encode essential geometric and meaningful patterns. For example, we may need detailed hand shape to infer the action "use a knife to cut", or facial shape for "eat and talk". And body shape would also largely affect human posture. In light of this, we argue that detailed 3D body can facilitate the HOI learning. Meanwhile, the object in HOI is also important, e.g. "hold an apple" and "hold the horse" have entirely different patterns. However, few studies considered how to embed 3D interacted objects in HOI. The reasons are two-fold: first, it is hard to reconstruct objects because of the 6D pose estimation and diverse object shapes (detailed point cloud or mesh <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b64">65]</ref>). Second, estimating 3D human-object spatial relationship is also difficult for single-view.</p><p>In this work, we propose a method to not only borrow essential discriminated clues from the detailed 3D body but also consider the 3D human-object spatial configuration. First, we represent the HOI in 3D. For human, we utilize the single-view human body capture <ref type="bibr" target="#b46">[47]</ref> to obtain detailed human shape. For object, referring to the 2D human-object spatial configuration and object category prior, we estimate its rough location and size through perspective projection, and use a hollow sphere to represent it. Then, we put the 3D detailed human and object sphere into a normalized volume as the 3D HOI spatial configuration volume, as shown in <ref type="figure">Fig. 1</ref>. Next, we propose Detailed Joint Representation Network (DJ-RN), which consists of two feature extractors: a 2D Representation Network (2D-RN) and a 3D Representation Network (3D-RN). Finally, we adopt several consistency tasks to learn the 2D-3D joint HOI representation. In detail, we align the 2D spatial features according to more robust 3D spatial features. And we perform semantic alignment to ensure the cross-modal semantic consistency. To better embed the body posture, we estimate body part attentions in a 2D-3D joint way with consistency. That is if 2D features tell us the hands and head are important for "work on laptop", so will the 3D features. DJ-RN is the first joint learning method to utilize single-view 3D recover for HOI. It is a novel paradigm instead of an ad-hoc model, and flexible to replace 2D/3D modules/extracted features. We believe it would promote not only HOI learning but also action related tasks, e.g., image caption, visual reasoning.</p><p>To better evaluate the ability of processing 2D ambiguities, we propose a new benchmark named Ambiguous-HOI, which includes ambiguous examples selected from existing datasets like HICO-DET <ref type="bibr" target="#b8">[9]</ref>, V-COCO <ref type="bibr" target="#b19">[20]</ref>, Open-Image <ref type="bibr" target="#b27">[28]</ref>, HCVRD <ref type="bibr" target="#b65">[66]</ref>. We conduct extensive experiments on widely-used HOI detection benchmark and Ambiguous-HOI. Our approach achieves significant improvements with 2D-3D joint learning. The main contributions are as follows: 1) We propose a 2D-3D joint representation learning paradigm to facilitate HOI detection. 2) A new benchmark Ambiguous-HOI is proposed to evaluate the disambiguation ability of models. 3) We achieve stateof-the-art results on HICO-DET <ref type="bibr" target="#b8">[9]</ref> and Ambiguous-HOI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human-Object Interaction Detection. Recently, great progress has been made in HOI detection. Large-scale datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref> have been released to promote this field. Meanwhile, lots of deep learning based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b47">48]</ref> have been proposed. Chao et al. <ref type="bibr" target="#b8">[9]</ref> proposed a multi-stream framework, which is proven effective and followed by subsequent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>. Differently, GPNN <ref type="bibr" target="#b50">[51]</ref> proposed a graph model and used message passing to address both image and video HOI detection. Gkioxari et al. <ref type="bibr" target="#b18">[19]</ref> adopted an action density map to estimate the 2D location of interacted objects. iCAN <ref type="bibr" target="#b16">[17]</ref> utilized self-attention to correlate the human-object and context. TIN <ref type="bibr" target="#b30">[31]</ref> proposed an explicit interactiveness learning network to identify the non-interactive human-object pairs and suppress them in inference. HAKE <ref type="bibr" target="#b29">[30]</ref> proposes a novel hierarchical paradigm based on human body part states <ref type="bibr" target="#b37">[38]</ref>. Previous methods mainly relied on the visual appearance and human-object spatial relative locations, some of them <ref type="bibr" target="#b30">[31]</ref> also utilized the 2D estimated pose. But the 2D ambiguity in HOI is not well studied before. 3D Pose-based Action Recognition. Recent deep learning based 3D pose estimation methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b44">45]</ref> have achieved substantial progresses. Besides 2D pose based action understanding <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b58">59]</ref>, many works also utilized the 3D human pose <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b23">24]</ref>. Yao et al. <ref type="bibr" target="#b62">[63]</ref> constructed a 2.5D graph with 2D appearance and 3D human pose, and selected exemplar graphs of different actions for the exemplar-based action classification. In <ref type="bibr" target="#b63">[64]</ref>, 2D pose is mapped to 3D pose and the actions are classified by comparing the 3D pose similarity. Luvizon et al. <ref type="bibr" target="#b38">[39]</ref> estimated the 2D/3D pose and recognized actions in a unified model from both image and video. Wang et al. <ref type="bibr" target="#b60">[61]</ref> used the RGB-D data to obtain the 3D human joints and adopted an actionlet ensemble method for HOI learning. Recently, Pham et al. <ref type="bibr" target="#b48">[49]</ref> proposed a multi-task model to operate 3D pose estimation and action recognition simultaneously from RGB video. Most 3D pose based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24]</ref> are using Recurrent Nerual Network (RNN) based framework for spatio-temporal action recognition, but few studies focus on the complex HOI understanding from single RGB image. Single-view 3D Body Recover. Recently the single-view human body capture and reconstruction methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b3">4]</ref> have made great progresses. With the help of deep learning and large-scale scanned 3D human database <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1]</ref>, they are able to directly recover 3D body shape and pose from single RGB images. SMPLify-X <ref type="bibr" target="#b46">[47]</ref> is a holistic and efficient model that takes the 2D human body, face and hand poses as inputs to capture 3D body, face and hands. To obtain more accurate and realistic body shape, SMPLify-X <ref type="bibr" target="#b46">[47]</ref> utilizes the Variational Human Body Pose prior (VPoser) trained on large-scale MoCap datasets, which carries lots of human body pose prior and knowledge. It supports us to recover 3D detailed human body from HOI images and embed more body posture knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Representing HOI in 3D</head><p>Our goal is to learn the 2D-3D joint HOI representation, thus we need to first represent HOI in 3D. Given a still image, we use object detection <ref type="bibr" target="#b51">[52]</ref> and pose estimation <ref type="bibr" target="#b6">[7]</ref> to obtain 2D instance boxes and human pose. Next, we adopt the 3D human body capture <ref type="bibr" target="#b46">[47]</ref> to estimate the 3D human body with above 2D detection (Sec. 3.1), and estimate the object location and size in 3D to construct the 3D spatial configuration volume (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Single-view 3D Body Capture</head><p>Rough 3D pose is not sufficient to discriminate various actions, especially the complex interactions with daily ob-  <ref type="figure">Figure 2</ref>. We adopt detailed human information. We use Open-Pose <ref type="bibr" target="#b6">[7]</ref> and SMPLify-X <ref type="bibr" target="#b46">[47]</ref> to estimate the 2D, 3D poses and shapes of face and hands. These information would largely help the HOI learning, especially on actions related to face and hands.</p><p>jects. Thus we need holistic and fine-grained 3D body information as a clue. To this end, we adopt a holistic 3D body capture method <ref type="bibr" target="#b46">[47]</ref> to recover detailed 3D body from single RGB images. Given the 2D detection of image I, i.e., 2D human and object boxes b h and b o , 2D human pose</p><formula xml:id="formula_0">θ 2D = {θ 2D b , θ 2D f , θ 2D h } (main body joints θ 2D b , jaw joints θ 2D f</formula><p>and finger joints θ 2D h in <ref type="figure">Fig. 2</ref>). We input them into SMPLify-X <ref type="bibr" target="#b46">[47]</ref> to recover 3D human estimations, i.e., fitting the SMPL-X [47] model to I and θ 2D . Then we can obtain the optimized shape parameters {θ 3D , β, ψ} by minimizing the body pose, shape objective function, where θ 3D are pose parameters and </p><formula xml:id="formula_1">θ 3D = {θ 3D b , θ 3D f , θ 3D h },</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Spatial Configuration Volume</head><p>After obtaining the 3D body, we further represent HOI in 3D, i.e. estimate the 3D object location and size. For robustness and efficiency, we do not reconstruct the object shape, but use a hollow sphere to represent it. Thus we can avoid the difficult 6D pose estimation under the circumstance of single-view and various categories. Our procedure has two stages: 1) locating the sphere center on a plane according to the camera perspective projection, 2) using the prior object size and human-object distance to estimate the depth of the sphere. For each image, we adopt the estimated camera parameters from SMPLify-X <ref type="bibr" target="#b46">[47]</ref>, where focal length f is set to a fixed value of 5000, and the camera distortions are not considered. For clarification, the camera optical center is expressed as C(t 1 , t 2 , t 3 ) in the world coordinate system, and the object sphere center is O(x O , y O , z O ). Object Sphere Center. As shown in <ref type="figure">Fig. 3</ref>, we assume that O is projected to the midperpendicular of object box bordertop, indicating the sphere center falls on plane P ABC . And we suppose the highest and lowest visible points of the  <ref type="figure">Figure 3</ref>. Object location estimation. Given prior radius r, we can get the sphere center location by solving projection equations, which restricts the sphere to be tangent to plane P1 and P2, and assures the sphere center falls on plane PABC . sphere are projected to the object box border-bottom and border-top respectively. Then we can get two tangent planes of the sphere: P 1 (contains points B, C, E) and P 2 (contains points A, C, D) as shown in <ref type="figure">Fig. 3</ref>. P 1 and P 2 intersect with P ABC , restricting a region on P ABC where the sphere center may locate. To get the depth of sphere center, we need to know the sphere radius, i.e., r = | − − → OD| = | − − → OE|. Object Sphere Radius. As for the sphere radius, we determine it by both considering the object box relative size (to the 2D human box) and the object category prior. With object detection in the first step, we can obtain the object category j. Thus, we can set a rough object size according to Wikipedia and daily life experience. In practice, we set prior sizes for COCO 80 objects <ref type="bibr" target="#b31">[32]</ref> to suit the HICO-DET setting <ref type="bibr" target="#b8">[9]</ref>. First, for small objects or objects with similar size along different axes (e.g. ball, table), we define the prior object scale ratio between the sphere radius and the human shoulder width. Second, for objects that are usually partly seen or whose projection is seriously affected by the 6D pose (e.g. boat, skis), we use the relative scale ratio of the human and object boxes as the referenced ratio. The estimated sphere center is denoted asÔ(</p><formula xml:id="formula_2">x O ,ŷ O ,ẑ O ).</formula><p>The sphere depth is very sensitive to the radius and may make the sphere away from human. Thus, we regularize the estimated depthẑ c using the maximum and minimum depth z max H , z min H of the recovered human. We define prior object depth regularization factor</p><formula xml:id="formula_3">Γ = {[γ min i , γ max i ]} 80</formula><p>i=1 for COCO objects <ref type="bibr" target="#b31">[32]</ref>. Specifically, with pre-defined depth bins (very close, close, medium, far, very far), we invite fifty volunteers from different backgrounds to watch HOI images and choose the degree of the object relative depth to the human. We then use their votes to set the empiri-</p><formula xml:id="formula_4">cal regularization factors Γ. For estimatedÔ(x O ,ŷ O ,ẑ O ), ifẑ O falls out of [γ min j z min H , γ max j z max H ], we shiftÔ to (x O ,ŷ O , γ max j z max H ) or (x O ,ŷ O , γ min j z min H )</formula><p>, depending on which is closer toÔ. Size and depth priors can effectively restrict the error boundaries. Without them, 3D volume would have large deviation and degrade performance.</p><p>Volume Formalization. Next, we perform translations to align different configurations in 3D. First, we set the coordinate origin as the human pelvis. The direction of gravity estimated is kept same with the negative direction of the z-axis, and the line between two human shoulder joints is rotated to be parallel to the x-axis. Then, we down-sample the 3D body to 916 points and randomly sample 312 points on spherical surface. The hollow sphere can keep the body information of the interacted body parts within the sphere. We then normalize the whole volume by setting unit length as the distance between the pupil joints. At last, we can obtain a normalized 3D volume including 3D body and object sphere, which not only carries essential 3D action information but also 3D human-object spatial configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">2D-3D Joint Learning</head><p>In this section, we aim to learn the joint representation. To this end, we propose Detailed Joint Representation Network (DJ-RN), as seen in <ref type="figure">Fig. 5</ref>. DJ-RN has two modules: 2D Representation Network (2D-RN) and 3D Representation Network (3D-RN). We use them to extract features from two modalities respectively (Sec. 4.1, 4.2). Then we align 2D spatial feature with 3D spatial feature (Sec. 4.3), and use body part attention consistency (Sec. 4.4) and semantic consistency (Sec. 4.5) to guide the learning .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">2D Feature Extraction</head><p>2D-RN is composed of human, object, and spatial streams following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. Within each stream, we adopt different blocks to take in 2D information with different properties and extract corresponding features ( <ref type="figure">Fig. 5</ref>). Human/Object Block. Human and object streams mainly utilize visual appearance. We use a COCO <ref type="bibr" target="#b31">[32]</ref> pretrained Faster-RCNN <ref type="bibr" target="#b51">[52]</ref> to extract ROI pooling features from detected boxes. To enhance the representation ability, we adopt the iCAN block <ref type="bibr" target="#b16">[17]</ref> which computes the selfattention via correlating the context and instances, and obtain the human feature f 2D H and object feature f 2D O . Spatial Block. Although appearance carries important clues, it also imports noise and misleading patterns from various viewpoints. Thus human-object spatial configuration can be used additionally to provide discriminative features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. Spatial stream mainly considers the 2D human-object relative locations. We input the 2D pose map and spatial map <ref type="bibr" target="#b30">[31]</ref> to the spatial block, which consists of convolution and fully-connected (FC) layers to extract the spatial feature f 2D sp . The spatial map consists of two channels, human and object maps, which are all 64 × 64 and generated from the human and object boxes. The value is 1 in the box and 0 elsewhere. The pose map consists of 17 joint heatmaps of size 64 × 64 from OpenPose <ref type="bibr" target="#b6">[7]</ref>.  <ref type="figure">Figure 4</ref>. 3D spatial configuration volume. After 3D body capture, we use 2D boxes, estimated camera parameters and object category prior to estimate the 3D object location and size, and then put 3D human and object together in a normalized volume. We also pair the 3D location with semantic knowledge (Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D Feature Extraction</head><p>3D-RN contains a 3D spatial stream with volume block which takes in the 3D spatial configuration volume, and a 3D human stream with 3D body block to encode 3D body. Volume Block. In 3D spatial stream, we adopt Point-Net <ref type="bibr" target="#b49">[50]</ref> to extract 3D spatial feature f 3D sp . We first pre-train it to segment the human and object points in the generated 3D spatial configuration volume. Thus it can learn to discriminate the geometric difference and shape of human and object. Then we use it to extract features from 3D spatial volume point cloud. To further embed the semantic information of 3D locations, we pair the spatial feature with the corresponding semantics, i.e., the word embedding of object or body part category. We first divide the volume point cloud into 18 sets: 17 part sets and an object sphere set. Then, for the feature of part set, we concatenate it with PCA reduced word embedding <ref type="bibr" target="#b40">[41]</ref> of part name (e.g. "hand"). Similarly, for the feature of the sphere set, we concatenate it with the object category word embedding (e.g. "bottle"), as seen in <ref type="figure">Fig. 4</ref>. The concatenated feature is used as f 3D sp . 3D Body Block. In 3D body block, we extract features based on SMPL-X <ref type="bibr" target="#b46">[47]</ref> parameters: joint body, face and hands shape β, face expression ψ and pose θ 3D , consisting of jaw joints θ 3D f , finger joints θ 3D h and body joints θ 3D b . For body shape and expression, we directly use their parameters. For pose, we adopt the VPoser <ref type="bibr" target="#b46">[47]</ref> to encode the 3D body into latent</p><formula xml:id="formula_5">representations {f 3D b , f 3D f , f 3D h } for body, face and hands corresponding to {θ 3D b , θ 3D f , θ 3D h }.</formula><p>VPoser is a variational auto-encoder trained with large-scale Mo-Cap datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1]</ref>. Thus it learns a latent space encoding the manifold of the physically plausible pose, and effectively embeds the 3D body pose. We concatenate the latent representations, shape parameters and face expression, feed them to two 1024 sized FC layers, and get the 3D human feature f 3D  <ref type="figure">Figure 5</ref>. Overview of DJ-RN. The framework consists of two main modules, named 2D Representation Network (2D-RN) and 3D Representation Network (3D-RN). They extract HOI representations from 2D and 3D information respectively. Hence, we can use spatial alignment, part attention consistency and semantic consistency to learn a joint 2D-3D representation for HOI learning.</p><formula xml:id="formula_6">H = F C 3D ({β, ψ, f 3D b , f 3D f , f 3D h }).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">2D-3D Spatial Alignment</head><p>In view of that 2D spatial features lack robustness and may bring in ambiguities, we propose the 2D spatial alignment. 3D spatial features are more robust, thus we refer them as anchors in the spatial space which describes the manifold of HOI spatial configuration. Given the 2D spatial feature f 2D sp of a sample, from the train set we randomly sample a positive 3D spatial feature f 3D sp+ with the same HOI label and a negative feature f 3D sp− with non-overlapping HOIs (a person may perform multiple actions at the same time). For a human-object pair, we use triplet loss <ref type="bibr" target="#b52">[53]</ref> to align its 2D spatial feature, i.e.,</p><formula xml:id="formula_7">L tri = [d(f 2D sp , f 3D sp+ ) − d(f 2D sp , f 3D sp− ) + α] +<label>(1)</label></formula><p>where d(·) indicates the Euclidean distance, and α = 0.5 is the margin value. For 2D samples with the same HOIs but different 2D spatial configurations, this spatial alignment will gather them together in the spatial space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Joint Body Part Attention Estimation</head><p>Body parts are important in HOI understanding, but not all parts make great contributions in inference. Thus, adopting attention mechanism is apparently a good choice. Different from previous methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, we generate body part attention by considering both 2D and 3D clues. Specifically, we use a part attention consistency loss to conduct self-attention learning, as shown in <ref type="figure">Fig. 6</ref>. With the 2D and 3D features, we can generate two sets of body attention. 2D Attention. We concatenate the input f 2D</p><formula xml:id="formula_8">H , f 2D O , f 2D</formula><p>sp to get f 2D , and apply global average pooling (GAP) to get the global feature vector f 2D g . Then we calculate the inner product f 2D g , f 2D and generate the attention map att 2D by att 2D = Sof tmax( f 2D g , f 2D ). Because 2D pose joints can indicate the part locations, we use joint attention to represent 2D part attention. If a joint location has high attention, its neighboring points should have high attention too. Thus we can calculate the pose joint attention by summarizing the attentions of its neighboring points. We represent the attention of 17 pose joints as</p><formula xml:id="formula_9">A 2D = {a 2D i } 17 i=1 , a 2D i = u,v att 2D (u,v) /(1 + d[(u, v), (u i , v i )]) u,v 1/(1 + d[(u, v), (u i , v i )]) ,<label>(2)</label></formula><p>and a 2D    <ref type="figure">Figure 6</ref>. Body part attention alignment. For 2D, we apply selfattention on f 2D sp to generate attention map att 2D and 2D part attention A 2D . For 3D, we use f 3D sp to generate 3D part attention A 3D , and get attention map att 3D using the correspondency between point cloud and joints. Finally, we construct consistency loss Latt with A 3D and A 2D . att 2D and att 3D are used to reweight and generate f 2D * sp and f 3D * sp .</p><formula xml:id="formula_10">i =â 2D i 17 i=1â</formula><formula xml:id="formula_11">u i , v i ); if (u, v) is close to (u i , v i ), it</formula><p>Attention Consistency. Whereafter, we operate the attention alignment via an attention consistency loss:</p><formula xml:id="formula_12">L att = 17 i a 2D i ln a 2D i a 3D i .<label>(3)</label></formula><p>where a 2D i and a 3D i are 2D and 3D attentions of the i-th joint. L att is the KullbackLeibler divergence between A 2D and A 3D , which enforces two attention estimators to generate similar part importance and keep the consistency.</p><p>Next, in 2D-RN, we multiply f 2D sp by att 2D , i.e. the Hadamard product f 2D * sp =f 2D sp • att 2D . In 3D-RN, we first assign attention to each 3D point in the spatial configuration volume (n points in total). For human 3D points, we divide them into different sets according to 17 joints, and each set is corresponding to a body part. Within the i-th set, we tile the body part attention a 3D i to each point. For object 3D points, we set all their attention as one. Because each element of f 3D sp is corresponding to a 3D point in the spatial configuration volume, we organize the attentions of both human and object 3D points as att 3D of size n × 1, where n is the number of elements in f 3D sp <ref type="figure">(Fig. 6</ref>). Thus we can calculate the Hadamard product f 3D * sp =f 3D sp • att 3D . After the part feature re-weighting, our model can learn to neglect the parts unimportant to the HOI inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">2D-3D Semantic Consistency</head><p>After the feature extraction and re-weighting, we perform the HOI classification. All classifiers in each stream are composed of two 1024 sized FC layers and Sigmoids. The HOI score of the 2D-RN is S 2D = (s 2D 2D and 3D representations, i.e. they should make the same prediction for the same sample, we construct:</p><formula xml:id="formula_13">H + s 2D O ) • s 2D sp , where s 2D H , s 2D O , s 2D</formula><formula xml:id="formula_14">L sem = m i ||S 2D i − S 3D i || 2 ,<label>(4)</label></formula><p>where m is the number of HOIs.</p><p>Multiple HOI Inferences. Moreover, we concatenate the features from the last FC layers in 2D-RN and 3D-RN as f joint (early fusion), and make the third classification to obtain the score S joint . The joint classifier is also composed of two 1024 sized FC layers and Sigmoids. The multi-label classification cross-entropy losses are expressed as L 2D cls , L 3D cls , L joint cls . Thus, the total loss of DJ-RN is:</p><formula xml:id="formula_15">L total = λ 1 L tri + λ 2 L att + λ 3 L sem + λ 4 L cls ,<label>(5)</label></formula><p>where L cls =L 2D cls +L 3D cls +L joint cls , and we set λ 1 =0.001, λ 2 =0.01, λ 3 =0.01, λ 4 =1 in experiments. The final score is</p><formula xml:id="formula_16">S = S 2D + S 3D + S joint .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>In this section, we first introduce the adopted datasets and metrics. Then we describe the detailed implementation of DJ-RN. Next, we compare DJ-RN with the state-of-theart on HICO-DET <ref type="bibr" target="#b8">[9]</ref> and Ambiguous-HOI. At last, ablation studies are operated to evaluate modules in DJ-RN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ambiguous-HOI</head><p>Existing benchmarks mainly focus on evaluating generic HOIs, but not to specially examine the ability to process 2D pose and appearance ambiguities. Hence, we propose a new benchmark named Ambiguous-HOI. Ambiguous-HOI consists of hard examples collected from the test set of HICO-DET <ref type="bibr" target="#b8">[9]</ref>, and other whole datasets such as V-COCO <ref type="bibr" target="#b19">[20]</ref>, OpenImage <ref type="bibr" target="#b27">[28]</ref>, HCVRD <ref type="bibr" target="#b65">[66]</ref> and Internet images. We choose HOI categories from HICO-DET <ref type="bibr" target="#b8">[9]</ref> for its welldesigned verbs and objects. For Internet images, we labeled the HOIs according to HICO-DET setting. The 2D pose and spatial configuration ambiguities are mainly considered in the selection. First, we put all images and corresponding labels in a candidate pool and manually choose some template 2D pose samples for each HOI. Then we use Procrustes transformation <ref type="bibr" target="#b4">[5]</ref> to align the 2D pose of samples to the templates. Next, we cluster all samples to find the samples far from the cluster center and repeat clustering according to different templates. The mean distance between a sample and multiple cluster centers is recorded as reference. Meanwhile, we train an MLP taking the 2D pose and spatial map as inputs on HICO-DET train set. Then we use it as an ambiguity probe to find the most easily misclassified samples. Combining the above two references, we finally select 8,996 images with 25,188 annotated humanobject pairs. Ambiguous-HOI finally includes 87 HOI categories, consisting of 48 verbs and 40 object categories from HICO-DET <ref type="bibr" target="#b8">[9]</ref>. Some sample are shown in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Dataset and Metric</head><p>Dataset. We adopt the widely-used HOI benchmark HICO-DET <ref type="bibr" target="#b8">[9]</ref> and our novel Ambiguous-HOI. HICO-DET <ref type="bibr" target="#b8">[9]</ref> is an instance-level benchmark consisting of 47,776 images <ref type="bibr" target="#b37">(38,</ref><ref type="bibr">118</ref> for training and 9,658 for testing) and 600 HOI categories. It contains 80 object categories from COCO <ref type="bibr" target="#b31">[32]</ref>, 117 verbs and more than 150k annotated HOI pairs. Metric. We use mAP metric from <ref type="bibr" target="#b8">[9]</ref> for two benchmarks: true positive need to contain accurate human and object locations (box IoU with reference to the ground truth box is larger than 0.5) and accurate interaction/verb classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>For 3D body recovery, we first use OpenPose <ref type="bibr" target="#b6">[7]</ref> to detect the 2D pose of body, face and hands. Then we feed them with the image to SMPLify-X <ref type="bibr" target="#b46">[47]</ref> to get 3D body. Since cases with severe occlusion might fail the 3D recovery, we only recover 3D bodies for those which at least includes detected 2D head, pelvis, one shoulder and one hip joints. For the rest, we assign them the body with standard template 3D pose, i.e. generated by setting all SMPL-X parameters to zero. Sometimes the recovered body can be implausible, i.e. "monsters". To exclude them, we use VPoser <ref type="bibr" target="#b46">[47]</ref> to extract the latent embedding of every recovered 3D body. With the mean latent embedding of the generated 3D body from HICO-DET train set as a reference, we assume that the farthest 10% embeddings from the mean embedding are "monsters". At last, 81.2% of the annotated instances are assigned with SMPLify-X <ref type="bibr" target="#b46">[47]</ref> generated mesh, and we assign standard templates for the rest to avoid importing noise.</p><p>For feature extraction, we use COCO <ref type="bibr" target="#b31">[32]</ref> pre-trained ResNet-50 <ref type="bibr" target="#b21">[22]</ref> in 2D-RN. In 3D-RN, we first train a Point-Net <ref type="bibr" target="#b49">[50]</ref> to segment the human and object points in 3D volume, and then use it to extract the 3D local feature of volume. The PointNet is trained for 10K iterations, using SGD with learning rate of 0.01, momentum of 0.9 and batch size of 32. In spatial alignment, we adopt the triplet loss with semi-hard sampling, i.e., for a sample, we only calculate the loss for its nearest negative and farthest positive samples in the same mini-batch with respect to their Euclidean distances. In joint training, we train the whole model for 400K iterations, using SGD with momentum of 0.9, following cosine learning rate restart <ref type="bibr" target="#b35">[36]</ref> with initial learning rate of 1e-3. For a fair comparison, we use object detection from iCAN <ref type="bibr" target="#b16">[17]</ref>. We also adopt the Non-Interaction Suppression (NIS) and Low-grade Instance Suppression (LIS) <ref type="bibr" target="#b30">[31]</ref> in inference. The interactiveness model from <ref type="bibr" target="#b30">[31]</ref> is trained on HICO-DET train set only. The thresholds of NIS are 0.9 and 0.1 and LIS parameters follow <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results and Comparisons</head><p>HICO-DET. We demonstrate our quantitative results in Tab. 1, compared with state-of-the-art methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b47">48]</ref>. The evaluation follows the settings in HICO-DET <ref type="bibr" target="#b8">[9]</ref>: Full(600 HOIs), Rare(138 HOIs) and Non-Rare(462 HOIs) in Default and Known Object mode. We also evaluate different streams in our model, i.e. 2D (S 2D ), 3D (S 3D ) and Joint (S joint ). Our 2D-RN has a similar multi-stream structure, object detection and backbone following HO-RCNN <ref type="bibr" target="#b8">[9]</ref>, iCAN <ref type="bibr" target="#b16">[17]</ref>, Interactiveness <ref type="bibr" target="#b30">[31]</ref> and PMFNet <ref type="bibr" target="#b57">[58]</ref>. With joint learning, 2D-RN (S 2D ) directly outperforms above methods with 13.53, 6.50, 4.31, 3.88 mAP on Default Full set. This strongly proves the effectiveness of the consistency tasks in joint learning. Meanwhile, 3D-RN (S 3D ) achieves 12.41 mAP on Default Full set and shows obvious complementarity for 2D-RN. Especially, 3D performs better on Rare set than Non-Rare set. This suggests that 3D representation has much weaker datadependence than 2D representation and is less affected by the long-tail data distribution. Joint learning (S Joint ) performs better than both 2D and 3D, achieving 20.61 mAP, while unified DJ-RN (late fusion) finally achieves 21.34 mAP, which outperforms the latest state-of-the-art [48] with 1.94 mAP. Facilitated by the detailed 3D body information, we achieve 21.71 mAP on 356 hand-related HOIs, which is higher than the 21.34 mAP on 600 HOIs. Ambiguous-HOI. To further evaluate our method, we conduct an experiment on the proposed Ambiguous-HOI. We choose methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48]</ref> with open-sourced code as baselines. All models are trained on HICO-DET train set art tion ge art tion </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ambiguious-HOI iCAN <ref type="bibr" target="#b16">[17]</ref> 8.14 Interactiveness <ref type="bibr" target="#b30">[31]</ref> 8.22 Julia et al. <ref type="bibr" target="#b47">[48]</ref> 9.72 DJ-RN 10.37  Visualizations. We visualize the part attention in <ref type="figure" target="#fig_8">Fig. 8</ref>. We can find that two kinds of attention are aligned well and both capture essential parts for various HOIs. We also visualize  HOI predictions paired with estimated 3D spatial configuration volumes in <ref type="figure" target="#fig_9">Fig. 9</ref>. Our method performs robustly in HOI inference and 3D spatial configuration estimation. Time Complexity. 2D-RN has similar complexity with iCAN <ref type="bibr" target="#b16">[17]</ref> and Interactiveness <ref type="bibr" target="#b30">[31]</ref>. 3D-RN is very efficient because of the pre-extracted features (about 50 FPS).</p><p>SMPLif-X runs with GPU acceleration is about 5 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Study</head><p>We evaluate different components of our method on HICO-DET. The results are shown in Tab. 3. 3D Formats. Using 3D pose or point cloud for 3D body block in 3D-RN performs worse than VPoser embedding. 3D Human Inputs. Without detailed face and hand shape, DJ-RN shows obvious degradation, especially DJ-RN without hand shape. Because about 70% verbs in HICO-DET are hand-related, which is consistent with daily experience. Blocks. Without volume or body block in 3D-RN hurts the performance with 1.00 and 1.33 mAP. Losses: Without L att , L tri and L sem , the performance degrades 0.64, 0.51 and 0.54 mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel 2D-3D joint HOI representation learning paradigm, DJ-RN. We first represent the HOI in 3D with detailed 3D body and estimated object location and size. Second, a 2D Representation Network and a 3D Representation Network are proposed to extract multi-modal features. Several cross-modal consistency tasks are finally adopted to drive the joint learning. On HICO-DET and our novel benchmark Ambiguous-HOI, DJ-RN achieves state-of-the-art results. <ref type="figure">Figure 10</ref>. Visualized human body part attentions and estimated 3D spatial configuration volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visualized Results</head><p>We report more 2D and 3D visualized body part attentions and estimated 3D spatial configuration volumes in <ref type="figure">Fig.10</ref>. From the results we can find that, our method can well handle various human-object interactions, e.g., from the small bottle to large chair, from local action "hold something" to whole body action "sit on something". Meanwhile, our method can also well estimated the interacted object size and location, e.g. from the simple banana to the hard chair. An interesting case is in the fourth row, the image includes a baby who sits in a big chair. We can find that our method successfully estimates appropriate chair size relative to the baby, and the location is also accurate which completely covers the baby in 3D volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prior Object Size and Depth Regularization Factor</head><p>When estimating the 3D spatial configuration volume, we utilize the prior object information collected from volunteers. The lists of the prior size and depth regularization factors of COCO 80 object <ref type="bibr" target="#b31">[32]</ref> in the HICO-DET <ref type="bibr" target="#b8">[9]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Volunteer Backgrounds</head><p>About 50 volunteers took part in our prior object information collection. The volunteer backgrounds are detailed in Tab. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Spatial Alignment Illustration</head><p>In addition, we give a visualized illustration of the spatial alignment between 2D and 3D spatial features in latent space. High School (25, 50%), Bachelor (13, 26%), Master (9, 18%), PhD (3, 6%) Major Law (2, 4%), Agriculture (9, 18%), Economics (1, 2%), Education (9, 18%), Medicine (5, 10%), Engineering (24, 48%)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Characteristics of Ambiguous-HOI</head><p>Ambiguous-HOI contains 87 kinds of HOIs, which consists of 40 verb categories and 48 object categories. The detailed statistics of Ambiguous-HOI are shown in Tab. 6, which includes the selected object categories, verbs and the number of annotated human-object pairs of each HOI. We also illustrate the detailed comparison between our method and TIN <ref type="bibr" target="#b30">[31]</ref> on Ambiguous-HOI in <ref type="figure" target="#fig_10">Fig. 11</ref>. We can find that our DJ-RN outperforms TIN on various HOIs and shows the effectiveness of the detailed 2D-3D joint HOI representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Verb</head><p>Human <ref type="table" target="#tab_3">-Object  Object  Verb  Human-Object  surfboard  load  46  carrot  stir  19  orange  wash  123  wine glass  wash  6  bottle  open  10  car  direct  7  bus  inspect  16  frisbee  spin  202  apple  wash  48  bowl  lick  5  spoon  wash  73  boat  wash  23  person  teach  13</ref>   <ref type="table" target="#tab_3">clean  7  cow  kiss  5  car  board  10  elephant  wash  42  dog  chase  121  giraffe  ride  20  vase  paint  64  backpack  open  7  surfboard  sit on  210  cat  kiss  136  knife  stick  224  dog  groom  21  horse  feed  6  giraffe  pet  8  hair drier  repair  5  dog  wash  229  umbrella  open  28  horse  load  209  teddy bear  kiss  49  boat  exit  268  train  exit  63  person  hug  211  car  park  13  backpack  inspect  211  sheep  wash  9  sheep  pet  13  motorcycle  wash  152  toaster  repair  13  bed  clean  6  sports ball  hold  221  skis  wear  264  bus  wash  24  sports ball  block  22  dog  feed  15  train  load  100  bird  chase  61  airplane  exit  23  book  carry  67  dog  run  233  kite  assemble  23  baseball bat  carry  17  fork  wash  7  couch  carry  13  bus  load  31  fire hydrant  open  5  person  greet  221  cow  ride  18  giraffe  kiss  49  dog  straddle  98  refrigerator  hold  127  car  inspect  29  airplane  inspect  52  parking meter  pay  12  car  wash  45  cow  walk  50  bird  release  78  horse  hop on  6  toilet  clean  5  elephant  hose  134  kite  inspect  238   Table 6</ref>. The selected HOIs of Ambiguous-HOI. "H-O" is the number of the annotated human-object pairs of the corresponding HOI.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>β are body, face and hands shape parameters, ψ are facial expression parameters. The template body mesh is finally blended and deformed to fit the target body posture and shape in images. With function M (θ 3D , β, ψ) : R |θ 3D |×|β|×|ψ| → R 3N , we can directly generate the 3D body mesh according to the estimated {θ 3D , β, ψ} from images and utilize it in the next stage, some examples are shown in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>2D i , where (u, v) denotes arbitrary point on attention map att 2D , (u i , v i ) indicates the coordinate of the i-th joint (calculated by scaling the joint coordinate on image). d[·] denotes the Euclidean distance between two points. Eq. 2 means: if point (u, v) is far from (u i , v i ), the attention value of (u, v) contributes less to the attention value of (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>sp are the scores of human, object and spatial stream. S 3D = s 3D H + s 3D sp indicates the final prediction of the 3D-RN. To maintain the semantic consistency of repair toaster pick up sports ball Ambiguous samples from Ambiguous-HOI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Visualized attention. Three rows are images, 2D and 3D attentions respectively. Red indicates high attention and blue is the opposite. 2D attention is in line with 3D attention, and they both capture reasonable part attentions for various HOIs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Visualized results and the corresponding 3D volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Performance comparison between our method and TIN [31] on Ambiguous-HOI. Background Content Age 18-20 (9, 18%), 21-25 (23, 46%), 26-30 (18, 36%) Education</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Illustration of the spatial alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>contributes more. After the summarizing and normalizing, we finally obtain the attention of (u i , v i ), i.e. a 2D i . 3D Attention. We use 3D joint attention to represent the 3D body part attention. Input f 3D sp is [1228 × 384] and f 3D Then we apply GAP to f 3D to get a [1408] tensor, and feed it to two 512 sized FC layers and Softmax, finally obtain the attention for 17 joints, i.e., A 3D = {a 3D</figDesc><table /><note>H is [1024]. We first tile f 3D H 1228 times to get shape [1228 × 1024], then concatenate it with f 3D sp to get f 3D ([1228 × 1408]).j } 17 j=1 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Results comparison on Ambiguous-HOI.</figDesc><table><row><cell></cell><cell></cell><cell>Default</cell><cell></cell><cell></cell><cell>Known Object</cell></row><row><cell>Method</cell><cell>Full</cell><cell cols="2">Rare Non-Rare</cell><cell>Full</cell><cell>Rare Non-Rare</cell></row><row><cell>DJ-RN</cell><cell cols="2">21.34 18.53</cell><cell>22.18</cell><cell cols="2">23.69 20.64</cell><cell>24.60</cell></row><row><cell>3D Pose</cell><cell cols="2">20.42 16.88</cell><cell>21.47</cell><cell cols="2">22.95 19.48</cell><cell>23.99</cell></row><row><cell>Point Cloud</cell><cell cols="2">20.05 16.52</cell><cell>21.10</cell><cell cols="2">22.61 19.11</cell><cell>23.66</cell></row><row><cell>w/o Face</cell><cell cols="2">21.02 17.56</cell><cell>22.05</cell><cell cols="2">23.48 19.80</cell><cell>24.58</cell></row><row><cell>w/o Hands</cell><cell cols="2">20.83 17.36</cell><cell>21.87</cell><cell cols="2">23.40 19.99</cell><cell>24.41</cell></row><row><cell>w/o Face &amp; Hands</cell><cell cols="2">20.74 17.36</cell><cell>21.75</cell><cell cols="2">23.33 19.82</cell><cell>24.37</cell></row><row><cell>w/o Volume Block</cell><cell cols="2">20.34 17.19</cell><cell>21.28</cell><cell cols="2">22.97 19.94</cell><cell>23.87</cell></row><row><cell cols="3">w/o 3D Body Block 20.01 16.14</cell><cell>21.17</cell><cell cols="2">22.73 18.88</cell><cell>23.88</cell></row><row><cell>w/o L att</cell><cell cols="2">20.70 16.56</cell><cell>21.93</cell><cell cols="2">23.32 19.13</cell><cell>24.57</cell></row><row><cell>w/o L tri</cell><cell cols="2">20.83 17.66</cell><cell>21.77</cell><cell cols="2">23.50 20.31</cell><cell>24.45</cell></row><row><cell>w/o L sem</cell><cell cols="2">20.80 17.51</cell><cell>21.78</cell><cell cols="2">23.45 20.27</cell><cell>24.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results of ablation studies.</figDesc><table><row><cell>and achieve respective best performances. To test the abil-</cell></row><row><cell>ity of disambiguation and generalization, we directly test</cell></row><row><cell>all models on Ambiguous-HOI. Ambiguous-HOI is much</cell></row><row><cell>more difficult, thus all methods get relatively low scores</cell></row><row><cell>(Tab. 2). DJ-RN outperforms previous method by 0.65, 2.15</cell></row><row><cell>and 2.23 mAP. This strongly verifies the advantage of our</cell></row><row><cell>joint representation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>are shown in Tab. 4. Object category Ratio γ min γ max Object category Ratio γ min γ max</figDesc><table><row><cell>airplane</cell><cell>195.640</cell><cell>1.0</cell><cell>1.0</cell><cell>apple</cell><cell>0.205</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>backpack</cell><cell>0.769</cell><cell>1.0</cell><cell>1.0</cell><cell>banana</cell><cell>0.385</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>baseball bat</cell><cell>2.564</cell><cell>1.0</cell><cell>1.0</cell><cell>baseball glove</cell><cell>0.769</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>bear</cell><cell>5.128</cell><cell>0.7</cell><cell>1.3</cell><cell>bed</cell><cell>5.128</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>bench</cell><cell>3.128</cell><cell>1.0</cell><cell>1.0</cell><cell>bicycle</cell><cell>1.051</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>bird</cell><cell>0.718</cell><cell>0.7</cell><cell>1.3</cell><cell>boat</cell><cell>12.821</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>book</cell><cell>0.590</cell><cell>1.0</cell><cell>1.0</cell><cell>bottle</cell><cell>0.769</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>bowl</cell><cell>0.487</cell><cell>1.0</cell><cell>1.0</cell><cell>broccoli</cell><cell>0.256</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>bus</cell><cell>3.590</cell><cell>1.0</cell><cell>1.0</cell><cell>cake</cell><cell>0.462</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>car</cell><cell>9.744</cell><cell>1.0</cell><cell>1.0</cell><cell>carrot</cell><cell>0.103</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>cat</cell><cell>1.179</cell><cell>1.0</cell><cell>1.0</cell><cell>cell phone</cell><cell>0.333</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>chair</cell><cell>1.103</cell><cell>1.0</cell><cell>1.0</cell><cell>clock</cell><cell>0.718</cell><cell>0.7</cell><cell>1.3</cell></row><row><cell>couch</cell><cell>4.744</cell><cell>1.0</cell><cell>1.0</cell><cell>cow</cell><cell>4.359</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>cup</cell><cell>0.564</cell><cell>1.0</cell><cell>1.0</cell><cell>dining table</cell><cell>4.615</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>dog</cell><cell>1.231</cell><cell>1.0</cell><cell>1.0</cell><cell>donut</cell><cell>0.128</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>elephant</cell><cell>7.436</cell><cell>0.8</cell><cell>1.2</cell><cell>fire hydrant</cell><cell>0.308</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>fork</cell><cell>0.410</cell><cell>1.0</cell><cell>1.0</cell><cell>frisbee</cell><cell>0.513</cell><cell>0.6</cell><cell>1.4</cell></row><row><cell>giraffe</cell><cell>10.769</cell><cell>0.8</cell><cell>1.2</cell><cell>hair drier</cell><cell>0.513</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>handbag</cell><cell>1.385</cell><cell>0.8</cell><cell>1.2</cell><cell>horse</cell><cell>5.385</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>hot dog</cell><cell>0.385</cell><cell>1.0</cell><cell>1.0</cell><cell>keyboard</cell><cell>0.641</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>kite</cell><cell>2.051</cell><cell>0.5</cell><cell>1.5</cell><cell>knife</cell><cell>0.410</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>laptop</cell><cell>0.846</cell><cell>1.0</cell><cell>1.0</cell><cell>microwave</cell><cell>1.154</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>motorcycle</cell><cell>3.949</cell><cell>1.0</cell><cell>1.0</cell><cell>mouse</cell><cell>0.256</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>orange</cell><cell>0.179</cell><cell>1.0</cell><cell>1.0</cell><cell>oven</cell><cell>1.538</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>parking meter</cell><cell>4.103</cell><cell>0.8</cell><cell>1.2</cell><cell>person</cell><cell>4.487</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>pizza</cell><cell>0.769</cell><cell>1.0</cell><cell>1.0</cell><cell>potted plant</cell><cell>0.590</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>refrigerator</cell><cell>4.231</cell><cell>1.0</cell><cell>1.0</cell><cell>remote</cell><cell>0.513</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>sandwich</cell><cell>0.359</cell><cell>1.0</cell><cell>1.0</cell><cell>scissors</cell><cell>0.385</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>sheep</cell><cell>3.333</cell><cell>0.8</cell><cell>1.2</cell><cell>sink</cell><cell>1.282</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>skateboard</cell><cell>1.821</cell><cell>1.0</cell><cell>1.0</cell><cell>skis</cell><cell>3.846</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>snowboard</cell><cell>3.949</cell><cell>1.0</cell><cell>1.0</cell><cell>spoon</cell><cell>0.410</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>sports ball</cell><cell>1.795</cell><cell>0.8</cell><cell>1.2</cell><cell>stop sign</cell><cell>3.923</cell><cell>0.6</cell><cell>1.4</cell></row><row><cell>suitcase</cell><cell>1.615</cell><cell>1.0</cell><cell>1.0</cell><cell>surfboard</cell><cell>6.231</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>teddy bear</cell><cell>2.462</cell><cell>1.0</cell><cell>1.0</cell><cell>tennis racket</cell><cell>1.897</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>tie</cell><cell>1.308</cell><cell>1.0</cell><cell>1.0</cell><cell>toaster</cell><cell>0.641</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>toilet</cell><cell>1.103</cell><cell>1.0</cell><cell>1.0</cell><cell>toothbrush</cell><cell>0.436</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>traffic light</cell><cell>0.651</cell><cell>0.6</cell><cell>1.4</cell><cell>train</cell><cell>512.82</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>truck</cell><cell>5.385</cell><cell>1.0</cell><cell>1.0</cell><cell>tv</cell><cell>1.821</cell><cell>0.7</cell><cell>1.3</cell></row><row><cell>umbrella</cell><cell>2.949</cell><cell>1.0</cell><cell>1.0</cell><cell>vase</cell><cell>0.846</cell><cell>0.8</cell><cell>1.2</cell></row><row><cell>wine glass</cell><cell>0.462</cell><cell>1.0</cell><cell>1.0</cell><cell>zebra</cell><cell>6.154</cell><cell>0.8</cell><cell>1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Object prior size ratio relative to the human shoulder width and object prior depth regularization factor Γ = {γ min i</figDesc><table><row><cell>, γ max i</cell><cell>} 80 i=1 .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Volunteer backgrounds.</figDesc><table><row><cell>Positive 3D</cell><cell></cell></row><row><cell>2D pose &amp; spatial</cell><cell></cell></row><row><cell cols="2">walk with bike</cell></row><row><cell>walk with</cell><cell></cell></row><row><cell>bike</cell><cell></cell></row><row><cell>Negative 3D</cell><cell>ride bike</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of robot learning from demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Brenna D Argall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and autonomous systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Poseconditioned spatio-temporal attention for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Modern multidimensional scaling: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Groenen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Cmu mocap dataset</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rpan: An end-toend recurrent pose-attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pairwise body-part attention for recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkun</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Instaboost: Boosting instance segmentation via probability map guided copypasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ican: Instancecentric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1808.10437</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1704.07333</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1505.04474</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Nofrills human-object interaction detection: Factorization, appearance and layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05967</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning on lie groups for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengde</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Senjian An, Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>arXiv preprint arXiv::1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for skeleton-based action recognition using temporal sliding lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seoungyoon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hake: Human activity knowledge engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06539</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global context-aware attention lstm networks for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond holistic object recognition: Enriching image understanding with part states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning features combination for human action recognition from skeleton sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Diogo Carbonera Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep rnn framework for visual sequential applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Further understanding videos through adverbs: A new video task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Detecting rare visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A unified deep framework for joint 3d pose estimation and action recognition from a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houssam</forename><surname>Huy Hieu Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louahdi</forename><surname>Salmane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Khoudour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Crouzil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">A</forename><surname>Zegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Velastin</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1907.06968</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Find and focus: Retrieve and localize video events with natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1909.08453</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Srda: Generating instance segmentation annotation via scanning, reasoning and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Action recognition with exemplar based 2.5 d graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Discovering object functionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Prin: Pointwise rotationinvariant network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09361</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Ian Reid, and Anton van den Hengel. Care about you: towards largescale human-centric visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09892</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
