<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eu</forename><surname>Wern Teh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
							<email>terrance@uoguelph.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<email>gwtaylor@uoguelph.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Metric Learning</term>
					<term>Zero-Shot Learning</term>
					<term>Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of distance metric learning (DML), where the task is to learn an effective similarity measure between images. We revisit ProxyNCA and incorporate several enhancements. We find that low temperature scaling is a performance-critical component and explain why it works. Besides, we also discover that Global Max Pooling works better in general when compared to Global Average Pooling. Additionally, our proposed fast moving proxies also addresses small gradient issue of proxies, and this component synergizes well with low temperature scaling and Global Max Pooling. Our enhanced model, called ProxyNCA++, achieves a 22.9 percentage point average improvement of Recall@1 across four different zero-shot retrieval datasets compared to the original ProxyNCA algorithm. Furthermore, we achieve state-of-theart results on the CUB200, Cars196, Sop, and InShop datasets, achieving Recall@1 scores of 72.2, 90.1, 81.4, and 90.9, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distance Metric Learning (DML) is the task of learning effective similarity measures between examples. It is often applied to images, and has found numerous applications such as visual products retrieval <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1]</ref>, person re-identification <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30]</ref>, face recognition <ref type="bibr" target="#b21">[22]</ref>, few-shot learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14]</ref>, and clustering <ref type="bibr" target="#b10">[11]</ref>. In this paper, we focus on DML's application on zero-shot image retrieval <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13]</ref>, where the task is to retrieve images from previously unseen classes.</p><p>Proxy-Neighborhood Component Analysis (ProxyNCA) <ref type="bibr" target="#b16">[17]</ref> is a proxy-based DML solution that consists of updatable proxies, which are used to represent class distribution. It allows samples to be compared with these proxies instead of one another to reduce computation. After the introduction of ProxyNCA, there are very few works that extend ProxyNCA <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b20">21]</ref>, making it less competitive when compared with recent DML solutions <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Our contributions are the following: First, we point out the difference between NCA and ProxyNCA, and propose to use proxy assignment probability which aligns ProxyNCA with NCA <ref type="bibr" target="#b6">[7]</ref>. Second, we explain why low temperature scaling works and show that it is a performance-critical component of ProxyNCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2004.01113v2 [cs.CV] 23 Jul 2020</head><p>Third, we explore different global pooling strategies and find out that Global Max Pooling (GMP) outperforms the commonly used Global Average Pooling (GAP), both for ProxyNCA and other methods. Fourth, we suggest using faster moving proxies that compliment well with both GMP and low temperature scaling, which also address the small gradient issue due to L 2 -Normalization of proxies. Our enhanced ProxyNCA, which we called ProxyNCA++, has a 22.9 percentage points of improvement over ProxyNCA on average for Recall@1 across four different zero-shot retrieval benchmarks (performance gains are highlighted in <ref type="figure" target="#fig_0">Figure 1</ref>). In addition, we also achieve state-of-the-art performance on all four benchmark dataset across all categories.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The core idea of Distance Metric Learning (DML) is to learn an embedding space where similar examples are attracted, and dissimilar examples are repelled. To restrict the scope, we limit our review to methods that consider image data. There is a large body of work in DML, and it can be traced back to the 90s, where Bromley et al. <ref type="bibr" target="#b1">[2]</ref> designed a Siamese neural network to verify signatures. Later, DML was used in facial recognition, and dimensionality reduction in the form of a contrastive loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, where pairs of similar and dissimilar images are selected, and the distance between similar pairs of images is minimized while the distance between dissimilar images is maximized. Like contrastive loss, which deals with the actual distance between two images, triplet loss optimizes the relative distance between positive pair (an anchor image and an image similar to anchor image) and negative pair (an anchor image and an image dissimilar to anchor image) <ref type="bibr" target="#b2">[3]</ref>. In addition to contrastive and triplet loss, there is a long line of work which proposes new loss functions, such as angular loss <ref type="bibr" target="#b30">[31]</ref>, histogram loss <ref type="bibr" target="#b25">[26]</ref>, margin-based loss <ref type="bibr" target="#b32">[33]</ref>, and hierarchical triplet loss <ref type="bibr" target="#b5">[6]</ref>. Wang et al. <ref type="bibr" target="#b31">[32]</ref> categorize this group as paired-based DML.</p><p>One weakness of paired-based methods is the sampling process. First, the number of possible pairs grows polynomially with the number of data points, which increases the difficulty of finding an optimal solution. Second, if a pair or triplet of images is sampled randomly, the average distance between two samples is approximately âˆš 2-away <ref type="bibr" target="#b32">[33]</ref>. In other words, a randomly sampled image is highly redundant and provides less information than a carefully chosen one.</p><p>In order to overcome the weakness of paired-based methods, several works have been proposed in the last few years. Schroff et al. <ref type="bibr" target="#b21">[22]</ref> explore a curriculum learning strategy where examples are selected based on the distances of samples to the anchored images. They use a semi-hard negative mining strategy to select negative samples where the distances between negative pairs are at least greater than the positive pairs. However, such a method usually generates very few semi-hard negative samples, and thus requires very large batches (on the order of thousands of samples) in order to be effective. Song et al. <ref type="bibr" target="#b22">[23]</ref> propose to utilize all pair-wise samples in a mini-batch to form triplets, where each positive pair compares its distance with all negative pairs. Wu et al. <ref type="bibr" target="#b32">[33]</ref> proposed a distance-based sampling strategy, where examples are sampled based on inverse n-dimensional unit sphere distances from anchored samples. Wang et al. <ref type="bibr" target="#b31">[32]</ref> propose a mining and weighting scheme, where informative pairs are sampled by measuring positive relative similarity, and then further weighted using selfsimilarity and negative relative similarity.</p><p>Apart from methods dedicated to addressing the weakness of pair-based DML methods, there is another line of work that tackles DML via class distribution estimation. The motivation for this camp of thought is to compare samples to proxies, and in doing so, reduce computation. One method that falls under this line of work is the Magnet Loss <ref type="bibr" target="#b18">[19]</ref> in which samples are associated with a cluster centroid, and at each training batch, samples are attracted to cluster centroids of similar classes and repelled by cluster centroids of different classes. Another method in this camp is ProxyNCA <ref type="bibr" target="#b16">[17]</ref>, where proxies are stored in memory as learnable parameters. During training, each sample is pushed towards its proxy while repelling against all other proxies of different classes. ProxyNCA is discussed in greater detail in Section 3.2.</p><p>Similar to ProxyNCA, Zhai et al. <ref type="bibr" target="#b35">[36]</ref> design a proxy-based solution that emphasizes on the Cosine distance rather than the Euclidean squared distance. They also use layer norm in their model to improve robustness against poor weight initialization of new parameters and introduces class balanced sampling during training, which improves their retrieval performance. In our work, we also use these enhancements in our architecture.</p><p>Recently, a few works in DML have explored ensemble techniques. Opitz et al. <ref type="bibr" target="#b17">[18]</ref> train an ensemble DML by reweighting examples using online gradient boosting. The downside of this technique is that it is a sequential process. Xuan et al. <ref type="bibr" target="#b34">[35]</ref> address this issue by proposing an ensemble technique where ensemble models are trained separately on randomly combined classes. Sanakoyeu et al. <ref type="bibr" target="#b20">[21]</ref> propose a unique divide-and-conquer strategy where the data is divided periodically via clustering based on current combined embedding during training. Each cluster is assigned to a consecutive chunk of the embedding, called learners, and they are randomly updated during training. Apart from ensemble techniques, there is recent work that attempts to improve DML in general. Jacob et al. <ref type="bibr" target="#b12">[13]</ref> discover that DML approaches that rely on Global Average Pooling (GAP) potentially suffer from the scattering problem, where features learned with GAP are sensitive to outlier. To tackle this problem, they propose HORDE, which is a high order regularizer for deep embeddings that computes higher-order moments of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we revisit NCA and ProxyNCA and discuss six enhancements that improve the retrieval performance of ProxyNCA. The enhanced version, which we call ProxyNCA++, is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neighborhood Component Analysis (NCA)</head><p>Neighborhood Component Analysis (NCA) is a DML algorithm that learns a Mahalanobis distance for k-nearest neighbors (KNN). Given two points, x i and x j , Goldberg et al. <ref type="bibr" target="#b6">[7]</ref> define p ij as the assignment probability of x i to x j :</p><formula xml:id="formula_0">p ij = âˆ’d(x i , x j ) k âˆˆi âˆ’d(x i , x k )<label>(1)</label></formula><p>where d(x i , x k ) is Euclidean squared distance computed on some learned embedding. In the original work, it was parameterized as a linear mapping, but nowadays, the method is often used with nonlinear mappings such as feedforward or convolutional neural networks. Informally, p ij is the probability that points i and j are said to be "neighbors". The goal of NCA is to maximize the probability that points assigned to the same class are neighbors, which, by normalization, minimizes the probability that points in different classes are neighbors:</p><formula xml:id="formula_1">L NCA = âˆ’ log jâˆˆCi exp(âˆ’d(x i , x j )) k âˆˆCi exp(âˆ’d(x i , x k ))</formula><p>.</p><p>(</p><p>Unfortunately, the computation of NCA loss grows polynomially with the number of samples in the dataset. To speed up computation, Goldberg et al. use random sampling and optimize the NCA loss with respect to the small batches of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ProxyNCA</head><p>ProxyNCA is a DML method which performs metric learning in the space of class distributions. It is motivated by NCA, and it attempts to address the computation weakness of NCA by using proxies. In ProxyNCA, proxies are stored as learnable parameters to faithfully represent classes by prototypes in an embedding space. During training, instead of comparing samples with one another in a given batch, which is quadratic in computation with respect to the batch size, ProxyNCA compares samples against proxies, where the objective aims to attract samples to their proxies and repel them from all other proxies.</p><p>Let C i denote a set of points that belong to the same class, f (a) be a proxy function that returns a corresponding class proxy, and ||a|| 2 be the L 2 -Norm of vector a. For each sample x i , we minimize the distance d(x i , f (x i )) between the sample, x i and its own proxy, f (x i ) and maximize the distance d(x i , f (z)) of that sample with respect to all other proxies Z, where f (z) âˆˆ Z and z âˆˆ C i .</p><formula xml:id="formula_3">L ProxyNCA = âˆ’ log ï£« ï£­ exp âˆ’d( xi ||xi||2 , f (xi) ||f (xi)||2 ) f (z)âˆˆZ exp âˆ’d( xi ||xi||2 , f (z) ||f (z)||2 ) ï£¶ ï£¸ .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aligning with NCA by optimizing proxy assignment probability</head><p>Using the same motivation as NCA (Equation 1), we propose to optimize the proxy assignment probability, P i . Let A denote the set of all proxies. For each x i , we aim to maximize P i .</p><formula xml:id="formula_4">P i = exp âˆ’d( xi ||xi||2 , f (xi) ||f (xi)||2 ) f (a)âˆˆA exp âˆ’d( xi ||xi||2 , f (a) ||f (a)||2 ) (4) L ProxyNCA++ = âˆ’ log(P i )<label>(5)</label></formula><p>Since P i is a probability score that must sum to one, maximizing P i for a proxy also means there is less chance for x i to be assigned to other proxies. In addition, maximizing P i also preserves the original ProxyNCA properties where x i is attracted toward its own proxy f (x i ) while repelling proxies of other classes, Z. It is important to note that in ProxyNCA, we maximize the distant ratio between âˆ’d(x i , y j ) and f (z)âˆˆZ âˆ’d(x i , f (z)), while in ProxyNCA++, we maximize the proxy assignment probability, P i , a subtle but important distinction. <ref type="table" target="#tab_7">Table 8</ref> shows the effect of proxy assignment probability to ProxyNCA and its enhancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">About Temperature Scaling</head><p>Temperature scaling is introduced in <ref type="bibr" target="#b11">[12]</ref>, where Hinton et al. use a high temperature (T &gt; 1) to create a softer probability distribution over classes for knowledge distillation purposes. Given a logit y i and a temperature variable T , a temperature scaling is defined as q i = exp(yi/T ) j exp(yj /T ) . By incorporating temperature scaling to the loss function of ProxyNCA++ in Equation 4, the new loss function has the following form:</p><formula xml:id="formula_5">L ProxyNCA++ = âˆ’ log ï£« ï£­ exp âˆ’d( xi ||xi||2 , f (xi) ||f (xi)||2 ) * 1 T f (a)âˆˆA exp âˆ’d( xi ||xi||2 , f (a) ||f (a)||2 ) * 1 T ï£¶ ï£¸<label>(6)</label></formula><p>When T = 1, we have a regular Softmax function. As T gets larger, the output of the softmax function will approach a uniform distribution. On the other hand, as T gets smaller, it leads to a peakier probability distribution. Low temperature scaling (T &lt; 1) is used in <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b35">[36]</ref>. In this work, we attempt to explain why low-temperature scaling works by visualizing its effect on synthetic data. In <ref type="figure">Figure 3</ref>, as T gets smaller, the decision boundary is getting more refined and can classify the samples better. In other words, as T becomes smaller, the model can overfit to the problem better and hence generating better decision boundaries.</p><p>In <ref type="figure">Figure 4</ref> (a), we show a plot of R@1 score with respect to temperature scale on the CUB200 dataset. The highest test average R@1 happens at T = 1 9 . Lowering T beyond this point will allow the model to overfit more to the training set and to make it less generalizable. Hence, we see a drop in test performance. <ref type="table">Table 9</ref> shows the effect of low temperature scaling to ProxyNCA and its enhancements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">About Global Pooling</head><p>In DML, the de facto global pooling operation used by the community is Global Average Pooling (GAP). In this paper, we investigate the effect of global pooling of spatial features on zero-shot image retrieval. We propose the use of Global K-Max Pooling <ref type="bibr" target="#b4">[5]</ref> to interpolate between GAP and Global Max Pooling (GMP). Given a convolution feature map of M Ã— M dimension with E channels, g âˆˆ R M Ã—M Ã—E and a binary variable, h i âˆˆ {0, 1}, Global K-Max Pooling is defined as:</p><formula xml:id="formula_6">Global k-Max(g ) = max h 1 k M 2 i=1 h i Â· g , s.t. M 2 i=1 h i = k, âˆ€ âˆˆ E<label>(7)</label></formula><p>When k = 1, we have GMP, and when k = M 2 , we have GAP. Figure 4 (b) is a plot of Recall@1 with different k value of Global K-Max Pooling on the CUB200 dataset. There is a negative correlation of 0.98 between k and Recall@1 performance, which shows that a lower k value results in better retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">About Fast moving proxies</head><p>In ProxyNCA, the proxies, the embedding layer, and the backbone model all share the same learning rate. We hypothesize that the proxies should be moving faster than the embedding space in order to represent the class distribution better. However, in our experiments, we discovered that the gradient of proxies is smaller than the gradient of the embedding layer and backbone model by three orders of magnitude, and this is caused by the L 2 -Normalization of proxies. To mitigate this problem, we use a higher learning rate for the proxies.</p><p>From our ablation studies in <ref type="table">Table 9</ref>, we observe that fast moving proxies synergize better with low temperature scaling and Global Max Pooling. We can see a 1.4pp boost in R@1 if we combine fast proxies and low temperature scaling. There is also a 2.1pp boost in the retrieval performance if we combine fast proxies, low temperature scaling, and Global Max Pooling. <ref type="figure">Figure 4</ref> (c) is a plot of Recall@1 with different proxy learning rates on CUB200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Layer Norm (Norm) and Class Balanced Sampling (CBS)</head><p>The use of layer normalization <ref type="bibr" target="#b26">[27]</ref> without affine parameters is explored by Zhai et al. <ref type="bibr" target="#b35">[36]</ref>. Based on our experiments, we also find that this enhancement helps to boost performance. Besides, we also use a class balanced sampling strategy in our experiments, where we have more than one instance per class in each training batch. To be specific, for every batch of size N b , we only sample N c classes from which we then randomly select N b /N c examples. This sampling strategy commonly appears in pair-based DML approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref> as a baseline and Zhai et al. is the first paper that uses it in a proxy-based DML method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We train and evaluate our model on four zero-shot image retrieval datasets: the Caltech-UCSD Birds dataset <ref type="bibr" target="#b28">[29]</ref> (CUB200), the Stanford Cars dataset <ref type="bibr" target="#b14">[15]</ref> (Cars196), the Stanford Online Products dataset <ref type="bibr" target="#b22">[23]</ref> (Sop), and the In Shop Clothing Retrieval dataset <ref type="bibr" target="#b15">[16]</ref> (InShop). The composition in terms of number of images and classes of each dataset is summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>For each dataset, we use the first half of the original training set as our training set and the second half of the original training set as our validation set. In all of our experiments, we use a two-stage training process. We first train our models on the training set and then use the validation set to perform hyper-parameter tuning (e.g., selecting the best epoch for early stopping, learning rate, etc.). Next, we train our models with the fine-tuned hyper-parameters on the combined training and validation sets (i.e., the complete original training set). <ref type="table">Table 1</ref>. We show the composition of all four zero-shot image retrieval datasets considered in this work. In addition, we also report the learning rates, the batch size, and cbs (class balanced sampling) instances for each dataset during training. The number of classes for the Sop and InShop datasets is large when compared to CUB200 and Cars196 dataset. However, the number of instances per class is very low for the Sop and InShop datasets. In general, ProxyNCA does not require a large batch size when compared to pairs-based DML methods. To illustrate this, we also show the batch sizes used in <ref type="bibr" target="#b31">[32]</ref>, which is current state-of-the-art among pairs-based methods. Their technique requires a batch size, which is several times larger compared to ProxyNCA++. We use the same learning rate for both stages of training. We also set the number of proxies to be the same as the number of classes in the training set. For our experiments with fast proxies, we use a different learning rate for proxies (see <ref type="table">Table 1</ref> for details). We also use a temperature value of <ref type="bibr">1 9</ref> across all datasets. In the first stage of training, we use the "reduce on loss plateau decay" annealing <ref type="bibr" target="#b7">[8]</ref> to control the learning rate of our model based on the recall performance (R@1) on the validation set. We set the patience value to four epochs in our experiments. We record the epochs where the learning rate is reduced and also save the best epochs for early stopping on the second stage of training.</p><p>In all of our experiments, we leverage the commonly used ImageNet <ref type="bibr" target="#b19">[20]</ref> pre-trained Resnet50 [10] model as our backbone (see <ref type="table" target="#tab_2">Table 2</ref> for commonly used backbone architectures). Features are extracted after the final convolutional block of the model and are reduced to a spatial dimension of 1 Ã— 1 using a global pooling operation. This procedure results in a 2048 dimensional vector, which is fed into a final embedding layer. In addition, we also experiment with various embedding sizes. We observe a gain in performance as we increase the size of the embedding. It is important to note that not all DML techniques yield better performance as embedding size increases. For some techniques such as <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23]</ref>, a larger embedding size hurts performance. During training, we scale the original images to a random aspect ratio (0.75 to 1.33) before applying a crop of random size (0.08 to 1.0 of the scaled image). After cropping, we resize the images to 256Ã—256. We also perform random horizontal flipping for additional augmentation. During testing, we resize the images to 288Ã—288 and perform a center crop of size 256Ã—256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>We evaluate retrieval performance based on two evaluation metrics: (a) Recall@K (R@K) and (b) Normalized Mutual Information, NMI(â„¦, C) = 2 * I(â„¦,C) H(â„¦)+H(C) , where â„¦ represents ground truth label, C represents the set of clusters computed by Kmeans, I stands for mutual information and H stands for entropy. The purpose of NMI is to measure the purity of the cluster on unseen data.</p><p>Using the same evaluation protocols detailed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>, we evaluate our model using unseen classes on four datasets. The InShop dataset <ref type="bibr" target="#b15">[16]</ref> is slightly different than all three other datasets. There are three groupings of data: training set, query set, and gallery set. The query and gallery set have the same classes, and these classes do not overlap with the training set. Evaluation is done based on retrieval performance on the gallery set. <ref type="table" target="#tab_3">Tables 3, 4</ref>, 5, and 6 show the results of our experiments 1 . For each dataset, we report the results of our method, averaged over five runs. We also report the standard deviation of our results to account for uncertainty. Additionally, we also show the results of ProxyNCA++ trained with smaller embedding sizes (512, 1024). Our ProxyNCA++ model outperforms ProxyNCA and all other stateof-the-art methods in all categories across all four datasets. Note, our model trained with a 512-dimensional embedding also outperform all other methods in the same embedding space except for The InShop dataset <ref type="bibr" target="#b15">[16]</ref>, where we tie in the R@1 category. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In <ref type="table">Table 7</ref>, we perform an ablation study on the performance of our proposed methods using the CUB200 dataset. The removal of the low temperature scaling component gives the most significant drop in R@1 performance (-10.8pt).  This is followed by Global Max Pooling (-3.2pt), Layer Normalization (-2.6pt), Class Balanced Sampling (-2.6pt), Fast proxies (-1.9pt) and Proxy Assignment Probability (-1.1pt).</p><p>We compare the effect of the Global Max Pooling (GMP) and the Global Average Pooling (GAP) on other metric learning methodologies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref> in <ref type="table" target="#tab_9">Table 11</ref> on CUB200 dataset. The performance of all other models improves when GAP is replaced with GMP, with the exception of HORDE <ref type="bibr" target="#b12">[13]</ref>. In HORDE, Jacob et al. <ref type="bibr" target="#b12">[13]</ref> include both the pooling features as well as the higher-order moment features in the loss calculation. We speculate that since this method is designed to reduce the effect of outliers, summing max-pooled features canceled out the effect of higher-order moment features, which may have lead to suboptimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We revisit ProxyNCA and incorporate several enhancements. We find that low temperature scaling is a performance-critical component and explain why it works. Besides, we also discover that Global Max Pooling works better in general <ref type="table">Table 7</ref>. An ablation study of ProxyNCA++ and its enhancements on CUB200 <ref type="bibr" target="#b28">[29]</ref>. when compared to Global Average Pooling. Additionally, our proposed fast moving proxies also addresses small gradient issue of proxies, and this component synergizes well with low temperature scaling and Global Average pooling. The new and improved ProxyNCA, which we call ProxyNCA++, outperforms the original ProxyNCA by 22.9 percentage points on average across four zero-shot image retrieval datasets for Recall@1. In addition, we also achieve state-of-art results on all four benchmark datasets for all categories.  <ref type="table">Table 9</ref>. An ablation study of the effect of low temperature scaling to ProxyNCA and its enhancements on CUB200 <ref type="bibr" target="#b28">[29]</ref>. Without low temperature scaling, three out of six enhancements (in red) get detrimental results when they are applied to ProxyNCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R@1</head><p>without scale with scale ProxyNCA <ref type="figure">(</ref>  <ref type="table">Table 10</ref>. An ablation study of ProxyNCA the effect of Global Max Pooling to Prox-yNCA and its enhancements on CUB200 <ref type="bibr" target="#b28">[29]</ref>. We can see a 2.1pp improvement on average after replacing GAP with GMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R@1</head><p>Global (max) improves NormSoftMax by 2.2pp on R@1. However, the fast proxies component (fast) reacts negatively with NormSoftMax by decreasing its performance by 0.6pp. By combining both the GMP and the fast proxies components into NormSoftMax, we see a small increase of R@1 performance (0.6pp).</p><p>On the CARS196 dataset, there is a slight increase in R@1 performance (0.3pp) by adding fast proxies component to NormSoftMax. When we add the GMP component to NormSoftMax, we observe an increase of R@1 by 1.1pp. Combining both the GMP and the fast proxies components, there is a 1.0pp increase in R@1 performance. <ref type="figure">Fig. 5</ref>. A sensitivity study of temperature scaling for NormSoftMax <ref type="bibr" target="#b35">[36]</ref> without layer norm (norm), class balanced sampling (cbs), and fast proxies (fast). We show a plot of R@1 with different temperature scales on CUB200 <ref type="bibr" target="#b28">[29]</ref>. The shaded areas represent one standard deviation of uncertainty.   <ref type="bibr" target="#b28">[29]</ref>. All models are experimented with embedding size of 2048. For NormSoftMax <ref type="bibr" target="#b36">[37]</ref>, we use a temperature scaling of T = 1/2, a proxy learning rate of 4e âˆ’1 (fast) and learning rates of 4e âˆ’ 3 for the backbone and embedding layers. It is important to note that, NormSoftMax <ref type="bibr" target="#b35">[36]</ref> does not have max pooling and fast proxy component.  <ref type="bibr" target="#b14">[15]</ref>. All models are experimented with embedding size of 2048. For NormSoftMax <ref type="bibr" target="#b36">[37]</ref>, we use a temperature scaling of T = 1/2, a proxy learning rate of 4e âˆ’1 (fast) and learning rates of 4e âˆ’ 3 for the backbone and embedding layers. It is important to note that, NormSoftMax <ref type="bibr" target="#b35">[36]</ref> does not have max pooling and fast proxy component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Two moon classifier</head><p>In Section 3.4 (About Temperature Scaling) in the main paper, we show a visualization of the effect of temperature scaling on the decision boundary of a softmax classifier on a two-moon synthetic dataset. In detail, we trained a twolayers linear model. The first layer has an input size of 2 and an output size of 100. This is followed by a ReLU unit. The second layer has an input size of 100 and an output size of 2. For the synthetic dataset, we use the scikit-learn's 2 moons data generator to generate 600 samples with noise of 0.3 and a random state of 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Regarding crop size of images</head><p>Image crop size can have a large influence on performance. Current SOTA method <ref type="bibr" target="#b12">[13]</ref> for embedding size 512 uses a crop size of 256 Ã— 256, which we also use for our experiments (See <ref type="table" target="#tab_14">Table 14</ref>). We repeat these experiments with a crop-size of 227 Ã— 227 to make it comparable with older SOTA method <ref type="bibr" target="#b31">[32]</ref> (See <ref type="table" target="#tab_4">Table 15</ref>). In this setting, we outperform SOTA for CARS and SOP. We tie on CUB, and we underperform on InShop. However, since no spread information is reported in SOTA <ref type="bibr" target="#b31">[32]</ref>, it is hard to make a direct comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Regarding the implementation of baseline</head><p>We follow Algorithm 1 in the original paper <ref type="bibr" target="#b16">[17]</ref> when implementing our baseline. In the original paper, there is an Î± variable that resembles temperature scaling. However, Î± choice is ambiguous and is used to prove the error bound theoretically. We replicate <ref type="bibr" target="#b16">[17]</ref> on CUB, with embedding size of 64, crop size of 227, and GoogLeNet <ref type="bibr" target="#b24">[25]</ref> backbone. With the temperature scale, T=1/3, we obtain a R@1 of 49.70, which is close to the reported R@1 49.2. This indicates our implementation of ProxyNCA is correct. The baseline ProxyNCA is implemented using the same training set up as the proposed ProxyNCA++. As mentioned in our paper (Sec 4.1), we split the original training set into the training (1st half) and validation set (2nd half). We did not perform an extensive sweep of hyperparameters. In our experiment, we first select the best hyperparameter for baseline ProxyNCA (i.e., learning rate [1e-3 to 5e-3]) before adding any enhancements corresponding to ProxyNCA++. We believe that it is possible to obtain better results for both ProxyNCA and ProxyNCA++ with a more extensive sweep of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Regarding the Global Max Pooling (GMP) vs. Global</head><p>Average Pooling (GAP)</p><p>In our paper, we show that GMP is better than GAP empirically. However, we could not find any consistent visual evidence as to why GMP works better. We initially hypothesized that GAP was failing for small objects. But after we controlled for object size, we did not observe any consistent visual evidence to support this hypothesis. In <ref type="figure" target="#fig_3">Figure 7</ref>, GMP consistently outperform GAP regardless of object size; this evidence disproved our initial hypothesis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Regarding the computation complexity of ProxyNCA++</head><p>The inference time to compute embeddings for ProxyNCA++ and baseline Prox-yNCA will depend on the base architecture. In our experiments, we used a ResNet-50 model as a backbone, so inference time would be comparable to that of a ResNet-50 classifier. There are two differences which have a negligible effect on inference time: (a) The removal of the softmax classification layer, and (b) the addition of layer norm. As for training time complexity, ProxyNCA++ is comparable to ProxyNCA both theoretically and in terms of runtime. Given a training batch size of B, we only need to compute the distance between each sample w.r.t the proxies, K. After that, we compute a cross-entropy of these distances, where we minimize the probability of a sample being assigned to its own proxy. Therefore the runtime complexity in a given batch is O(BK).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A summary of the average performance on Recall@1 for all datasets. With our proposed enhancements, we improve upon the original ProxyNCA by 22.9pp, and outperform current state-of-the-art models by 2.0pp on average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>We show an overview of our architecture, ProxyNCA++, which consists of the original building blocks of ProxyNCA and six enhancements, which are shown in the dashed boxes. ProxyNCA consists of a pre-trained backbone model, a randomly initialized embedding layer, and randomly initialized proxies. The six enhancements in ProxyNCA++ are proxy assignment probability (+prob), low temperature scaling (+scale), class balanced sampling (+cbs), layer norm (+norm), global max pooling (+max) and fast-moving proxies (+fast).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>The effect of temperature scaling on the decision boundary of a Softmax Classifier trained on the two moons synthetic dataset T=4 We show three plots of R@1 with different (a) temperature scales , (b) k values for K-Max Pooling and (c) proxy learning rates on on CUB200<ref type="bibr" target="#b28">[29]</ref>. The shaded areas represent one standard deviation of uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Performance summary (R@1) between GMP and GAP on various object sizes (in percent w.r.t. image size) in the CUB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Commonly used backbone architectures for zero-shot image retrieval, with associated ImageNet Top-1 Error % for each architecture</figDesc><table><row><cell>Architecture</cell><cell cols="2">Abbreviation Top-1 Error (%)</cell></row><row><cell>Resnet18 [10]</cell><cell>R18</cell><cell>30.24</cell></row><row><cell>GoogleNet [25]</cell><cell>I1</cell><cell>30.22</cell></row><row><cell>Resnet50 [10]</cell><cell>R50</cell><cell>23.85</cell></row><row><cell>InceptionV3 [24]</cell><cell>I3</cell><cell>22.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Recall@k ProxyNCA++ 72.2Â±0.8 82.0Â±0.6 89.2Â±0.6 93.5Â±0.4 75.8Â±0.8 R50 2048 1Â±0.2 94.5Â±0.2 97.0Â±0.2 98.4Â±0.1 76.6Â±0.7 R50 2048</figDesc><table><row><cell></cell><cell></cell><cell cols="5">for k = 1,2,4,8 and NMI on CUB200-2011 [29]</cell></row><row><cell>R@k</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell><cell>Arch Emb</cell></row><row><cell>ProxyNCA[17]</cell><cell>49.2</cell><cell>61.9</cell><cell>67.9</cell><cell>72.4</cell><cell>59.5</cell><cell>I1 128</cell></row><row><cell>Margin[33]</cell><cell>63.6</cell><cell>74.4</cell><cell>83.1</cell><cell>90.0</cell><cell>69.0</cell><cell>R50 128</cell></row><row><cell>MS [32]</cell><cell>65.7</cell><cell>77.0</cell><cell>86.3</cell><cell>91.2</cell><cell>-</cell><cell>I3 512</cell></row><row><cell>HORDE [13]</cell><cell>66.8</cell><cell>77.4</cell><cell>85.1</cell><cell>91.0</cell><cell>-</cell><cell>I3 512</cell></row><row><cell>NormSoftMax [36]</cell><cell>61.3</cell><cell>73.9</cell><cell>83.5</cell><cell>90.0</cell><cell>-</cell><cell>R50 512</cell></row><row><cell>NormSoftMax [36]</cell><cell>65.3</cell><cell>76.7</cell><cell>85.4</cell><cell>91.8</cell><cell>-</cell><cell>R50 2048</cell></row><row><cell>ProxyNCA</cell><cell cols="6">59.3Â±0.4 71.2Â±0.3 80.7Â±0.2 88.1Â±0.3 63.3Â±0.5 R50 2048</cell></row><row><cell>ProxyNCA++</cell><cell cols="6">69.0Â±0.8 79.8Â±0.7 87.3Â±0.7 92.7Â±0.4 73.9Â±0.5 R50 512</cell></row><row><cell>ProxyNCA++</cell><cell cols="6">70.2Â±1.6 80.7Â±1.4 88.0Â±0.9 93.0Â±0.4 74.2Â±1.0 R50 1024</cell></row><row><cell>ProxyNCA++</cell><cell cols="6">69.1Â±0.5 79.6Â±0.4 87.3Â±0.3 92.7Â±0.2 73.3Â±0.7 R50 2048</cell></row><row><cell>(-max, -fast)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Table 4. Recall@k for k = 1,2,4,8 and NMI on CARS196 [15]</cell><cell></cell></row><row><cell>R@k</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell><cell>Arch Emb</cell></row><row><cell>ProxyNCA [17]</cell><cell>73.2</cell><cell>82.4</cell><cell>86.4</cell><cell>88.7</cell><cell>64.9</cell><cell>I1 128</cell></row><row><cell>Margin [33]</cell><cell>79.6</cell><cell>86.5</cell><cell>91.9</cell><cell>95.1</cell><cell>69.1</cell><cell>R50 128</cell></row><row><cell>MS [32]</cell><cell>84.1</cell><cell>90.4</cell><cell>94.0</cell><cell>96.1</cell><cell>-</cell><cell>I3 512</cell></row><row><cell>HORDE [13]</cell><cell>86.2</cell><cell>91.9</cell><cell>95.1</cell><cell>97.2</cell><cell>-</cell><cell>I3 512</cell></row><row><cell>NormSoftMax [36]</cell><cell>84.2</cell><cell>90.4</cell><cell>94.4</cell><cell>96.9</cell><cell>-</cell><cell>R50 512</cell></row><row><cell>NormSoftMax [36]</cell><cell>89.3</cell><cell>94.1</cell><cell>96.4</cell><cell>98.0</cell><cell>-</cell><cell>R50 2048</cell></row><row><cell>ProxyNCA</cell><cell cols="6">62.6Â±9.1 73.6Â±8.6 82.2Â±6.9 88.9Â±4.8 53.8Â±7.0 R50 2048</cell></row><row><cell>ProxyNCA++</cell><cell cols="6">86.5Â±0.4 92.5Â±0.3 95.7Â±0.2 97.7Â±0.1 73.8Â±1.0 R50 512</cell></row><row><cell>ProxyNCA++</cell><cell cols="6">87.6Â±0.3 93.1Â±0.1 96.1Â±0.2 97.9Â±0.1 75.7Â±0.3 R50 1024</cell></row><row><cell>ProxyNCA++</cell><cell cols="6">87.9Â±0.2 93.2Â±0.2 96.1Â±0.2 97.9Â±0.1 76.0Â±0.5 R50 2048</cell></row><row><cell>(-max, -fast)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ProxyNCA++ 90.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Recall@k for k = 1,10,100,1000 and NMI on Stanford Online Products<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell>R@k</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>1000</cell><cell>Arch Emb</cell></row><row><cell>ProxyNCA [17]</cell><cell>73.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>I1 128</cell></row><row><cell>Margin [33]</cell><cell>72.7</cell><cell>86.2</cell><cell>93.8</cell><cell>98.0</cell><cell>R50 128</cell></row><row><cell>MS [32]</cell><cell>78.2</cell><cell>90.5</cell><cell>96.0</cell><cell>98.7</cell><cell>I3 512</cell></row><row><cell>HORDE [13]</cell><cell>80.1</cell><cell>91.3</cell><cell>96.2</cell><cell>98.7</cell><cell>I3 512</cell></row><row><cell>NormSoftMax [36]</cell><cell>78.2</cell><cell>90.6</cell><cell>96.2</cell><cell>-</cell><cell>R50 512</cell></row><row><cell>NormSoftMax [36]</cell><cell>79.5</cell><cell>91.5</cell><cell>96.7</cell><cell>-</cell><cell>R50 2048</cell></row><row><cell>ProxyNCA</cell><cell cols="5">62.1Â±0.4 76.2Â±0.4 86.4Â±0.2 93.6Â±0.3 R50 2048</cell></row><row><cell>ProxyNCA++</cell><cell cols="5">80.7Â±0.5 92.0Â±0.3 96.7Â±0.1 98.9Â±0.0 R50 512</cell></row><row><cell>ProxyNCA++</cell><cell cols="5">80.7Â±0.4 92.0Â±0.2 96.7Â±0.1 98.9Â±0.0 R50 1024</cell></row><row><cell cols="6">ProxyNCA++(-max, -fast) 72.1Â±0.2 85.4Â±0.1 93.0Â±0.1 96.7Â±0.2 R50 2048</cell></row><row><cell>ProxyNCA++</cell><cell cols="5">81.4Â±0.1 92.4Â±0.1 96.9Â±0.0 99.0Â±0.0 R50 2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Recall@k</figDesc><table><row><cell></cell><cell cols="6">for k = 1,10,20,30,40 on the In-Shop Clothing Retrieval dataset [23]</cell></row><row><cell>R@k</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>Arch Emb</cell></row><row><cell>MS [32]</cell><cell>89.7</cell><cell>97.9</cell><cell>98.5</cell><cell>98.8</cell><cell>99.1</cell><cell>I3 512</cell></row><row><cell>HORDE [13]</cell><cell>90.4</cell><cell>97.8</cell><cell>98.4</cell><cell>98.7</cell><cell>98.9</cell><cell>I3 512</cell></row><row><cell>NormSoftMax [36]</cell><cell>88.6</cell><cell>97.5</cell><cell>98.4</cell><cell>98.8</cell><cell>-</cell><cell>R50 512</cell></row><row><cell>NormSoftMax [36]</cell><cell>89.4</cell><cell>97.8</cell><cell>98.7</cell><cell>99.0</cell><cell>-</cell><cell>R50 2048</cell></row><row><cell>ProxyNCA</cell><cell cols="6">59.1Â±0.7 80.6Â±0.6 84.7Â±0.3 86.7Â±0.4 88.1Â±0.5 R50 2048</cell></row><row><cell>ProxyNCA++</cell><cell cols="6">90.4Â±0.2 98.1Â±0.1 98.8Â±0.0 99.0Â±0.1 99.2Â±0.0 R50 512</cell></row><row><cell>ProxyNCA++</cell><cell cols="6">90.4Â±0.4 98.1Â±0.1 98.8Â±0.1 99.1Â±0.1 99.2Â±0.1 R50 1024</cell></row><row><cell>ProxyNCA++</cell><cell cols="6">82.5Â±0.3 93.5Â±0.1 95.4Â±0.2 96.3Â±0.0 96.8Â±0.0 R50 2048</cell></row><row><cell>(-max, -fast)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">ProxyNCA++ 90.9Â±0.3 98.2Â±0.0 98.9Â±0.0 99.1Â±0.0 99.4Â±0.0 R50 2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>An ablation study of the effect of Proxy Assignment Probability (+prob) to ProxyNCA and its enhancements on CUB200<ref type="bibr" target="#b28">[29]</ref>.</figDesc><table><row><cell>R@1</cell><cell>without prob with prob</cell></row><row><cell>ProxyNCA (Emb: 2048)</cell><cell>59.3 Â± 0.4 59.0 Â± 0.4</cell></row><row><cell>+scale</cell><cell>62.9 Â± 0.4 63.4 Â± 0.6</cell></row><row><cell>+scale +norm</cell><cell>65.3 Â± 0.7 65.7 Â± 0.8</cell></row><row><cell>+scale +max</cell><cell>65.1 Â± 0.3 66.2 Â± 0.3</cell></row><row><cell>+scale +norm +cbs</cell><cell>67.2 Â± 0.8 69.1 Â± 0.5</cell></row><row><cell>+scale +norm +cbs +max</cell><cell>68.8 Â± 0.7 70.3 Â± 0.9</cell></row><row><cell cols="2">+scale +norm +cbs +max +fast 71.1 Â± 0.7 72.2 Â± 0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .</head><label>11</label><figDesc>Comparing the effect of Global Max Pooling and Global Average Pooling on the CUB200 dataset for a variety of methods.</figDesc><table><row><cell>Average Pooling Global Max Pooling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>A comparison of ProxyNCA++ and NormSoftMax<ref type="bibr" target="#b35">[36]</ref> on CUB200</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 .</head><label>13</label><figDesc>4Â±1.1 76.1Â±0.7 85.0Â±0.7 91.4Â±0.3 70.0Â±1.1 A comparison of ProxyNCA++ and NormSoftMax [36] on CARS196</figDesc><table><row><cell>R@k</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell></row><row><cell cols="6">ProxyNCA++ 72.2Â±0.8 82.0Â±0.6 89.2Â±0.6 93.5Â±0.4 75.8Â±0.8</cell></row><row><cell>-max</cell><cell cols="5">69.0Â±0.6 80.3Â±0.5 88.1Â±0.4 93.1Â±0.1 74.3Â±0.4</cell></row><row><cell>-fast</cell><cell cols="5">70.3Â±0.9 80.6Â±0.4 87.7Â±0.5 92.5Â±0.3 73.5Â±0.9</cell></row><row><cell>-max -fast</cell><cell cols="5">69.1Â±0.5 79.6Â±0.4 87.3Â±0.3 92.7Â±0.2 73.3Â±0.7</cell></row><row><cell cols="6">NormSoftMax 65.0Â±1.7 76.6Â±1.1 85.5Â±0.6 91.6Â±0.4 69.6Â±0.8</cell></row><row><cell>(+max, +fast)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">NormSoftMax 63.8Â±1.3 75.9Â±1.0 84.9Â±0.8 91.4Â±0.6 70.8Â±1.1</cell></row><row><cell>(+fast)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">NormSoftMax 67.6Â±0.4 78.4Â±0.2 86.7Â±0.4 92.2Â±0.3 71.2Â±0.9</cell></row><row><cell>(+max)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">NormSoftMax 64.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>9Â±0.2 93.2Â±0.2 96.1Â±0.2 97.9Â±0.1 76.0Â±0.5 NormSoftMax 86.0Â±0.1 92.0Â±0.1 95.5Â±0.1 97.6Â±0.1 68.6Â±0.6 (+max, +fast) NormSoftMax 85.3Â±0.4 91.6Â±0.3 95.5Â±0.2 97.6Â±0.1 72.2Â±0.7 (+fast) NormSoftMax 86.1Â±0.4 92.1Â±0.3 95.5Â±0.2 97.6Â±0.2 68.0Â±0.5 (+max) NormSoftMax 85.0Â±0.6 91.4Â±0.5 95.3Â±0.4 97.5Â±0.3 70.7Â±1.1</figDesc><table><row><cell>R@k</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>NMI</cell></row><row><cell cols="6">ProxyNCA++ 90.1Â±0.2 94.5Â±0.2 97.0Â±0.2 98.4Â±0.1 76.6Â±0.7</cell></row><row><cell>-max</cell><cell cols="5">87.8Â±0.6 93.2Â±0.4 96.3Â±0.2 98.0Â±0.1 76.4Â±1.3</cell></row><row><cell>-fast</cell><cell cols="5">89.2Â±0.4 93.9Â±0.2 96.5Â±0.1 98.0Â±0.1 74.8Â±0.7</cell></row><row><cell>-max -fast</cell><cell>87.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 .</head><label>14</label><figDesc>A comparison of ProxyNCA++ and the current SOTA<ref type="bibr" target="#b12">[13]</ref> in the embedding size of 512 and a crop size of 256 Ã— 256.Table 15. A comparison of ProxyNCA++ and the current SOTA<ref type="bibr" target="#b31">[32]</ref> in the embedding size of 512 and a crop size of 227 Ã— 227.</figDesc><table><row><cell>R@k</cell><cell>SOTA [13]</cell><cell>Ours</cell></row><row><cell>CUB</cell><cell>66.8</cell><cell>69.0Â±0.8</cell></row><row><cell>CARS</cell><cell>86.2</cell><cell>86.5Â±0.4</cell></row><row><cell>SOP</cell><cell>80.1</cell><cell>80.7Â±0.5</cell></row><row><cell>InShop</cell><cell>90.4</cell><cell>90.4Â±0.2</cell></row><row><cell>R@k</cell><cell>SOTA [32]</cell><cell>Ours</cell></row><row><cell>CUB</cell><cell>65.7</cell><cell>64.7Â±1.6</cell></row><row><cell>CARS</cell><cell>84.2</cell><cell>85.1Â±0.3</cell></row><row><cell>SOP</cell><cell>78.2</cell><cell>79.6Â±0.6</cell></row><row><cell>InShop</cell><cell>89.7</cell><cell>87.6Â±1.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For additional experiments on different crop sizes, please refer to the corresponding supplementary materials in the appendix</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://scikit-learn.org/stable/modules/generated/sklearn.datasets. make_moons.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main difference between Equation 8 and 9 is the distance function. In ProxyNCA++, we use a euclidean squared distance function instead of cosine distance function.</p><p>Based on our sensitivity studies on temperature scaling and proxy learning rate, we show that NormSoftMax perform best when the temperature scale, T is set to 1/2 and the proxy learning rate is set to 4e âˆ’1 (see <ref type="figure">Figure 5</ref> and 6).</p><p>We perform an ablation study of NormSoftMax in <ref type="table">Table 12</ref> and 13. On the CUB200 dataset, we show that the Global Max Pooling (GMP) component</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A A comparison with NormSoftMax <ref type="bibr" target="#b35">[36]</ref> In this section, we compare the differences between ProxyNCA++ and Norm-SoftMax <ref type="bibr" target="#b35">[36]</ref>. Both ProxyNCA++ and NormSoftMax are proxy-based DML solutions. By borrowing notations from Equation 6 in the main paper, Prox-yNCA++ has the following loss function:</p><p>And NormSoftMax has the following loss function:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="98" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>SÃ¤ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Neural Information Processing Systems, NIPS&apos;93</title>
		<meeting>the 6th International Conference on Neural Information Processing Systems, NIPS&apos;93<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weldon: Weakly supervised learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org.9" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymeric</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02735</idno>
		<title level="m">Metric learning with horde: High-order regularizer for deep embeddings</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bier -boosting independent embeddings robustly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05939</idno>
		<title level="m">Metric learning with adaptive density discrimination</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Divide and conquer the embedding space for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Tschernezki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc. 1</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3637" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia, MM &apos;18</title>
		<meeting>the 26th ACM International Conference on Multimedia, MM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving generalization via scalable neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="685" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep randomized ensembles for metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Classification is a strong baseline for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Us San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francisco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multiloss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
