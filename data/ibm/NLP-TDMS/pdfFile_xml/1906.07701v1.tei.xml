<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distilling Translations with Visual Awareness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Ive</surname></persName>
							<email>j.ive@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">DCS</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
							<email>pranava@imperial.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
							<email>l.specia@imperial.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distilling Translations with Visual Awareness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal machine translation (MMT) is an area of research that addresses the task of translating texts using context from an additional modality, generally static images. The assumption is that the visual context can help ground the meaning of the text and, as a consequence, generate more adequate translations. Current work has focused on datasets of images paired with their descriptions, which are crowdsourced in English and then translated into different languages, namely the Multi30K dataset .</p><p>Results from the most recent evaluation campaigns in the area <ref type="bibr" target="#b1">Barrault et al., 2018)</ref> have shown that visual information can be helpful, as humans generally prefer translations generated by multimodal models than by their text-only counterparts. However, previous work has also shown that images are only needed in very specific cases . This is also the case for humans.  (see <ref type="figure" target="#fig_0">Figure 1</ref>) concluded that visual information is needed by humans in the presence of the following: incorrect or ambiguous source words and gender-neutral words that need to be marked for gender in the target language. In an experiment where human translators were asked to first translate descriptions based on their textual context only and then revise their translation based on a corresponding image, they report that these three cases accounted for 62-77% of the revisions in the translations in two subsets of Multi30K.</p><p>Ambiguities are very frequent in Multi30K, as in most language corpora. <ref type="bibr" target="#b1">Barrault et al. (2018)</ref> shows that in its latest test set, 358 (German) and 438 (French) instances (out of 1,000) contain at least one word that has more than one translation in the training set. However, these do not always represent a challenge for translation models: often the text context can easily disambiguate words (see baseline translation in <ref type="figure">Figure 4(a)</ref>); additionally, the models are naturally biased to generate the most frequent translation of the word, which by definition is the correct one in most cases.</p><p>The need to gender-mark words in a target language when translating from English can be thought of as a disambiguation problem, except that the text context is often less telling and the frequency bias plays ends up playing a bigger role (see baseline translation in <ref type="figure">Figure 4(c)</ref>). This has been shown to be a common problem in neural machine translation <ref type="bibr" target="#b42">(Vanmassenhove et al., 2018;</ref><ref type="bibr" target="#b15">Font and Costa-Jussà, 2019)</ref>, as well as in areas such as image captioning <ref type="bibr" target="#b22">(Hendricks et al., 2018)</ref> and co-reference resolution <ref type="bibr" target="#b49">(Zhao et al., 2018)</ref>.</p><p>Incorrect source words are common in Multi30K, as in many other crowdsourced or usergenerated dataset. In this case the context may not be enough (see DE translation in <ref type="figure" target="#fig_0">Figure 1(c)</ref>). We posit that models should be robust to such a type of noise and note that similar treatment would be  required for out of vocabulary (OOV) words, i.e. correct words that are unknown to the model. We propose an approach that takes into account the strengths of a text-only baseline model and only refines its translations when needed. Our approach is based on deliberation networks <ref type="bibr" target="#b46">(Xia et al., 2017)</ref> to jointly learn to generate draft translations and refine them based on left and right side target context as well as structured visual information. This approach outperforms previous work.</p><p>In order to further probe how well our models can address the three problems mentioned above, we perform a controlled experiment where we minimise the interference of the frequency bias by masking ambiguous and gender-related words, as well as randomly selected words (to simulate noise and OOV). This experiment shows that our multimodal refinement approach outperforms the textonly one in more complex linguistic setups.</p><p>Our main contributions are: (i) a novel approach to MMT based on deliberation networks and structured visual information which gives state of the art results (Sections 3.2 and 5.1); (ii) a frequency bias-free investigation on the need for visual context in MMT (Sections 4.2 and 5.2); and (iii) a thorough investigation on different vi-sual representations for transformer-based architectures (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>MMT: Approaches to MMT vary with regards to how they represent images and how they incorporate this information in the models. Initial approaches use RNN-based sequence to sequence models <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> enhanced with a single, global image vector, extracted as one of the layers of a CNN trained for object classification <ref type="bibr" target="#b20">(He et al., 2016)</ref>, often the penultimate or final layer.</p><p>The image representation is integrated into the MT models by initialising the encoder or decoder <ref type="bibr" target="#b12">(Elliott et al., 2015;</ref><ref type="bibr">Caglayan et al., 2017;</ref><ref type="bibr" target="#b33">Madhyastha et al., 2017)</ref>; element-wise multiplication with the source word annotations <ref type="bibr">(Caglayan et al., 2017)</ref>; or projecting the image representation and encoder context to a common space to initialise the decoder . <ref type="bibr" target="#b14">Elliott and Kádár (2017)</ref> and <ref type="bibr" target="#b21">Helcl et al. (2018)</ref> instead model the source sentence and reconstruct the image representation jointly via multi-task learning.</p><p>An alternative way of exploring image rep-resentations is to have an attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> on the output of the last convolutional layer of a CNN <ref type="bibr" target="#b47">(Xu et al., 2015)</ref>. The layer represents the activation of K different convolutional filters on evenly quantised N × N spatial regions of the image. <ref type="bibr">Caglayan et al. (2017)</ref>   <ref type="bibr" target="#b21">Helcl et al. (2018)</ref> is the closest to our work: we also use a doubly-attentive transformer architecture and explore spatial visual information. However, we differ in two main aspects (Section 3): (i) our approach explores additional textual context through a second pass decoding process and uses visual information only at this stage, and (ii) in addition to convolutional filters we use objectlevel visual information. The latter has only been explored to generate a single global representation <ref type="bibr" target="#b17">(Grönroos et al., 2018)</ref> and used for example to initialise the encoder <ref type="bibr" target="#b25">(Huang et al., 2016)</ref>. We note that translation refinement is different translation re-ranking from a text-only model based on image representation <ref type="bibr" target="#b40">(Shah et al., 2016;</ref><ref type="bibr" target="#b23">Hitschler et al., 2016;</ref>, since the latter assumes that the correct translation can already be produced by a text-only model. <ref type="bibr" target="#b5">Caglayan et al. (2019)</ref> investigate the importance and the contribution of multimodality for MMT. They perform careful experiments by using input degradation and observe that, specially under limited textual context, multimodal models exploit the visual input to generate better translations. <ref type="bibr" target="#b5">Caglayan et al. (2019)</ref> also show that MMT systems exploit visual cues and obtain correct translations even with typographical errors in the source sentences. In this paper, we build upon this idea and investigate the potential of visual cues for refining translation.</p><p>Translation refinement: The idea of treating machine translation as a two step approach dates back to statistical models, e.g. in order to improve a draft sentence-level translation by exploring document-wide context through hill-climbing for local refinements <ref type="bibr" target="#b18">(Hardmeier et al., 2012)</ref>. Iterative refinement approaches have also been pro-posed that start with a draft translation and then predict discrete substitutions based on an attention mechanism <ref type="bibr" target="#b37">(Novak et al., 2016)</ref>, or using nonautoregressive methods with a focus on speeding up decoding <ref type="bibr" target="#b31">(Lee et al., 2018)</ref>. Translation refinement can also be done through learning a separate model for automatic post-editing <ref type="bibr" target="#b36">(Niehues et al., 2016;</ref><ref type="bibr" target="#b26">Junczys-Dowmunt and Grundkiewicz, 2017;</ref><ref type="bibr" target="#b8">Chatterjee et al., 2018)</ref>, but this requires additional training data with draft translations and their correct version.</p><p>An interesting approach is that of deliberation networks, which jointly train an encoder and first and second stage decoders <ref type="bibr" target="#b46">(Xia et al., 2017)</ref>. The second stage decoder has access to both left and right side context and this has been shown to improve translation <ref type="bibr" target="#b46">(Xia et al., 2017;</ref>. We follow this approach as it offers a very flexible framework to incorporate additional information in the second stage decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We base our model on the transformer architecture <ref type="bibr" target="#b44">(Vaswani et al., 2017)</ref> for neural machine translation. Our implementation is a multilayer encoder-decoder architecture that uses the tensor2tensor 1 <ref type="bibr" target="#b43">(Vaswani et al., 2018)</ref> library. The encoder and decoder blocks are as follows:</p><p>Encoder Block (E): The encoder block comprises of 6 layers, with each containing two sublayers of multi-head self-attention mechanism followed by a fully connected feed forward neural network. We follow the standard implementation and employ residual connections between each layer, as well as layer normalisation. The output of the encoder forms the encoder memory which consists of contextualised representations for each of the source tokens (M E ).</p><p>Decoder Block (D): The decoder block also comprises of 6 layers. It contains an additional sublayer which performs multi-head attention over the outputs of the encoder block. Specifically, decoding layer d l i is the result of a) multi-head attention over the outputs of the encoder which in turn is a function of the encoder memory and the outputs from the previous layer:</p><formula xml:id="formula_0">A D→E = f (M E , d l i−1 )</formula><p>where, the keys and values are the encoder outputs and the queries correspond to the decoder input, and b) the multi-head self attention which is a function of the generated outputs from the previous layer:</p><formula xml:id="formula_1">A D = f (d l i−1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deliberation networks</head><p>Deliberation networks <ref type="bibr" target="#b46">Xia et al., 2017)</ref> build on the standard sequence to sequence architecture to add an additional decoder block (in our case, with 3 layers -see <ref type="figure" target="#fig_1">Figure 2</ref>). The additional decoder (also referred to as secondpass decoder) is conditioned on the source and sampled outputs from the standard transformer decoder (the first-pass decoder). More concretely, the second-pass decoder (D ) at layer d l consists of A D , A D →E , A D →D , where, A D and A D →E is similar to the standard deliberation architecture multi-head attention over the encoder memory and self attention respectively while, A D →D is the multi-head attention over outputs O d from the first-pass decoder (D) (</p><formula xml:id="formula_2">A D→E = f (O d , d l i−1 )). 2</formula><p>In our experiments, we obtain samples as a set of translations from the first-pass decoder using beam-search. Given a translation candidate, O d consists of the first-pass decoder's hidden layer before softmax concatenated with the embeddings of the resultant words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal transformer &amp; deliberation</head><p>Our multimodal transformer models follow one of the two formulations below for conditioning trans-2 In the implementation we used, the deliberation network trains 345M parameters, as compared to the Transformer with 210M parameters. lations on image information:</p><p>Additive image conditioning (AIC): A projected image vector is added to each of the outputs of the encoder. The projections matrices are parameters that are jointly learned with the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention over image features (AIF):</head><p>The model attends over image features, as in <ref type="bibr" target="#b21">Helcl et al. (2018)</ref>, where the decoder block now contains an additional cross-attention sub-layer A D →V which attends to the visual information (V). The keys and values correspond to the visual information.</p><p>Within the deliberation network framework, based on the previously discussed observation (Section 1) that images are only needed in a small number of cases, we propose to add visual crossattention only to the second-pass decoder block (see <ref type="figure" target="#fig_1">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image features</head><p>Motivated by previous work that indicates the importance of structured information from images <ref type="bibr">(Caglayan et al., 2017;</ref>, we focus on structural forms of image representations, including the spatially aware feature maps from CNNs and information extracted from automatic object detectors.</p><p>Spatial image features: We use spatial feature maps from the last convolutional layer of a pretrained ResNet-50 <ref type="bibr" target="#b20">(He et al., 2016)</ref> CNN-based image classifier for every image. 3 These feature maps contain output activations for various filters while preserving spatial information. They have been used in various vision to language tasks including image captioning <ref type="bibr" target="#b47">(Xu et al., 2015)</ref> and multimodal machine translation (Section 2). Our formulation for the integration of these features into the deliberation network is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, setup (b). We use the the AIF setup and refer to models that use the representation as att.</p><p>Object-based image features: We use a bag-of-objects representation where the objects are obtained using an off-shelf object detector <ref type="bibr" target="#b29">(Kuznetsova et al., 2018)</ref> based on the Open Images dataset. This representations is a sparse 545-dimensional vector with the frequency of each (545) given object in an image. This is inspired by previous research that investigates the potential of object-based information for vision to language tasks <ref type="bibr" target="#b35">(Mitchell et al., 2012;</ref>. We use the the AIC setup and refer to models that use the representation as sum.</p><p>Object-based embedding features: The bagof-objects representations makes it hard to exploit object-to-object similarity, since visual representations of different objects can be very different. To mitigate this, we propose a simple extension using bag-of-object embeddings. We represent each object using the pre-trained GLoVebased <ref type="bibr" target="#b38">(Pennington et al., 2014)</ref> 50-dimensional word vectors for their categories (e.g. woman). We use the the AIF based setup and refer to models that use the representation as obj <ref type="figure" target="#fig_1">(Figure 2  setup (a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental settings 4.1 Data</head><p>We build and test our MMT models on the Multi30K dataset . Each image in Multi30K contains one English (EN) description taken from Flickr30K <ref type="bibr" target="#b48">(Young et al., 2014)</ref> and human translations into German (DE), French (FR) and Czech <ref type="bibr" target="#b1">Barrault et al., 2018)</ref>. The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present. In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test 2018 <ref type="bibr" target="#b1">(Barrault et al., 2018)</ref>. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Degradation of source</head><p>In addition to using the Multi30K dataset as is (standard setup), we probe the ability of our models to address the three linguistic phenomena where additional context has been proved important (Section 1): ambiguities, gender-neutral words and noisy input. In a controlled experiment where we aim to remove the influence of frequency biases, we degrade the source sentences by masking words through three strategies to replace words by a placeholder: random source words, ambiguous source words and gender unmarked source words. The procedure is applied to the train, validation and test sets. For the resulting dataset generated for each setting, we compare models having access to text-only context versus additional text and multimodal contexts. We seek to get insights into the contribution of each type of context to address each type of degradation.</p><p>Random content words In this setting (RND) we simulate erroneous source words by randomly dropping source content words. We first tag the entire source sentences using the spacy toolkit <ref type="bibr">(Honnibal and Montani, 2017)</ref> and then drop nouns, verbs, adjectives and adverbs and replace these with a default BLANK token. By focusing on content words, we differ from previous work that suggests that neural machine translation is robust to non-content word noise in the source <ref type="bibr" target="#b28">(Klubička et al., 2017)</ref>.</p><p>Ambiguous words In this setting (AMB), we rely on the MLT dataset  which provides a list of source words with multiple translations in the Multi30k training set. We replace ambiguous words with the BLANK token in the source language, which results in two languagespecific datasets.</p><p>Person words In this setting (PERS), we use the Flickr Entities dataset <ref type="bibr" target="#b39">(Plummer et al., 2017)</ref> to identify all the words that were annotated by humans as corresponding to the category person. <ref type="bibr">5</ref> We then replace such source words with the BLANK token.</p><p>The statistics of the resulting datasets for the three degradation strategies are shown in <ref type="table" target="#tab_3">Table 1</ref>. We note that RND and PERS are the same for language pairs as the degradation only depends on the source side, while for AMB the words replaced depend on the target language.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Models</head><p>Based on the models described in Section 3 we experiment with eight variants: (a) baseline transformer model (base); (b) base with AIC (base+sum); (c) base with AIF using spacial (base+att) or object based (base+obj) image features; (d) standard deliberation model (del); (e) deliberation models enriched with image information: del+sum, del+att and del+obj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>In all cases, we optimise our models with cross entropy loss. For deliberation network models, we first train the standard transformer model until convergence, and use it to initialise the encoder and first-pass decoder. For each of the training samples, we follow <ref type="bibr" target="#b46">(Xia et al., 2017)</ref> and obtain a set of 10-best samples from the first pass decoder, with a beam search of size 10. We use these as the first-pass decoder samples. We use Adam as optimiser <ref type="bibr" target="#b27">(Kingma and Ba, 2014)</ref>    <ref type="bibr" target="#b21">(Helcl et al., 2018)</ref>. <ref type="bibr">6</ref> We built on the tensor2tensor implementation of deliberation nets in https://github.com/ustctf/ delibnet using the transformer big parameters with a learning rate of 0.05 with 8K warmup steps for both the first and the second-pass decoders, and early stopping with the patience of 10 epochs based on the validation BLEU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section we present results of our experiments, first in the original dataset without any source degradation (Section 5.1) and then in the setup with various source degradation strategies (Section 5.2). <ref type="table" target="#tab_5">Table 2</ref> shows the results of our main experiments on the 2016 and 2018 test sets for French and German. We use Meteor <ref type="bibr" target="#b10">(Denkowski and Lavie, 2014)</ref> as the main metric, as in the WMT tasks <ref type="bibr" target="#b1">(Barrault et al., 2018)</ref>. We compare our transformer baseline to transformer models enriched with image information, as well as to the deliberation models, with or without image information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Standard setup</head><p>We first note that our multimodal models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to <ref type="bibr" target="#b21">(Helcl et al., 2018)</ref>. Second, our deliberation models lead to significant improvements over this baseline across test sets (average ∆METEOR = 1, ∆BLEU = 1).</p><p>Transformer-based models enriched with image information (base+sum, base+att and base+obj), on the other hand, show no major improvements with respect to the base performance. This is also the case for deliberation models with image information (del+sum, del+att, del+obj), which do not show significant improvement over the vanilla deliberation performance (del).</p><p>However, as it has been shown in the WMT shared tasks on MMT <ref type="bibr" target="#b1">Barrault et al., 2018)</ref>, automatic metrics often fail to capture nuances in translation quality, such as, the ones we expect the visual modality to help with, which -according to human perception -lead to better translations. To test this assumption in our settings, we performed human evaluation involving professional translators and native speakers of both French and German (three annotators).</p><p>The annotators were asked to rank randomly selected test samples according to how well they convey the meaning of the source, given the image (50 samples per language pair per annotator). For each source segment, the annotator was shown the outputs of three systems: base+att, the current MMT state-of-the-art <ref type="bibr" target="#b21">(Helcl et al., 2018)</ref>, del EN:</p><p>Two men work under the hood of a white race car.</p><p>base+att: Zwei Männer arbeiten unter der Motorhaube eines weißen Rennens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Zwei Männer arbeiten unter der Motorhaube eines weißen Autos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Zwei Männer arbeiten unter der Motorhaube eines weißen Rennwagen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE:</head><p>Zwei Männer arbeiten unter der Haube eines weißen Rennautos.</p><p>(a) base+att translates race car with Rennen (race), del with Auto (car) and del+obj with Rennwagen (race car). Objects: land, vehicle, car, wheel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN:</head><p>A young child holding an oar paddling a blue kayak in a body of water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base+att:</head><p>Un jeune enfant tenant une rame dans un kayak bleu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Un jeune enfant tenant une rame dans un kayak bleu sur un plan d'eau.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Un jeune enfant tenant une rame dans un kayak bleu pagayant sur un plan d'eau.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FR:</head><p>Un jeune enfant avec une rame pagayant dans un kayak bleu sur un plan d'eau.</p><p>(b) del and del+obj translate in a body of water with sur un plan d'eau (on a body of water), missing in base+att. del+obj translates the word paddling with pagayant (paddling). Objects: paddle, canoe and del+obj. A rank could be assigned from 1 to 3, allowing ties <ref type="bibr" target="#b2">(Bojar et al., 2017</ref>). Annotators could assign zero rank to all translations if they were judged incomprehensible. Following the common practice in WMT <ref type="bibr" target="#b2">(Bojar et al., 2017)</ref>, each system was then assigned a score which reflects the proportion of times it was judged to be better or equal other systems. <ref type="table" target="#tab_7">Table 3</ref> shows the human evaluation results. They are consistent with the automatic evaluation results when it comes to the preference of humans towards the deliberation-based setups, but show a more positive outlook regarding the addition of visual information (del+obj over del) for French.  Manual inspection of translations suggests that deliberation setups tend to improve both the grammaticality and adequacy of the first pass outputs. For German, the most common modifications performed by the second-pass decoder are substitutions of adjectives and verbs (for test 2016, 15% and 12% respectively, of all the edit distance operations). Changes to adjectives are mainly gram-matical, changes to verbs are contextual (e.g., changing laufen to rennen, both verbs mean run, but the second refers to running very fast). For French, 15% of all the changes are substitutions of nouns (for test 2016). These are again very contextual. For example, the French word travailleur (worker) is replaced by ouvrier (manual worker) in the contexts where tools, machinery or buildings are mentioned. For our analysis we used again spacy.</p><p>The information on detected objects is particularly helpful for specific adequacy issues. <ref type="figure">Figure</ref> 3 demonstrates some such cases. In the first case, the base+att model misses the translation of race car: the German word Rennen translates only the word race. del introduces the word car (Auto) into the translation. Finally, del+obj correctly translates the expression race car (Rennwagen) by exploiting the object information. For French, del translates the source part in a body of water, missing from the base+att translation. del+obj additionally translated the word paddling according to the detected object Paddle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Source degradation setup</head><p>Results of our source degradation experiments are shown in <ref type="table" target="#tab_9">Table 4</ref>. A first observation is that -as with the standard setup -the performance of our deliberation models is overall better than that of the base models. The results of the multimodal  models differ for German and French. For German, del+obj is the most successful configuration and shows statistically significant improvements over base for all setups. Moreover, for RND and AMB, it shows statistically significant improvements over del. However, especially for RND and AMB, del and del+sum are either the same or slightly worse than base. For French, all the deliberation models show statistically significant improvements over base (average ∆METEOR = 1, ∆BLEU = 1.1), but the image information added to del only improve scores significantly for test 2018 RND.</p><p>This difference in performances for French and German is potentially related to the need of more significant restructurings while translating from English into German. 7 This is where a more complex del+obj architecture is more helpful. This is especially true for RND and AMB setups where blanked words could also be verbs, the part-ofspeech most influenced by word order differences between English and German (see the decreasing complexity of translations for del and del+obj for the example (c) in <ref type="figure">Figure 4</ref>).</p><p>To get an insight into the contribution of different contexts to the resolution of blanks, we performed manual analysis of examples coming from the English-German base, del and del+obj setups (50 random examples per setup), where we count correctly translated blanks per system.</p><p>The results are shown in  <ref type="table" target="#tab_10">Table 5</ref>: Results of human annotation of blanked translations (English-German). We report counts of blanks resolved by each system, as well as total source blank count for each selection (50 sentences selected randomly).</p><p>difficult to resolve (at most 40% resolved as compared to 61% for PERS). Translations of the majority of those blanks tend to be guessed by the textual context alone (especially for verbs). Image information is more helpful for PERS: we observe an increase of 10% in resolved blanks for del+obj as compared to del. However, for PERS the textual context is still enough in the majority of the cases: models tend to associate men with sports or women with cooking and are usually right (see <ref type="figure">Figure 4</ref> example (c)). The cases where image helps seem to be those with rather generic contexts: see <ref type="figure">Figure 4</ref> </p><formula xml:id="formula_3">(b)</formula><p>where enjoying a summer day is not associated with any particular gender and make other models choose homme (man) or femme (woman), and only base+obj chooses enfant (child) (the option closest to the reference).</p><p>In some cases detected objects are inaccurate or not precise enough to be helpful (e.g., when an object Person is detected) and can even harm correct translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have proposed a novel approach to multimodal machine translation which makes better EN:</p><p>Three farmers harvest rice out in a rice field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base:</head><p>Drei Bauern ernten sich mit einem Reisfeld.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Drei Bauern ernten Reis mit einem Reisfeld.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Drei Bauern ernten sich mit einem Reishut auf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE:</head><p>Drei Farmer ernten Reis auf einem Feld.</p><p>(a) Example of a blank resolved by the textual context for AMB: field translated as Reisfeld (rice field) by base. del+obj incorrectly translated the blank into Reishut (rice hat) due to detected objects. Objects: person, clothing, mammal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN:</head><p>The boy is outside enjoying a summer day.</p><p>base: L'homme profite d'une journée d'été.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>La femme profite d'une journée d'été.</p><p>del+obj: L'enfant profite d'une journée d'été.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FR:</head><p>Le garçon est dehors, profitant d'une journée d'été.</p><p>(b) Example of a blank resolved by the multimodal context for PERS. The textual context is too generic and del+obj uses the detected objects to correctly translate boy into l'enfant (child). Objects: clothing, face, tree, boy, jeans EN: Dirt biker makes a sloping turn in a forest during the fall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base:</head><p>Geländemotorradfahrer macht in einem Wald eine Kurve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Geländemotorradfahrer macht in einem Herbst während Zuschauer eine Kurve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Geländemotorradfahrer macht in einem Herbst eine Kurve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE:</head><p>Ein offroad-biker fährt im Herbst durch eine steile Kurve.</p><p>(c) Example of a blank resolved by the textual context for PERS. biker correctly translated into the Masc. form Geländemotorradfahrer (dirt biker) by base. Objects: person, tree, bike, helmet <ref type="figure">Figure 4</ref>: Examples of resolved blanks for test set 2016. Underlined text denotes blanked words and their translations. Object field indicates the detected objects.</p><p>use of context, both textual and visual. Our results show that further exploring textual context through deliberation networks already leads to better results than the previous state of the art. Adding visual information, and in particular structural representations of this information, proved beneficial when input text contains noise and the language pair requires substantial restructuring from source to target. Our findings suggest that the combination of a deliberation approach and information from additional modalities is a promising direction for machine translation that is robust to noisy input. Our code and pre-processing scripts are available at https:// github.com/ImperialNLP/MMT-Delib.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of lexical and gender ambiguity, and inaccurate English description where post-edits (PE) required the image to correct human translation from English (EN) to German (DE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our deliberation architecture: The secondpass decoder is conditioned on the source and samples output from the first-pass decoder. The second-pass decoder has access to (a) the object based features represented by embeddings, or (b) spacial image features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples of improvements of del and del+obj over base+att for test set 2016 for French and German. Underlined words represent some of the improvements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1906.07701v1 [cs.CL] 18 Jun 2019 EN: Three children in football uniforms are playing football. DE: Drei Kinder in Fußballtrikots spielen Fußball. PE: Drei Kinder in Footballtrikots spielen Football. (a) Ambiguous word football translated as soccer (Fußball) EN: A baseball player in a black shirt just tagged a player in a white shirt. DE: Ein Baseballspieler in einem schwarzen Shirt fängt einen Spieler in einem weißen Shirt.</figDesc><table><row><cell>PE: Eine Baseballspielerin in einem schwarzen Shirt fängt eine Spielerin in einem weißen</cell></row><row><cell>Shirt.</cell></row><row><cell>(b) Gender-neutral word player translated as male player (Spieler)</cell></row><row><cell>EN: A woman wearing a white shirt works out on an elliptical machine.</cell></row></table><note>DE: Eine Frau in einem weißen Shirt trainiert auf einem Crosstrainer.PE: Eine Frau in einem weißen Pullover trainiert auf einem Crosstrainer.(c) Inaccurate English word shirt instead of sweater or pullover</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>setup% sent. avg. blanks per sent.</figDesc><table><row><cell>RND</cell><cell>100</cell><cell>1.5</cell></row><row><cell>AMB DE</cell><cell>83</cell><cell>2</cell></row><row><cell>AMB FR</cell><cell>77</cell><cell>1.8</cell></row><row><cell>PERS</cell><cell>92</cell><cell>1.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets after applying source degradation strategies</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results for the test sets 2016 and 2018. M denotes METEOR, B -BLEU; * marks statistically significant changes for METEOR (p-value ≤ 0.05) as compared to base, † -as compared to del. Bold highlights statistically significant improvements. We report previous state of the art results for multimodal models from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Human ranking results: normalised rank (micro-averaged). Bold highlights best results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results for the test sets 2016 and 2018 for the three degradation configurations: RND, AMB and PERS. M denotes METEOR, B -BLEU; * marks statistically significant changes as computed for METEOR (p-value ≤ 0.05) as compared to base, † -as compared to del. Bold highlights statistically significant improvements over base.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>As expected, they show that the RND and AMB blanks are more 7 English and French are both languages with the subjectverb-object (SVO) sentence structure. German, on the other hand, can have subject-object-verb (SOV) constructions. For example, a German sentence Gestern bin ich in London gewesen (Yesterday have I to London been) would need to be restructured to Yesterday I have been to London in English.</figDesc><table><row><cell cols="5">setup base del del+obj gold</cell></row><row><cell>RND</cell><cell>22</cell><cell>23</cell><cell>24</cell><cell>79</cell></row><row><cell>AMB</cell><cell>29</cell><cell>25</cell><cell>33</cell><cell>88</cell></row><row><cell>PERS</cell><cell>43</cell><cell>46</cell><cell>51</cell><cell>84</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorflow/ tensor2tensor</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Provided athttp://statmt.org/wmt18/ multimodal-task.html.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The pre-processed datasets provided by the organisers were used without additional pre-processing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We pre-processed the initial dataset to remove noise. We also add the gender-marked pronouns he, she, her and his to the person word list.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the anonymous reviewers for their useful feedback. This work was supported by the MultiMT (H2020 ERC Starting Grant No. 678017) and MMVC (Newton Fund Institutional Links Grant, ID 352343575) projects. We also thank the annotators for their valuable help.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN:</head><p>A bride and groom kiss under the bride's veil.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base:</head><p>Ein Mann und eine Frau küssen sich unter den Blicken der Frau.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Ein Mann und eine Frau küssen sich unter dem Brautschleier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Ein Mann und eine Frau küssen sich unter den hin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE:</head><p>Eine Braut und Bräutigam küssen sich unter dem Brautschleier .</p><p>(a) PERS example: bride and groom translated are correctly translated by base into Frau (wife) and Mann (husband). Objects: face, woman, dress</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN:</head><p>A brown dog runs down the sandy beach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base:</head><p>Ein brauner Hund läuft an einem sandigen Strand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Ein brauner Hund rennt den Sandstrand hinunter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Ein brauner Hund läuft an einem sandigen Strand hinunter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FR:</head><p>Ein brauner Hund läuftüber den Sandstrand.</p><p>(b) AMB example: runs is correctly translated by base into läuft. Objects: dog <ref type="figure">Figure 5</ref>: Examples of blanks for test set 2016 that were correctly resolved by the textual context. The underlined words denote blanked words and their translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN:</head><p>A woman and a dog sit on a white bench near a beach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base:</head><p>Eine Frau und ein Hund sitzen an einem weißen Strand nahe einem Strand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Eine Frau und ein Hund sitzen auf einem weißen Sofa in der nähe eines Strands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Eine Frau und ein Hund sitzen auf einer weißen Bank nahe einem Strand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE:</head><p>Eine Frau und eine Hund sitzen auf einer weißen Bank in der nähe eines Strandes.</p><p>(a) RND example: the blank bench is correctly translated by del+obj into Bank due to the detected object Bench. Objects: person, dog, bench EN: Two men dressed in green are preparing food in a restaurant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base:</head><p>Deux femmes vêtues de vert préparent des aliments dans un restaurant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Deux femmes vêtues de vert préparent de la nourriture dans un restaurant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Deux asiatiques en vert préparent de la nourriture dans un restaurant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FR:</head><p>Deux hommes habillés en vert préparent de la nourriture dans un restaurant.</p><p>(b) PERS example. men correctly translated into asiatiques (asians) by del+obj. Objects: person, clothing, man, food, cake <ref type="figure">Figure 6</ref>: Examples of blanks for test set 2016 that were correctly resolved by the multimodal context. The underlined words denote blanked words and their translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN:</head><p>A guy give a kiss to a guy also.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base:</head><p>Ein Mann, der sich vor, um eine Frau zu knüssen .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Ein Mann, der sich vor, um eine Frau zu küssen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Ein Mann, der einem kuss küsst, um eine Frau zu küssen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE:</head><p>Ein Typ küsst einen anderen Typ .</p><p>(a) PERS example: the second mention of guy is consistently translated into Frau (woman). Objects: clothing, man, face</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN:</head><p>A group of students sit and listen to the speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>base:</head><p>Eine Gruppe von Studenten sitzt und schaut nach rechts .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del:</head><p>Eine Gruppe Schüler sitzt und schaut nach rechts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>del+obj:</head><p>Eine Gruppe Schüler sitzt und schaut zu rechts auf das Wasser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DE:</head><p>Eine Gruppe von Studenten sitzt und hört der Sprecherin zu.</p><p>(b) AMB example. The blanks listen and speaker are consistently translated into schaut (look) and rechts (right) or Wasser (water). Objects: person, clothing, man, food, cake <ref type="figure">Figure 7</ref>: Examples of unresolved blanks. The underlined words denote blanked words and their translations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Findings of the third shared task on multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiraag</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="304" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the 2017 conference on machine translation (WMT17)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4717</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="169" to="214" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LIUM-CVC submissions for WMT17 multimodal translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weijer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4746</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probing the need for visual context in multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4159" to="4170" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating global visual features into attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Doubly-attentive decoder for multi-modal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1175</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1913" to="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Findings of the WMT 2018 shared task on automatic post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation</title>
		<meeting>the Third Conference on Machine Translation<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="723" to="738" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical study on the effectiveness of images in multimodal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Dupont</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1095</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="910" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Findings of the second shared task on multimodal machine translation and multilingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="215" to="233" />
		</imprint>
	</monogr>
	<note>Shared Task Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-language image description with neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<idno>abs/1510.04709</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi30k: Multilingual englishgerman image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Workshop on Vision and Language</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagination improves multimodal translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kádár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="130" to="141" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Equalizing gender biases in neural machine translation with word embeddings techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Escudé</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<idno>abs/1901.03116</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Assessing multilingual multimodal image description: Studies of native speaker preferences and translator choices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324918000074</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">393413</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The MeMAD submission to the WMT18 multimodal translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig-Arne</forename><surname>Grönroos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Merialdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mats</forename><surname>Sjöberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Sulubacak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Troncy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raúl</forename><surname>Vázquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="603" to="611" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Document-wide decoding for phrasebased statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1179" to="1190" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Achieving human parity on automatic Chinese to English news translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1803.05567</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CUNI system for the WMT18 multimodal translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinďrich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinďrich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dusan</forename><surname>Varis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="616" to="623" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Women also snowboard: Overcoming bias in captioning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="793" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal pivots for image caption translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigehiko</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1227</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2399" to="2409" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">2017. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention-based multimodal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sz-Rung</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="639" to="645" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An exploration of neural sequence-tosequence architectures for automatic post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
	<note>Long Papers). Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fine-grained human evaluation of neural versus phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Klubička</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Víctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sánchez-Cartagena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<title level="m">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sheffield submissions for WMT18 multimodal translation shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiraag</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranava</forename><surname>Swaroop Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation</title>
		<meeting>the Third Conference on Machine Translation<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="630" to="637" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention strategies for multi-source sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinďrich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinďrich</forename><surname>Helcl</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sheffield MultiMT: Using object posterior predictions for multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranava</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4752</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="470" to="476" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">End-to-end image captioning exploits multimodal distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Pranava Swaroop Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specia</surname></persName>
		</author>
		<idno>abs/1809.04144</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Midge: Generating image descriptions from computer vision detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="747" to="756" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pre-translation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Iterative refinement for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno>abs/1610.06602</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0965-7</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="74" to="93" />
			<pubPlace>Hingham, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SHEF-Multimodal: Grounding machine translation on images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Machine Translation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="657" to="662" />
		</imprint>
	</monogr>
	<note>Shared Task Papers, WMT. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A shared task on multimodal machine translation and crosslingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Getting gender right in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Vanmassenhove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3003" to="3008" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Tensor2Tensor for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<editor>Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object counts! Bringing explicit detections back into image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranava</forename><surname>Swaroop Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1784" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
