<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pseudo Shots: Few-Shot Learning with Auxiliary Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Esfandiarpoor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Hajabdollahi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
						</author>
						<title level="a" type="main">Pseudo Shots: Few-Shot Learning with Auxiliary Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https://github.com/Reza-esfandiarpoor/pseudo-shots</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many practical few-shot learning problems, even though labeled examples are scarce, there are abundant auxiliary data sets that potentially contain useful information. We propose a framework to address the challenges of efficiently selecting and effectively using auxiliary data in image classification. Given an auxiliary dataset and a notion of semantic similarity among classes, we automatically select pseudo shots, which are labeled examples from other classes related to the target task. We show that naively assuming that these additional examples come from the same distribution as the target task examples does not significantly improve accuracy. Instead, we propose a masking module that adjusts the features of auxiliary data to be more similar to those of the target classes. We show that this masking module can improve accuracy by up to 18 accuracy points, particularly when the auxiliary data is semantically distant from the target task. We also show that incorporating pseudo shots improves over the current state-of-the-art few-shot image classification scores by an average of 4.81 percentage points of accuracy on 1-shot tasks and an average of 0.31 percentage points on 5-shot tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large labeled datasets for new computer vision tasks are costly and time-consuming to gather, so there is a great demand for methods that can learn new tasks with less labeling. One such area of work is few-shot learning (FSL), where we seek to learn a model with only one or a few labeled examples per class <ref type="bibr" target="#b8">[9]</ref>. Learning a model that generalizes well is challenging with such limited information. However, for many tasks, there are rich, abundant sources of auxiliary <ref type="bibr" target="#b0">1</ref> Computer Science Department, Brown University, Providence, RI, USA 2 Electrical and Computer Engineering Department, Isfahan University of Technology, Isfahan, Isfahan, Iran. Correspondence to: Reza Esfandiarpoor &lt;reza esfandiarpoor@brown.edu&gt;.</p><p>data that are publicly available. Auxiliary data consists of labeled data for related tasks and semantic knowledge in the form of knowledge graphs. Such data has the potential to significantly improve FSL accuracy, but-as we shownaive approaches fail to improve over the the current state of the art and even harm performance. In this work, we address the two challenges of selecting which auxiliary data to use and using it more effectively.</p><p>Due to its effectiveness, the use of knowledge and data external to the target task appears in many problems such as transfer learning <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b46">46]</ref>, multi-task learning (MTL) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">56]</ref>, and semi-supervised learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b48">48]</ref>. Such methods can improve generalization and compensate for the lack of labeled data using readily available resources. While these methods have been adapted to FSL problems to help with representation learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b4">5]</ref>, the problem of exploiting auxiliary data chosen specifically for test classes in FSL has received considerably less attention.</p><p>Recent work has considered alternative FSL scenarios related to, but distinct from our proposed setting. Several recent studies on few-shot image classification have investigated a transductive setup for FSL where in addition to limited labeled examples (support set), they take advantage of the unlabeled test examples (query set) which are drawn from the same support classes <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12]</ref>. These methods assume that we have access to large query sets during testing. However, this assumption does not always hold in practice. There are also a number of semi-supervised FSL methods. Their difference with transductive method is that their unlabeled set is not necessarily the same as the query set <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b26">26]</ref>. However, they generally assume that unlabeled examples of the test classes are available, sometimes mixed with distractor images in a 1:1 ratio. The problem of auxiliary data that we are solving differs from either transductive or semi-supervised FSL. Here we are assuming that we have no access to the query set during training, nor do we have unlabeled examples of the test classes. All we have access to are labeled samples from a set of semantically related classes that is disjoint from the set of test classes.</p><p>Although auxiliary data is often abundant, selecting which data to use and using it effectively pose several challenges. Sources like ImageNet <ref type="bibr" target="#b5">[6]</ref> are huge, and we inevitably have to select a subset of it. Also, a significant portion of the auxiliary data is not informative about the target task. Therefore, auxiliary data selection has a key role, and naive approaches like random sampling are unlikely to provide relevant information. Using visual similarity to select relevant data is computationally expensive. Instead, we propose to use semantic similarity between classes in the target task and classes in the auxiliary data. A second set of challenges comes from the fact that, even if we select auxiliary data that is similar to that of our target task, they are not sampled from the same distribution. Given the limited labeled information available in a FSL task, the inevitable noise can easily outweigh the training set if we naively treat the auxiliary data the same as the original training set. We will address this problem by learning how to adjust the auxiliary data to be more useful for the target task.</p><p>In this paper, we propose a framework for few-shot image classification with semantically related auxiliary data. We propose the concept of pseudo shots, which are automatically curated subsets of auxiliary data that share semantic information with the training and test classes. To select pseudo shots, we use common sense knowledge graphs as a form of semantic similarity among classes in the training, test, and auxiliary datasets. This approach has the advantages of broad applicability to many tasks and computational efficiency. However, we show that naively treating pseudo shots as additional data in a semi-supervised fashion generally does not improve accuracy. More sophisticated semi-supervised methods for FSL <ref type="bibr" target="#b37">[37]</ref> that attempt to downweight irrelevant examples also fail to significantly improve performance over the state of the art. We argue that this failure is due to the assumption that the additional data contains a mix of examples from target classes and irrelevant examples, which is not the case with pseudo shots.</p><p>To cope with the discrepancy between the target task and pseudo shot classes, we design a masking module that filters each pseudo shot in the feature space based on the corresponding target class. We propose a module of convolutional blocks that takes the averaged target class support set and corresponding pseudo shots as inputs to produce a feature mask. We train this module with a meta-learning approach, and show that it learns to adjust the pseudo shots to be more helpful for learning, particularly when the pseudo shots are semantically distant from the target classes.</p><p>To evaluate pseudo shots, we propose extending established few-shot image classification benchmarks with subsets of ImageNet 22k <ref type="bibr" target="#b5">[6]</ref> as auxiliary data. These subsets exclude the test classes and their WordNet <ref type="bibr" target="#b9">[10]</ref> descendants, and we consider four different amounts of additional pruning to create increasing semantic distance from the test classes. This setup captures our scenarios of interest, in which we have related labeled data sets but not any additional examples of the test classes, labeled or otherwise. With the growth of public benchmark datasets, this is an increasingly common scenario in practice. We evaluate on tieredImageNet <ref type="bibr" target="#b37">[37]</ref>, miniImageNet <ref type="bibr" target="#b47">[47]</ref>, CIFAR-FS <ref type="bibr" target="#b23">[24]</ref>, and FC-100 <ref type="bibr" target="#b31">[31]</ref>, and show that our model surpasses the state of the art for both 1and 5-shot classification tasks (up to 6.26 percentage points of accuracy on 1-shot tasks and up to 1.82 on 5-shot tasks). Further, most of the gains come from our masking mechanism, significantly improving over semi-supervised FSL methods. Our results show that, while using auxiliary data is intuitively appealing, efficiently selecting and effectively adapting to the target task is critical for best performance.</p><p>Our main contributions are:</p><p>• We propose pseudo shots as a framework for incorporating semantically related auxiliary data into FSL. Pseudo shots are automatically curated auxiliary data that are combined with the support set of target classes at training and test time to improve accuracy.</p><p>• We design a masking module that adjusts the pseudo shots in feature space to remove irrelevant information from each example. We show that this mechanism improves performance by up to 18 percentage points of accuracy, particularly when the pseudo shots are semantically distant from the target classes.</p><p>• We introduce new tasks based on current FSL benchmarks that simulate the common scenario where novel classes need to be predicted and semantically related data is available, but no additional examples of the target classes are available, labeled or otherwise.</p><p>• We evaluate our masking model on these tasks, using auxiliary data with four different levels of semantic distance from the target classes. On one-shot tasks, when the target classes and all WordNet subtypes are excluded from the auxiliary data, we find that our proposed approach achieves accuracies on average 3.62 percentage points higher than a naive semi-supervised approach, 4.81 higher than state-of-the-art methods without any auxiliary data, and 17 higher than the semi-supervised approach of Ren et al. <ref type="bibr" target="#b37">[37]</ref>. On the analogous five-shot tasks, the average improvements are 5.3, 0.31, and 2.18 percentage points, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In FSL, we often learn from a large dataset divided into a set of training classes C train . Then, at test time, we will make predictions on data divided into a set of distinct test classes C test . Note that the training classes and testing classes are disjoint, i.e., C train ∩ C test = ∅. Following many previous works, we use a episodic meta-learning paradigm <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24</ref>]. An episodic paradigm simulates the testing situation during training. To create a K-shot N-way episode, we randomly sample a set of N classes as support classes, C. (To create a testing or training episode, support classes are sampled from C test or C train , respectively.) Then we randomly sample K images from each of them. This set of N × K samples is known as the support set, S. We take another q samples from each of the N classes as our query set, Q. An FSL model is aware of the support set labels and uses them to predict the labels of the query set samples, often in a nearest-neighbor fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Few Shot Learning with Pseudo Shots</head><p>In this section, we first explain the concept of pseudo shots and discuss how FSL benefits from them. Then, we describe our proposed method for selecting pseudo shots based on knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pseudo Shots</head><p>In many real-world FSL scenarios, there exist auxiliary labeled data sets with related classes. For example, public data sets like ImageNet 22k cover a wide range of classes.</p><p>We introduce the concept of pseudo shots to capture the labeled samples in these auxiliary data sets that are particularly useful for a novel task. Pseudo shots are automatically selected, with the goal that they share a greater amount of visual and semantic information with the support set than the rest of auxiliary samples. From another perspective, pseudo shots simulate extra labeled samples which might provide useful information by being more similar to one target class than the others. For example, when classifying images of horses, additional examples of related animals like zebras, deer, and moose might help separate horses from more dissimilar classes.</p><p>To select pseudo shots, for each support class c, we choose a set of K samples similar to class c. We call this the pseudo shot set for class c which is denoted by S c . We extend supervised, episodic learning such that there is a support, query, and pseudo shot set for each support class. We now discuss one useful method for selecting pseudo shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Selecting Pseudo Shots with Knowledge Graphs</head><p>We propose using a notion of semantic similarity between two classes to select pseudo shots, which has several benefits versus visual similarity. First, semantic similarity is applicable in a wide range of tasks and does not require learning a feature representation for all the available auxiliary data. Second, semantic similarity is a computationally efficient proxy for visual similarity. It allows us to avoid expensive per-image comparisons between the support sets and the auxiliary data. We still can discover visually related examples because the similarity between the aggregated visual features of a class is directly related to the semantic similarity between the classes <ref type="bibr" target="#b6">[7]</ref>. This setup also has a potential benefit of discovering a wider visual range of auxiliary data that pertain to each support class.</p><p>To estimate the semantic similarity between two classes, we rely on common sense knowledge graphs, which are graph structures that model natural concepts and the relation between them. We focus on the ConceptNet graph <ref type="bibr" target="#b42">[42]</ref>, since it covers concepts from many different domains, has multiple relationship types, and provides Euclidean embeddings for nodes to capture semantic similarity. These node vectors are created using a method called NumberBatch, which starts with Word2Vec <ref type="bibr" target="#b28">[28]</ref> and GloVe <ref type="bibr" target="#b33">[33]</ref> embeddings for the name of each node, and then uses the graph structure to adjust their positions. The result is that these embeddings capture a wide range of semantic relationships, with nodes having a smaller distance not just to those with names that often co-occur in text, but also those that are related in other ways, such as being of similar types, having similar functions, etc.</p><p>Here, we use the cosine similarity between the corresponding node vectors in ConceptNet as a measure of similarity between two classes. For each support class c, we select the N most similar classes from the set of auxiliary classes, C A , and choose K samples from them as the pseudo shot set S c :</p><formula xml:id="formula_0">C c = argmax {c1,...,c N }⊆C A ci E c , E ci .<label>(1)</label></formula><p>where · is cosine similarity and E is the corresponding ConceptNet node vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model</head><p>In this section, we introduce our proposed model, which is designed to improve the usefulness of the selected pseudo shots. First, we explain the details of our embedding function in Section 4.1, which is similar to prior work. In Section 4.2, we explain our basic model which naively treats the pseudo shots and support set samples the same. As shown by our experiments, the performance of this basic model is mediocre, with performance often being harmed by semantically distant pseudo shots. We address this deficiency in Section 4.3, where we describe our masking model, which filters the irrelevant information in pseudo shots by comparing them with support set samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Embedding Function</head><p>We need an embedding function to represent images as lower-dimensional features. We train the parameters of our embedding function, f φ , with a typical classification loss. Previous works <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b4">5]</ref> show that it extracts high quality  <ref type="figure">Figure 1</ref>. Our masking model pipeline. We train our embedding function, f φ , with a classification loss (green box). The masking module takes the features of support set and pseudo shots and generates a class specific mask (red box). We mask the pseudo shots and use cosine similarity on the outputs of the universal transformation function, f β , for meta-training/testing. features for FSL. We use all the training classes and their corresponding pseudo shots for training the embedding function. Here, there is no notion of episodes. So, we consider all the training dataset as a single large episode. We select the corresponding pseudo shots by following the steps in 3.2. To learn the parameters of the embedding function, we add a fully connected layer that maps the features to the |C| + |C | training and pseudo shot classes. Then we minimize the cross entropy loss on all the pseudo shots and training dataset. After training, we drop the fully connected layer and freeze the parameters of the feature embedding function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Basic Model</head><p>Our basic model assumes that pseudo shots and support samples are from the same distribution, i.e., pseudo shots are extra labeled samples from support classes. Given the embedding function, f φ , the class centroids are calculated as</p><formula xml:id="formula_1">h c = x∈Sc f φ (x) + x∈S c f φ (x) |S c | + |S c | ∀c ∈ C . (2)</formula><p>For each input x, eq. 3 gives the probability distribution over support classes. The model predicts the highest probability class as the label. We let</p><formula xml:id="formula_2">p(y = c|x) = exp( f φ (x), h c ) c ∈C exp( f φ (x), h c ) ,<label>(3)</label></formula><p>where · is cosine similarity between the embedded features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Masking Model</head><p>This section describes our masking model, which is designed to maximize the benefits of pseudo shots by filtering their irrelevant information. Our masking model further refines the embedded features by comparing pseudo shots with support set feature embeddings and masks the irrelevant parts of pseudo shot features.</p><p>For both pseudo shots and support images, we use a mean aggregator to calculate a single representation that allows us to compare them against each other. Let f φ (x) be of shape (C * , W, H) where C * , W , and H are the number of channels, width, and height of the feature embedding, respectively. <ref type="bibr" target="#b0">1</ref> The mean aggregator transforms the dimensions as</p><formula xml:id="formula_3">mean : (N, K * , C * , W, H) → (N, C * , W, H) ,<label>(4)</label></formula><p>where K * equals K and K for support images and pseudo shots, respectively. Next, we define f θ . As input, f θ takes the concatenated features for each class, and its output shows the similar parts between pseudo shot and support sample feature embeddings. We denote the mask for each class with m c which is equal to σ(f θ (cat(a c , a c ))). a c and a c are the mean-aggregated features for support samples and pseudo shots, respectively. The sigmoid function, σ, gives us the final mask with values in the [0, 1] interval:</p><formula xml:id="formula_4">σ(f θ (.)) : (N, 2C * , W, H) → (N, 1, W, H) . (5)</formula><p>To exploit the inherent spatial correlation of images, we choose to calculate a scalar mask value for each position in the (H, W ) plane that is the same across all channels rather than having a scalar mask value for each channel that masks all the positions. To achieve this spatial mask, we follow a pyramid structure for designing f θ that consists of three ResBlocks <ref type="bibr" target="#b16">[17]</ref> with (2C * , C * , 1) feature maps.</p><p>Since we use a different mask for pseudo shots of each class, they go through different transformations in the feature space. These transformations do not preserve the relative interclass distance between pseudo shots, nor the relative distance between pseudo shots and support and query samples that are not transformed. This makes it harder to use a cosine similarity function in this feature space. We learn a function, f β , that transforms all the features to a final space in which the cosine similarity between feature vectors is directly related to the similarity between original input images. Now, we can calculate the class centroids by averaging the support features and the masked pseudo shot features:</p><formula xml:id="formula_5">h c = x∈Sc (f β • f φ )(x) + x∈S c f β (f φ (x) m c ) |S c | + |S c | ,<label>(6)</label></formula><p>where is the Hadamard product. For calculating the probability distribution over support classes for any input x, we update eq. 3 and include f β :</p><formula xml:id="formula_6">p(y = c|x) = exp( (f β • f φ )(x), h c ) c ∈C exp( (f β • f φ )(x), h c ) .<label>(7)</label></formula><p>For testing, we use an argmax operator over this distribution. During training, we use this distribution to learn the model parameters by minimizing the cross entropy loss on the query set, Q.</p><p>Helper Dataset. We observe a behavior similar to overfitting when training the masking module. However, unlike overfitting, adding more training data does not help since the set of training and testing classes are disjoint <ref type="bibr" target="#b4">[5]</ref>. The masking module takes as input the output of the embedding function. The embedding function generates well clustered features for the seen classes during training, but more scattered features for the unseen test classes. Therefore, we fail to simulate the testing situation during training and we train the masking module on a much easier problem than that of testing which explains the behavior similar to overfitting. To address this problem, we add an unseen subset of auxiliary classes to the training data to simulate the data distribution during testing. To have more scattered classes, we choose the least similar classes to that of the training classes as helper classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Auxiliary Data Benchmarks</head><p>When learning with auxiliary data, the similarity of the available data to the target task directly affects the quality of the model. Ideally, an algorithm would do well even with distantly related auxiliary data, and, if that is not possible, degrade gracefully. We propose extensions to common FSL benchmarks that provide a standardized way to measure how algorithms perform in such scenarios.</p><p>To create auxiliary data sets of increasing semantic distance from the target tasks, we start with ImageNet22k and use the corresponding hierarchical relationships among classes in WordNet <ref type="bibr" target="#b9">[10]</ref> to prune data. In WordNet, organizing words with respect to the hyponym (subtype) relation creates a directed tree in which child classes are subtypes of their parents. As our baseline constraint in all scenarios, we eliminate all the descendants of each test class c (and c itself) from ImageNet22k. To create additional auxiliary datasets of increasing semantic distance from the targets, we move up the tree and prune ancestors of the test classes. We refer to each such dataset as a level from 0 (pruning the test classes and their descendants) to 3 (pruning the great-grandparents of the test classes and all descendants).</p><p>In general, we constrain the auxiliary data with respect to class c at level l by removing all the descendants of the l th ancestor of c. Formally, let p c l be the l th ancestor of class c such that p c 0 = c and p c n is the root of the WordNet tree where n is the depth of class c in the tree. Also, let d c be the set of all the descendants of class c. Given the set of support classes, C, we constrain the auxiliary classes at level l and use C A as the set of available auxiliary classes in the current episode as</p><formula xml:id="formula_7">C A = C T \ c∈C d p c l ,<label>(8)</label></formula><p>where C T is the set of all the classes in the auxiliary data source. In our experiments, we use this technique to augment several FSL benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Evaluation</head><p>In this section, we evaluate the pseudo shots framework. Using our proposed auxiliary data benchmark tasks, we find that pseudo shots can significantly outperform stateof-the-art FSL methods that do not consider auxiliary data, particularly on 1-shot tasks. We also find that our proposed masking module is critical. It increases accuracy versus no masking by up to 18 percentage points of accuracy. It is particularly effective when the auxiliary data is more semantically distant from the test classes. We also show that it matches or outperforms a previously introduced persample mask designed for semi-supervised learning <ref type="bibr" target="#b37">[37]</ref>, particularly on 1-shot tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>We For auxiliary data, we use a subset of ImageNet22k <ref type="bibr" target="#b5">[6]</ref>.</p><p>We choose the subset of ImageNet22k classes with more than 500 samples as the set of auxiliary classes C T . To evaluate the model at level l, we use eq. 8 to get the set of available classes, C A , for the current episode. We then follow the steps in section 3.2 for choosing the pseudo shots. We select the pseudo shots for training similar to testing. But, we use all the testing classes, C test , in eq. 8 rather than training classes. It ensures that training and testing data are completely disjoint. See the appendix for additional details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Models and Architecture</head><p>We evaluate three models on the auxiliary data benchmarks. We include a Basic Model (Section 4.2) and our proposed Masking Model (Section 4.3). The Basic Model is effectively a naive semi-supervised baseline that attempts to label the auxiliary data. To evaluate a more sophisticated semi-supervised method, we also include the masking soft K-means method of Ren et al. <ref type="bibr" target="#b37">[37]</ref> (MS K-Means). As discussed in the introduction, there are many semi-supervised methods, but we select this one because it has a distinct masking module that can be integrated into other architectures for FSL. It is a soft masking mechanism for updating class centroids. This mechanism masks entire images based on the assumption that the unlabeled data is a mix of target classes and distractors. In contrast, our proposed Masking Model masks individual feature patches of images based on the assumption that the auxiliary data contains related images, but no actual samples of the target class. To have a fair comparison, we implement MS K-Means with our (higher capacity) embedding function trained on the same data. We freeze the embedding function and train the parameters of their masking mechanism. With this implementation, MS K-Means is identical to our Masking Model other than their masking module.</p><p>In all cases, we use ResNet12 <ref type="bibr" target="#b16">[17]</ref> as our embedding function with DropBlock <ref type="bibr" target="#b13">[14]</ref> for regularization. We use (640, 320, 160, 64) filters instead of the original (512, 256, 128, 64) filters in ResNet12. With this modification, our embedding function is similar to that of Tian et al. <ref type="bibr" target="#b45">[45]</ref>. We use three ResBlocks with (640, 320, 1) filters for f θ . f β consists of one ResBlock with 640 filters. See the appendix for details on the hyperparameters and optimizer setup for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>We evaluate all the models with respect to auxiliary data benchmarks introduced in Section 5 up to level 3. We emphasize the results at level 0, which is still a challenging setting because the auxiliary data contains no samples of the test classes nor any subtypes of the test classes. We also include some of the most recent state-of-the-art work on FSL to illustrate the potential benefits of using auxiliary data.</p><p>Results from other methods are included in the appendix.</p><p>ImageNet. As shown in <ref type="table">Table 1</ref>, on the ImageNet derivatives, our Masking Model outperforms all previous state-ofthe-art methods with a large margin in 1-shot classification with level-0 auxiliary data. For miniImageNet dataset, our model improves the accuracy of previous methods by six percentage points. Our Masking Model also benefits from significant boosts in performance for in 1-shot classification task on tieredImageNet. The performance of the Masking Model is also comparable to other recent methods on 5-shot classification tasks. Our Basic Model also has a comparable performance to other recent work on 1-shot classification tasks. This shows the utility of pseudo shots in low-shot regimes, even before attempting to mask them for improved performance. Pseudo shots are likely more effective in 1shot than 5-shot settings because finding samples from the noisy auxiliary data that improve the information in 5 support samples is much harder than finding such samples for 1-shot classification.</p><p>On higher levels of auxiliary data, the masking mechanism becomes more helpful. Even at level 1-which contains no WordNet parents of the test classes nor their subtypes-the Masking Model significantly outperforms the state of the art methods on 1-shot tasks. The last section of <ref type="table">Table 1</ref> shows the improvement by the Masking Model relative to the basic model for different levels. We observe that the effect of masking module increases as the quality of data decreases. The masking module makes the most improvement in level 3 where auxiliary samples are distant from target classes.</p><p>CIFAR-100. our Masking Model gives a large boost in performance for 1-shot classification tasks relative to the baselines. With level-0 auxiliary data, on the 1-shot tasks, the Masking Model surpasses more sophisticated state-of-the-art methods that do not use auxiliary data. On the 5-shot tasks, the Masking Model also has comparable performance to the best performing other methods.</p><p>The trends on other levels of auxiliary data are also similar to the previous experiments. On 1-shot tasks, the Masking Model continues to match the state-of-art accuracies, even with level-1 auxiliary data. MS K-Means does very well on 5-shot tasks, approaching the Masking Model, and even outperforming it on FC-100. We believe that the Masking Model is less effective here because (1) the 5 shots provide ample information for classification already, and (2) the smaller dimensions of CIFAR-100 means the spatial masking of features is less useful. The embedded feature maps of CIFAR-100 and ImageNet are of size 2 × 2 and 5 × 5, respectively. The larger feature maps of ImageNet provide more spatial information that results in a more effective masking. Finally, we again note that the Masking Model is increasingly effective at higher level auxiliary data sets. This result demonstrates the importance of carefully selecting information from more distantly related data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Analysis</head><p>Due to space constraints, we defer additional analysis to the appendix and summarize the findings here. First, we perform ablation experiments to measure the improvement gained from each component of the Masking Model. We find that the use of auxiliary data in the form of pseudo shots and the helper dataset to train the masking module all contribute to its success. Second, we investigate why pseudo shots are helpful. We calculate the mutual information of the features between the support samples and the pseudo shots at different stages of the pipeline. We find that the masking module consistently increases the mutual information. We also find that resulting class centroids have increased mutual information with the correct query images. Finally, we visualize the pseudo shot masks on some examples to see that the masks localize seemingly useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Works</head><p>Meta-learning is the dominant approach for solving FSL problems. Meta-learning tries to learn transferable knowledge based on the training classes and use this learned knowledge during test time. This transferable knowledge can be a discriminating metric space <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">23]</ref>, or a Model Level CIFAR-FS FC-100 1-shot 5-shot 1-shot 5-shot RFS-Distill <ref type="bibr" target="#b45">[45]</ref> 73.9 ± 0.8 86.9 ± 0.5 44.6 ± 0.7 60.9 ± 0.6 DSN-MR <ref type="bibr" target="#b40">[40]</ref> 78.0 ± 0.9 87.  <ref type="table" target="#tab_2">Table 2</ref>. Mean accuracy of 800 episodes with 95% confidence intervals for 1/5-shot classification on CIFAR-100 derivatives.</p><p>fast converging algorithm or initial state <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b30">30]</ref>. All these methods rely solely on the support set.</p><p>Transductive models have gained attention for FSL recently. These models use the query set as unlabeled data in each episode <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b21">22]</ref>. EGNN <ref type="bibr" target="#b21">[22]</ref> uses a graph convolutional edge labeling network to propagate the support set labels to query samples. TPN <ref type="bibr" target="#b27">[27]</ref> propagates the labels of the support set to unlabeled (query) samples by learning to construct a graph structure. There are a number of other transductive FSL solutions that rely on the query set as well as the support set in each episode <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Semi-supervised FSL methods are similar to transductive methods, but their unlabeled set is not the same as the query set <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b26">26]</ref>. Ren et al. <ref type="bibr" target="#b37">[37]</ref> include unlabeled samples in each episode and propose a masking mechanism to control the effect of unrelated unlabeled samples. As the source of unlabeled samples, they use a mix of samples from random and support classes with a 1:1 ratio. Several other works <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref> use the same unlabeled set as Ren et al. <ref type="bibr" target="#b37">[37]</ref>. TransMatch <ref type="bibr" target="#b51">[51]</ref> draws unlabeled samples from support classes in each episode and measures robustness against distractors, but does not try to exploit them. Gidaris et al. <ref type="bibr" target="#b14">[15]</ref> use tieredImageNet as the source of unlabeled data for self-supervised learning for miniImageNet.</p><p>Some recent studies aim for more discriminative features and propose related masking methods that further refine the embedded features <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">25]</ref>. CTM <ref type="bibr" target="#b25">[25]</ref> looks at all the support samples together and generates a mask that indicates the most discriminative features for the current task. CAN <ref type="bibr" target="#b18">[19]</ref> calculates a cross attention map for for each pair of class centroid features and query features. The cross attention map improves the discriminative power of features by localizing the target object. Our masking module differs from these works by comparing support sets and pseudo shots to import information from the pseudo shots.</p><p>Other related work includes Xing et al. <ref type="bibr" target="#b49">[49]</ref>, which uses word vectors directly to update class prototypes, whereas we use word vectors as a tool to select auxiliary samples. Ge and Yu <ref type="bibr" target="#b12">[13]</ref> use visual similarity and Zhang et al.</p><p>[55] use meta-learning to select auxiliary data for fine-grained image classification. Finally, concurrent work <ref type="bibr" target="#b1">[2]</ref> considers transductive FSL for multi-domain problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we introduced pseudo shots as a framework to incorporate auxiliary data into FSL. It is highly effective on four benchmark datasets, even as the available auxiliary data grows semantically distant from the test classes. We believe that the problem of exploiting auxiliary data for novel classes will be an increasingly important challenge as shared datasets continue to multiply. Pseudo shots provide a straightforward, versatile answer for this growing problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results</head><p>Quantitative Results. Here, we compare the performance of our method using level 0 auxiliary data with recent FSL methods. <ref type="table" target="#tab_7">Table 3</ref> reports the experimental results on miniImageNet and tieredImageNet. Our method achieves a significantly higher accuracy than others in 1-shot classification for both datasets. We also get better results in 5-shot classification compared to previous FSL methods.</p><p>We are significantly more accurate than many of the previous works on 5-shot classification while our method is far simpler than many of those. Pseudo shots simulate extra labeled examples and reduce the gap between the performance in 1-shot and 5-shot classification. As demonstrated in <ref type="table" target="#tab_7">Table 3</ref>, the 5-shot accuracy of our method is roughly 10 points higher than its 1-shot accuracy which is the smallest gap between 1-shot and 5-shot performance compared to other FSL methods. <ref type="table">Table 4</ref> compares the performance of our method with other works on CIFAR-FS and FC-100. Our observations for CIFAR-100 derivatives are similar to ImageNet derivatives. We outperform recent methods with a large margin on 1shot classification for both datasets. For 1-shot classification on FC-100, even our basic model has a significantly higher accuracy than others which further emphasizes the importance and efficacy of pseudo shots. Our method has the smallest gap between the accuracy for 1-shot and 5-shot classification (9 percentage points) compared to previous methods. For FC-100, we have comparable performance to previous methods for 5-shot classification. The 5-shot accuracy of our method on CIFAR-FS is 2 points higher than previous methods.</p><p>Visual Results. In <ref type="figure" target="#fig_0">Fig. 2</ref>, we show the visual results of the masking model. In each row, the support image, selected pseudo shot, and generated mask are shown in column (a), (b), and (c), respectively. The selected pseudo shots represent similar visual structure to the support images. This shows the effectiveness of our pseudo shot selection method in practice. Moreover, the masking module successfully learns to identify the most similar region to support images in pseudo shots. In the third column of <ref type="figure" target="#fig_0">Fig. 2</ref>, the mask has higher values (shown with warmer colors) in the most similar regions to support images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis Details</head><p>We experimentally show the effect of each component of the proposed framework. We also run experiments to give more insight into the proposed making model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Ablation Studies</head><p>Here, we ablate different components of our framework and evaluate the performance. The results of these experiments are reported in <ref type="table">Table 5</ref> and 6 where each row corresponds to ablating one component. The performance of the original framework with all its components is reported in the last row. We use level 0 auxiliary data for these experiments. The experimental setup is the same as our main experiments other than the variable under experiment.</p><p>Helper Dataset. We run the main experiments in the absence of the helper dataset during training the masking module. As shown in the third row of <ref type="table">Table 5</ref>, the helper dataset has a greater effect in 1-shot classification than 5-shot for both miniImageNet and tieredImageNet. The helper dataset has a more significant impact on the accuracy for larger  <ref type="bibr" target="#b15">[16]</ref> 56.20 ± 0.86 73.00 ± 0.64 − − Relation Networks <ref type="bibr" target="#b44">[44]</ref> 50.44 ± 0.82 65.32 ± 0.70 54.48 ± 0.93 71.32 ± 0.78 TADAM <ref type="bibr" target="#b31">[31]</ref> 58.50 ± 0.30 76.70 ± 0.30 − − Shot-Free <ref type="bibr" target="#b36">[36]</ref> 59.04 ± n/a 77.64 ± n/a 63.52 ± n/a 82.59 ± n/a TEWAM <ref type="bibr" target="#b34">[34]</ref> 60.07 ± n/a 75.90 ± n/a − − MTL <ref type="bibr" target="#b43">[43]</ref> 61. <ref type="bibr" target="#b19">20</ref>   datasets such as tieredImageNet. With larger datasets, the feature embedding function is trained with a greater amount of data and generates well clustered features for training samples. Therefore, there is a greater discrepancy between training and testing situation. <ref type="table">Table 6</ref> reports the results of the same experiment for CIFAR-FS and FC-100 in the third row. The 5-shot task benefits from helper dataset more than 1-shot task on FC-100. The helper dataset downgrades the performance for both 1/5-shot tasks on CIFAR-FS unlike the other three datasets. CIFAR-FS splits the classes randomly and does not guarantee the isolation of test data, i.e., there might be similar training and testing classes. It is likely that the model is exploiting this leaked information and introducing helper dataset makes the problem harder.</p><p>Auxiliary Data. We present experimental results to support the main idea of using auxiliary data external to target classes. Since the masking module is only helpful in presence of auxiliary data, we train and test the basic model with only the training set. The first rows of <ref type="table">Table 5 and 6</ref> show the results of these experiments. The accuracy is extremely low in the absence of auxiliary data for 1-shot classification. This is true across all four datasets. We observe the same for 5-shot classification, and including auxiliary samples boosts the 5-shot classification accuracy Pseudo Shot Selection. We show the crucial role of the proposed pseudo shot selection method in section 3.2 in the success of our framework. We replace the pseudo shot selection method with random sampling and run the main experiments. The second rows of <ref type="table">Table 5</ref> and 6 report the corresponding results. The proposed selection method has a pivotal role in 1-shot classification task. Our experiments show that random auxiliary examples (second row) drastically downgrades the accuracy of the fully supervised method (first row) in 1-shot classification across three datasets, tieredImageNet, CIFAR-FS, and FC-100. For 5shot classification, the masking module is more effective and filters the irrelevant information in the random auxiliary data. Hence, random auxiliary data increases the accuracy of the fully supervised method in 5-shot classification. However, our proposed pseudo shot selection method carefully chooses the auxiliary classes to maximize the benefits of auxiliary samples and we gain a greater boost in performance compared to random sampling for 5-shot classification task across all four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Analysis of Embedded Features</head><p>We inspect the embedded features at different stages of the masking model to give some insight into its different parts. We consider the mutual information (MI) between the histograms of two embedded features as a measure of their similarity. We calculate the MI for many episodes. The histogram of the obtained values is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. We measure MI for two pairs of embedded features, features of the query samples and the class centroids, <ref type="figure" target="#fig_1">Fig. 3b</ref>, and features of the support samples and pseudo shots <ref type="figure" target="#fig_1">Fig. 3a</ref>.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 3a</ref>, masking pseudo shots decreases their MI with the original support sample features. The universal transformation function transforms pseudo shot and support sample features to a common feature space. In this common feature space, the refined pseudo shot features have a greater MI with support sample features compared to their original MI. Since pseudo shots contribute to class centroids, the same argument holds for the MI between the features of query samples and class centroids in <ref type="figure" target="#fig_1">Fig. 3b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>Dataset Alignment. ImageNet classes are WordNet IDs. So, it is straight forward to map classes to WordNet nodes. To map the classes to ConceptNet nodes, we use the corre-sponding noun in WordNet database for each class. CIFAR-100 classes are nouns. We use all the corresponding Word-Net IDs to the selected support classes as C in eq. 8. This is the hardest scenario. We directly use nouns for mapping CIFAR-100 classes to ConceptNet nodes.</p><p>Optimizer. We use stochastic gradient descent (SGD) as optimizer with initial learning rate of 0.05, momentum of 0.9, weight decay of 5.e-4, and learning rate decay of 0.1. We train the embedding function for 100 epochs and decay the learning rate at epochs 60 and 80. We use the parameters at the last epoch as f φ . We train the masking model for 300 epochs and decay the learning rate at epochs 100 and 200. We use the parameters of the masking model at the epoch with highest validation accuracy. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Three support images and the corresponding selected pseudo shot. Each row corresponds to one class. Support images, pseudo shots, and masked pseudo shots are shown in columns (a), (b), and (c), respectively. Warmer colors represent higher mask values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Mutual information (MI) between the histogram of the embedded features at different stages of the masking model. a) Histogram of MI values between the features of the pseudo shots and the support samples. Since f θ only masks the pseudo shots, the MI between the masked pseudo shot features and original support sample features decreases. The universal transformation function, f β , transforms all the features into a common space where the MI between masked pseudo shot features and support sample features increases. b) The same plot as part (a) but for MI between class centroid and query sample features. The same argument holds for this part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>selected classes with 600 images per class. Classes are split into 64, 20, and 16 classes for training, testing, and validation, respectively. tieredImageNet<ref type="bibr" target="#b37">[37]</ref> is a larger dataset with with 608 classes. Classes are split into 351, 160, and 97 classes for training, testing, and validation, respectively. tieredImageNet classes are grouped into high level semantic categories. Classes of each category only appear in one of the splits. It makes sure that there is no overlapping information in testing and training data (i.e. testing classes are completely unseen). Both miniImageNet and tieredImageNet contain 84 × 84 RGB images.</figDesc><table /><note>create four auxiliary data benchmarks from popular FSL datasets. miniImageNet and tieredImageNet are both subsets of the ILSVRC dataset. miniImageNet [47] consists of 100 randomlyCIFAR-FS and FC-100 are both derivatives of the CIFAR- 100 dataset for FSL. CIFAR-FS [24] randomly splits CIFAR- 100 classes into 64, 20, and 16 classes for training, test- ing, and validation, respectively. FC-100 [31] tries to pre- vent overlapping information between training and test- ing classes, similar to tieredImageNet. FC-100 splits the CIFAR-100 classes into 60, 20, and 20 classes for training, testing, and validation, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>64.82 ± 0.60 82.14 ± 0.43 71.52 ± 0.69 86.03 ± 0.49 DSN-MR [40] 67.09 ± 0.68 81.65 ± 0.69 68.44 ± 0.77 83.32 ± 0.66 FEAT [50] 66.78 ± 0.20 82.05 ± 0.14 70.80 ± 0.23 84.79 ± 0.16 DeepEMD [53] 65.91 ± 0.82 82.41 ± 0.56 71.16 ± 0.87 86.03 ± 0.58</figDesc><table><row><cell>shows the evaluation results on</cell></row><row><cell>CIFAR-100 derivatives. Similar to previous experiments,</cell></row></table><note>Table 1. Mean accuracy of 800 episodes with 95% confidence intervals for 1/5-shot classification on ILSVRC derivatives.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>65.85 ± 0.84 88.24 ± 0.54 40.20 ± 0.64 62.79 ± 0.69 Basic Model 75.20 ± 0.80 85.27 ± 0.55 49.05 ± 0.66 56.70 ± 0.65 Masking Model 81.87 ± 0.66 89.12 ± 0.51 50.57 ± 0.65 61.58 ± 0.32</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3 ± 0.6</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepEMD [53]</cell><cell></cell><cell>-</cell><cell>-</cell><cell cols="2">46.47 ± 0.78 63.22 ± 0.71</cell></row><row><cell>MS K-Means [37]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS K-Means [37]</cell><cell></cell><cell>61.17 ± 0.96</cell><cell>87.36 ± 0.52</cell><cell cols="2">39.75 ± 0.69 62.71 ± 0.57</cell></row><row><cell>Basic Model</cell><cell>1</cell><cell>66.97 ± 0.98</cell><cell>80.96 ± 0.70</cell><cell>42.93 ± 0.79</cell><cell>51.85 ± 0.72</cell></row><row><cell>Masking Model</cell><cell></cell><cell>77.94 ± 0.73</cell><cell>87.61 ± 0.53</cell><cell>46.04 ± 0.71</cell><cell>60.71 ± 0.28</cell></row><row><cell>MS K-Means [37]</cell><cell></cell><cell>47.60 ± 0.88</cell><cell>82.38 ± 0.57</cell><cell cols="2">38.36 ± 0.70 62.97 ± 0.66</cell></row><row><cell>Basic Model</cell><cell>2</cell><cell>56.74 ± 1.08</cell><cell>75.27 ± 0.75</cell><cell>35.66 ± 0.77</cell><cell>48.04 ± 0.72</cell></row><row><cell>Masking Model</cell><cell></cell><cell>69.44 ± 0.87</cell><cell>84.45 ± 0.56</cell><cell>44.68 ± 0.69</cell><cell>60.46 ± 0.65</cell></row><row><cell>MS K-Means [37]</cell><cell></cell><cell>43.92 ± 0.83</cell><cell>78.94 ± 0.63</cell><cell>31.11 ± 0.51</cell><cell>56.60 ± 0.58</cell></row><row><cell>Basic Model</cell><cell>3</cell><cell>47.57 ± 1.03</cell><cell>73.50 ± 0.75</cell><cell>29.86 ± 0.63</cell><cell>42.41 ± 0.65</cell></row><row><cell>Masking Model</cell><cell></cell><cell>66.26 ± 0.81</cell><cell>84.41 ± 0.53</cell><cell>39.32 ± 0.62</cell><cell>54.54 ± 0.60</cell></row><row><cell>Masking Improvement vs. Basic</cell><cell>0 1 2 3</cell><cell>6.67 10.97 12.70 18.69</cell><cell>3.85 6.65 9.18 10.91</cell><cell>1.52 3.11 9.02 9.46</cell><cell>4.88 8.86 12.42 12.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>[53] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen.Deepemd: Few-shot image classification with differentiable earth mover's distance and structured classifiers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.</figDesc><table><row><cell>[54] Jian Zhang, Chenglong Zhao, Bingbing Ni, Minghao Xu,</cell></row><row><cell>and Xiaokang Yang. Variational few-shot learning. In Pro-</cell></row><row><cell>ceedings of the IEEE International Conference on Computer</cell></row><row><cell>Vision, pages 1685-1694, 2019.</cell></row><row><cell>[55] Yabin Zhang, Hui Tang, and Kui Jia. Fine-grained visual</cell></row><row><cell>categorization using meta-learning optimization with sample</cell></row><row><cell>selection of auxiliary data. In Proceedings of the European</cell></row><row><cell>Conference on Computer Vision (ECCV), 2018.</cell></row><row><cell>[56] Yu Zhang, Ying Wei, and Qiang Yang. Learning to multitask.</cell></row><row><cell>CoRR, abs/1805.07541, 2018.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Performance of the proposed models on ILSVRC derivatives compared to previous works.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .Table 5 .Table 6 .</head><label>456</label><figDesc>Performance of the proposed models on CIFAR-100 derivatives compared to previous works. 77.99 67.02 83.65 Selection Method 58.07 80.40 62.72 85.33 Helper Dataset 72.14 82.46 74.83 84.68 -73.35 82.51 76.55 86.82 Ablation studies for ImageNet derivatives. 86.05 40.90 56.86 Selection Method 58.78 88.03 33.73 59.36 Helper Dataset 82.90 89.25 49.79 59.94 -81.87 89.12 50.57 61.58 Ablation studies for CIFAR-100 derivatives.</figDesc><table><row><cell>Ablated Variable</cell><cell cols="2">miniImageNet tieredImageNet</cell></row><row><cell></cell><cell cols="2">1-shot 5-shot 1-shot 5-shot</cell></row><row><cell cols="2">Auxiliary Data 58.16 Ablated Variable CIFAR-FS</cell><cell>FC-100</cell></row><row><cell></cell><cell cols="2">1-shot 5-shot 1-shot 5-shot</cell></row><row><cell>Auxiliary Data</cell><cell>69.12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To avoid confusion with the set of support classes, we use C * rather than the conventional C to denote the number of channels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This material is based on research sponsored by Defense Advanced Research Projects Agency (DARPA) and Air Force Research Laboratory (AFRL) under agreement number FA8750-19-2-1006. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Defense Advanced Research Projects Agency (DARPA) and Air Force Research Laboratory (AFRL) or the U.S. Government. We gratefully acknowledge support from Google. Disclosure: Stephen Bach is an advisor to Snorkel AI, a company that provides software and services for weakly supervised machine learning.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving few-shot visual classification with unlabelled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarred</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Willem</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12245</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08136</idno>
		<title level="m">Meta-learning with differentiable closed-form solvers</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A new meta-baseline for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04390</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual and semantic similarity in imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diversity with cooperation: Ensemble methods for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3723" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8059" to="8068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4003" to="4014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Leveraging the feature distribution in transfer-based few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03806</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11719" to="11727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">Pseudo Shots: Few-Shot Learning with Auxiliary Data</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finding task-relevant features for fewshot learning by category traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shoestring: Graph-based semi-supervised classification with severely limited labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaolin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4174" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodríguez López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3603" to="3612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<title level="m">Meta-learning with latent embedding optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-learning with memoryaugmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03975</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11539</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jude</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</title>
		<imprint>
			<publisher>IGI global</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adaptive cross-modal few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Negar</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Oo</forename><surname>Pinheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4847" to="4857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fewshot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">TransMatch: A transfer-learning scheme for semi-supervised few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno>CIFAR-FS FC-100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
