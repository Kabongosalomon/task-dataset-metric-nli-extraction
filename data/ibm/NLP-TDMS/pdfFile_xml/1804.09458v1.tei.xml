<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Few-Shot Visual Learning without Forgetting Spyros Gidaris</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">University</forename><surname>Paris-Est</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIGM Ecole des Ponts ParisTech</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nikos Komodakis University</orgName>
								<address>
									<addrLine>Paris-Est, LIGM Ecole des Ponts ParisTech</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Few-Shot Visual Learning without Forgetting Spyros Gidaris</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on "unseen" categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick <ref type="bibr" target="#b4">[4]</ref> where we also achieve stateof-the-art results. The code and models of our paper will be published on: https://github.com/gidariss/ FewShotWithoutForgetting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on "unseen" categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick <ref type="bibr" target="#b4">[4]</ref> where we also achieve stateof-the-art results. The code and models of our paper will be published on: https://github.com/gidariss/ FewShotWithoutForgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last few years, deep convolutional neural networks <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b5">5]</ref> (ConvNets) have achieved impressive results on image classification tasks, such as object recognition <ref type="bibr" target="#b19">[19]</ref> or scene classification <ref type="bibr" target="#b28">[28]</ref>. In order for a ConvNet to successfully learn to recognize a set of visual categories (e.g., object categories or scene types), it requires to manually collect and label thousands of training examples per target category and to apply on them an iterative gradient based optimization routine <ref type="bibr" target="#b10">[10]</ref> that is extremely computationally expensive, e.g., it can consume hundreds or even thousands of GPU hours. Moreover, the set of categories that the ConvNet model can recognize remains fixed after training. In case we would like to expand the set of categories that the ConvNet can recognize, then we need to collect training data for the novel categories (i.e., those that they were not in the initial training set) and restart the aforementioned computationally costly training procedure this time on the enhanced training set such that we will avoid catastrophic interference. Even more, it is of crucial importance to have enough training data for the novel categories (e.g., thousands of examples per category) otherwise we risk overfitting on them.</p><p>In contrast, the human visual system exhibits the remarkably ability to be able to effortlessly learn novel concepts from only one or a few examples and reliably recognize them later on. It is assumed that the reason the human visual system is so efficient when learning novel concepts is that it exploits its past experiences about the (visual) world. For example, a child, having accumulated enough knowledge about mammal animals and in general the visual world, can easily learn and generalize the visual concept of "rhinoceros" from only a single image. Mimicking that behavior on artificial vision systems is an interesting and very challenging research problem with many practical advantages, such as developing real-time interactive vision applications for portable devices (e.g., cell-phones).</p><p>Research on this subject is usually termed few-shot learning. However, most prior methods neglect to fulfill two very important requirements for a good few-shot learning system: (a) the learning of the novel categories needs to be fast, and (b) to not sacrifice any recognition accuracy on the initial categories that the ConvNet was trained on, i.e., to not "for-get" (from now on we will refer to those initial categories by calling them base categories). Motivated by this observation, in this work we propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training data is assumed to exist for a set of base categories and, using these data as the sole input, we want to develop an object recognition learning system that, not only is able to recognize these base categories, but also learns to dynamically recognize novel categories from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (dynamic few-shot learning without forgetting). Compared to prior approaches, we believe that this setting more closely resembles the human visual system behavior (w.r.t. how it learns novel concepts). In order to achieve our goal, we propose two technical novelties.</p><p>Few-shot classification-weight generator based on attention. A typical ConvNet based recognition model, in order to classify an image, first extracts a high level feature representation from it and then computes per category classification scores by applying a set of classification weight vectors (one per category) to the feature. Therefore, in order to be able to recognize novel categories we must be able to generate classification weight vectors for them. In this context, the first technical novelty of our work is that we enhance a typical object recognition system with an extra component, called few-shot classification weight generator that accepts as input a few training examples of a novel category (e.g., no more than five examples) and, based on them, generates a classification weight vector for that novel category. Its key characteristic is that in order to compose novel classification weight vectors, it explicitly exploits the acquired past knowledge about the visual world by incorporating an attention mechanism over the classification weight vectors of the base categories. This attention mechanism offers a significant boost on the recognition performance of novel categories, especially when there is only a single training example available for learning them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cosine-similarity based ConvNet recognition model.</head><p>In order for the few-shot classification weight generator to be successfully incorporated into the rest of the recognition system, it is essential the ConvNet model to be able to simultaneously handle the classification weight vectors of both base and novel categories. However, as we will explain in the methodology, this is not feasible with the typical dot-product based classifier (i.e., the last linear layer of a classification neural network). Therefore, in order to overcome this serious issue, our second technical novelty is to implement the classifier as a cosine similarity function between the feature representations and the classification weight vectors. Apart from unifying the recognition of both base and novel categories, features learned with the cosine-similarity based classifier turn out to generalize significantly better on novel categories than those learned with a dot-product based classifier. Moreover, we demonstrate in the experimental section that, by simply training a cosine-similarity based ConvNet recognition model, we are able to learn feature extractors that when used for image matching they surpass prior stateof-the-art approaches on the few-shot recognition task.</p><p>To sum up, our contributions are: (1) We propose a fewshot object recognition system that is capable of dynamically learning novel categories from only a few training data while at the same time does not forget the base categories on which it was trained. (2) In order to achieve that we introduced two technical novelties, an attention based few-shot classification weight generator, and to implement the classifier of a Con-vNet model as a cosine similarity function between feature representations and classification vectors. (3) We extensively evaluate our object recognition system on Mini-ImageNet, both w.r.t. its few-shot object recognition performance and its ability to not forget the base categories, and we report state-of-the-art results that surpass prior approaches by a very significant margin. (4) Finally, we apply our approach on the recently introduced fews-shot benchmark of Bharath and Girshick <ref type="bibr" target="#b4">[4]</ref> where we achieve state-of-the-art results.</p><p>In the following sections, we provide related work in §2, we describe our few-shot object learning methodology in §3, we provide experimental results in §4, and finally we conclude in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Recently, there is resurgence of interest on the few-shot learning problem. In the following we briefly discuss the most relevant approaches to our work.</p><p>Meta-learning based approaches. Meta-learning approaches typical involve a meta-learner model that given a few training examples of a new task it tries to quickly learn a learner model that "solves" this new task <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b20">20]</ref>. Specifically, Ravi and Larochelle <ref type="bibr" target="#b17">[17]</ref> propose a LSTM <ref type="bibr" target="#b6">[6]</ref> based meta-learner that is trained given as input a few training examples of a new classification task to sequentially generate parameter updates that will optimize the classification performance of a learner model on that task. Their LSTM also learns the parameter initialization of the learner model. Finn et al. <ref type="bibr" target="#b3">[3]</ref> simplified the above meta-learner model and only learn the initial learner parameters such that only a few gradient descent steps w.r.t. those initial parameters will achieve the maximal possible performance on the new task. Mishra et al. <ref type="bibr" target="#b13">[13]</ref> instead propose a generic temporal convolutional network that given as input a sequence of a few labeled training examples and then an unlabeled test example, it predicts the label of that test example. Our system also includes a meta-learner network component, the few-shot classification weight generator.</p><p>Metric-learning based approaches. In general, metric learning approaches attempt to learn feature representations that preserve the class neighborhood structure (i.e., features of the same object are closer than features of different objects). Specifically, Koch et al. <ref type="bibr" target="#b8">[8]</ref> formulated the one-shot object recognition task as image matching and train Siamese neural networks to compute the similarity between a training example of a novel category and a test example. Vinyals et al. <ref type="bibr" target="#b26">[26]</ref> proposed Matching Networks that in order to classify a test example it employs a differentiable nearest neighbor classifier implemented with an attention mechanism over the learned representations of the training examples. Prototypical Networks <ref type="bibr" target="#b23">[23]</ref> learn to classify test examples by computing distances to prototype feature vectors of the novel categories. They propose to learn the prototype feature vector of a novel category as the average of the feature vectors extracted by the training examples of that category. A similar approach was proposed before by Mensink et al. <ref type="bibr" target="#b12">[12]</ref> and Prototypical Networks can be viewed as an adaption of that work for ConvNets. Despite their simplicity, Prototypical Networks demonstrated state-of-the-art performance. Our few-shot classification weight generator also includes a feature averaging mechanism. However, more than that, it also explicitly exploits past knowledge about the visual world with an attention based mechanism and the overall framework allows to perform unified recognition of both base and novel categories without altering the way base categories are learnt and recognized.</p><p>In a different line of work, Bharath and Girshick <ref type="bibr" target="#b4">[4]</ref> propose to use during training a l 2 regularization loss on the feature representations that forces them to better generalize on "unseen" categories. In our case, the cosine-similarity based classifier, apart from unifying the recognition of both base and novel categories, it also leads to feature representations that are able to better generalize on "unseen" categories. Also, their framework is able to recognize both base and novel categories as ours. However, to achieve that goal they re-train the classifier on both the base categories (with a large set of training data) and the novel categories (with few training data), which is in general slow and requires constantly maintaining in disc a large set of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As an input to our object recognition learning system we assume that there exists a dataset of K base base categories:</p><formula xml:id="formula_0">D train = K base b=1 {x b,i } N b i=1 ,<label>(1)</label></formula><p>where N b is the number of training examples of the b-th category and x b,i is its i-th training example. Using this as the only input, the goal of our work is to be able to both learn to accurately recognize base categories and to learn to perform few-shot learning of novel categories in a dynamic manner and without forgetting the base ones. An overview of our framework is provided in <ref type="figure">Figure 1</ref>. It consists of two main components, a ConvNet-based recognition model that is able to recognize both base and novel categories and a few-shot classification weight generator that dynamically generates classification weight vectors for the novel categories at test time:</p><p>ConvNet-based recognition model. It consists of (a) a feature extractor F (.|θ) (with learnable parameters θ) that</p><formula xml:id="formula_1">extracts a d-dimensional feature vector z = F (x|θ) ∈ R d from an input image x, and (b) a classifier C(.|W * ), where W * = {w * k ∈ R d } K * k=1</formula><p>are a set of K * classification weight vectors -one per object category, that takes as input the feature representation z and returns a K * -dimensional vector with the probability classification scores p = C(z|W * ) of the K * categories. Note that in a typical convolutional neural network the feature extractor is the part of the network that starts from the first layer and ends at the last hidden layer while the classifier is the last classification layer. During the single training phase of our algorithm, we learn the θ parameters and the classification weight vectors of the base categories W base = {w k } K base k=1 such that by setting W * = W base the ConvNet model will be able to recognize the base object categories.</p><p>Few-shot classification weight generator. This comprises a meta-learning mechanism that, during test time, takes as input a set of K novel novel categories with few training examples per category</p><formula xml:id="formula_2">D novel = K novel n=1 {x n,i } N n i=1 ,<label>(2)</label></formula><p>where N n is the number of training examples of the n-th novel category and x n,i is its i-th training example, and is able to dynamically assimilate the novel categories on the repertoire of the above ConvNet model. More specifically, for each novel category n ∈ [1, N novel ], the few-shot classification weight generator G(., .|φ) gets as input the feature vectors Z n = {z n,i } N n i=1 of its N n training examples, where z n,i = F (x n,i |θ), and the classification weight vectors of the base categories W base and generates a classification weight vector w n = G(Z n , W base |φ) for that novel category. Note that φ are the learnable parameters of the few-shot weight generator, which are learned during the single training phase of our framework. Therefore, if W novel = {w n } K novel n=1 are the classification weight vectors of the novel categories inferred by the few-shot weight generator, then by setting W * = W base ∪ W novel on the classifier C(.|W * ) we enable the ConvNet model to recognize both base and novel categories.</p><p>A key characteristic of our framework is that it is able to effortlessly (i.e., quickly during test time) learn novel categories and at the same time recognize both base and novel categories in a unified manner. In the following subsections, we will describe in more detail the ConvNet-based recognition model in §3.1 and the few-shot weight generator in §3.2.</p><p>Finally, we will explain the training procedure in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cosine-similarity based recognition model</head><p>A crucial difference of our ConvNet based recognition model compared to a standard one is that it should be able to dynamically incorporate at test time a variable number of novel categories (through the few-shot weight generator).</p><p>The standard setting for classification neural networks is, after having extracted the feature vector z, to estimate the classification probability vector p = C(z|W * ) by first computing the raw classification score s k of each category k ∈ [1, K * ] using the dot-product operator:</p><formula xml:id="formula_3">s k = z w * k ,<label>(3)</label></formula><p>where w k is the k-th classification weight vector in W * , and then applying the softmax operator across all the K * classification scores, i.e., p k = sof tmax(s j ), where p k is the k-th classification probability of p. In our case the classification weight vectors w * k could come both from the base categories, i.e., w * k ∈ W base , and the novel categories, i.e., w * k ∈ W novel . However, the mechanisms involved during learning those classification weights are very different. The base classification weights, starting from their initial state, are slowly modified (i.e., slowly learned) with small SGD steps and thus their magnitude changes slowly over the course of their training. In contrast, the novel classification weights are dynamically predicted (i.e., quickly learned) by the weight generator based on the input training feature vectors and thus their magnitude depends on those input features. Due to those differences, the weight values in those two cases (i.e., base and novel classification weights) can be completely different, and so the same applies to the raw classification scores computed with the dot-product operation, which can thus have totally different magnitudes depending on whether they come from the base or the novel categories. This can severely impede the training process and, in general, does not allow to have a unified recognition of both type of categories. In order to overcome this critical issue, we propose to modify the classifier C(.|W * ) and compute the raw classification scores using the cosine similarity operator:</p><formula xml:id="formula_4">s k = τ · cos(z, w * k ) = τ · z w * k ,<label>(4)</label></formula><p>where z = z z and w * k = w * k w * k are the l 2 -normalized vectors (from now on we will use the overline symbol z to indicate that a vector z is l 2 -normalized), and τ is a learnable scalar value 1 . Since the cosine similarity can be implemented by first l 2 -normalizing the feature vector z and the classification weight vector w * k and then applying the dot-product operator, the absolute magnitudes of the classification weight vectors can no longer affect the value of the raw classification score (as a result of the l 2 normalization that took place).</p><p>In addition to the above modification, we also choose to remove the ReLU non-linearity <ref type="bibr" target="#b15">[15]</ref> after the last hidden  layer of the feature extractor, which allows the feature vector z to take both positive and negative values, similar to the classification weight vectors. Note that the removal of the ReLU non-linearity does not make the composition of the last hidden layer with the classification layer a linear operation, since we l 2 -normalize the feature vectors, which is a non-linear operation. In our initial experiments with the cosine similarity based classifier we found that such a modification can significantly improve the recognition performance of novel categories.</p><p>We note that, although cosine similarity is already well established as an effective similarity function for classifying a test feature by comparing it with the available training features vectors <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b18">18]</ref>, in this work we use it for a different purpose, i.e., to replace the dot-product operation of the last linear layer of classification ConvNets used for applying the learnable weights of that layer to the test feature vectors. The proposed modification in the architecture of a classification ConvNet allows to unify the recognition of base and novel categories without significantly altering the classification pipeline for the recognition of base categories (in contrast to <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b18">18]</ref>). To the best of our knowledge, employing the cosine similarity operation in such a way is novel in the context of few shot learning. Interestingly, concurrently to us, Qi et al. <ref type="bibr" target="#b16">[16]</ref> also propose to use the cosine similarity function in a similar way for the few-shot learning task. In a different line of work, very recently Chunjie et al. <ref type="bibr" target="#b2">[2]</ref> also explored cosine similarity for the typical supervised classification task.</p><p>Advantages of cosine-similarity based classifier. Apart from making possible the unified recognition of both base and novel categories, the cosine-similarity based classifier leads the feature extractor to learn features that generalize significantly better on novel categories than features learned with the dot-product based classifier. A possible explanation for this is that, in order to minimize the classification loss of a cosine-similarity based ConvNet model, the l 2 -normalized feature vector of an image must be very closely matched with the l 2 -normalized classification weight vector of its ground truth category. As a consequence, the feature extractor is forced to (a) learn to encode on its feature activations exactly those discriminative visual cues that also the classification weight vectors of the ground truth categories learn to look for, and (b) learn to generate l 2 -normalized feature vectors with low intraclass variance, since all the feature vectors that belong to the same category must be very closely matched with the single classification weight vector of that category. This is visually illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, where we visualize t-SNE scatter plots of cosine-similarity-based and dot-product-based features related to categories "unseen" during training. As can be clearly observed, the features generated from the cosine-similarity-based ConvNet form more compact and distinctive category-specific clusters (i.e., they provide more discriminative features). Moreover, our cosine-similarity based classification objective resembles the training objectives typically used by metric learning approaches <ref type="bibr" target="#b7">[7]</ref>. In fact, it turns out that our feature extractor trained solely on cosine-similarity based classification of base categories, when used for image matching, it manages to surpass all prior state-of-the-art approaches on the fewshot object recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Few-shot classification weight generator</head><p>The few-shot classification weight generator G(., .|φ) gets as input the feature vectors Z = {z i } N i=1 of the N training examples of a novel category (typically N ≤ 5) and (optionally) the classification weight vectors of the base categories W base . Based on them, it infers a classification weight vector w = G(Z , W base |φ) for that novel category. Here we explain how the above few-shot classification weight generator is constructed.</p><p>Feature averaging based weight inference. Since, as we explained in section § 3.1, the cosine similarity based classifier of the ConvNet model forces the feature extractor to learn feature vectors that form compact category-wise clusters and the classification weight vectors to learn to be representative feature vectors of those clusters, an obvious choice is to infer the classification weight vector w by averaging the feature vectors of the training examples (after they have been l 2 -normalized):</p><formula xml:id="formula_5">w avg = 1 N N i=1 z i .<label>(5)</label></formula><p>The final classification weight vector in case we only use the feature averaging mechanism is:</p><formula xml:id="formula_6">w = φ avg w avg ,<label>(6)</label></formula><p>where is the Hadamard product, and φ avg ∈ R d is a learnable weight vector. Similar strategy has been previously proposed by Snell et al. <ref type="bibr" target="#b23">[23]</ref> and has demonstrated very good results. However, it does not fully exploit the knowledge about the visual world that the ConvNet model acquires during its training phase. Furthermore, in case there is only a single training example for the novel category, the averaging cannot infer an accurate classification weight vector.</p><p>Attention-based weight inference. We enhance the above feature averaging mechanism with an attention based mechanism that composes novel classification weight vectors by "looking" at a memory that contains the base classification weight vectors W base = {w b } K base b=1 . More specifically, an extra attention-based classification weight vector w att is computed as:</p><formula xml:id="formula_7">w att = 1 N N i=1 K base b=1 Att(φ q z i , k b ) · w b ,<label>(7)</label></formula><p>where φ q ∈ R d×d is a learnable weight matrix that transforms the feature vector z i to query vector used for querying the memory,</p><formula xml:id="formula_8">{k b ∈ R d } K base b</formula><p>is a set of K base learnable keys (one per base category) used for indexing the memory, and Att(., .) is an attention kernel implemented as a cosine similarity function 2 followed by a softmax operation over the K base base categories. The final classification weight vector is computed as a weighted sum of the average based classification vector w avg and the attention based classification vector w att :</p><formula xml:id="formula_9">w = φ avg w avg + φ att w att ,<label>(8)</label></formula><p>where is the Hadamard product, and φ avg , φ att ∈ R d are learnable weight vectors. Why using an attention-based weight composition? Thanks to the cosine-similarity based classifier, the base classification weight vectors learn to be representative feature vectors of their categories. Thus, the base classification weight vectors also encode visual similarity, e.g., the classification vector of a mammal animal should be closer to the classification vector of another mammal animal rather than the classification vector of a vehicle. Therefore, the classification weight vector of a novel category can be composed as a linear combination of those base classification weight vectors that are most similar to the few training examples of that category. This allows our few-shot weight generator to explicitly exploit the acquired knowledge about the visual word (here represented by the base classification weight vectors) in order to improve the few-shot recognition performance. This improvement is very significant especially in the one-shot recognition setting where averaging cannot provide an accurate classification weight vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training procedure</head><p>In order to learn the ConvNet-based recognition model (i.e. the feature extractor F (.|θ) as well as the classifier C(.|W * )) and the few-shot classification weight generator G(., .|φ), we use as the sole input a training set</p><formula xml:id="formula_10">D train = K base b=1 {x b,i } N b i=1 of K base base categories.</formula><p>We split the training procedure into 2 stages and at each stage we minimize a different cross-entropy loss of the following form:</p><formula xml:id="formula_11">1 K base K base b=1 1 N b N b i=1 loss(x b,i , b),<label>(9)</label></formula><p>where loss(x, y) is the negative log-probability −log(p y ) of the y-th category in the probability vector p = C(F (x|θ)|W * ). The meaning of W * is different on each of the training stages, as we explain below. 1st training stage: During this stage we only learn the ConvNet recognition model without the few-shot classification weight generator. Specifically, at this stage we learn the parameters θ of the feature extractor F (.|θ) and the base classification weight vectors W base = {w b } K base b=1 . This is done in exactly the same way as for any other standard recognition model. In this case W * is equal to the base classification weight vectors W base .</p><p>2nd training stage: During this stage we train the learnable parameters φ of the few-shot classification weight generator while we continue training the base classification weight vectors W base (in our experiments during that training stage we freezed the feature extractor). In order to train the fewshow classification weight generator, in each batch we randomly pick K novel "fake" novel categories from the base categories and we treat them in the same way as we will treat the actual novel categories after training. Specifically, instead of using the classification weight vectors in W base for those "fake" novel categories, we sample N training examples (typically N ≤ 5) for each of them, compute their feature vectors Z = {z i } N i=1 , and give those feature vectors to the few-shot classification weight generator G(., .|φ) in order to compute novel classification weight generators. The inferred classification weight vectors are used for recognizing the "fake" novel categories. Everything is trained end-to-end. Note that we take care to exclude from the base classification weight vectors that are given as a second argument to the few-shot weight generator G(., .|φ) those classification vectors that correspond to the "fake" novel categories. In this case W * is the union of the "fake" novel classification weight vectors generated by G(., .|φ) and the classification weight vectors of the remaining base categories. More implementation details of this training stage are provided in appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>We extensively evaluate the proposed few-shot recognition system w.r.t. both its few-shot recognition performance of novel categories and its ability to not "forget" the base categories on which it was trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Mini-ImageNet experiments</head><p>Evaluation setting for recognition of novel categories. We evaluate our few-shot object recognition system on the Mini-ImageNet dataset <ref type="bibr" target="#b26">[26]</ref> that includes 100 different categories with 600 images per category, each of size 84×84. For our experiments we used the splits by Ravi and Laroche <ref type="bibr" target="#b17">[17]</ref> that include 64 categories for training, 16 categories for validation, and 20 categories for testing. The typical evaluation setting on this dataset is first to train a few-shot model on the training categories and then during test time to use the validation (or the test) categories in order to form few-shot tasks on which the trained model is evaluated. Those few-shot tasks are formed by first sampling K novel categories and one or five training example per category (1-shot and 5-shot settings respectively), which the trained model uses for metalearning those categories, and then evaluating it on some test examples that come from the same novel categories but do not overlap with the training examples.</p><p>Evaluation setting for the recognition of the base categories. When we evaluate our model w.r.t. few-shot recognition task on the validation / test categories, we consider as base categories the 64 training categories on which we trained the model. Since the proposed few-shot object recognition system has the ability to not forget the base categories, we would like to also evaluate the recognition performance of our model on those base categories. Therefore, we sampled 300 extra images for each training category that we use as validation image set for the evaluation of the recognition performance of the base categories and also another 300 extra images that are used for the same reason as test image set. Therefore, when we evaluate our model w.r.t. the few-shot learning task on the validation / test categories we also evaluate w.r.t. recognition performance of the base categories on the validation / test image set of the training categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Ablation study</head><p>In <ref type="table" target="#tab_1">Table 1</ref> we provide an ablation study of the proposed object recognition framework on the validation set of mini-ImageNet. We also compare with two prior state-of-theart approaches, Prototypical Networks <ref type="bibr" target="#b23">[23]</ref> and Matching Nets <ref type="bibr" target="#b26">[26]</ref>, that we re-implemented ourselves in order to ensure a fair comparison. The feature extractor used in all cases is a ConvNet model that has 4 convolutional modules, with 3 × 3 convolutions, followed by batch normalization, ReLU nonlinearity 3 , and 2 × 2 max-pooling. Given as input images of size 84 × 84 it yields feature maps with spatial size 5 × 5. The first two convolutional layers have 64 feature channels and the latter two have 128 feature channels.</p><p>Cosine-similarity based ConvNet model. First we examine the performance of the cosine-similarity based Con-vNet recognition model (entry Cosine Classifier) without training the few-shot classification weight generator (i.e., we only perform the 1st training stage as was described in section 3.3). In order to test its performance on the novel categories, during test time we estimate classification weight vectors using feature averaging. We want to stress out that  <ref type="table">Table 2</ref>: Average classification accuracies on the test set of Mini-ImageNet. In order to report those results we sampled 600 tasks in a similar fashion as for the validation set of Mini-ImageNet.</p><p>in this case there are no learnable parameters involved in the generation of the novel classification weight vectors and also the ConvNet model it was never trained on the fewshot recognition task. Despite that, the features learned by the cosine-similarity based ConvNet model matches or even surpasses the performance of the Matching-Nets and Prototypical Networks, which are explicitly trained on the few-shot object recognition task. By comparing the cosine-similarity based ConvNet models (Cosine Classifier entries) with the dot-product based models (Dot Product entries) we observe that the former drastically improve the few-shot object recognition performance, which means that the feature extractor that is learned with the cosine-similarity classifier generalizes significantly better on "unseen" categories than the feature extractor learned with the dot-product classifier. Notably, the cosine-similarity classifier significantly improves also the recognition performance on the base categories.</p><p>Removing the last ReLU unit. In our work we propose to remove the last ReLU non-linearity from the feature extractor when using a cosine classifier. Instead, keeping the ReLU units (Cosine w/ ReLU entries) decreases the accuracy on novel categories while increasing it on base categories.</p><p>Few-shot classification weight generator. Here we examine the performance of our system when we also incorporate on it the proposed few-shot classification weight generator. In <ref type="table" target="#tab_1">Table 1</ref> we provide two solutions for the few-shot weight generator: the entry Cosine Classifier &amp; Avg. Weight Gen that uses only the feature averaging mechanism described in section 3.2 and the entry Cosine Classifier &amp; Att. Weight Gen that uses both the feature averaging and the attention based mechanism. Both types of few-shot weight generators are trained during the 2nd training stage that is described in section 3.3. We observe that both of them offer a very significant boost on the few-shot recognition performance of the cosine similarity based model (entry Cosine Classifier). Among the two, the attention based solution exhibits better few-shot recognition behavior, especially in the 1-shot setting where it has more than 3 percentage points higher performance. Also, it is easy to see that the few-shot classification weight generator does not affect the recognition performance of the base categories, which is around 70.50% in all the cosine-similarity based models. Moreover, by introducing the few-shot weight generator, the recognition performance in both type of categories (columns Both) increases significantly, which means that the ConvNet model achieves better behavior w.r.t. our goal of unified recognition of both base and novel categories. The few-shot recognition performance of our full system, which is the one that includes the attention based few-shot weight generator (entry Cosine classifier &amp; Att. Weight Gen), offers a very significant improvement w.r.t. the prior state-of-the-art approaches on the few-shot object recognition task, i.e., from 72.67% to 74.92% in the 5-shot setting and from 55.53% to 58.55% in the 1-shot setting. Also, our system achieves significantly higher performance on the recognition of base categories compared to Prototypical Networks 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison with state-of-the-art</head><p>Here we compare the proposed few-shot object recognition system with other state-of-the-art approaches on the Mini-ImageNet test set.</p><p>Explored feature extractor architectures. Because prior approaches use several different network architectures for implementing the feature extractor of the ConvNet model, we evaluate our model with each of those architectures. Specifically the architectures that we evaluated are: C32F is a 4 module ConvNet network (which was described in § 4.1.1) with 32 feature channels on each convolutional layer, C64F has 64 feature channels on each layer, and in C128F the first two layers have 64 channels and the latter two have 128 channels (exactly the same as the model that was used in § 4.1.1). With RESNET we refer to the ResNet <ref type="bibr" target="#b5">[5]</ref> like network that was used from Mishra et al. <ref type="bibr" target="#b13">[13]</ref> (for more details we refer to <ref type="bibr" target="#b13">[13]</ref>).</p><p>In <ref type="table">Table 2</ref> we provide the experimental results. In all cases, our models (that include the cosine-similarity based ConvNet model and the attention-based few-shot weight generator) achieve better few-shot object recognition performance than prior approaches. Moreover, it is very important to note that our approach is capable to achieve such excellent accuracy on the novel categories while at the same time it does not sacrifice the recognition performance of the base categories, which is an ability that prior methods lack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Qualitative evaluation with t-SNE scatter plots</head><p>Here we compare qualitatively the feature representations learned by the proposed cosine-similarity based ConvNet recognition model with those learned by the typical dotproduct based ConvNet recognition model. For that purpose in <ref type="figure" target="#fig_1">Figure 2</ref> we provide the t-SNE <ref type="bibr" target="#b11">[11]</ref> scatter plots that visualize the local-structures of the feature representations learned in those two cases. Note that the visualized features are from the validation categories of the Mini-ImageNet dataset that are "unseen" during training. Also, in the case of the cosine-similarity based ConvNet recognition model, we visualize the l 2 -normalized features, which are the features that are actually learned by the feature extractor.</p><p>We observe that the feature extractor learned with the cosine-similarity based ConvNet recognition model, when applied on the images of "unseen" categories (in this case the validation categories of Mini-ImageNet), it generates features that form more compact and distinctive categoryspecific clusters (i.e., more discriminative features). Due to that, as it was argued in section §3.1, the features learned with the proposed cosine-similarity based recognition model generalize better on the "unseen" categories than the features learned with the typical dot-product based recognition model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Few-shot benchmark of Bharath &amp; Girshick [4]</head><p>Here we evaluate our approach on the ImageNet based few-shot benchmark proposed by Bharath and Girshick <ref type="bibr" target="#b4">[4]</ref> using the improved evaluation metrics proposed by Wang et al. <ref type="bibr" target="#b27">[27]</ref>. Briefly, this benchmark splits the ImageNet categories into 389 base categories and 611 novel categories; 193 of the base categories and 300 of the novel categories are used for cross validation and the remaining 196 base categories and 311 novel categories are used for the final evaluation (for more details we refer to <ref type="bibr" target="#b4">[4]</ref>). We use the same categories split as they did. However, because it was not possible to use the same training images that they did for the novel categories 5 , we sample ourselves N training images per novel category and, similar to them, evaluate using the images in the validation set of ImageNet. We repeat the above experiment 100 times (sampling each time a different set of training images for the novel categories) and report in <ref type="table">Table 3</ref> the mean accuracies and the 95% confidence intervals for the recognition accuracy metrics proposed in <ref type="bibr" target="#b27">[27]</ref>.</p><p>Comparison to prior and concurrent work. We compare our full system (Cosine Classifier &amp; Att. Weight Gen entry) against prior work, such as Prototypical-Nets <ref type="bibr">[</ref>  <ref type="table">Table 3</ref>: Top-5 accuracy on the novel categories and on all categories (with and without priors) fot the ImageNet based few-shot benchmark proposed in <ref type="bibr" target="#b4">[4]</ref> (for more details about the evaluation metrics we refer to <ref type="bibr" target="#b27">[27]</ref> Matching Networks <ref type="bibr" target="#b26">[26]</ref>, and the work of Bharath and Girshick <ref type="bibr" target="#b4">[4]</ref>. We also compare against the work of Wang et al. <ref type="bibr" target="#b27">[27]</ref>, which is concurrent to ours. We observe that in all cases our approach achieves superior performance than prior approaches and even exceeds (in all but one cases) the Prototype Matching Net <ref type="bibr" target="#b27">[27]</ref> based approaches that are concurrent to our work. Feature extractor: The feature extractor of all approaches is implemented with a ResNet-10 [5] network architecture 6 that gets as input images of 224 × 224 size. Also, when training the attention based few-shot classification weight generator component of our model (2nd training stage) we found helpful to apply dropout with 0.5 probability on the feature vectors generated by the feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In our work we propose a dynamic few-shot object recognition system that is able to quickly learn novel categories without forgetting the base categories on which it was trained, a property that most prior approaches on the few-shot learning task neglect to fulfill. To achieve that goal we propose a novel attention based few-shot classification weight generator as well as a cosine-similarity based ConvNet classifier. This allows to recognize in a unified way both novel and base categories and also leads to learn feature representations with better generalization capabilities. We evaluate our framework on Mini-ImageNet and the recently introduced fews-shot benchmark of Bharath and Girshick <ref type="bibr" target="#b4">[4]</ref> where we demonstrate that our approach is capable of both maintaining high recognition accuracy on base categories and to</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Cosine-similarity based features of novel categories (b) Dot-product based features of novel categories</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Here we visualize the t-SNE<ref type="bibr" target="#b11">[11]</ref> scatter plots of the feature representations learned with (a) the cosine-similarity based ConvNet recognition model, and (b) the dot-product based ConvNet recognition model. Note that in the case of the cosine-similarity based ConvNet recognition model, we visualize the l2-normalized features. The visualized feature data points are from the "unseen" during training validation categories of Mini-ImageNet. Each data point in the t-SNE scatter plots is colored according to its category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Feature Extractor Dynamic Few-Shot Learning without Forgetting Classifier</head><label></label><figDesc>Overview of our system. It consists of: (a) a ConvNet based recognition model (that includes a feature extractor and a classifier) and (b) a few-shot classification weight generator. Both are trained on a set of base categories for which we have available a large set of training data. During test time, the weight generator gets as input a few training data of a novel category and the classification weight vectors of base categories (green rectangle inside the classifier box) and generates a classification weight vector for this novel category (blue rectangle inside the classifier box). This allows the ConvNet to recognize both base and novel categories.</figDesc><table><row><cell>Test image</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Base Novel</cell></row><row><cell></cell><cell>Classification weight vectors</cell><cell>Few-shot classification weight generator</cell><cell>Probability scores of base &amp; novel categories</cell></row><row><cell>Training data for base categories</cell><cell>Training procedure</cell><cell>Few training data of novel category</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>± 0.37% 62.10% 32.70% 54.44 ± 0.48% 52.35% 26.68% Avg. Weight Gen 74.66 ± 0.35% 70.92% 60.26% 55.33 ± 0.46% 70.45% 48.56% Cosine Classifier &amp; Att. Weight Gen 74.92 ± 0.36% 70.88% 60.50% 58.55 ± 0.50% 70.73% 50.50% Average classification accuracies on the validation set of Mini-ImageNet. The Novel columns report the average 5-way and 1-shot or 5-shot classification accuracies of novel categories (with 95% confidence intervals), the Base and Both columns report the classification accuracies of base categories and of both type of categories respectively. In order to report those results we sampled 2000 tasks each with 15 × 5 test examples of novel categories and 15 × 5 test examples of base categories. 62% 68.13% 57.72% 56.20 ± 0.86% 68.08% 48.09% Ours C128F 73.00 ± 0.64% 70.90% 59.35% 55.95 ± 0.84% 70.72% 49.08% Ours RESNET 70.13 ± 0.68% 80.16% 56.04% 55.45 ± 0.89% 80.24% 51.23%</figDesc><table><row><cell>Models</cell><cell></cell><cell cols="3">5-Shot learning -K novel =5 Novel Base Both</cell><cell cols="3">1-Shot learning -K novel =5 Novel Base Both</cell></row><row><cell>Matching-Nets [26]</cell><cell></cell><cell>68.87 ± 0.38%</cell><cell>-</cell><cell cols="2">-55.53 ± 0.48%</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Prototypical-Nets [23] 72.67 Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cosine Classifier</cell><cell></cell><cell cols="6">72.83 ± 0.35% 70.68% 51.89% 54.55 ± 0.44% 70.68% 39.17%</cell></row><row><cell>Cosine Classifier &amp; Ablations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dot Product</cell><cell></cell><cell cols="6">64.58 ± 0.38% 63.59% 31.80% 46.09 ± 0.40% 63.59% 24.76%</cell></row><row><cell cols="2">Dot Product &amp; Avg. Weight Gen</cell><cell cols="6">60.30 ± 0.39% 62.15% 46.41% 44.31 ± 0.40% 61.99% 39.05%</cell></row><row><cell cols="2">Dot Product &amp; Att. Weight Gen</cell><cell cols="6">67.81 ± 0.37% 62.11% 48.70% 53.88 ± 0.48% 62.28% 42.41%</cell></row><row><cell>Ablations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cosine w/ ReLU.</cell><cell></cell><cell cols="6">71.04 ± 0.36% 72.51% 58.16% 52.91 ± 0.45% 72.51% 43.17%</cell></row><row><cell cols="8">Cosine w/ ReLU. &amp; Avg. Weight Gen 71.30 ± 0.38% 72.47% 59.33% 53.19 ± 0.45% 71.70% 49.53%</cell></row><row><cell cols="2">Cosine w/ ReLU. &amp; Att. Weight Gen</cell><cell cols="6">73.03 ± 0.38% 72.26% 61.05% 56.09 ± 0.54% 72.34% 51.25%</cell></row><row><cell>Models</cell><cell>Feature Extractor</cell><cell cols="3">5-Shot learning -K novel =5 Novel Base Both</cell><cell cols="3">1-Shot learning -K novel =5 Novel Base Both</cell></row><row><cell>Matching-Nets [26]</cell><cell>C64F</cell><cell>55.30%</cell><cell>-</cell><cell>-</cell><cell>43.60%</cell><cell>-</cell><cell>-</cell></row><row><cell>Ravi and Laroche [17]</cell><cell cols="2">C32F 60.20 ± 0.71%</cell><cell>-</cell><cell cols="2">-43.40 ± 0.77%</cell><cell>-</cell><cell>-</cell></row><row><cell>Finn et al. [3]</cell><cell cols="2">C64F 63.10 ± 0.92%</cell><cell>-</cell><cell cols="2">-48.70 ± 1.84%</cell><cell>-</cell><cell>-</cell></row><row><cell>Prototypical-Nets [23]</cell><cell cols="2">C64F 68.20 ± 0.66%</cell><cell>-</cell><cell cols="2">-49.42 ± 0.78%</cell><cell>-</cell><cell>-</cell></row><row><cell>Mishra et al. [13]</cell><cell cols="2">RESNET 68.88 ± 0.92%</cell><cell>-</cell><cell cols="2">-55.71 ± 0.99%</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="7">C32F 70.27 ± 0.64% 61.08% 52.45% 54.33 ± 0.81% 61.09% 43.05%</cell></row><row><cell>Ours</cell><cell cols="2">C64F 72.81 ± 0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Prior workPrototypical-Nets<ref type="bibr" target="#b23">[23]</ref> (from<ref type="bibr" target="#b27">[27]</ref>) 39.3 54.4 66.3 71.2 73.9 49.5 61.0 69.7 72.9 74.6 53.6 61.4 68.8 72.0 73.8 Matching Networks [26] (from [27]) 43.6 54.0 66.0 72.5 76.9 54.4 61.0 69.0 73.7 76.5 54.5 60.7 68.2 72.6 75.6 Logistic regression (from [27]) 38.4 51.1 64.8 71.6 76.6 40.8 49.9 64.2 71.9 76.9 52.9 60.4 68.6 72.9 76.3 Logistic regression w/ H [4] (from [27]) 40.7 50.8 62.0 69.3 76.5 52.2 59.4 67.6 72.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Novel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>All</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">All with prior</cell><cell></cell></row><row><cell>Approach</cell><cell>N =1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>N =1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>N =1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">8 76.9</cell><cell cols="5">53.2 59.1 66.8 71.7 76.3</cell></row><row><cell>SGM w/ H [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">54.3 62.1 71.3 75.8 78.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Batch SGM [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">49.3 60.5 71.4 75.8 78.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>23],</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). For each novel category we use N = 1, 2, 5, 10 or 20 training examples. Methods with "w/ H" use mechanisms that hallucinate extra training examples for the novel categories. The second rows in our entries report the 95% confidence intervals.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This work was supported by the ANR SEMAPOLIS project, an INTEL gift, and hardware donation by NVIDIA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The scalar parameter τ is introduced in order to control the peakiness of the probability distribution generated by the softmax operator since the range of the cosine similarity is fixed to[−1, 1]. In all of our experiments τ is initialized to 10.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The cosine similarity scores are also scaled by a learnable scalar parameter γ in order to increase the peakiness of the softmax distribution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Unless otherwise stated, our cosine-similarity based models as well as the re-implementation of Matching-Nets do not have a ReLU nonlinearity after the last convolutional layer, since in both cases this modification improved the recognition performance on the few-shot recognition task</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In order to recognize base categories with Prototypical Networks, the prototypes for the base categories are computed by averaging all the available training features vectors</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">It was not possible to establish a correspondence between the index files that they provide and the ImageNet images</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Similar to what it is already explained, our model does not include the last ReLU non-linearity of the ResNet-10 feature extractor achieve excellent few-shot recognition accuracy on novel categories that surpasses prior state-of-the-art approaches by a significant margin.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A. Implementation details of training procedure followed during the 2nd training stage</p><p>As explained in §3.3, in order to train the few-shot classification weight generator, during the 2nd training stage we sample K novel "fake" novel categories from the base training categories and we treat them in the same way as we will treat the actual novel categories after training. More specifically, during the 2nd training stage we form "training episodes"; each "training episode" is created by sampling: (a) K novel "fake" novel categories with N training examples per "fake" novel category, (b) T novel test image examples from the "fake" novel categories, and (c) T base test image examples from the remaining base categories (i.e., the base categories without the "fake" novel categories). Given such a "training episode", we first use the N training examples of each "fake" novel category to infer with the few-shot weight generator a "fake" novel classification weight vector for that category and then we use the union of "fake" novel classification weight vectors and the classification weight vectors of the remaining base categories in order to learn to classify the T = T novel + T base test image examples. To conclude, in order to train the few-shot classification weight generator and the classification weight vectors of the base categories we use stochastic gradient descent based routines with training batches that include multiple different instances of the above "training episodes".</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>46.02 57.51 69.16 74.83 78.11</idno>
	</analytic>
	<monogr>
		<title level="j">Cosine Classifier &amp; Att. Weight Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cosine normalization: Using cosine similarity instead of dot product in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chunjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02819</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03141</idno>
		<title level="m">Meta-learning with temporal convolutions</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00837</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Meta networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07136</idno>
		<title level="m">Learning with imprinted weights</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06065</idno>
		<title level="m">One-shot learning with memory-augmented neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shifting inductive bias with success-story algorithm, adaptive levin search, and incremental self-improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="105" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<title level="m">Prototypical networks for few-shot learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Lifelong learning algorithms. Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05401</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
