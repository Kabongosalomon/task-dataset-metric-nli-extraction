<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RRPN: RADAR REGION PROPOSAL NETWORK FOR OBJECT DETECTION IN AUTONOMOUS VEHICLES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Nabati</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The University of Tennessee Knoxville</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hairong</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The University of Tennessee Knoxville</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RRPN: RADAR REGION PROPOSAL NETWORK FOR OBJECT DETECTION IN AUTONOMOUS VEHICLES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Region Proposal Network</term>
					<term>Autonomous Driving</term>
					<term>Object Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Region proposal algorithms play an important role in most state-of-the-art two-stage object detection networks by hypothesizing object locations in the image. Nonetheless, region proposal algorithms are known to be the bottleneck in most two-stage object detection networks, increasing the processing time for each image and resulting in slow networks not suitable for real-time applications such as autonomous driving vehicles. In this paper we introduce RRPN, a Radarbased real-time region proposal algorithm for object detection in autonomous driving vehicles. RRPN generates object proposals by mapping Radar detections to the image coordinate system and generating pre-defined anchor boxes for each mapped Radar detection point. These anchor boxes are then transformed and scaled based on the object's distance from the vehicle, to provide more accurate proposals for the detected objects. We evaluate our method on the newly released NuScenes dataset [1] using the Fast R-CNN object detection network <ref type="bibr" target="#b1">[2]</ref>. Compared to the Selective Search object proposal algorithm [3], our model operates more than 100× faster while at the same time achieves higher detection precision and recall. Code has been made publicly available at https://github.com/mrnabati/RRPN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Real-time object detection is one of the most challenging problems in building perception systems for autonomous vehicles. Most self-driving vehicles take advantage of several sensors such as cameras, Radars and LIDARs. Having different types of sensors provides an advantage in tasks such as object detection and may result in more accurate and reliable detections, but at the same time makes designing a real-time perception system more challenging.</p><p>Radars are one of the most popular sensors used in autonomous vehicles and have been studied for a long time in different automotive applications. Authors in <ref type="bibr" target="#b3">[4]</ref> were among the first researchers discussing such applications for Radars, providing a detailed approach for utilizing them on vehicles.</p><p>While Radars can provide accurate range and range-rate information on the detected objects, they are not suitable for tasks such as object classification. Cameras on the other hand, are very effective sensors for object classification, making Radar and camera sensor fusion a very interesting topic in autonomous driving applications. Unfortunately, there has been very few studies in this area in recent years, mostly due to the lack of a publicly available dataset with annotated and synchronized camera and Radar data in an autonomous driving setting.</p><p>2D object detection has seen a significant progress over the past few years, resulting in very accurate and efficient algorithms mostly based on convolutional neural networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. These methods usually fall under two main categories, one-stage and two-stage algorithms. One-stage algorithms treat object detection as a regression problem and learn the class probabilities and bounding boxes directly from the input image <ref type="bibr" target="#b7">[8]</ref>. YOLO <ref type="bibr" target="#b8">[9]</ref> and SSD <ref type="bibr" target="#b6">[7]</ref> are among the most popular algorithms in this category. Two-stage algorithms such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> on the other hand, use a Region Proposal Network (RPN) in the first stage to generate regions of interests, and then use these proposals in the second stage to do classification and bounding box regression. One-stage algorithms usually reach lower accuracy rates, but are much faster than their two-stage counterparts. The bottleneck in two-stage algorithms is usually the RPN, processing every single image to generate ROIs for the object classifier, although yielding higher accuracy. This makes two-stage object detection algorithms not suitable for applications such as autonomous driving where it's extremely important for the perception system to operate in real time.</p><p>In this paper we propose Radar Region Proposal Network (RRPN), a real-time RPN based on Radar detections in autonomous vehicles. By relying only on Radar detections to propose regions of interest, we bypass the computationally expensive vision-based region proposal step, while improving detection accuracy. We demonstrate the effectiveness of our approach in the newly released NuScenes dataset <ref type="bibr" target="#b0">[1]</ref>, featuring data from Radars and cameras among other sensors integrated on a vehicle. When used in the Fast R-CNN object detection network, our proposed method achieves higher mean Average Precision (AP) and mean Average Recall (AR) compared to the Selective Search algorithm originally used in Fast R-CNN, while operating more than 100× faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Authors in <ref type="bibr" target="#b9">[10]</ref> discussed the application of Radars in navigation for autonomous vehicles, using an extended Kalman filter to fuse the radar and vehicle control signals for estimating vehicle position. In <ref type="bibr" target="#b10">[11]</ref> authors proposed a correlation based pattern matching algorithm in addition to a range-window to detect and track objects in front of a vehicle. In <ref type="bibr" target="#b11">[12]</ref> Ji et al.</p><p>proposed an attention selection system based on Radar detections to find candidate targets and employ a classification network to classify those objects. They generated a single attention window for each Radar detection, and used a Multilayer In-place Learning Network (MILN) as the classifier.</p><p>Authors in <ref type="bibr" target="#b12">[13]</ref> proposed a LIDAR and vision-based pedestrian detection system using both a centralized and decentralized fusion architecture. In the former, authors proposed a feature level fusion system where features from LIDAR and vision spaces are combined in a single vector which is classified using a single classifier. In the latter, two classifiers are employed, one per sensorfeature space. More recently, Choi et al. in <ref type="bibr" target="#b13">[14]</ref> proposed a multi-sensor fusion system addressing the fusion of 14 sensors integrated on a vehicle. This system uses an Extended Kalman Filter to process the observations from individual sensors and is able to detect and track pedestrian, bicyclists and vehicles.</p><p>Vision based object proposal algorithms have been very popular among object detection networks. Authors in <ref type="bibr" target="#b2">[3]</ref> proposed the Selective Search algorithm, diversifying the search for objects by using a variety of complementary image partitionings. Despite its high accuracy, Selective Search is computationally expensive, operating at 2-7 seconds per image. Edge Boxes <ref type="bibr" target="#b14">[15]</ref> is another vision based object proposal algorithm using edges to detect objects. Edge Boxes is faster than the Selective Search algorithm with a run time of 0.25 seconds per image, but it is still considered very slow in realtime applications such as autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RADAR REGION PROPOSAL NETWORK</head><p>We propose RRPN for object detection and classification in autonomous vehicles, a real-time algorithm using Radar detections to generate object proposals. The generated proposals can be used in any two-stage object detection network such as Fast-RCNN. Relying only on Radar detections to generate object proposals makes an extremely fast RPN, making it suitable for autonomous driving applications. Aside from being a RPN for an object detection algorithm, the proposed network also inherently acts as a sensor fusion algorithm by fusing the Radar and camera data to obtain higher accuracy and reliability. The objects' range and range-rate information obtained from the Radar can be easily associated with the proposed regions of interest, providing accurate depth and velocity information for the detected objects.</p><p>RRPN also provides an attention mechanism to focus the underlying computational resources on the more important parts of the input data. While in other object detection applications the entire image may be of equal importance. In an autonomous driving application more attention needs to be given to objects on the road. For example in a highway driving scenario, the perception system needs to be able to detect all the vehicles on the road, but there is no need to dedicate resources to detect a picture of a vehicle on a billboard. A Radar based RPN focuses only on the physical objects surrounding the vehicle, hence inherently creating an attention mechanism focusing on parts of the input image that are more important.</p><p>The proposed RRPN consists of three steps: perspective transformation, anchor generation and distance compensation, each individually discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Perspective Transformation</head><p>The first step in generating ROIs is mapping the radar detections from the vehicle coordinates to the camera-view coordinates. Radar detections are reported in a bird's eye view perspective as shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>, with the object's range and azimuth measured in the vehicle's coordinate system. By mapping these detections to the camera-view coordinates, we are able to associate the objects detected by the Radars to those seen in the images obtained by the camera.</p><p>In general, the projective relation between a 3D point P = [X; Y ; Z; 1] and its image p = [x; y; 1] in the camera-view plane can be expressed as below: In an autonomous driving application, the matrix H can be obtained from the calibration parameters of the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Anchor Generation</head><p>Once the Radar detections are mapped to the image coordinates, we have the approximate location of every detected object in the image. These mapped Radar detections, hereafter called Points of Interest (POI), provide valuable information about the objects in each image, without any processing on the image itself. Having this information, a simple approach for proposing ROIs would be introducing a bounding box centered at every POI. One problem with this approach is that Radar detections are not always mapped to the center of the detected objects in every image. Another problem is the fact that Radars do not provide any information about the size of the detected objects and proposing a fixed-size bounding box for objects of different sizes would not be an effective approach. We use the idea of anchor bounding boxes from Faster R-CNN <ref type="bibr" target="#b5">[6]</ref> to alleviate the problems mentioned above. For every POI, we generate several bounding boxes with different sizes and aspect ratios centered at the POI, as shown in <ref type="figure" target="#fig_0">Fig. 1  (b)</ref>. We use 4 different sizes and 3 different aspect ratios to generate these anchors.</p><p>To account for the fact that the POI is not always mapped to the center of the object in the image coordinate, we also generate different translated versions of the anchors. These translated anchors provide more accurate bounding boxes when the POI is mapped towards the right, left or the bottom of the object as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> c-e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Distance Compensation</head><p>The distance of each object from the vehicle plays an important role in determining its size in the image. Generally, objects' sizes in an image have an inverse relationship with their distance from the camera. Radar detections have the range information for every detected object, which is used in this step to scale all generated anchors. We use the following formula to determine the scaling factor to use on the anchors:</p><formula xml:id="formula_0">S i = α 1 d i + β<label>(2)</label></formula><p>where d i is the distance to the ith object, and α and β are two parameters used to adjust the scale factor. These parameters are learned by maximizing the Intersection Over Union (IOU) between the generated bounding boxes and the ground truth bounding boxes in each image, as shown in Eq. 3 below.</p><formula xml:id="formula_1">argmax α,β N i=1 Mi j=1 max 1&lt;k&lt;Ai IOU i jk (α, β)<label>(3)</label></formula><p>In this equation, N is the number of training images, M i is the number of ground truth bounding boxes in image i, A i is the number of anchors generated in image i, and IOU i jk is the IOU between the jth ground truth bounding box in image i and the kth proposed anchor in that image. This equation finds the parameters α and β that maximize the IOU between the ground truth and proposed bounding boxes. We use a simple grid search approach over a range of values to find α and β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>To evaluate the proposed RPN, we use the recently released NuScenes dataset. NuScenes is a publicly available largescale dataset for autonomous driving, featuring a full sensor suite including Radars, cameras, LIDAR and GPS units. Having 3D bounding boxes for 25 object classes and 1.3M Radar sweeps, NuScenes is the first large-scale dataset to publicly provide synchronized and annotated camera and Radar data collected in highly challenging driving situations. To use this dataset in our application, we have converted all 3D bounding boxes to 2D and also merged some of the similar classes, such as child, adult and police officer. The classes used for our experiments are Car, Truck, Person, Motorcycle, Bicycle and Bus.</p><p>The NuScenes dataset includes images from 6 different cameras in the front, sides and back of the vehicle. The Radar detections are obtained from four corner Radars and one front Radar. We use two subsets of the samples available in the dataset for our experiments. The first subset contains data from the front camera and front Radar only, with 23k samples. We refer to this subset as NS-F . The second subset contains data from the rear camera and two rear Radars, in addition to all the samples from NS-F . This subset has 45k images and we call it NS-FB . Since front Radars usually have a longer range compared to the corner Radars, NS-F gives us more accurate detections for objects far away from the vehicle. On the other hand, NS-FB includes samples from the rear camera and Radar that are more challenging for our network. We further split each dataset with a ratio of 0.85-0.15 for training and testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use the RRPN to propose ROIs for a Fast R-CNN object detection network. Two different backbone networks have been used with Fast R-CNN: the original ResNet-101 network <ref type="bibr" target="#b15">[16]</ref> hereafter called R101 and the ResNeXt-101 <ref type="bibr" target="#b16">[17]</ref>, an improved version of ResNet, hereafter called X101. In the training stage, we start from a model pre-trained on the COCO dataset and further fine-tune it on NS-F and NS-FB .  We compare the results of detection using RRPN proposals with that of the Selective Search algorithm <ref type="bibr" target="#b2">[3]</ref>, which uses a variety of complementary image partitionings to find objects in images. In both RRPN and Selective Search, we limit the number of object proposals to 2000 per image.</p><p>The evaluation metrics used in our experiments are the same metrics used in the COCO dataset <ref type="bibr" target="#b17">[18]</ref>, namely mean Average Precision (AP) and mean Average Recall (AR). We also report the AP calculated with 0.5 and 0.75 IOU, as well as AR for small, medium and large objects areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>The Fast R-CNN object detection results for the two RPN networks on NS-F and NS-FB datasets are shown in <ref type="table" target="#tab_2">Table 1</ref>. According to these results, RRPN is outperforming Selective Search in almost all metrics. <ref type="table" target="#tab_3">Table 2</ref> shows the per-class AP results for the NS-F and NS-FB datasets, respectively. For the NS-F dataset, RRPN outperforms Selective Search in the Person, Motorcycle and Bicycle classes with a wide margin, while following Selective Search closely in other classes. For the NS-FB dataset, RRPN outperforms Selective Search in all classes except for the Bus class. <ref type="figure">Figure 2</ref> shows selected examples of the object detection results, with the first row showing the ground truth and mapped Radar detections. The next two rows are the detected bounding boxes using the region proposals from Selective Search and RRPN respectively. According to these figures, RRPN has been very successful in proposing accurate bounding boxes even under hard circumstances such as object occlusion and overlap. In our experiments, RRPN was able to generate proposals for anywhere between 70 to 90 images per second, depending on the number of Radar detections, while Selective Search took between 2-7 seconds per image.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We presented RRPN, a real-time region proposal network for object detection in autonomous driving applications. By only relying on Radar detections to propose ROIs, our method is extremely fast while at the same time achieving a higher precision and recall compared to the Selective Search algorithm. Additionally, RRPN inherently performs as a sensor fusion algorithm, fusing the data obtained from Radars with vision data to obtain faster and more accurate detections. We evaluated RRPN on the NuScenes dataset and compared the results to the Selective Search algorithm. Our experiments show RRPN operates more than 100x faster than the Selective Search algorithm, while resulting in better detection average precision and recall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Bird's eye view (b) Centered anchors (c) Right aligned anchors (d) Bottom-aligned anchors (e) Left aligned anchors Generating anchors of different shapes and sizes for each Radar detection, shown here as the blue circle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>truck</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0.652 0.463 0.478 0.041 0.406 0.573 RRPN + R101 -F 0.430 0.649 0.485 0.486 0.040 0.412 0.582 SS + X101 -FB 0.332 0.545 0.352 0.382 0.001 0.291 0.585 SS + R101 -FB 0.336 0.548 0.357 0.385 0.001 0.291 0.591 RRPN + X101 -FB 0.354 0.592 0.369 0.420 0.202 0.391 0.510 RRPN + R101 -FB 0.355 0.590 0.370 0.421 0.211 0.391 0.514</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Detection results for the NS-F and NS-FB datasets</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Per-class AP for the NS-F and NS-FB datasets</figDesc><table><row><cell>method</cell><cell cols="3">Car Truck Person Motorcycle Bicycle Bus</cell></row><row><cell>SS + X101 -F</cell><cell>0.424 0.509 0.117</cell><cell>0.288</cell><cell>0.190 0.680</cell></row><row><cell>SS + R101 -F</cell><cell>0.472 0.545 0.155</cell><cell>0.354</cell><cell>0.241 0.722</cell></row><row><cell>RRPN + X101 -F</cell><cell>0.428 0.501 0.212</cell><cell>0.407</cell><cell>0.304 0.660</cell></row><row><cell>RRPN + R101 -F</cell><cell>0.442 0.516 0.220</cell><cell>0.434</cell><cell>0.306 0.664</cell></row><row><cell>SS + X101 -FB</cell><cell>0.390 0.415 0.122</cell><cell>0.292</cell><cell>0.179 0.592</cell></row><row><cell>SS + R101 -FB</cell><cell>0.392 0.420 0.121</cell><cell>0.291</cell><cell>0.191 0.600</cell></row><row><cell cols="2">RRPN + X101 -FB 0.414 0.449 0.174</cell><cell>0.294</cell><cell>0.215 0.579</cell></row><row><cell cols="2">RRPN + R101 -FB 0.418 0.447 0.171</cell><cell>0.305</cell><cell>0.214 0.572</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selective Search for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automotive Radar: A Brief Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Owen</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="804" to="822" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Optimizing the trade-off between single-stage and two-stage object detectors using image difficulty prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petru</forename><surname>Soviany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08707</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An autonomous fuzzy logic architecture for multisensor data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D L</forename><surname>R E Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1994 IEEE International Conference on MFI 94 Multisensor Fusion and Integration for Intelligent Systems</title>
		<meeting>1994 IEEE International Conference on MFI 94 Multisensor Fusion and Integration for Intelligent Systems</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Target tracking by a single camera based on range-window algorithm and pattern matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunji</forename><surname>Miyahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Sielagoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli</forename><surname>Koulinitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faroog</forename><surname>Ibrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAE Technical Paper. 04 2006, SAE International</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Radar-vision fusion for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danil</forename><surname>Prokhorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Information Fusion, FU-SION 2008</title>
		<meeting>the 11th International Conference on Information Fusion, FU-SION 2008</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lidar and vision-based pedestrian detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristiano</forename><surname>Premebida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswaldo</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urbano</forename><surname>Nunes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="696" to="711" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multi-sensor fusion system for moving object detection and tracking in urban driving environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunggi</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Woo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragunathan Raj</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -IEEE International Conference on Robotics and Automation</title>
		<meeting>-IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1836" to="1843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page">740755</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
