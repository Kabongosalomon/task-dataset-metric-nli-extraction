<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Person Re-identification via Multi-label Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkai</forename><surname>Wang</surname></persName>
							<email>dongkai.wang@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Person Re-identification via Multi-label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The challenge of unsupervised person re-identification (ReID) lies in learning discriminative features without true labels. This paper formulates unsupervised person ReID as a multi-label classification task to progressively seek true labels. Our method starts by assigning each person image with a single-class label, then evolves to multi-label classification by leveraging the updated ReID model for label prediction. The label prediction comprises similarity computation and cycle consistency to ensure the quality of predicted labels. To boost the ReID model training efficiency in multi-label classification, we further propose the memory-based multi-label classification loss (MMCL). MMCL works with memory-based non-parametric classifier and integrates multi-label classification and single-label classification in a unified framework. Our label prediction and MMCL work iteratively and substantially boost the ReID performance. Experiments on several large-scale person ReID datasets demonstrate the superiority of our method in unsupervised person ReID. Our method also allows to use labeled person images in other domains. Under this transfer learning setting, our method also achieves state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed the great success of person re-identification (ReID), which learns discriminative features from labeled person images with deep Convolutional Neural Network (CNN) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Because it is expensive to annotate person images across multiple cameras, recent research efforts start to focus on unsupervised person ReID. Unsupervised person ReID aims to learn discriminative features from unlabeled person images. Compared with supervised learning, unsupervised learning relieves the requirement for expensive data annotation, hence shows better potential to push person ReID towards real applications.</p><p>The challenge of unsupervised person ReID lies in learning discriminative features without true labels. To conquer . Illustrations of multi-label classification for unsupervised person ReID. We target to assign each unlabeled person image with a multi-class label reflecting the person identity. This is achieved by iteratively running MPLP for prediction and MMCL for multi-label classification loss computation. This procedure guides CNN to produce discriminative features for ReID.</p><p>this challenge, most of recent works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref> define unsupervised person ReID as a transfer learning task, which leverages labeled data on other domains for model initialization or label transfer. Among them, some works assign each image with a single-class label <ref type="bibr" target="#b45">[46]</ref>. Some others leverage spatio-temporal cues or additional attribute annotations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>. Detailed review of existing methods will be presented in Sec. 2. Thanks to the above efforts, the performance of unsupervised person ReID has been significantly boosted. However, there is still a considerable gap between supervised and unsupervised person ReID. Meanwhile, the setting of transfer learning leads to limited flexibility. For example, as discussed in many works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31]</ref>, the performance of transfer learning is closely related to the domain gap, e.g., large domain gap degrades the performance. It is non-trivial to estimate the domain gap and select suitable source datasets for transfer learning in unsupervised person ReID. This paper targets to boost unsupervised person ReID without leveraging any labeled data. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, we treat each unlabeled person image as a class and train the ReID model to assign each image with a multi-class label. In other words, the ReID model is trained to classify each image to multiple classes belonging to the same identity. Because each person usually has multiple images, multilabel classification effectively identifies images of the same identity and differentiates images from different identities. This in-turn facilitates the ReID model to optimize inter and intra class distances. Compared with previous methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>, which classify each image into a single class, the multi-label classification has potential to exhibit better efficiency and accuracy.</p><p>Our method iteratively predicts multi-class labels and updates the network with multi-label classification loss. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, to ensure the quality of predicted labels, we propose the Memory-based Positive Label Prediction (MPLP), which considers both visual similarity and cycle consistency for label prediction. Namely, two images are assigned with the same label if they a) share large similarity and b) share similar neighbors. To further ensure the accuracy of label prediction, MPLP utilizes image features stored in the memory bank, which is updated with augmented features after each training iteration to improve feature robustness.</p><p>Predicted labels allow for CNN training with a multilabel classification loss. Since each image is treated as a class, the huge number of classes makes it hard to train classifiers like Fully Connected (FC) layers. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we adopt the feature of each image stored in the memory bank as a classifier. Specifically, a Memory-based Multilabel Classification Loss (MMCL) is introduced. MMCL accelerates the loss computation and addresses the vanishing gradient issue in traditional multi-label classification loss <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5]</ref> by abandoning the sigmoid function and enforcing the classification score to 1 or -1. MMCL also involves hard negative class mining to deal with the imbalance between positive and negative classes.</p><p>We test our approach on several large-scale person ReID datasets including Market-1501 <ref type="bibr" target="#b41">[42]</ref>, DukeMTMC-reID <ref type="bibr" target="#b24">[25]</ref> and MSMT17 <ref type="bibr" target="#b30">[31]</ref> without leveraging other labeled data. Comparison with recent works shows our method achieves competitive performance. For instance, we achieve rank-1 accuracy of 80.3% on Market-1501, significantly outperforming the recent BUC <ref type="bibr" target="#b19">[20]</ref> and DBC <ref type="bibr" target="#b3">[4]</ref> by 14.1% and 11.1%, respectively. Our performance is also better than the HHL <ref type="bibr" target="#b44">[45]</ref> and ECN <ref type="bibr" target="#b45">[46]</ref>, which use extra DukeMTMC-reID <ref type="bibr" target="#b24">[25]</ref> for transfer learning. Our method is also compatible with transfer learning. Leveraging DukeMTMC-reID for training, we further achieve rank-1 accuracy of 84.4% on Market-1501.</p><p>In summary, our method iteratively runs MPLP and MMCL to seek true labels for multi-label classification and CNN training. As shown in our experiments, this strategy, although does not leverage any labeled data, achieves promising performance. The maintained memory bank reinforces both label prediction and classification. Our work also shows that, unsupervised training has potential to achieve better flexibility and accuracy than existing transfer learning strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section briefly reviews related works on unsupervised person ReID, unsupervised feature learning, and multi-label classification.</p><p>Unsupervised person ReID works can be summarized into three categories. The first category utilizes hand-craft features <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42]</ref>. However, it is difficult to design robust and discriminative features by hand. The second category <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> adopts clustering to estimate pseudo labels to train the CNN. However, these methods require good pretrained model. Apart from this, Lin et al. <ref type="bibr" target="#b19">[20]</ref> treat each image as a cluster, through training and merging clusters, this method achieves good performance.</p><p>The third category <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref> utilizes transfer learning to improve unsupervised person ReID. Some works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref> use transfer learning and minimize the attribute-level discrepancy by utilizing extra attribute annotations. MAR <ref type="bibr" target="#b37">[38]</ref> uses the source dataset as a reference to learn soft labels, which hence supervise the ReID model training. Generative Adversarial Network (GAN) is also utilized for transfer learning. PT-GAN <ref type="bibr" target="#b30">[31]</ref> and SPGAN <ref type="bibr" target="#b2">[3]</ref> first generate transferred images from source datasets, then uses transferred image for training. HHL <ref type="bibr" target="#b44">[45]</ref> generates images under different cameras and trains network using triplet loss. ECN <ref type="bibr" target="#b45">[46]</ref> utilizes transfer learning and minimizes the target invariance. Transfer learning requires a labeled source dataset for training. Our method differs with them that, it does not require any labeled data. It also achieves better performance than many transfer learning methods.</p><p>Unsupervised feature learning aims to relieve the requirement on labeled data for feature learning. It can be applied in different tasks. Some works adopt unsupervised feature learning for neural network initialization. For example, RotNet <ref type="bibr" target="#b12">[13]</ref> predicts the rotation of image to learn a good representation. Li et al. <ref type="bibr" target="#b14">[15]</ref> use motion and view as supervision to learn an initialization for action recognition. Some other works utilize unsupervised feature learning to acquire features for image classification and retrieval. <ref type="bibr" target="#b10">[11]</ref> utilizes manifold learning to seek positive and negative samples to compute the triplet loss. Wu et al. <ref type="bibr" target="#b33">[34]</ref> regard each image as single class, and propose a non-parametric softmax classifier to train CNN. Our work shares certain similarity with <ref type="bibr" target="#b33">[34]</ref>, in that we also use non-parametric classifiers. However, we consider multi-label classification, which is important in identifying images of the same identity as well as differentiating different identities.</p><p>Multi-label classification is designed for classification tasks with multi-class labels <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19]</ref>. Durand et al. <ref type="bibr" target="#b4">[5]</ref> deal with multi-label learning based on partial labels and utilize GNN to predict missing labels. Wang et al. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b18">19]</ref> use multi-label classification to learn attribute feature. This paper utilizes multi-label classification to predict multi-class labels and focuses on learning identity feature for person ReID. To the best of our knowledge, this is an early work utilizing multi-label classification for unsupervised person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Given an unlabeled person image dataset X = {x 1 , x 2 , ..., x n }, our goal is to train a person ReID model on X . For any query person image q, the person ReID model is expected to produce a feature vector to retrieve image g containing the same person from a gallery set G. In other words, the ReID model should guarantee q share more similar feature with g than with other images in G. We could conceptually denote the goal of person ReID as,</p><formula xml:id="formula_0">g * = arg min g∈G dist(f g , f q ),</formula><p>(</p><p>where f ∈ R d is a d-dimensional L2-normalized feature vector extracted by the person ReID model. dist(·) is the distance metric, e.g., the L2 distance. To make training on X possible, we start by treating each image as an individual class and assign x i with a label y i . This pseudo label turns X into a labeled dataset, and allows for the ReID model training. y i is initialized to a two-valued vector, where only the value at index i is set to 1 and the others are set to −1, i.e.,</p><formula xml:id="formula_2">y i [j] = 1 j = i −1 j = i<label>(2)</label></formula><p>Since each person may have multiple images in X , the initial label vector is not valid in representing person identity cues. Label prediction is required to assign multi-class labels to each image, which can be used for ReID model training with a multi-label classification loss. Labels of x i can be predicted by referring its feature f i to features of other images, and find consistent feature groups. On the other hand, due to the huge number of image classes in X , it is hard to train a multi-label classifier. One efficient solution is to use the f i as the classifier for the i-th class. This computes the classification score for any image x j as,</p><formula xml:id="formula_3">c j [i] = f i × f j ,<label>(3)</label></formula><p>where c j denotes the multi-label classification score for x j . It is easy to infer that, both label prediction and multilabel classification require features of images in X . We hence introduce a n × d sized memory bank M to store those features, where M[i] = f i . With M, we propose the Memory-based Positive Label Prediction (MPLP) for label prediction and Memory-based Multi-label Classification Loss (MMCL) for ReID model training, respectively.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, MPLP takes a single-class label as input and outputs the multi-label predictionȳ i based on memory bank M, i.e.,</p><formula xml:id="formula_4">y i = MPLP(y i , M),<label>(4)</label></formula><p>where MPLP(·) denotes the MPLP module andȳ is the multi-class label. MMCL computes the multi-label classification loss by taking the image feature f , labelȳ, and the memory bank M as inputs. The computed loss L M M CL can be represented as,</p><formula xml:id="formula_5">L M M CL = n i=1 D(M × f i ,ȳ i ),<label>(5)</label></formula><p>where M × f i computes the classification score, and D(·) computes the loss by comparing classification scores and predicted labels. M is updated after each training iteration as,</p><formula xml:id="formula_6">M[i] t = α · f i + (1 − α) · M[i] t−1 ,<label>(6)</label></formula><p>where the superscript t denotes the t-th training epoch,</p><formula xml:id="formula_7">α is the updating rate. M[i] t is then L2-normalized by M[i] t ← ||M[i] t || 2 .</formula><p>It is easy to infer that, both MPLP and MMCL require robust features in M to seek reliable labels and classification scores, respectively. We use many data argumentation techniques to reinforce M. In other words, each M[i] combines features of different augmented samples form x i , hence it presents better robustness. More details are given in Sec. 4.2. MPLP considers both similarity and cycle consistency to predictȳ i , making it more accurate than the classification score. This makes the loss computed with Eq. (5) valid in boosting the ReID model, which in-turn produces positive feedbacks to M[i] and label prediction. This loop makes it possible to train discriminative ReID models on unlabeled dataset. Implementations to MPLP and MMCL can be found in the following parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory-based Positive Label Prediction</head><p>As shown in Eq. (4), given an initial two-valued label y i of image x i , MPLP aims to find other classes that x i may belong to. For x i , MPLP first computes a rank list R i according to the similarity between x i and other features, i.e., where s i,j denotes the similarity score of x i and x j . R i finds candidates for reliable labels for x i , e.g., labels at the top of rank list. However, variances of illumination, viewpoint, backgrounds, etc., would degrade the robustness of the rank list. E.g., noisy labels may appear at the top of rank list. To ensure the quality of predicted labels, MPLP refers to the similarity score and cycle consistency for label prediction.</p><formula xml:id="formula_8">R i = arg sort j (s i,j ), j ∈ [1, n],<label>(7)</label></formula><formula xml:id="formula_9">s i,j = M[i] × M[j],<label>(8)</label></formula><p>Label filtering by similarity score: We first select positive label candidates for x i based on its rank list. Inspired by <ref type="bibr" target="#b38">[39]</ref>, that uses a threshold to select relevant labels for query, we select candidate labels with a predefined similarity threshold. Given a similarity score threshold t, k i label candidates can be generated by removing labels with similarity smaller than t, i.e.,</p><formula xml:id="formula_10">P i = R i [1 : k i ],<label>(9)</label></formula><p>where R i [k i ] is the last label with similarity score higher than t, P i is the collection of label candidates for x i . t largely decides the quantity of label candidates. It will be tested in Sec. 4.3. Eq. (9) adaptively finds different numbers of candidates labels for different images, which is better than selecting fixed number of labels, i.e., the KNN in <ref type="figure" target="#fig_1">Fig. 2</ref>. We proceed to introduce cycle consistency to find positive labels from P i . Label filtering by cycle consistency: Inspired by kreciprocal nearest neighbor <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b43">44]</ref>, we assume that, if two images belong to the same class, their neighbor image sets should also be similar. In other words, two images should be mutual neighbor for each other if they can be assigned with similar labels. With this intuition, we propose a cycle consistency scheme to filter hard negative labels in P i . MPLP traverses labels in P i from head to tail. For a label j in P i , MPLP computes its top-k i nearest labels with Eq. <ref type="bibr" target="#b6">(7)</ref>. If label i is also one of the top-k i nearest labels of j, j is considered as a positive label for x i . Otherwise, it is treated as a hard negative label. The traverse is stopped when the first hard negative label is found. This leads to a positive label set P * i as well as a hard negative label for image x i . We denote the positive label set as,</p><formula xml:id="formula_11">P * i = P i [1 : l],<label>(10)</label></formula><p>where l satisfies i ∈ R Pi[l] [1 :</p><formula xml:id="formula_12">k i ] &amp; i / ∈ R Pi[l+1] [1 : k i ].</formula><p>As P * i contains l labels, x i would be assigned with a multiclass labelȳ i with l positive classes,</p><formula xml:id="formula_13">y i [j] = 1 j ∈ P * i −1 j / ∈ P * i<label>(11)</label></formula><p>As <ref type="figure" target="#fig_1">Fig. 2</ref> shows that, MPLP predicts accurate positive labels. Experimental evaluations will be presented in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Memory-based Multi-label Classification Loss</head><p>Traditional multi-label classification loss: The predicted multi-class labels are used for training the ReID model with a multi-label classification loss. In traditional multi-label classification methods, sigmoid and logistic regression loss is a common option <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19]</ref>. For a task with n classes, it adopts n independent binary classifiers for classification. The loss of classifying image x i to class j can be computed as,</p><formula xml:id="formula_14">(j|x i ) = log(1 + exp(−ȳ i [j] × M[j] × f i )),<label>(12)</label></formula><p>where M[j] × f i computes the classification score of image x i for the class j.ȳ i [j] is the label of image x i for class j. With the loss at a single class, we can obtain the Multi-Label Classification (MCL) loss, i.e., L M CL ,</p><formula xml:id="formula_15">L M CL = n i=1 n j=1 (j|x i ),<label>(13)</label></formula><p>where n is the number of images in the dataset X , which equals to the class number in our setting. Because the M[j] and f i are L2 normalized, the classification score is restricted between [−1, 1]. This limits the range of sigmoid function in Eq. (12), making the loss nonzero even for correct classifications. This issue can be addressed by introducing a scalar τ on the classification score. This updates Eq. (12) as,</p><formula xml:id="formula_16">τ (j|x i ) = log(1 + exp(−ȳ i [j] × M[j] × f i /τ )). (14)</formula><p>We denote the corresponding MCL loss as L M CL−τ . The gradient of L M CL−τ can be computed as,</p><formula xml:id="formula_17">∂L M CL−τ ∂f i = − exp(−ȳ i [j]M[j] f i /τ ) 1 + exp(−ȳ i [j]M[j] f i /τ )ȳ i [j]M[j] τ .<label>(15)</label></formula><p>vanishing gradient  <ref type="figure" target="#fig_2">Fig. 3</ref>. It is clear that, the updated MCL loss still suffers from substantial vanishing gradient issue as the classification score larger than 0.25 or smaller than -0.25.</p><p>Another issue with MCL loss is that, our task involves a large number of classes, making the positive and negative classes unbalanced. Treating those negative classes equally in Eq. (14) may cause a model collapse. We hence proceed to propose MMCL to address those issues.</p><p>Memory-based Multi-label Classification Loss: MMCL is proposed to address two issues in traditional MCL. For the first issue, since the score is bounded by [−1, 1], we can abandon the sigmoid function and directly compute the loss by regressing the classification score to 1 and -1. This simplifies the loss computation and improves the training efficiency. The loss of classifying image x i to class j can be updated as,</p><formula xml:id="formula_18">* (j|x i ) = ||M[j] × f i −ȳ i [j]|| 2 ,<label>(16)</label></formula><p>where f i is the feature of image x i .</p><p>The second problem is the imbalance between positive and negative classes. MMCL introduces hard negative class mining to solve it. This is inspired by the sample mining in deep metric learning <ref type="bibr" target="#b32">[33]</ref>, where hard negative samples are more informative for training. Similarly in our multilabel classification, the training should focus more on hard negative classes than easy negative classes. For x i , its negative classes can be denoted as R i \P * i . We rank them by their classification scores and select the top r% classes as the hard negative classes. The collection of hard negative classes for x i can be denoted as N i , |N i | = (n − |P * i |) · r%. The MMCL is computed on positive classes and sampled hard negative classes as follows,</p><formula xml:id="formula_19">L M M CL = n i=1 δ |P * i | p∈P * i * (p|x i )+ 1 |N i | s∈Ni * (s|x i )<label>(17)</label></formula><p>where δ is a coefficient measuring the importance of positive class loss and negative class loss, which will be tested in experiments.</p><p>We also illustrate the gradients of L M M CL similarly whenȳ i [j] = 1, in <ref type="figure" target="#fig_2">Fig. 3</ref>, where the gradient of L M M CL can be computed as, <ref type="figure" target="#fig_2">Fig. 3</ref> clearly shows that, the vanishing gradient issue is effectively addressed by MMCL. Because of vanishing gradient, L M CL−τ won't enforce the classifier to classify positive labels with large scores. This is harmful for decreasing the intra-class variance. Therefore, MMCL is more effective than MCL in optimizing the ReID model. <ref type="figure" target="#fig_2">Fig. 3</ref> also shows that, δ controls the magnitude of the gradient of MMCL. As discussed in <ref type="bibr" target="#b40">[41]</ref>, mean square loss is inferior to log-based loss (e.g. cross entropy) when classification score is near the decision boundary. δ effectively solves this issue by scaling the gradient magnitude of MMCL.</p><formula xml:id="formula_20">∂L M M CL /∂f i = 2δ(M[j] × f i −ȳ i [j])M[j]. (18)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussions: Comparison between MCL and MMCL in</head><p>By adopting the hard negative class mining strategy, MMCL not only works for multi-label classification, it also could be applied in single-label classification, where the unbalanced class issue still exists. Compared with cross entropy loss and MCL, MMCL abandons activation functions like softmax and sigmoid, leading to more efficient computation. As discussed in many works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref>, a huge number of classes degrades the speed of softmax computation. Existing solutions include hierarchical softmax <ref type="bibr" target="#b22">[23]</ref> and noisecontrastive estimation <ref type="bibr" target="#b7">[8]</ref>. As MMCL does not involve softmax computation, it does not suffer from such issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Transfer Learning with Labeled Dataset</head><p>Our method is also compatible with transfer learning setting. Given a dataset containing labeled person images, we can adopt the commonly used cross entropy loss and triplet loss on labeled data to train the model. The training loss can be denoted as L labeled . The overall training loss for transfer learning can be represented as the sum of MMCL and loss on labeled dataset, i.e.,</p><formula xml:id="formula_21">L transf er = L labeled + L M M CL .<label>(19)</label></formula><p>The performance of our methods on transfer learning will be tested in the next section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Metrics</head><p>Market-1501 <ref type="bibr" target="#b41">[42]</ref> contains 32,668 labeled person images of 1,501 identities collected from 6 non-overlapping camera views. DukeMTMC-reID <ref type="bibr" target="#b24">[25]</ref> has 8 cameras and 36,411 labeled images of 1,404 identities. MSMT17 <ref type="bibr" target="#b30">[31]</ref> is a newly released person ReID dataset. It is composed of 126,411 person images from 4,101 identities collected by 15 cameras. The dataset suffers from substantial variations of scene and lighting, and is more challenging than the other two datasets. All three datasets are collected under similar scenario, i.e., campus, which makes the transfer learning possible. We follow the standard settings <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref> on them to conduct experiments. Performance is evaluated by the Cumulative Matching Characteristic (CMC) and mean Average Precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>All experiments are implemented on PyTorch. We use ResNet-50 <ref type="bibr" target="#b8">[9]</ref> as backbone to extract the feature and initialize it with parameters pre-trained on ImageNet <ref type="bibr" target="#b1">[2]</ref>. After pooling-5 layer, we remove subsequent layers and add a batch normalization layer <ref type="bibr" target="#b9">[10]</ref>, which produces a 2048-dim feature. During testing, we also extract the pooling-5 feature to calculate the distance. For multi-label classification, we allocate a memory bank to store L2 normalized image features. The memory bank is initialized to all zeros, and we start using MPLP for label prediction when the memory is fully updated 5 times (after 5 epochs). As mentioned in section 3.2, we leverage CamStyle <ref type="bibr" target="#b46">[47]</ref> as a data augmentation strategy for unlabeled images. Strategies like random crop, random rotation, color jitter, and random erasing are also introduced to improve the feature robustness.</p><p>The input image is resized to 256*128. We use SGD to optimize the model, the learning rate for ResNet-50 base layers are 0.01, and others are 0.1. The memory updating rate α starts from 0 and grows linearly to 0.5. We train the model for 60 epochs, and the learning rate is divided by 10 after every 40 epochs. The batch size for model training is 128. We fix the similarity threshold t in MPLP as 0.6. In MMCL, the weight δ is fixed to 5 and we select the 1% top-ranked negative classes to compute the loss through the parameter analysis in Sec. 4.3. For transfer learning with labeled dataset, we apply the same batch size on the labeled dataset. A fully connected layer is added after batch normalization layer for classification. We optimize the L transf er in section 3.4 following the same baseline training strategy as described in <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter Analysis</head><p>This section aims to investigate some important hyperparameters in our method, including the similarity score threshold t in MPLP, coefficient δ, and hard negative mining ratio r% in MMCL. Each experiment varies the value of one hyper-parameter while keeping others fixed. All experiments are conducted with unsupervised ReID setting on both Market-1501 and DukeMTMC-reID.</p><p>Similarity threshold t: <ref type="figure" target="#fig_4">Fig.4</ref> investigates the effect of similarity threshold t in MPLP. We vary t from 0.3 to 0.7 and test the model performance. A low similarity score t will harm the model performance. For example, when t is in range [0.3, 0.5], a substantial performance drop can be observed compared with larger t. This is because that, low similarity threshold introduces many negative labels. More accurate labels can be selected as t becomes larger. However, too large t decreases the number of selected labels. The best t is 0.6 for both Market-1501 and DukeMTMC-reID. We hence set t = 0.6.</p><p>Coefficient δ: <ref type="table" target="#tab_0">Table 1</ref> reports the analysis on coefficient δ of MMCL. As discussed in Sec. 3.3, δ plays a role to scale the gradient of MMCL. δ = 1 means that we do not scale the gradient. In this case, the MMCL cannot produce large gradients to pull positive samples together, leading to bad performance. For example, the rank-1 accuracy is dropped to 59.3% on Market-1501 and 52.6% on DukeMTMC-reID. As δ becomes larger, MMCL effectively improves the similarity of positive samples, leading to better performance. However, too large δ may make the training unstable. According to <ref type="table" target="#tab_0">Table 1</ref>, we set δ = 5.</p><p>Hard negative mining ratio r%: <ref type="figure" target="#fig_5">Fig. 5</ref> shows effects of hard negative mining ratio r% in network training. r = 100 means using all negative classes for loss computation. It is clear that, r = 100 is harmful for the performance. This implies that, not all of the negative classes are helpful for unsupervised ReID training. As r becomes smaller, hard negative mining would be activated and it boosts the performance. Too small r selects too few negative classes, hence   is also harmful for the performance. According to <ref type="figure" target="#fig_5">Fig. 5</ref>, r = 1 is used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>This part evaluates the effectiveness of MPLP and MMCL by making comparison with supervised learning, MMCL+single-class label, and MMCL+MPLP. Experimental results are reported in <ref type="table" target="#tab_2">Table 2</ref>, where the performance of ImageNet pre-trained model is also reported as the baseline. As shown in the table, the supervised learning achieves high accuracy, e.g., 87.1% in rank-1 accuracy and 68.3% in mAP on Market-1501. ImageNet pretrained model performs badly on both Market-1501 and DukeMTMC-reID. MMCL with single-class pseudo labels boosts the baseline performance, indicating the validity of leveraging unlabeled dataset in training. <ref type="table" target="#tab_2">Table 2</ref> also shows that, combining MMCL with MPLP significantly boosts the performance, e.g., from baseline 7.8% to 80.3% in rank-1 accuracy on Market-1501. <ref type="table" target="#tab_2">Table 2</ref> also shows that, Cam-Style <ref type="bibr" target="#b46">[47]</ref> boosts the performance, indicating the importance of data augmentation as discussed in Sec. 3.1.</p><p>Effectiveness of MPLP: To verify that MPLP is a reasonably good solution for label prediction, we compare MPLP against several other label prediction methods, e.g., the KNN search and selection by Similarity Score (SS). Table 3 (a) summarizes the results. From <ref type="table" target="#tab_3">Table 3</ref> (a), we can observe that KNN (K=8, which achieves the best peformance) achieves 73.3% rank-1 accuracy and 35.4% mAP on Market-1501. Selecting positive labels by Similarity Score (SS) gains improvements over KNN. This indicates that, adaptively select positive labels with similarity threshold is more reasonable than fixing the positive label number for different images.  ing cycle consistency and similarity score effectively ensure the quality of predicted labels. Visualization of predicted labels by MPLP can be found in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Effectiveness of MMCL: To test the validity of MMCL, this part proceeds to compare it against Cross Entropy (CE) loss with different training settings. Experimental results are summarized in <ref type="table" target="#tab_3">Table 3</ref> (b). We first test MPLP and CE using single-class labels for model learning. MMCL gets 49.0% rank-1 accuracy and 17.8% mAP on Market-1501, substantially better than CE. We further test MMCL and CE using ground truth labels for learning. Note that, we modify CE according to <ref type="bibr" target="#b45">[46]</ref> to make it applicable in multi-class learning. With ground truth labels, MMCL still performs better than CE, especially on DukeMTMC-reID. We finally test MMCL and CE with labels predicted by MPLP. MMCL still outperforms CE by large margins. Note that, MMCL uses non-parameters classifiers for training. It still achieves comparable performance with the supervised learning in <ref type="table" target="#tab_2">Table 2</ref>. <ref type="table" target="#tab_3">Table 3</ref> (b) thus demonstrates the effectiveness of MMCL and the training paradigm of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the State of the Art</head><p>We compare our method against state-of-the-art unsupervised learning and transfer learning approaches on Market-1501 <ref type="bibr" target="#b41">[42]</ref>, DukeMTMC-reID <ref type="bibr" target="#b24">[25]</ref> and MSMT17 <ref type="bibr" target="#b30">[31]</ref>. <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table">Table 5</ref> summarize the comparison. <ref type="table" target="#tab_5">Table 4</ref> reports comparisons on Market-1501 and DukeMTMC-reID. We compare two types of methods, including unsupervised learning methods: LOMO <ref type="bibr" target="#b17">[18]</ref>, BOW <ref type="bibr" target="#b41">[42]</ref>, BUC <ref type="bibr" target="#b19">[20]</ref> and DBC <ref type="bibr" target="#b3">[4]</ref>, and transfer learning based approaches: PUL <ref type="bibr" target="#b5">[6]</ref>, PTGAN <ref type="bibr" target="#b30">[31]</ref>, SPGAN <ref type="bibr" target="#b2">[3]</ref>, CAMEL <ref type="bibr" target="#b36">[37]</ref>, MMFA <ref type="bibr" target="#b18">[19]</ref>, TJ-AIDL <ref type="bibr" target="#b29">[30]</ref>, HHL <ref type="bibr" target="#b44">[45]</ref>, ECN <ref type="bibr" target="#b45">[46]</ref>, MAR <ref type="bibr" target="#b37">[38]</ref>, PAUL <ref type="bibr" target="#b35">[36]</ref>, SSG <ref type="bibr" target="#b6">[7]</ref>, CR-GAN <ref type="bibr" target="#b0">[1]</ref>, CASCL <ref type="bibr" target="#b31">[32]</ref>, PDA-Net <ref type="bibr" target="#b16">[17]</ref>, UCDA <ref type="bibr" target="#b23">[24]</ref> and PAST <ref type="bibr" target="#b39">[40]</ref>.</p><p>We first compare with unsupervised learning methods. LOMO and BOW utilize hand-crafted features, which show lower performance. BUC and DBC treat each image as a single cluster then merges clusters, thus share certain similarity to our work. However, our method outperforms them by large margins. The reasons could be because: 1) BUC  <ref type="table">Table 5</ref>. Comparison with state-of-the-art methods on MSMT17.</p><p>tries to keep different clusters with similar size, hence could suffer from the issue of imbalanced number of positive labels. MPLP could alleviate this issue by adaptively selecting positive labels for different images. 2) As discussed in ablation studies, MMCL performs better than the cross entropy loss when using memory bank as classifier. Therefore, MPLP and MMCL effectively boost our performance. Under the transfer learning setting, our method achieves the best performance on Market-1501 in <ref type="table" target="#tab_5">Table 4</ref>. For example, our rank-1 accuracy of on Market-1501 achieves 84.4%, when using DukeMTMC-reID as the source dataset. Similarly, we get 72.4% rank-1 accuracy on DukeMTMC-reID using Market-1501 as the source dataset. Although SSG and PAST achieve slightly better performance, our method is more flexible and can be used without labeled dataset. It is also interesting to observe that, with unsupervised learning setting, our method still outperforms several recent transfer learning methods, e.g., our rank-1 accuracy of 80.3% on Market-1501 vs. 78.38% and 64.3% of PAST <ref type="bibr" target="#b39">[40]</ref> and UCDA <ref type="bibr" target="#b23">[24]</ref>.</p><p>We also conduct experiments on MSMT17, a larger and more challenging dataset. A limited number of works report performance on MSMT17, i.e., PTGAN <ref type="bibr" target="#b30">[31]</ref>, ECN <ref type="bibr" target="#b45">[46]</ref>, and SSG <ref type="bibr" target="#b6">[7]</ref>. As table 5 shows, our approach outperforms existing methods by large margins under both unsupervised and transfer learning settings. For example, our method achieves 35.4% and 43.6%/40.8% rank-1 accuracy respectively. This outperforms SSG by 11.4% in rank-1 accuracy. The above experiments on three datasets demonstrate the promising performance of our MPLP and MMCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a multi-label classification method to address unsupervised person ReID. Different from previous works, our method works without requiring any labeled data or a good pre-trained model. This is achieved by iteratively predicting multi-class labels and updating the network with a multi-label classification loss. MPLP is proposed for multi-class label prediction by considering both visual similarity and cycle consistency. MMCL is introduced to compute the multi-label classification loss and address the vanishing gradient issue. Experiments on several large-scale datasets demonstrate the effectiveness of the proposed methods in unsupervised person ReID.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1. Illustrations of multi-label classification for unsupervised person ReID. We target to assign each unlabeled person image with a multi-class label reflecting the person identity. This is achieved by iteratively running MPLP for prediction and MMCL for multi-label classification loss computation. This procedure guides CNN to produce discriminative features for ReID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the label prediction by MPLP. (a) reports the precision and recall of MPLP in finding true positive labels, where MPLP consistently outperforms KNN at different training stages. (b) shows positive labels and negative labels selected by MPLP, where MPLP effectively rejects hard negative labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Gradient Analysis for MCL-τ and MMCL. It is clear that, MMCL does not suffer from the vanishing gradient issue. With Eq. (15), we illustrate the gradient of L M CL−τ with different values of τ whenȳ i [j] = 1 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Evaluation of similarity score t in MPLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Evaluation of hard negative mining ratio r in MMCL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of parameter δ in Eq. (17).</figDesc><table><row><cell>δ</cell><cell cols="2">Market-1501 Rank-1 mAP</cell><cell cols="2">DukeMTMC-reID Rank-1 mAP</cell></row><row><cell>1</cell><cell>59.3</cell><cell>19.4</cell><cell>52.6</cell><cell>22.8</cell></row><row><cell>2</cell><cell>71.3</cell><cell>31.1</cell><cell>58.8</cell><cell>31.0</cell></row><row><cell>3</cell><cell>76.6</cell><cell>40.0</cell><cell>62.6</cell><cell>35.6</cell></row><row><cell>4</cell><cell>79.9</cell><cell>44.9</cell><cell>64.9</cell><cell>39.1</cell></row><row><cell>5</cell><cell>80.3</cell><cell>45.5</cell><cell>65.2</cell><cell>40.2</cell></row><row><cell>6</cell><cell>78.1</cell><cell>45.0</cell><cell>65.0</cell><cell>39.8</cell></row><row><cell>7</cell><cell>73.2</cell><cell>41.3</cell><cell>64.1</cell><cell>39.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Test of validity of MMCL and MPLP. † denotes not using CamStyle<ref type="bibr" target="#b46">[47]</ref> for data augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>(a) also shows that, MPLP</cell></row><row><cell>achieves the best performance. This indicates that combin-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on different label prediction algorithms and loss functions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Unsupervised person re-ID performance comparison with state-of-the-art methods on Market-1501 and DukeMTMC-reID.</figDesc><table><row><cell>Method</cell><cell cols="2">Reference</cell><cell cols="2">Source</cell><cell>Rank-1</cell><cell cols="2">Market-1501 Rank-5</cell><cell>Rank-10</cell><cell>mAP</cell><cell>Source</cell><cell cols="2">DukeMTMC-reID Rank-1 Rank-5 Rank-10</cell><cell>mAP</cell></row><row><cell>LOMO[18]</cell><cell cols="2">CVPR15</cell><cell></cell><cell>None</cell><cell>27.2</cell><cell cols="2">41.6</cell><cell>49.1</cell><cell>8.0</cell><cell>None</cell><cell>12.3</cell><cell>21.3</cell><cell>26.6</cell><cell>4.8</cell></row><row><cell>BOW[42]</cell><cell cols="2">ICCV15</cell><cell></cell><cell>None</cell><cell>35.8</cell><cell cols="2">52.4</cell><cell>60.3</cell><cell>14.8</cell><cell>None</cell><cell>17.1</cell><cell>28.8</cell><cell>34.9</cell><cell>8.3</cell></row><row><cell>BUC[20]</cell><cell cols="2">AAAI19</cell><cell></cell><cell>None</cell><cell>66.2</cell><cell cols="2">79.6</cell><cell>84.5</cell><cell>38.3</cell><cell>None</cell><cell>47.4</cell><cell>62.6</cell><cell>68.4</cell><cell>27.5</cell></row><row><cell>DBC[4]</cell><cell cols="2">BMVC19</cell><cell></cell><cell>None</cell><cell>69.2</cell><cell cols="2">83.0</cell><cell>87.8</cell><cell>41.3</cell><cell>None</cell><cell>51.5</cell><cell>64.6</cell><cell>70.1</cell><cell>30.0</cell></row><row><cell>Ours</cell><cell cols="2">This paper</cell><cell></cell><cell>None</cell><cell>80.3</cell><cell cols="2">89.4</cell><cell>92.3</cell><cell>45.5</cell><cell>None</cell><cell>65.2</cell><cell>75.9</cell><cell>80.0</cell><cell>40.2</cell></row><row><cell>PUL[6]</cell><cell cols="2">TOMM18</cell><cell></cell><cell>Duke</cell><cell>45.5</cell><cell cols="2">60.7</cell><cell>66.7</cell><cell>20.5</cell><cell>Market</cell><cell>30.0</cell><cell>43.4</cell><cell>48.5</cell><cell>16.4</cell></row><row><cell>PTGAN[31]</cell><cell cols="2">CVPR18</cell><cell></cell><cell>Duke</cell><cell>38.6</cell><cell>-</cell><cell></cell><cell>66.1</cell><cell>-</cell><cell>Market</cell><cell>27.4</cell><cell>-</cell><cell>50.7</cell><cell>-</cell></row><row><cell>SPGAN[3]</cell><cell cols="2">CVPR18</cell><cell></cell><cell>Duke</cell><cell>51.5</cell><cell cols="2">70.1</cell><cell>76.8</cell><cell>22.8</cell><cell>Market</cell><cell>41.1</cell><cell>56.6</cell><cell>63.0</cell><cell>22.3</cell></row><row><cell>CAMEL[37]</cell><cell cols="2">ICCV17</cell><cell></cell><cell>Multi</cell><cell>54.5</cell><cell>-</cell><cell></cell><cell>-</cell><cell>26.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMFA[19]</cell><cell cols="2">BMVC19</cell><cell></cell><cell>Duke</cell><cell>56.7</cell><cell cols="2">75.0</cell><cell>81.8</cell><cell>27.4</cell><cell>Market</cell><cell>45.3</cell><cell>59.8</cell><cell>66.3</cell><cell>24.7</cell></row><row><cell>TJ-AIDL[30]</cell><cell cols="2">CVPR18</cell><cell></cell><cell>Duke</cell><cell>58.2</cell><cell cols="2">74.8</cell><cell>81.1</cell><cell>26.5</cell><cell>Market</cell><cell>44.3</cell><cell>59.6</cell><cell>65.0</cell><cell>23.0</cell></row><row><cell>HHL[45]</cell><cell cols="2">ECCV18</cell><cell></cell><cell>Duke</cell><cell>62.2</cell><cell cols="2">78.8</cell><cell>84.0</cell><cell>31.4</cell><cell>Market</cell><cell>46.9</cell><cell>61.0</cell><cell>66.7</cell><cell>27.2</cell></row><row><cell>ECN[46]</cell><cell cols="2">CVPR19</cell><cell></cell><cell>Duke</cell><cell>75.1</cell><cell cols="2">87.6</cell><cell>91.6</cell><cell>43.0</cell><cell>Market</cell><cell>63.3</cell><cell>75.8</cell><cell>80.4</cell><cell>40.4</cell></row><row><cell>MAR[38]</cell><cell cols="2">CVPR19</cell><cell cols="2">MSMT</cell><cell>67.7</cell><cell cols="2">81.9</cell><cell>-</cell><cell>40.0</cell><cell>MSMT</cell><cell>67.1</cell><cell>79.8</cell><cell>-</cell><cell>48.0</cell></row><row><cell>PAUL[36]</cell><cell cols="2">CVPR19</cell><cell cols="2">MSMT</cell><cell>68.5</cell><cell cols="2">82.4</cell><cell>87.4</cell><cell>40.1</cell><cell>MSMT</cell><cell>72.0</cell><cell>82.7</cell><cell>86.0</cell><cell>53.2</cell></row><row><cell>SSG[7]</cell><cell cols="2">ICCV19</cell><cell></cell><cell>Duke</cell><cell>80.0</cell><cell cols="2">90.0</cell><cell>92.4</cell><cell>58.3</cell><cell>Market</cell><cell>73.0</cell><cell>80.6</cell><cell>83.2</cell><cell>53.4</cell></row><row><cell>CR-GAN[1]</cell><cell cols="2">ICCV19</cell><cell></cell><cell>Duke</cell><cell>77.7</cell><cell cols="2">89.7</cell><cell>92.7</cell><cell>54.0</cell><cell>Market</cell><cell>68.9</cell><cell>80.2</cell><cell>84.7</cell><cell>48.6</cell></row><row><cell>CASCL[32]</cell><cell cols="2">ICCV19</cell><cell cols="2">MSMT</cell><cell>65.4</cell><cell cols="2">80.6</cell><cell>86.2</cell><cell>35.5</cell><cell>MSMT</cell><cell>59.3</cell><cell>73.2</cell><cell>77.5</cell><cell>37.8</cell></row><row><cell>PDA-Net[17]</cell><cell cols="2">ICCV19</cell><cell></cell><cell>Duke</cell><cell>75.2</cell><cell cols="2">86.3</cell><cell>90.2</cell><cell>47.6</cell><cell>Market</cell><cell>63.2</cell><cell>77.0</cell><cell>82.5</cell><cell>45.1</cell></row><row><cell>UCDA[24]</cell><cell cols="2">ICCV19</cell><cell></cell><cell>Duke</cell><cell>64.3</cell><cell>-</cell><cell></cell><cell>-</cell><cell>34.5</cell><cell>Market</cell><cell>55.4</cell><cell>-</cell><cell>-</cell><cell>36.7</cell></row><row><cell>PAST[40]</cell><cell cols="2">ICCV19</cell><cell></cell><cell>Duke</cell><cell>78.38</cell><cell>-</cell><cell></cell><cell>-</cell><cell>54.62</cell><cell>Market</cell><cell>72.35</cell><cell>-</cell><cell>-</cell><cell>54.26</cell></row><row><cell>Ours (transfer)</cell><cell cols="2">This paper</cell><cell></cell><cell>Duke</cell><cell>84.4</cell><cell cols="2">92.8</cell><cell>95.0</cell><cell>60.4</cell><cell>Market</cell><cell>72.4</cell><cell>82.9</cell><cell>85.0</cell><cell>51.4</cell></row><row><cell>Method</cell><cell>Source</cell><cell cols="2">Rank-1</cell><cell cols="3">MSMT17 Rank-5 Rank-10</cell><cell>mAP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>None</cell><cell>35.4</cell><cell></cell><cell>44.8</cell><cell cols="2">49.8</cell><cell>11.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PTGAN [31]</cell><cell>Market</cell><cell>10.2</cell><cell></cell><cell>-</cell><cell cols="2">24.4</cell><cell>2.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECN [46]</cell><cell>Market</cell><cell>25.3</cell><cell></cell><cell>36.3</cell><cell cols="2">42.1</cell><cell>8.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSG [7]</cell><cell>Market</cell><cell>31.6</cell><cell></cell><cell>-</cell><cell cols="2">49.6</cell><cell>13.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (transfer)</cell><cell>Market</cell><cell>40.8</cell><cell></cell><cell>51.8</cell><cell cols="2">56.7</cell><cell>15.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PTGAN [31]</cell><cell>Duke</cell><cell>11.8</cell><cell></cell><cell>-</cell><cell cols="2">27.4</cell><cell>3.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECN [46]</cell><cell>Duke</cell><cell>30.2</cell><cell></cell><cell>41.5</cell><cell cols="2">46.8</cell><cell>10.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSG [7]</cell><cell>Duke</cell><cell>32.2</cell><cell></cell><cell>-</cell><cell cols="2">51.2</cell><cell>13.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (transfer)</cell><cell>Duke</cell><cell>43.6</cell><cell></cell><cell>54.3</cell><cell cols="2">58.9</cell><cell>16.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Instanceguided context rendering for cross-domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Qingze Yin, and Zhenmin Tang. Dispersion based clustering for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a deep convnet for multi-label classification with partial labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Mehrasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mining on manifolds: Metric learning without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Yannis Avrithis, and Ondřej Chum</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A contextual dissimilarity measure for accurate and efficient image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of view-invariant action representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pose-guided representation learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-dataset person re-identification via unsupervised pose disentanglement and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ci-Siang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Bo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Tsun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Chichung</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel unsupervised camera-aware domain adaptation framework for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task learning with low rank attribute embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attributes driven tracklet-to-tracklet person re-identification using latent prototypes space mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by camera-aware similarity consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Patch-based discriminative feature learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qize</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Crossview asymmetric metric learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
