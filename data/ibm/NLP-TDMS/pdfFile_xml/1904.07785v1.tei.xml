<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH WAVELET NEURAL NETWORK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Xu</surname></persName>
							<email>xubingbing@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer and Control Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
							<email>shenhuawei@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer and Control Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cao</surname></persName>
							<email>caoqi@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer and Control Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
							<email>qiuyunqi@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer and Control Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer and Control Engineering</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPH WAVELET NEURAL NETWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN), leveraging graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b15">(LeCun et al., 1998)</ref> have been successfully used in many machine learning problems, such as image classification <ref type="bibr" target="#b10">(He et al., 2016)</ref> and speech recognition <ref type="bibr" target="#b11">(Hinton et al., 2012)</ref>, where there is an underlying Euclidean structure. The success of CNNs lies in their ability to leverage the statistical properties of Euclidean data, e.g., translation invariance. However, in many research areas, data are naturally located in a non-Euclidean space, with graph or network being one typical case. The non-Euclidean nature of graph is the main obstacle or challenge when we attempt to generalize CNNs to graph. For example, convolution is not well defined in graph, due to that the size of neighborhood for each node varies dramatically .</p><p>Existing methods attempting to generalize CNNs to graph data fall into two categories, spatial methods and spectral methods, according to the way that convolution is defined. Spatial methods define convolution directly on the vertex domain, following the practice of the conventional CNN. For each vertex, convolution is defined as a weighted average function over all vertices located in its neighborhood, with the weighting function characterizing the influence exerting to the target vertex by its neighbors . The main challenge is to define a convolution operator that can handle neighborhood with different sizes and maintain the weight sharing property of CNN. Although spatial methods gain some initial success and offer us a flexible framework to generalize CNNs to graph, it is still elusive to determine appropriate neighborhood.</p><p>Spectral methods define convolution via graph Fourier transform and convolution theorem. Spectral methods leverage graph Fourier transform to convert signals defined in vertex domain into spectral domain, e.g., the space spanned by the eigenvectors of the graph Laplacian matrix, and then filter is defined in spectral domain, maintaining the weight sharing property of CNN. As the pioneering work of spectral methods, spectral CNN <ref type="bibr" target="#b3">(Bruna et al., 2014)</ref> exploited graph data with the graph Fourier transform to implement convolution operator using convolution theorem. Some subsequent works make spectral methods spectrum-free <ref type="bibr" target="#b4">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b14">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b12">Khasanova &amp; Frossard, 2017)</ref>, achieving locality in spatial domain and avoiding high computational cost of the eigendecomposition of Laplacian matrix.</p><p>In this paper, we present graph wavelet neural network to implement efficient convolution on graph data. We take graph wavelets instead of the eigenvectors of graph Laplacian as a set of bases, and define the convolution operator via wavelet transform and convolution theorem. Graph wavelet neural network distinguishes itself from spectral CNN by its three desirable properties: (1) Graph wavelets can be obtained via a fast algorithm without requiring the eigendecomposition of Laplacian matrix, and thus is efficient; (2) Graph wavelets are sparse, while eigenvectors of Laplacian matrix are dense. As a result, graph wavelet transform is much more efficient than graph Fourier transform;</p><p>(3) Graph wavelets are localized in vertex domain, reflecting the information diffusion centered at each node <ref type="bibr" target="#b27">(Tremblay &amp; Borgnat, 2014)</ref>. This property eases the understanding of graph convolution defined by graph wavelets.</p><p>We develop an efficient implementation of the proposed graph wavelet neural network. Convolution in conventional CNN learns an individual convolution kernel for each pair of input feature and output feature, causing a huge number of parameters especially when the number of features is high. We detach the feature transformation from convolution and learn a sole convolution kernel among all features, substantially reducing the number of parameters. Finally, we validate the effectiveness of the proposed graph wavelet neural network by applying it to graph-based semi-supervised classification. Experimental results demonstrate that our method consistently outperforms previous spectral CNNs on three benchmark datasets, i.e., Cora, Citeseer, and Pubmed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OUR METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PRELIMINARY</head><p>Let G = {V, E, A} be an undirected graph, where V is the set of nodes with |V| = n, E is the set of edges, and A is adjacency matrix with A i,j = A j,i to define the connection between node i and node j. The graph Laplacian matrix L is defined as L = D −A where D is a diagonal degree matrix with D i,i = j A i,j , and the normalized Laplacian matrix is L = I n − D −1/2 AD −1/2 where I n is the identity matrix. Since L is a real symmetric matrix, it has a complete set of orthonormal eigenvectors U = (u 1 , u 2 , ..., u n ), known as Laplacian eigenvectors. These eigenvectors have associated real, non-negative eigenvalues {λ l } n l=1 , identified as the frequencies of graph. Eigenvectors associated with smaller eigenvalues carry slow varying signals, indicating that connected nodes share similar values. In contrast, eigenvectors associated with larger eigenvalues carry faster varying signals across connected nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GRAPH FOURIER TRANSFORM</head><p>Taking the eigenvectors of normalized Laplacian matrix as a set of bases, graph Fourier transform of a signal x ∈ R n on graph G is defined asx = U x, and the inverse graph Fourier transform is x = Ux (Shuman et al., 2013). Graph Fourier transform, according to convolution theorem, offers us a way to define the graph convolution operator, denoted as * G . Denoting with y the convolution kernel, * G is defined as</p><formula xml:id="formula_0">x * G y = U (U y) (U x) ,<label>(1)</label></formula><p>where is the element-wise Hadamard product. Replacing the vector U y by a diagonal matrix g θ , then Hadamard product can be written in the form of matrix multiplication. Filtering the signal x by the filter g θ , we can write Equation (1) as U g θ U x.</p><p>However, there are some limitations when using Fourier transform to implement graph convolution:</p><p>(1) Eigendecomposition of Laplacian matrix to obtain Fourier basis U is of high computational cost with O(n 3 ); (2) Graph Fourier transform is inefficient, since it involves the multiplication between a dense matrix U and the signal x;</p><p>(3) Graph convolution defined through Fourier transform is not localized in vertex domain, i.e., the influence to the signal on one node is not localized in its neighborhood. To address these limitations, ChebyNet <ref type="bibr" target="#b4">(Defferrard et al., 2016)</ref> restricts convolution kernel g θ to a polynomial expansion</p><formula xml:id="formula_1">g θ = K−1 k=0 θ k Λ k ,<label>(2)</label></formula><p>where K is a hyper-parameter to determine the range of node neighborhoods via the shortest path distance, θ ∈ R K is a vector of polynomial coefficients, and Λ =diag({λ l } n l=1 ). However, such a polynomial approximation limits the flexibility to define appropriate convolution on graph, i.e., with a smaller K, it's hard to approximate the diagonal matrix g θ with n free parameters. While with a larger K, locality is no longer guaranteed. Different from ChebyNet, we address the aforementioned three limitations through replacing graph Fourier transform with graph wavelet transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GRAPH WAVELET TRANSFORM</head><p>Similar to graph Fourier transform, graph wavelet transform projects graph signal from vertex domain into spectral domain. Graph wavelet transform employs a set of wavelets as bases, defined as ψ s = (ψ s1 , ψ s2 , ..., ψ sn ), where each wavelet ψ si corresponds to a signal on graph diffused away from node i and s is a scaling parameter. Mathematically, ψ si can be written as</p><formula xml:id="formula_2">ψ s = U G s U ,<label>(3)</label></formula><p>where U is Laplacian eigenvectors, G s =diag g(sλ 1 ), ..., g(sλ n ) is a scaling matrix and g(sλ i ) = e λis .</p><p>Using graph wavelets as bases, graph wavelet transform of a signal x on graph is defined asx = ψ −1 s x and the inverse graph wavelet transform is x = ψ sx . Note that ψ −1 s can be obtained by simply replacing the g(sλ i ) in ψ s with g(−sλ i ) corresponding to a heat kernel <ref type="bibr" target="#b6">(Donnat et al., 2018)</ref>. Replacing the graph Fourier transform in Equation (1) with graph wavelet transform, we obtain the graph convolution as</p><formula xml:id="formula_3">x * G y = ψ s ((ψ −1 s y) (ψ −1 s x)).<label>(4)</label></formula><p>Compared to graph Fourier transform, graph wavelet transform has the following benefits when being used to define graph convolution:</p><p>1. High efficiency: graph wavelets can be obtained via a fast algorithm without requiring the eigendecomposition of Laplacian matrix. In <ref type="bibr" target="#b9">Hammond et al. (2011)</ref>, a method is proposed to use Chebyshev polynomials to efficiently approximate ψ s and ψ −1 s , with the computational complexity O(m × |E|), where |E| is the number of edges and m is the order of Chebyshev polynomials.</p><p>2. High spareness: the matrix ψ s and ψ −1 s are both sparse for real world networks, given that these networks are usually sparse. Therefore, graph wavelet transform is much more computationally efficient than graph Fourier transform. For example, in the Cora dataset, more than 97% elements in ψ −1 s are zero while only less than 1% elements in U are zero <ref type="table" target="#tab_3">(Table 4</ref>). 3. Localized convolution: each wavelet corresponds to a signal on graph diffused away from a centered node, highly localized in vertex domain. As a result, the graph convolution defined in Equation <ref type="formula" target="#formula_3">(4)</ref> is localized in vertex domain. We show the localization property of graph convolution in Appendix A. It is the localization property that explains why graph wavelet transform outperforms Fourier transform in defining graph convolution and the associated tasks like graph-based semisupervised learning. 4. Flexible neighborhood: graph wavelets are more flexible to adjust node's neighborhoods. Different from previous methods which constrain neighborhoods by the discrete shortest path distance, our method leverages a continuous manner, i.e., varying the scaling parameter s. A small value of s generally corresponds to a smaller neighborhood. <ref type="figure" target="#fig_0">Figure 1</ref> shows two wavelet bases at different scale on an example network, depicted using GSP toolbox <ref type="bibr" target="#b22">(Perraudin et al., 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">GRAPH WAVELET NEURAL NETWORK</head><p>Replacing Fourier transform with wavelet transform, graph wavelet neural network (GWNN) is a multi-layer convolutional neural network. The structure of the m-th layer is</p><formula xml:id="formula_4">X m+1 [:,j] = h(ψ s p i=1 F m i,j ψ −1 s X m [:,i] ) j = 1, · · · , q,<label>(5)</label></formula><p>where ψ s is wavelet bases, ψ −1 s is the graph wavelet transform matrix at scale s which projects signal in vertex domain into spectral domain, X m [:,i] with dimensions n × 1 is the i-th column of X m , F m i,j is a diagonal filter matrix learned in spectral domain, and h is a non-linear activation function. This layer transforms an input tensor X m with dimensions n × p into an output tensor X m+1 with dimensions n × q.</p><p>In this paper, we consider a two-layer GWNN for semi-supervised node classification on graph. The formulation of our model is</p><formula xml:id="formula_5">first layer : X 2 [:,j] = ReLU(ψ s p i=1 F 1 i,j ψ −1 s X 1 [:,i] ) j = 1, · · · , q,<label>(6)</label></formula><p>second layer :</p><formula xml:id="formula_6">Z j = softmax(ψ s q i=1 F 2 i,j ψ −1 s X 2 [:,,i] ) j = 1, · · · , c,<label>(7)</label></formula><p>where c is the number of classes in node classification, Z of dimensions n × c is the prediction result. The loss function is the cross-entropy error over all labeled examples:</p><formula xml:id="formula_7">Loss = − l∈y L c i=1 Y li lnZ li ,<label>(8)</label></formula><p>where y L is the labeled node set, Y li = 1 if the label of node l is i, and Y li = 0 otherwise. The weights F are trained using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">REDUCING PARAMETER COMPLEXITY</head><p>In Equation <ref type="formula" target="#formula_4">(5)</ref>, the parameter complexity of each layer is O(n × p × q), where n is the number of nodes, p is the number of features of each vertex in current layer, and q is the number of features of each vertex in next layer. Conventional CNN methods learn convolution kernel for each pair of input feature and output feature. This results in a huge number of parameters and generally requires huge training data for parameter learning. This is prohibited for graph-based semi-supervised learning.</p><p>To combat this issue, we detach the feature transformation from graph convolution. Each layer in GWNN is divided into two components: feature transformation and graph convolution. Spectially, we have feature transformation :</p><formula xml:id="formula_8">X m = X m W ,<label>(9)</label></formula><p>graph convolution :</p><formula xml:id="formula_9">X m+1 = h(ψ s F m ψ −1 s X m ).<label>(10)</label></formula><p>where W ∈ R p×q is the parameter matrix for feature transformation, X m with dimensions n × q is the feature matrix after feature transformation, F m is the diagonal matrix for graph convolution kernel, and h is a non-linear activation function.</p><p>After detaching feature transformation from graph convolution, the parameter complexity is reduced from O(n × p × q) to O(n + p × q). The reduction of parameters is particularly valuable fro graphbased semi-supervised learning where labels are quite limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORKS</head><p>Graph convolutional neural networks on graphs. The success of CNNs when dealing with images, videos, and speeches motivates researchers to design graph convolutional neural network on graphs. The key of generalizing CNNs to graphs is defining convolution operator on graphs. Existing methods are classified into two categories, i.e., spectral methods and spatial methods.</p><p>Spectral methods define convolution via convolution theorem. Spectral CNN <ref type="bibr" target="#b3">(Bruna et al., 2014)</ref> is the first attempt at implementing CNNs on graphs, leveraging graph Fourier transform and defining convolution kernel in spectral domain. <ref type="bibr" target="#b1">Boscaini et al. (2015)</ref> developed a local spectral CNN approach based on the graph Windowed Fourier Transform. <ref type="bibr" target="#b4">Defferrard et al. (2016)</ref> introduced a Chebyshev polynomial parametrization for spectral filter, offering us a fast localized spectral filtering method. Kipf &amp; Welling (2017) provided a simplified version of ChebyNet, gaining success in graph-based semi-supervised learning task. <ref type="bibr" target="#b12">Khasanova &amp; Frossard (2017)</ref> represented images as signals on graph and learned their transformation invariant representations. They used Chebyshev approximations to implement graph convolution, avoiding matrix eigendecomposition. <ref type="bibr" target="#b16">Levie et al. (2017)</ref> used rational functions instead of polynomials and created anisotropic spectral filters on manifolds.</p><p>Spatial methods define convolution as a weighted average function over neighborhood of target vertex. GraphSAGE takes one-hop neighbors as neighborhoods and defines the weighting function as various aggregators over neighborhood <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref>. Graph attention network (GAT) proposes to learn the weighting function via self-attention mechanism <ref type="bibr" target="#b28">(Velickovic et al., 2017)</ref>. MoNet offers us a general framework for design spatial methods, taking convolution as the weighted average of multiple weighting functions defined over neighborhood . Some works devote to making graph convolutional networks more powerful. <ref type="bibr" target="#b20">Monti et al. (2018)</ref> alternated convolutions on vertices and edges, generalizing GAT and leading to better performance. GraphsGAN <ref type="bibr" target="#b5">(Ding et al., 2018)</ref> generalizes GANs to graph, and generates fake samples in low-density areas between subgraphs to improve the performance on graph-based semi-supervised learning.</p><p>Graph wavelets. Sweldens (1998) presented a lifting scheme, a simple construction of wavelets that can be adapted to graphs without learning process. <ref type="bibr" target="#b9">Hammond et al. (2011)</ref> proposed a method to construct wavelet transform on graphs. Moreover, they designed an efficient way to bypass the eigendecomposition of the Laplacian and approximated wavelets with Chebyshev polynomials. <ref type="bibr" target="#b27">Tremblay &amp; Borgnat (2014)</ref> leveraged graph wavelets for multi-scale community mining by modulating a scaling parameter. Owing to the property of describing information diffusion, <ref type="bibr" target="#b6">Donnat et al. (2018)</ref> learned structural node embeddings via wavelets. All these works prove that graph wavelets are not only local and sparse but also valuable for signal processiong on graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS</head><p>To evaluate the proposed GWNN, we apply GWNN on semi-supervised node classification, and conduct experiments on three benchmark datasets, namely, Cora, Citeseer and Pubmed <ref type="bibr" target="#b23">(Sen et al., 2008)</ref>. In the three citation network datasets, nodes represent documents and edges are citation links.</p><p>Details of these datasets are demonstrated in <ref type="table" target="#tab_0">Table 1</ref>. Here, the label rate denotes the proportion of labeled nodes used for training. Following the experimental setup of GCN (Kipf &amp; Welling, 2017), we fetch 20 labeled nodes per class in each dataset to train the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BASELINES</head><p>We compare with several traditional semi-supervised learning methods, including label propagation (LP) <ref type="bibr" target="#b31">(Zhu et al., 2003)</ref>, semi-supervised embedding (SemiEmb) <ref type="bibr" target="#b29">(Weston et al., 2012)</ref>, manifold regularization (ManiReg) <ref type="bibr" target="#b0">(Belkin et al., 2006)</ref>, graph embeddings (DeepWalk) <ref type="bibr" target="#b21">(Perozzi et al., 2014)</ref>, iterative classification algorithm (ICA) <ref type="bibr" target="#b17">(Lu &amp; Getoor, 2003)</ref> and Planetoid <ref type="bibr" target="#b30">(Yang et al., 2016)</ref>.</p><p>Furthermore, along with the development of deep learning on graph, graph convolutional networks are proved to be effective in semi-supervised learning. Since our method is a spectral method based on convolution theorem, we compare it with the Spectral CNN <ref type="bibr" target="#b3">(Bruna et al., 2014)</ref>. ChebyNet <ref type="bibr" target="#b4">(Defferrard et al., 2016)</ref> and <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>, two variants of the Spectral CNN, are also included as our baselines. Considering spatial methods, we take MoNet  as our baseline, which also depends on Laplacian matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EXPERIMENTAL SETTINGS</head><p>We train a two-layer graph wavelet neural network with 16 hidden units, and prediction accuracy is evaluated on a test set of 1000 labeled samples. The partition of datasets is the same as GCN (Kipf &amp; Welling, 2017) with an additional validation set of 500 labeled samples to determine hyper-parameters.</p><p>Weights are initialized following <ref type="bibr" target="#b7">Glorot &amp; Bengio (2010)</ref>. We adopt the Adam optimizer (Kingma &amp; Ba, 2014) for parameter optimization with an initial learning rate lr = 0.01. For computational efficiency, we set the elements of ψ s and ψ −1 s smaller than a threshold t to 0. We find the optimal hyper-parameters s and t through grid search, and the detailed discussion about the two hyperparameters is introduced in Appendix B. For Cora, s = 1.0 and t = 1e − 4. For Citeseer, s = 0.7 and t = 1e − 5. For Pubmed, s = 0.5 and t = 1e − 7. To avoid overfitting, dropout <ref type="bibr" target="#b25">(Srivastava et al., 2014)</ref> is applied. Meanwhile, we terminate the training if the validation loss does not decrease for 100 consecutive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ANALYSIS ON DETACHING FEATURE TRANSFORMATION FROM CONVOLUTION</head><p>Since the number of parameters for the undetached version of GWNN is O(n × p × q), we can hardly implement this version in the case of networks with a large number n of nodes and a huge number p of input features. Here, we validate the effectiveness of detaching feature transformation form convolution on ChebyNet (introduced in Section 2.2), whose parameter complexity is O(K × p × q). For ChebyNet of detaching feature transformation from graph convolution, the number of parameters is reduced to O(K + p × q). <ref type="table" target="#tab_1">Table 2</ref> shows the performance and the number of parameters on three datasets. Here, the reported performance is the optimal performance varying the order K = 2, 3, 4. As demonstrated in <ref type="table" target="#tab_1">Table 2</ref>, with fewer parameters, we improve the accuracy on Pubmed by a large margin. This is due to that the label rate of Pubmed is only 0.003. By detaching feature transformation from convolution, the parameter complexity is significantly reduced, alleviating overfitting in semi-supervised learning and thus remarkably improving prediction accuracy. On Citeseer, there is a little drop on the accuracy. One possible explanation is that reducing the number of parameters may restrict the modeling capacity to some degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">PERFORMANCE OF GWNN</head><p>We now validate the effectiveness of GWNN with detaching technique on node classification. Experimental results are reported in <ref type="table" target="#tab_2">Table 3</ref>. GWNN improves the classification accuracy on all the three datasets. In particular, replacing Fourier transform with wavelet transform, the proposed GWNN is comfortably ahead of Spectral CNN, achieving 10% improvement on Cora and Citeseer, and 5% improvement on Pubmed. The large improvement could be explained from two perspectives: (1) Convolution in Spectral CNN is non-local in vertex domain, and thus the range of feature diffusion is not restricted to neighboring nodes;</p><p>(2) The scaling parameter s of wavelet transform is flexible to adjust the diffusion range to suit different applications and different networks. GWNN consistently outperforms ChebyNet, since it has enough degree of freedom to learn the convolution kernel, while ChebyNet is a kind of approximation with limited degree of freedom. Furthermore, our GWNN also performs better than GCN and MoNet, reflecting that it is promising to design appropriate bases for spectral methods to achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">ANALYSIS ON SPARSITY</head><p>Besides the improvement on prediction accuracy, wavelet transform with localized and sparse transform matrix holds sparsity in both spatial domain and spectral domain. Here, we take Cora as an example to illustrate the sparsity of graph wavelet transform.</p><p>The sparsity of transform matrix. There are 2,708 nodes in Cora. Thus, the wavelet transform matrix ψ −1 s and the Fourier transform matrix U both belong to R 2,708×2,708 . The first two rows in <ref type="table" target="#tab_3">Table 4</ref> demonstrate that ψ −1 s is much sparser than U . Sparse wavelets not only accelerate the computation, but also well capture the neighboring topology centered at each node.</p><p>The sparsity of projected signal. As mentioned above, each node in Cora represents a document and has a sparse bag-of-words feature. The input feature X ∈ R n×p is a binary matrix, and X [i,j] = 1 when the i-th document contains the j-th word in the bag of words, it equals 0 otherwise. Here, X <ref type="bibr">[:,j]</ref> denotes the j-th column of X, and each column represents the feature vector of a word. Considering a specific signal X <ref type="bibr">[:,984]</ref> , we project the spatial signal into spectral domain, and get its projected vector. Here, p = ψ −1 s X <ref type="bibr">[:,984]</ref> denotes the projected vector via wavelet transform, q = U X <ref type="bibr">[:,984]</ref> denotes the projected vector via Fourier transform, and p, q ∈ R 2,708 . The last row in <ref type="table" target="#tab_3">Table 4</ref> lists the numbers of non-zero elements in p and q. As shown in <ref type="table" target="#tab_3">Table 4</ref>, with wavelet transform, the projected signal is much sparser. Each feature, i.e. word in the bag of words, has a projected vector, and each element in this vector is associated with a spectral wavelet basis. Here, each basis is centered at a node, corresponding to a document. The value can be regarded as the relation between the word and the document. Thus, each value in p can be interpreted as the relation between W ord 984 and a document. In order to elaborate the interpretability of wavelet transform, we analyze the projected values of different feature as following.</p><p>Considering two features W ord 984 and W ord 1177 , we select the top-10 active bases, which have the 10 largest projected values of each feature. As illustrated in <ref type="figure">Figure 2</ref>, for clarity, we magnify the local structure of corresponding nodes and marked them with bold rims. The central network in each subgraph denotes the dataset Cora, each node represents a document, and 7 different colors represent 7 classes. These nodes are clustered by OpenOrd <ref type="bibr" target="#b18">(Martin et al., 2011)</ref> based on the adjacency matrix. <ref type="figure">Figure 2a</ref> shows the top-10 active bases of W ord 984 . In Cora, this word only appears 8 times, and all the documents containing W ord 984 belong to the class " Case-Based ". Consistently, all top-10 nodes activated by W ord 984 are concentrated and belong to the class " Case-Based ". And, the frequencies of W ord 1177 appearing in different classes are similar, indicating that W ord 1177 is a universal word. In concordance with our expectation, the top-10 active bases of W ord 1177 are discrete and belong to different classes in <ref type="figure">Figure 2b</ref>.</p><p>(a) (b) <ref type="figure">Figure 2</ref>: Top-10 active bases of two words in Cora. The central network of each subgraph represents the dataset Cora, which is split into 7 classes. Each node represents a document, and its color indicates its label. The nodes that represent the top-10 active bases are marked with bold rims. (a) W ord 984 only appears in documents of the class " Case-Based " in Cora. Consistently, all its 10 active bases also belong to the class " Case-Based ". (b) The frequencies of W ord 1177 appearing in different classes are similar in Cora. As expected, the top-10 active bases of W ord 1177 also belong to different classes.</p><p>Owing to the properties of graph wavelets, which describe the neighboring topology centered at each node, the projected values of wavelet transform can be explained as the correlation between features and nodes. These properties provide an interpretable domain transformation and ease the understanding of graph convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Replacing graph Fourier transform with graph wavelet transform, we proposed GWNN. Graph wavelet transform has three desirable properties: (1) Graph wavelets are local and sparse; (2) Graph wavelet transform is computationally efficient; (3) Convolution is localized in vertex domain. These advantages make the whole learning process interpretable and efficient. Moreover, to reduce the number of parameters and the dependence on huge training data, we detached the feature transformation from convolution. This practice makes GWNN applicable to large graphs, with remarkable performance improvement on graph-based semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A LOCALIZED GRAPH CONVOLUTION VIA WAVELET TRANSFORM</head><p>We use a diagonal matrix Θ to represent the learned kernel transformed by wavelets ψ −1 s y, and replace the Hadamard product with matrix muplication. Then Equation <ref type="formula" target="#formula_3">(4)</ref> is:</p><formula xml:id="formula_10">x * G y = ψ s Θψ −1 s x.<label>(11)</label></formula><p>We set ψ s = (ψ s1 , ψ s2 , ..., ψ sn ), ψ −1 s = (ψ * s1 , ψ * s2 , ..., ψ * sn ), and Θ = diag({θ k } n k=1 ). Equation <ref type="formula" target="#formula_0">(11)</ref> becomes :</p><formula xml:id="formula_11">x * G y = n k=1 θ k ψ sk (ψ * sk ) x.<label>(12)</label></formula><p>As proved by <ref type="bibr" target="#b9">Hammond et al. (2011)</ref>, both ψ s and ψ −1 s are local in small scale (s). <ref type="figure" target="#fig_1">Figure 3</ref> shows the locality of ψ s1 and ψ * s1 , i.e., the first column in ψ s and ψ −1 s when s = 3. Each column in ψ s and ψ −1 s describes the neighboring topology of target node, which means that ψ s and ψ −1 s are local. The locality of ψ sk and ψ * sk leads to the locality of the resulting matrix of multiplication between the column vector ψ sk and row vector (ψ * sk  Since each M k is local, for any convolution kernel Θ, ψ s Θψ −1 s is local, and it means that convolution is localized in vertex domain. By replacing Θ with an identity matrix in Equation <ref type="formula" target="#formula_0">(12)</ref>, we get x * G y = n k=1 M k x. We define H = n k=1 M k , and <ref type="figure" target="#fig_2">Figure 4</ref> shows H <ref type="bibr">[1,:]</ref> in different scaling, i.e., correlation between the first node and other nodes during convolution. The locality of H suggests that graph convolution is localized in vertex domain. Moreover, as the scaling parameter s becomes larger, the range of feature diffusion becomes larger. GWNN leverages graph wavelets to implement graph convolution, where s is used to modulate the range of neighborhoods. From <ref type="figure">Figure 5</ref>, as s becomes larger starting from 0, the range of neighboring nodes becomes large, resulting the increase of accuracy on Cora. However when s becomes too large, some irrelevant nodes are included, leading to decreasing of accuracy. The hyperparameter t only used for computational efficiency, has any slight influence on its performance.</p><p>For experiments on specific dataset, s and t are choosen via grid search using validation. Generally, a appropriate s is in the range of [0.5, 1], which can not only capture the graph structure but also guarantee the locality of convolution, and t is less insensive to dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C PARAMETER COMPLEXITY OF NODE CLASSIFICATION</head><p>We show the parameter complexity of node classification in <ref type="table" target="#tab_5">Table 5</ref>. The high parameter complexity O(n * p * q) of Spectral CNN makes it difficult to generalize to real world networks. ChebyNet approximates the convolution kernel via polynomial function of the diagonal matrix of Laplacian eigenvalues, reducing parameter complexity to O(K * p * q) with K being the order of polynomial function. GCN simplifies ChebyNet via setting K=1. We detach feature transformation from graph convolution to implement GWNN and Spectral CNN in our experiments, which can reduce parameter to O(n + p * q). In Cora and Citeseer, with smaller parameter complexity, GWNN achieves better performance than ChebyNet, reflecting that it is promising to implement convolution via graph wavelet transform. As Pubmed has a large number of nodes, the parameter complexity of GWNN is larger than ChebyNet. As future work, it is an interesting attempt to select wavelets associated with a subset of nodes, further reducing parameter complexity with potential loss of performance. With the stable recurrence relation T k (y) = 2yT k−1 (y) − T k−2 (y), we can generate the Chebyshev polynomials T k (y). Here T 0 = 1 and T 1 = y. For y sampled between -1 and 1, the trigonometric expression T k (y) = cos(k arccos(y)) is satisfied. It shows that T k (y) ∈ [−1, 1] when y ∈ [−1, 1]. Through the Chebyshev polynomials, an orthogonal basis for the Hilbert space of square integrable functions L 2 ([−1, 1], dy √ 1−y 2 ) is formed. For each h in this Hilbert space, we have a uniformly convergent Chebyshev series h(y) = 1 2 c 0 + ∞ k=1 c k T k (y), and the Chebyshev coefficients c k = 2 π 1 −1 T k (y)h(y) √ 1−y 2 dy = 2 π π 0 cos(kθ)h(cos(θ))dθ. A fixed scale s is assumed. To approximate g(sx) for x ∈ [0, λ max ], we can shift the domain through the transformation x = a(y + 1), where a = λmax 2 . T k (x) = T k ( x−a a ) denotes the shifted Chebyshev polynomials, with x−a a ∈ [−1, 1]. Then we have g(sx) = 1 2 c 0 + ∞ k=1 c k T k (x), and x ∈ [0, λ max ], c k = 2 π π 0 cos(kθ)g(s(a(cos(θ) + 1)))dθ. we truncate the Chebyshev expansion to m terms and achieve Polynomial approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D FAST GRAPH</head><p>Here we give the example of the ψ −1 s and g(sx) = e −sx , the graph signal is f ∈ R n . Then we can give the fast approximation wavelets by ψ −1 s f = 1 2 c 0 f + </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E ANALYSIS ON SPASITY OF SPECTRAL TRANSFORM AND LAPLACIAN MATRIX</head><p>The sparsity of the graph wavelets depends on the sparsity of the Laplacian matrix and the hyperparameter s, We show the sparsity of spectral transform matrix and Laplacian matrix in <ref type="table" target="#tab_6">Table 6</ref>. The sparsity of Laplacian matrix is sparser than graph wavelets, and this property limits our method, i.e., the higher time complexity than some methods depending on Laplacian matrix and identity matrix, e.g., GCN. Specifically, both our method and GCN aim to improve Spectral CNN via designing localized graph convolution. GCN, as a simplified version of ChebyNet, leverages Laplacian matrix as weighted matrix and expresses the spectral graph convolution in spatial domain, acting as spatial-like method . However, our method resorts to using graph wavelets as a new set of bases, directly designing localized spectral graph convolution. GWNN offers a localized graph convolution via replacing graph Fourier transform with graph wavelet transform, finding good spectral basis with localization property and good interpretability. This distinguishes GWNN from ChebyNet and GCN, which express the graph convolution defined via graph Fourier transform in vertex domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Wavelets on an example graph at (a) small scale and (b) large scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Locality of (a) ψ s1 and (b) ψ * s1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Correlation between first node and other nodes at (a) small scale and (b) large scale. Nonzero value of node represents correlation between this node and target node during convolution. Locality of H suggests that graph convolution is localized in vertex domain. Moreover, with scaling parameter s becoming larger, the range of feature diffusion becomes larger. APPENDIX B INFLUENCE OF HYPER-PARAMETERS Figure 5: Influence of s and t on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>WAVELETS WITH CHEBYSHEV POLYNOMIAL APPROXIMATION Hammond et al. (2011) proposed a method, using Chebyshev polynomials to efficiently approximate ψ s and ψ −1 s . The computational complexity is O(m × |E|), where |E| is the number of edges and m is the order of Chebyshev polynomials. We give the details of the approximation proposed in Hammond et al. (2011).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>m</head><label></label><figDesc>k=1 c k T k (L)f . The efficient computation of T k (L) determines the utility of this approach, where T k (L)f = 2 a (L − I)(T k−1 (L)f ) − T k−2 (L)f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The Statistics of Datasets</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell cols="4">Edges Classes Features Label Rate</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>0.052</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>0.036</cell></row><row><cell cols="3">Pubmed 19,717 44,338</cell><cell>3</cell><cell>500</cell><cell>0.003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of Detaching Feature Transformation from Convolution</figDesc><table><row><cell></cell><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>Prediction Accuracy</cell><cell>ChebyNet Detaching-ChebyNet</cell><cell>81.2% 81.6%</cell><cell>69.8% 68.5%</cell><cell>74.4% 78.6%</cell></row><row><cell>Number of Parameters</cell><cell cols="4">ChebyNet Detaching-ChebyNet 23,048 (K=4) 46,080 (K=2) 178,032 (K=3) 24,144 (K=3) 59,348 (K=2) 8,054 (K=3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of Node Classification</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell cols="2">Citeseer Pubmed</cell></row><row><cell>MLP</cell><cell>55.1%</cell><cell>46.5%</cell><cell>71.4%</cell></row><row><cell>ManiReg</cell><cell>59.5%</cell><cell>60.1%</cell><cell>70.7%</cell></row><row><cell>SemiEmb</cell><cell>59.0%</cell><cell>59.6%</cell><cell>71.7%</cell></row><row><cell>LP</cell><cell>68.0%</cell><cell>45.3%</cell><cell>63.0%</cell></row><row><cell>DeepWalk</cell><cell>67.2%</cell><cell>43.2%</cell><cell>65.3%</cell></row><row><cell>ICA</cell><cell>75.1%</cell><cell>69.1%</cell><cell>73.9%</cell></row><row><cell>Planetoid</cell><cell>75.7%</cell><cell>64.7%</cell><cell>77.2%</cell></row><row><cell cols="2">Spectral CNN 73.3%</cell><cell>58.9%</cell><cell>73.9%</cell></row><row><cell>ChebyNet</cell><cell>81.2%</cell><cell>69.8%</cell><cell>74.4%</cell></row><row><cell>GCN</cell><cell>81.5%</cell><cell>70.3%</cell><cell>79.0%</cell></row><row><cell>MoNet</cell><cell cols="2">81.7±0.5% -</cell><cell>78.8±0.3%</cell></row><row><cell>GWNN</cell><cell>82.8%</cell><cell>71.7%</cell><cell>79.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Statistics of wavelet transform and Fourier transform on Cora</figDesc><table><row><cell></cell><cell>Statistical Property</cell><cell cols="2">wavelet transform Fourier transform</cell></row><row><cell>Transform Matrix</cell><cell>Density Number of Non-zero Elements</cell><cell>2.8% 205,774</cell><cell>99.1% 7,274,383</cell></row><row><cell>Projected Signal</cell><cell>Density Number of Non-zero Elements</cell><cell>10.9% 297</cell><cell>100% 2,708</cell></row><row><cell cols="2">4.7 ANALYSIS ON INTERPRETABILITY</cell><cell></cell><cell></cell></row></table><note>Compare with graph convolution network using Fourier transform, GWNN provides good inter- pretability. Here, we show the interpretability with specific examples in Cora.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>) . For convenience, we setM k = ψ sk (ψ * sk ) , M k[i,j] &gt; 0 only when ψ sk [i] &gt; 0 and (ψ * sk ) [j] &gt; 0.In other words, if M k[i,j] &gt; 0, vertex i and vertex j can correlate with each other through vertex k.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Parameter complexity of Node Classification</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>Spectral CNN</cell><cell>62,392,320</cell><cell>197,437,488</cell><cell>158,682,416</cell></row><row><cell>Spectral CNN (detaching)</cell><cell>28,456</cell><cell>65,379</cell><cell>47,482</cell></row><row><cell>ChebyNet</cell><cell cols="3">46,080 (K=2) 178,032 (K=3) 24,144 (K=3)</cell></row><row><cell>GCN</cell><cell>23,040</cell><cell>59,344</cell><cell>8,048</cell></row><row><cell>GWNN</cell><cell>28,456</cell><cell>65,379</cell><cell>47,482</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Statistics of spectral transform and Laplacian matrix on Cora</figDesc><table><row><cell></cell><cell cols="2">Density Num of Non-zero Elements</cell></row><row><cell cols="2">wavelet transform 2.8%</cell><cell>205,774</cell></row><row><cell cols="2">Fourier transform 99.1%</cell><cell>7,274,383</cell></row><row><cell>Laplacian matrix</cell><cell>0.15%</cell><cell>10,858</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENTS</head><p>This work is funded by the National Natural Science Foundation of China under grant numbers 61425016, 61433014, and 91746301. Huawei Shen is also funded by K.C. Wong Education Foundation and the Youth Innovation Promotion Association of the Chinese Academy of Sciences.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR2014)</title>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning structural node embeddings via diffusion wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph-based isometry invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renata</forename><surname>Khasanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1847" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07664</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03)</title>
		<meeting>the 20th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Openord: an opensource toolbox for large graph layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Klavans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Boyack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">7868</biblScope>
		</imprint>
	</monogr>
	<note>Visualization and Data Analysis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00770</idno>
		<title level="m">Dual-primal graph convolutional networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gspbox: A toolbox for signal processing on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanaël</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Paratte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David K</forename><surname>Hammond</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5781</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The lifting scheme: A construction of second generation wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><surname>Sweldens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on mathematical analysis</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="546" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph wavelets for multiscale community mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Borgnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="5227" to="5239" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
