<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ryerson University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach outperforms all state-of-theart methods on standard benchmarks achieving a relative error reduction greater than 30% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our endto-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the full-body 3D pose of a human from a single monocular image is an open challenge, which has garnered significant attention since the early days of computer vision <ref type="bibr" target="#b17">[18]</ref>. Given its ill-posed nature, researchers have generally approached 3D human pose estimation in simplified settings, such as assuming background subtraction is feasible <ref type="bibr" target="#b0">[1]</ref>, relying on groundtruth 2D joint locations to estimate 3D pose <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42]</ref>, employing additional camera views <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>, and capitalizing on temporal consistency to improve upon single frame predictions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b2">3]</ref>. This di-Image ConvNet Volumetric Output <ref type="figure">Figure 1</ref>: Illustration of our volumetric representation for 3D human pose. We discretize the space around the subject and use a ConvNet to predict per voxel likelihoods for each joint from a single color image.</p><p>versity of assumptions and additional information sources exemplifies the challenge presented by the task.</p><p>With the introduction of more powerful discriminative approaches, such as Convolutional Networks (ConvNets), many of these restrictive assumptions have been relaxed. End-to-end learning approaches attempt to estimate 3D pose directly from a single image by addressing it as coordinate regression <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>, nearest neighbor between images and poses <ref type="bibr" target="#b19">[20]</ref>, or classification over a set of pose classes <ref type="bibr" target="#b26">[27]</ref>. Yet to date, these approaches have been outperformed by more traditional two-step pipelines, e.g., <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b5">6]</ref>. In these cases, ConvNets are used only for 2D joint localization and 3D poses are generated during a postprocessing optimization step. Combining accurate 2D joint localization with strong and expressive 3D priors has been proven to be very effective. In this work, we show that ConvNets are able to provide much richer information than simply 2D joint locations.</p><p>To fully exploit the potential of ConvNets in the context of 3D human pose, we propose the following items, and justify them empirically. First, we cast 3D pose estimation as a keypoint localization problem in a discretized 3D space. Instead of directly regressing the coordinates of the joints (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>), we train a ConvNet to predict per voxel likelihoods for each joint in this volume. This volumetric representation, illustrated in <ref type="figure">Figure 1</ref>, is much more sensible for the 3D nature of our problem and improves learning.</p><p>Effectively, for every joint, the volumetric supervision provides the network with groundtruth for each voxel in the 3D space. This provides much richer information than a set of world coordinates. The empirical results also validate the superiority of our proposed form of supervision.</p><p>Second, to deal with the increased dimensionality of the volumetric representation, we propose a coarse-to-fine prediction scheme. As demonstrated in the 2D pose case, intermediate supervision and iterative estimation are particularly effective strategies <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21]</ref>. For our volumetric representation though, naively stacking an increasing number of components and refining the estimates is not an effective solution, as shown empirically. Instead, we gradually increase the resolution of the supervision volume for the most challenging z-dimension (depth), during the processing. This coarse-to-fine supervision, illustrated schematically in <ref type="figure">Figure</ref> 2, allows for more accurate estimates after each step. We empirically demonstrate the advantage of this practice over naively stacking more components together.</p><p>Our proposed approach achieves state-of-the-art results on standard benchmarks, outperforming both ConvNet-only and hybrid approaches that post-process the 2D output of a ConvNet. Additionally, we investigate using our volumetric representation within a related architecture that decouples 2D joint localization and 3D joint reconstruction. In particular, we use two separate networks (the output of one serves as the input to the other) and two non-corresponding data sources, i.e., 2D labeled imagery to train the first component and an independent 3D data source (e.g., MoCap) to train the second one separately. While this architecture has practical benefits (e.g., predicting 3D pose for in-the-wild images), we show empirically that it underperforms compared to our end-to-end approach when images with corresponding 3D groundtruth are available for training. This finding further underlines the benefit of predicting 3D pose directly from an image, whenever this is possible, instead of using 2D joint localization as an intermediate step.</p><p>In summary, we make the four following contributions:</p><p>• we are the first to cast 3D human pose estimation as a 3D keypoint localization problem in a voxel space using the end-to-end learning paradigm;</p><p>• we propose a coarse-to-fine prediction scheme to deal with the large dimensionality of our representation and enable iterative processing to realize further benefits;</p><p>• our proposed approach achieves state-of-the-art results on standard benchmarks, surpassing both ConvNetonly and hybrid approaches that employ ConvNets for 2D pose estimation, with a relative error reduction that exceeds 30% on average;</p><p>• we show the practical use of our volumetric representation in cases when end-to-end training is not an option and present compelling results on in-the-wild images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The literature on 3D human pose estimation is vast with approaches addressing the problem in a variety of settings. Here, we survey works that are most relevant to ours with a focus on ConvNet-based approaches; we refer the reader to a recent survey <ref type="bibr" target="#b28">[29]</ref> for a more complete literature review.</p><p>The majority of recent ConvNet-only approaches cast 3D pose estimation as a coordinate regression task, with the target output being the spatial x, y, z coordinates of the human joints with respect to a known root joint, such as the pelvis. Li and Chan <ref type="bibr" target="#b18">[19]</ref> pretrain their network with maps for 2D joint classification. Tekin et al. <ref type="bibr" target="#b33">[34]</ref> include a pretrained autoencoder within the network to enforce structural constraints on the output. Ghezelghieh et al. <ref type="bibr" target="#b12">[13]</ref> employ viewpoint prediction as a side task to provide the network with global joint configuration information. Zhou et al. <ref type="bibr" target="#b42">[43]</ref> embed a kinematic model to guarantee the validity of the regressed pose. Park et al. <ref type="bibr" target="#b21">[22]</ref> concatenate the 2D joint predictions with image features to improve 3D joint localization. Tekin et al. <ref type="bibr" target="#b34">[35]</ref> include temporal information in the joint predictions by extracting spatiotemporal features from a sequence of frames. In contrast to all these approaches, we adopt a volumetric representation of the human pose, and regress the per voxel likelihood for each joint separately. This proves to have significant advantages for the network performance and provides a richer output compared to the low-dimensional vector of joint coordinates.</p><p>An alternative approach to the classical regression paradigm is proposed by Li et al. <ref type="bibr" target="#b19">[20]</ref>. During training, they learn a common embedding between color images and 3D poses. At test time, the test image is coupled with each candidate pose and forwarded through the network; the input image is assigned the candidate pose with the maximum network score. This is a form of nearest neighbor classification which is highly inefficient due to the requirement of multiple forward network passes. On the other hand, Rogez and Schmid <ref type="bibr" target="#b26">[27]</ref> cast pose estimation as a classification problem. Given a predefined set of pose classes, each image is assigned to the class with the highest score. This guarantees a valid global pose prediction, but the approach is constrained by the poses in the original classes and thus returns only a rough pose estimate. In contrast to the inefficient nearest neighbor approach and the coarse classification approach, our volume regression allows for much more accurate 3D joint localization, while also being efficient.</p><p>Despite the interest in end-to-end learning, ConvNetonly approaches underperform those that employ a ConvNet for the 2D localization of joints, and produce 3D pose with a subsequent optimization step. Zhou et al. <ref type="bibr" target="#b43">[44]</ref> utilize a standard 2D pose ConvNet to localize the joints and retrieve the 3D pose using an optimization scheme over a sequence of monocular images. Similarly, Du et al. <ref type="bibr" target="#b9">[10]</ref> include height-maps of the human body to improve 2D joint <ref type="figure">Figure 2</ref>: Illustration of our coarse-to-fine volumetric approach for 3D human pose estimation from a single image. The input is a single color image and the output is a dense 3D volume with separate per voxel likelihoods for each joint. The network consists of multiple fully convolutional components <ref type="bibr" target="#b20">[21]</ref>, which are supervised in a coarse-to-fine fashion, to deal with the large dimensionality of our representation. 3D heatmaps are synthesized for supervision by increasing the resolution for the most challenging z-dimension (depth) after each component. The dashed lines indicate that the intermediate heatmaps are fused with image features to produce the input for the next fully convolutional component. For presentation simplicity, the illustrated heatmaps correspond to the location of only one joint. localization. Bogo et al. <ref type="bibr" target="#b5">[6]</ref> use the joints predicted by a 2D ConvNet and fit a statistical body shape model to recover the full shape of the human body. In contrast, our approach achieves state-of-the-art results with a single network. Furthermore, it provides a rich 3D output, amenable to post-processing, such as pictorial structures optimization to constrain limb lengths, or temporal filtering.</p><p>Another issue that has been addressed in the context of using ConvNets for 3D human pose is the scarcity of training data. Chen et al. <ref type="bibr" target="#b8">[9]</ref> use a graphics renderer to create images with known groundtruth. Similarly, Ghezelghieh et al. <ref type="bibr" target="#b12">[13]</ref> augment the training set with synthesized examples. A collage approach is proposed by Rogez and Schmid <ref type="bibr" target="#b26">[27]</ref>, where parts from in-the-wild images are combined to create additional images with known 3D poses. However, there is no guarantee that the statistics of the synthetic examples match those of real images. To investigate the data scarcity issue, we take inspiration from the 3D Interpreter Network <ref type="bibr" target="#b39">[40]</ref>, which decouples the 3D pose estimation task into 2D localization and 3D reconstruction within a single ConvNet. In contrast, rather than using a predefined linear basis for 3D reconstruction, we predict 3D joint locations directly with our volumetric representation. This demonstrates the practical use of our volumetric representation even when end-to-end training is not an option.</p><p>Finally, while we do not compare explicitly with multiview pose estimation work (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11]</ref>), it is interesting to note that the representation of 3D human pose in a discretized 3D space has also been previously adopted in multi-view settings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref>, where it was used to accommodate predictions from different viewpoints. For single view pose estimation, it has been considered in the context of random forests <ref type="bibr" target="#b15">[16]</ref>. This approach suffered from large execution time (around three minutes), and required an additional refinement step using a pictorial structures model.</p><p>In stark contrast, our network can provide complete volume predictions with a single forward pass in a few milliseconds, needs no additional refinement (although it is still a possibility) to provide state-of-the-art results, and is integrated within a coarse-to-fine prediction scheme to deal with excessive dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Technical approach</head><p>The following subsections summarize our technical approach. Section 3.1 describes the proposed volumetric representation for 3D human pose and discusses its merits. Next, Section 3.2 describes our coarse-to-fine prediction approach that addresses the high dimensional nature of our output representation. Finally, Section 3.3 describes the use of our volumetric representation within a related decoupled architecture and discusses its relative merits compared to our coarse-to-fine volumetric prediction approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Volumetric representation for 3D human pose</head><p>The problem of 3D human pose estimation using ConvNets has been primarily approached as a coordinate regression problem. In this case, the target of the network is a 3N -dimensional vector comprised of the concatenation of the x, y, z coordinates of the N joints of the human body. For training, an L 2 regression loss is employed:</p><formula xml:id="formula_0">L = N n=1 x n gt − x n pr 2 2 ,<label>(1)</label></formula><p>where x n gt is the groundtruth and x n pr is the predicted location for joint n. The location of each joint is expressed globally, with respect to a root joint, or locally, with respect to its parent joint in the kinematic tree. The second formulation has some benefits, as discussed also by Li et al. <ref type="bibr" target="#b18">[19]</ref> (e.g., easier to learn to predict small, local deviations), but still suffers from the fact that small errors can easily propagate hierarchically to children joints of the kinematic tree. In general, despite its simplicity, the coordinate regression approach makes the problem highly non-linear and presents problems for the learning procedure. These issues have previously been identified in the context of 2D human pose <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>To improve learning, we propose a volumetric representation for 3D human pose. The volume around the subject is discretized uniformly in each dimension. For each joint we create a volume of size w × h × d. Let p n (i,j,k) denote the predicted likelihood of joint n being in voxel (i, j, k). To train this network, the supervision is also provided in volumetric form. The target for each joint is a volume with a 3D Gaussian centered around the groundtruth position x n gt = (x, y, z) of the joint in the 3D grid:</p><formula xml:id="formula_1">G i,j,k (x n gt ) = 1 2πσ 2 e − (x−i) 2 +(y−j) 2 +(z−k) 2 2σ 2 ,<label>(2)</label></formula><p>where the value σ = 2 is used for our experiments. For training, we use the mean squared error loss:</p><formula xml:id="formula_2">L = N n=1 i,j,k G (i,j,k) (x n gt ) − p n (i,j,k) 2 .<label>(3)</label></formula><p>In theory, the output of the network is four dimensional, i.e., (w × h × d × N ), but in practice we organize it in channels, thus our output is three dimensional, i.e., w × h × dN . The voxel with the maximum response in each 3D grid is selected as the joint's 3D location.</p><p>A major advantage of the volumetric representation is that it casts the highly non-linear problem of direct 3D coordinate regression to a more manageable form of prediction in a discretized space. In this case, the predictions do not necessarily commit to a unique location for each joint, but instead an estimate of the confidence is provided for each voxel. This makes it easier for the network to learn the target mapping. A similar argument has been previously put forth in the 2D pose case, validating the benefit of predicting per pixel likelihoods instead of pixel coordinates <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref>. In terms of the network architecture, an important benefit of the volumetric representation is that it enables the use of a fully convolutional network for prediction. Here, we adopt the hourglass design <ref type="bibr" target="#b20">[21]</ref>. This leads to less network parameters than using fully connected layers for coordinate regression or pose classification. Finally, in terms of the predicted output, besides being more accurate, our network predictions in the form of dense 3D heatmaps are useful for subsequent post-processing applications. For example, structural constraints can be enforced with the use of a 3D Pictorial Structures model, e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref>. Another option is to use the dense predictions in a filtering framework in cases where multiple input frames are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Coarse-to-fine prediction</head><p>A design choice that has been particularly effective in the case of 2D human pose is the iterative processing of the network output <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21]</ref>. Instead of using a single component with a single output, the network is forced to produce predictions in multiple processing stages. These intermediate predictions are gradually refined to produce more accurate estimates. Additionally, the use of intermediate supervision on the "earlier" outputs allows for a richer gradient signal, which has been demonstrated empirically as an effective learning practice <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Inspired by the success of iterative refinement in the context of 2D pose, we also consider a gradual refinement scheme. Empirically, we found that naively stacking multiple components yielded diminishing returns because of the large dimensionality of our representation. In fact, for the highest 3D resolution of 64 × 64 × 64 with 16 joints, we would need to estimate the likelihood for more that 4 million voxels. To deal with this curse of dimensionality, we propose to use a coarse-to-fine prediction scheme. In particular, the first steps are supervised with lower resolution targets for the (most challenging and technically unobserved) z-dimension. Precisely, we use targets of size 64 × 64 × d per joint, where d typically takes values from the set {1, 2, 4, 8, 16, 32, 64}. An illustration of this supervision approach is given in <ref type="figure">Figure 2</ref>.</p><p>This strategy makes training more effective, and allows us to benefit from stacking multiple components together without suffering from overfitting or dimensionality issues. Intuitively, easier versions of the task are presented to the network during the early stages of processing, and the complexity increases gradually. This postpones the harder decisions until the very end of the processing, when all the available information has been processed and consolidated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoupled architecture with volumetric target</head><p>To further show the versatility of the proposed volumetric representation, we also employ it in a scenario where end-to-end training is not an option. This is usually the case for in-the-wild images, where accurate, large-scale acquisition of 3D groundtruth is not feasible. Inspired by the 3D Interpreter Network <ref type="bibr" target="#b39">[40]</ref>, we decouple 3D pose estimation in two sequential steps consisting of predicting 2D keypoint heatmaps, followed by an inference step of the 3D joint positions with our volumetric representation. The first step can be trained with 2D labeled in-the-wild imagery, while the second step requires only 3D data (e.g., MoCap). Independently, each of these sources are abundantly available.</p><p>This training strategy is useful for practical scenarios, and we present compelling results for in-the-wild images (Sec. 4.6). However, it remains suboptimal compared to our end-to-end approach when images with corresponding 3D groundtruth are available for training. <ref type="figure" target="#fig_1">Figure 3</ref>   illustration of each architecture in a simplified setting with two hourglasses. It can be seen that the decoupled case is related to our course-to-fine architecture when the resolution of the intermediate supervision is set to d = 1 resulting in 2D heatmaps. A crucial difference between the two architectures is that our coarse-to-fine approach combines the produced 2D heatmaps with intermediate image features. This way, the rest of the network can process information both about the image and the 2D joint locations. On the other hand, a decoupled network processes the 2D heatmaps directly and attempts to reconstruct 3D locations without further aid by image-based evidence. In cases where the heatmaps are grossly erroneous, the 3D predictions can be lead astray. In Sec. 4.4, we show empirically that when images with corresponding 3D groundtruth are available, our coarse-to-fine architecture outperforms the decoupled one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Empirical evaluation 4.1. Datasets</head><p>We present extensive quantitative evaluation of our coarse-to-fine volumetric approach on three standard benchmarks for 3D human pose: Human3.6M <ref type="bibr" target="#b13">[14]</ref>, HumanEva-I <ref type="bibr" target="#b29">[30]</ref> and KTH Football II <ref type="bibr" target="#b14">[15]</ref>. Additionally, qualitative results are presented on the MPII human pose dataset <ref type="bibr" target="#b1">[2]</ref>, since no 3D groundtruth is available. Human3.6M: It contains video of 11 subjects performing a variety of actions, such as "Walking", "Sitting" and "Phoning". We follow the same evaluation protocol as prior work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b43">44]</ref>. In particular, Subjects S1, S5, S6, S7 and S8 were used for training, while subjects S9 and S11 were used for testing. The original videos were downsampled from 50fps to 10fps. We employed all camera views and trained a single model for all actions, instead of training action-specific models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>HumanEva-I: It is a smaller dataset compared to Hu-man3.6M, with fewer subjects and actions. Following the standard protocol <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>, we evaluated on "Walking" and "Jogging" from subjects S1, S2 and S3. The training sequences of these subjects and actions were used for training, and the corresponding validation sequences for testing. As done with the Human3.6M evaluation, we train a single model using the frames for all users and actions.</p><p>KTH Football II: The images are taken from a professional football match, and 3D groundtruth is provided only for a very small number of them. The limited available groundtruth is not very accurate, since it was generated by combining manual 2D annotations from multiple views. In this case, image-to-3D training is not a practical option. Instead, we report results using our volumetric representation within the decoupled architecture described in Sec. 3.3. More specifically, we train the first network component (image to 2D heatmaps) using images from this dataset which provide 2D groundtruth. For the second network component (2D heatmaps to 3D heatmaps), we use all the training MoCap data from Human3.6M dataset. As others <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35]</ref>, we report results using "Sequence 1" from "Player 2" and frames taken from "Camera 1".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII:</head><p>It is a large scale 2D pose dataset containing inthe-wild imagery. It provides 2D annotations but no 3D groundtruth. Like KTH, direct image-to-3D training is not a practical option with this dataset. Instead, we use the decoupled architecture with our volumetric representation. Since we cannot quantify the performance here, we only provide qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation metrics</head><p>For Human3.6M, most approaches report the per joint 3D error, which is the average Euclidean distance of the estimated joints to the groundtruth. This is done after aligning the root joints (here the pelvis) of the estimated and groundtruth 3D pose. An alternative metric, which is used by some methods to report results on Human3.6M and HumanEva-I is the reconstruction error. It is defined as the per joint 3D error up to a similarity transformation. Effectively, the estimated 3D pose is aligned to the groundtruth by the Procrustes method. Finally, for KTH the percentage of the correctly estimated parts in 3D (3D PCP <ref type="bibr" target="#b6">[7]</ref>) is reported. Again, the root joints (here we use the center of the chest) are aligned to resolve the depth ambiguity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>In our volumetric space, the x-y grid is a uniform discretization within the 2D bounding box in the image and the z grid is a uniform discretization in [−1, 1] (meters) centered at the root joint. This means that we predict the image coordinates for each joint and its metric depth relative to the root. Given the depth of the root joint we can recover the absolute depth for each joint and its metric coordinates in the x-y dimensions. For the component analysis (Section 4.4) we use the groundtruth depth of the root joint, while for the comparison to other methods (Section 4.5) we estimate this depth based on the average skeleton size of each dataset. More details are provided in the supplementary material.</p><p>In terms of the architecture, the fully convolutional components of our network, illustrated in <ref type="figure">Figure 2</ref>, are based on the hourglass design <ref type="bibr" target="#b20">[21]</ref>. We use the publicly available code to faithfully replicate the architecture. Similarly, we adopt the same training practices, employing rotation augmentation (±30 o ), scaling augmentation (0.75-1.25), leftright flipping, and using rmsprop for optimization with the batch size equal to 4 and the learning rate set to 2.5e−4.</p><p>Regarding training on individual datasets, for Hu-man3.6M, the network models are trained from scratch, typically for four epochs (approximately 310k iterations). For HumanEva-I, the model is trained from scratch for 120 epochs (approximately 235k iterations), because of the significantly smaller training set size. Finally, for the 2D joint localization network on KTH, we use the publicly available stacked hourglass model trained on MPII <ref type="bibr" target="#b20">[21]</ref> and fine-tune it for 20 epochs (approximately 30k iterations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Component evaluation</head><p>To evaluate the components of our approach, we use Hu-man3.6M to report results, since it is the most complete available benchmark.</p><p>Volumetric representation: Our first goal is to demonstrate that regression in a discretized space provides great benefit over coordinate regression. Both versions are implemented with the simplest setting of one hourglass. The only difference between the architectures is that the network for the volumetric predictions is fully convolutional, while for coordinate regression we use a fully connected layer at the end. The results are presented in <ref type="table" target="#tab_0">Table 1</ref>. The error of 112.41mm for coordinate regression is comparable to recent reported results with a coordinate regression target output <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref>. In comparison, a significant decrease in the error is observed, down to 85.82mm at the highest depth resolution, by using the volumetric output target.</p><p>Coarse-to-fine prediction: The next significant improvement to our network is provided by iterative processing of the image features. The naive way to achieve this is by stacking together multiple hourglasses. This is helpful, but  <ref type="table">Table 2</ref>: Comparison of the Naive Stacking (left) versus Coarse-to-Fine (right) approaches on Human3.6M. The column Li denotes the z-dimension resolution for the supervision provided at the i-th hourglass component (empty if the network has less than i components). We report mean per joint errors (mm) following the standard protocol.</p><p>offers only diminishing returns, as is demonstrated in Table 2 (Naive stacking). Instead, our coarse-to-fine supervision approach outperforms the naive one when we use two, three, or four hourglasses ( <ref type="table">Table 2</ref>, Coarse-to-Fine). In fact, our coarse-to-fine version with two hourglasses (69.77mm error) outperforms the naive stacking network with four hourglasses (75.06mm error), despite using less than half of the parameters compared to the deeper network.</p><p>Decoupled network with volumetric representation: We investigate the use of a decoupled network combined with our volumetric representation, as described in Sec. 3.3. Our goal is to demonstrate the benefit of predicting the 3D pose directly from image features versus using 2D locations as an intermediate representation. We refer back to <ref type="figure" target="#fig_1">Fig. 3</ref> for a schematic representation of the two relevant networks. We train both networks end-to-end to evaluate the architecture performance rather than the benefit of end-to-end training. (In fact we observed that training the two components of the decoupled network independently leads to inferior performance.) Comparative results are provided in <ref type="table" target="#tab_2">Table 3</ref>. We present the average across all actions, as well as the six actions with the largest difference between the two networks. Besides being more accurate for every action and on average, our coarse-to-fine approach shows significant improvement for more challenging actions, such as "Sitting" or "Sitting Down". In these cases, 2D joint localization often fails because of severe self-occlusions, providing the second subnetwork with inaccurate 2D heatmaps. Unless we process   image features as well, 3D localization is bound to fail. This demonstrates the benefit of using information directly from the image for 3D localization versus decoupling the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with state-of-the-art</head><p>Human3.6M: We compare the performance of our approach with previously reported results on Human3.6M. <ref type="table">Table 4</ref> presents the mean per joint 3D error results. Note that some previous works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b9">10]</ref> leverage a sequence of frames for pose prediction rather than a single frame as considered by our approach. Nonetheless, our network achieves state-of-the-art results across the vast majority of actions and outperforms all other methods on average. Since some works use reconstruction error to report results, we also evaluate using this metric in <ref type="table" target="#tab_3">Table 5</ref>. Again, our approach outperforms the other baselines by large margins. HumanEva-I: Our empirical results for HumanEva-I are presented in <ref type="table" target="#tab_6">Table 6</ref>, along with reported results from stateof-the-art approaches. Similar to Human3.6M, our network outperforms all other published approaches. KTH Football II: We report results for our approach on this dataset in  <ref type="figure" target="#fig_2">Figure 4</ref> presents qualitative results for images taken from the aforementioned datasets. Additionally, we demon-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Walking</head><p>Jogging Avg S1 S2 S3 S1 S2 S3 Radwan et al. <ref type="bibr" target="#b24">[25]</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>Our paper addressed the challenging problem of 3D human pose estimation from a single color image. Departing from recent ConvNet approaches, we cast the problem as 3D keypoint localization in a discretized space around the subject. We integrated this volumetric representation with a coarse-to-fine supervision scheme to deal with the high dimensionality and enable iterative processing. We demonstrated that our contributions were crucial to achieve stateof-the-art results on the standard benchmarks with a relative error reduction greater than 30% on average. Furthermore, we used our volumetric representation within a decoupled architecture, making it of practical use for in-the-wild images even when end-to-end training is not feasible.  <ref type="table">Table 4</ref>: Quantitative comparison on Human3.6M. The numbers are the average 3D joint error (mm). Baseline numbers are taken from the respective papers. Note, several approaches use video for prediction rather than a single frame <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b9">10]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>provides an (a) Decoupled architecture (b) Coarse-to-fine architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Schematic comparison of a decoupled architecture versus our coarse-to-fine architecture with intermediate supervision at the coarsest level (2D heatmaps). Blue blocks indicate 3D heatmaps, while green blocks indicate 2D heatmaps. Decoupled architecture: The 2D heatmaps are provided directly as input to the second part of the network, which effectively operates as a 2D-to-3D reconstruction component. Note, no image features are processed in the second component, only information about 2D joint locations. Coarse-to-fine architecture: We use 2D heatmaps as intermediate supervision, which are then combined with image features, effectively carrying information both from the image and the 2D locations of the joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Sample qualitative results for all the datasets used in the empirical evaluation. First row: Human3.6M. Second row: HumanEva. Third row: KTH Football II. Fourth and fifth row: MPII. For each image, the original viewpoint and a novel viewpoint are shown. Red and green indicate left and right, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Coordinate versus volume regression on Hu-man3.6M. The mean per joint error (mm) across all actions and subjects in the test set are shown.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Average</cell></row><row><cell cols="3">Coordinate Regression</cell><cell>112.41</cell></row><row><cell cols="4">Volume Regression (d = 32) 92.23</cell></row><row><cell cols="4">Volume Regression (d = 64) 85.82</cell></row><row><cell cols="2">Naive Stacking</cell><cell cols="2">Coarse-to-Fine</cell></row><row><cell cols="4">L1 L2 L3 L4 Avg. L1 L2 L3 L4 Avg.</cell></row><row><cell>64 64</cell><cell>80.14</cell><cell>1 64 8 64</cell><cell>69.77 77.52</cell></row><row><cell>64 64 64</cell><cell>78.17</cell><cell>1 2 64 1 4 64</cell><cell>68.49 72.02</cell></row><row><cell cols="4">64 64 64 64 75.06 1 2 4 64 64.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Comparison of our coarse-to-fine network using</cell></row><row><cell cols="5">2D heatmaps for intermediate supervision (Coarse-to-Fine)</cell></row><row><cell cols="5">versus a decoupled network with a volumetric representa-</cell></row><row><cell cols="5">tion (Decoupled). The reported results are for the six classes</cell></row><row><cell cols="5">of Human3.6M with the largest difference between the two</cell></row><row><cell cols="4">approaches, as well as the average across all actions.</cell><cell></cell></row><row><cell></cell><cell cols="4">Sanzari et al. [28] Rogez et al. [27] Bogo et al. [6] Ours</cell></row><row><cell>Average</cell><cell>93.2</cell><cell>87.3</cell><cell>82.3</cell><cell>51.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Quantitative comparison on Human3.6M among approaches that report reconstruction error (mm). Baseline numbers are taken from the respective papers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7</head><label>7</label><figDesc>, comparing with relevant methods. Note, Tekin et al.<ref type="bibr" target="#b34">[35]</ref> use video for prediction instead of a single frame, while Burenius et al.<ref type="bibr" target="#b6">[7]</ref> is a multi-view method. Despite this, we outperform the single view approaches and we are competitive with the multi-view results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>75.1 99.8 93.8 79.2 89.8 99.4 89.5 Wang et al. [38] 71.9 75.7 85.3 62.6 77.7 54.4 71.3 Simo-Serra et al. [32] 65.1 48.6 73.5 74.2 46.6 32.2 56.7 Bo et al. [5] 46.4 30.3 64.9 64.5 48.0 38.2 48.7 Kostrikov et al. [16] 44.0 30.9 41.7 57.2 35.0 33.3 40.3 Yasin et al. [41] 35.8 32.4 41.6 46.6 41.4 35.4 38.9 Ours 22.3 19.5 29.7 28.9 21.9 23.8 24.3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Quantitative results on HumanEva-I. The numbers are the mean reconstruction errors (mm). Baseline numbers are taken from the respective papers.</figDesc><table><row><cell></cell><cell cols="3">[7] [7] [7]</cell><cell>[35]</cell><cell>Ours</cell></row><row><cell>Cameras</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>1 (video)</cell><cell>1</cell></row><row><cell>Upper Arms</cell><cell>60</cell><cell cols="2">53 14</cell><cell>74</cell><cell>96</cell></row><row><cell cols="2">Lower Arms 35</cell><cell cols="2">28 06</cell><cell>49</cell><cell>83</cell></row><row><cell>Upper Legs</cell><cell cols="3">100 88 63</cell><cell>98</cell><cell>98</cell></row><row><cell>Lower Legs</cell><cell>90</cell><cell cols="2">82 41</cell><cell>77</cell><cell>88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Quantitative results on KTH Football II. The numbers are the mean PCP scores (the higher the better). Baseline numbers are taken from the respective papers. We indicate how many cameras each approach uses, and highlight the best performance for single view approaches. strate 3D reconstructions on MPII which offers greater visual variety due to its in-the-wild nature. Despite the challenging poses present in the MPII examples, our volumetric representation produces compelling 3D predictions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Project Page: https://www.seas.upenn.edu/˜pavlakos/ projects/volumetric Directions Discussion Eating Greeting Phoning Photo Posing Purchases LinKDE [14] 132.71 183.55 132.37 164.39 162.12 205.94 150.61 171.31 Li et al. [20] -134.13 97.37 122.33 -166.15 --Tekin et al. [35] 102.41 147.72 88.83 125.28 118.02 182.73 112.38 129.17 Zhou et al. [44] 87.36 109.31 87.05 103.16 116.18 143.32 106.88 99.78 Tekin et al.</figDesc><table><row><cell>[34]</cell><cell>-</cell><cell>129.06</cell><cell cols="2">91.43 121.68</cell><cell>-</cell><cell>162.17</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Ghezelghieh et al. [13] 80.30</cell><cell>80.39</cell><cell>78.13</cell><cell>89.72</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Du et al. [10] 85.07</cell><cell>112.68</cell><cell cols="4">104.90 122.05 139.08 135.91</cell><cell>105.93</cell><cell>166.16</cell></row><row><cell cols="2">Park et al. [22] 100.34</cell><cell>116.19</cell><cell cols="4">89.96 116.49 115.34 149.55</cell><cell>117.57</cell><cell>106.94</cell></row><row><cell cols="2">Zhou et al. [43] 91.83</cell><cell>102.41</cell><cell>96.65</cell><cell>98.75</cell><cell cols="2">113.35 125.22</cell><cell>90.04</cell><cell>93.84</cell></row><row><cell cols="2">Ours 67.38</cell><cell>71.95</cell><cell>66.70</cell><cell>69.07</cell><cell>71.95</cell><cell>76.97</cell><cell>65.03</cell><cell>68.30</cell></row><row><cell></cell><cell cols="8">Sitting SittingDown Smoking Waiting WalkDog Walking WalkTogether Average</cell></row><row><cell cols="2">LinKDE [14] 151.57</cell><cell>243.03</cell><cell cols="3">162.14 170.69 177.13</cell><cell>96.60</cell><cell>127.88</cell><cell>162.14</cell></row><row><cell>Li et al. [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>134.13</cell><cell>68.51</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Tekin et al. [35] 138.89</cell><cell>224.9</cell><cell cols="3">118.42 138.75 126.29</cell><cell>55.07</cell><cell>65.76</cell><cell>124.97</cell></row><row><cell cols="2">Zhou et al. [44] 124.52</cell><cell>199.23</cell><cell cols="3">107.42 118.09 114.23</cell><cell>79.39</cell><cell>97.70</cell><cell>113.01</cell></row><row><cell>Tekin et al. [34]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>130.53</cell><cell>65.75</cell><cell>-</cell><cell>-</cell></row><row><cell>Ghezelghieh et al. [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>95.07</cell><cell>82.22</cell><cell>-</cell></row><row><cell cols="2">Du et al. [10] 117.49</cell><cell>226.94</cell><cell cols="3">120.02 117.65 137.36</cell><cell>99.26</cell><cell>106.54</cell><cell>126.47</cell></row><row><cell cols="2">Park et al. [22] 137.21</cell><cell>190.82</cell><cell cols="3">105.78 125.12 131.90</cell><cell>62.64</cell><cell>96.18</cell><cell>117.34</cell></row><row><cell cols="2">Zhou et al. [43] 132.16</cell><cell>158.97</cell><cell cols="2">106.91 94.41</cell><cell>126.04</cell><cell>79.02</cell><cell>98.96</cell><cell>107.26</cell></row><row><cell cols="2">Ours 83.66</cell><cell>96.51</cell><cell>71.74</cell><cell>65.83</cell><cell>74.89</cell><cell>59.11</cell><cell>63.24</cell><cell>71.90</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We gratefully appreciate support through the following grants: NSF-DGE-0966142 (IGERT), NSF-IIP-1439681 (I/UCRC), NSF-IIS-1426840, ARL MAST-CTA W911NF-08-2-0004, ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093, an ONR STTR (Robotics Research), NSERC Discovery, and the DARPA FLA program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Recovering 3D human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3D pose estimation and tracking by detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Twin Gaussian processes for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="28" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Synthesizing training images for boosting human 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinsk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marker-less 3D human motion capture with monocular image sequence and height-maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient ConvNet-based marker-less motion capture in general scenes with a low number of cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimization and filtering for human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning camera viewpoint using CNN to improve 3D body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Ghezelghieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiview body part recognition with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3D human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Determination of 3D human body postures from a single view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="168" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D human pose estimation using convolutional neural networks with 2D pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular image 3D human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reconstructing 3D human pose from 2D image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MoCap-guided data augmentation for 3D pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian image based 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="4" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Loose-limbed people: Estimating 3D human pose and motion using non-parametric belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Joint Model for 2D and 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structured prediction of 3D human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Direct prediction of 3D body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3D people tracking with gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust estimation of 3D human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Single image 3D interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A dualsource approach for 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3D shape estimation from 2D landmarks: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3D human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
