<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-RISE: Graph-Regularized Image Semantic Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>zhenli@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Futang</forename><surname>Peng</surname></persName>
							<email>futangpeng@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
							<email>altimofeev@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
							<email>yitingchen@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxi</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
							<email>tduerig@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
							<email>tomkins@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
							<email>sravi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Ta</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Futang</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxi</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><forename type="middle">Ravi</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-RISE: Graph-Regularized Image Semantic Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<note>ACM Reference Format: Graph-RISE: Graph-Regularized Image Semantic Embedding. In Woodstock &apos;18: ACM Symposium on Neural Gaze Detection, June 03-05, 2018, Woodstock, NY . ACM, New York, NY, USA, Article 4, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Image representations</term>
					<term>• In- formation systems → Web searching and information discovery</term>
					<term>KEYWORDS Image embeddings, semantic understanding, graph regularization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning image representations to capture fine-grained semantics has been a challenging and important task enabling many applications such as image search and clustering. In this paper, we present Graph-Regularized Image Semantic Embedding (Graph-RISE), a large-scale neural graph learning framework that allows us to train embeddings to discriminate an unprecedented O(40M) ultra-fine-grained semantic labels. Graph-RISE outperforms stateof-the-art image embedding algorithms on several evaluation tasks, including image classification and triplet ranking. We provide case studies to demonstrate that, qualitatively, image retrieval based on Graph-RISE effectively captures semantics and, compared to the state-of-the-art, differentiates nuances at levels that are closer to human-perception.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>"Is it possible to learn image content descriptors (a.k.a., embeddings) that capture image semantics and similarity close to human perception?" Learning image embeddings that capture fine-grained semantics is the core of many modern image-related applications such as image search, either querying by traditional keywords or by an example query image <ref type="bibr" target="#b13">[14]</ref>. Learning such embeddings is a challenging task, partly due to the large variations seen among images that belong to the same category or class. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Spectrum of semantic similarity bridge steel red bridge golden gate bridge <ref type="figure">Figure 1</ref>: Spectrum of image semantic similarity. We provide six image examples (two for each granularity) to illustrate the difference from coarser (left) to ultra-fine granularity (right). We refer to ultra fine-grained as "instance-level" to contrast with category-level and fine-grained semantics.</p><p>Several previous works consider category-level image semantics <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>, in which two images are considered semantically similar if they belong to the same category. As illustrated in <ref type="figure">Figure 1</ref>, category-level similarity may not be sufficient for modern visionbased applications such as query-by-image, which often require the distinction of nuances among images within the same category.</p><p>Recently, deep ranking models <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> have been proposed to learn fine-grained image similarity. These ranking models are trained with image triplets, where each entry contains {query image, positive image, negative image}. The goal is to rank (query, positive image) as more similar than (query, negative image); this training formulation can encode distinctions that are as fine-grained as the construction of the triplets allows. In practice, however, it becomes increasingly difficult to generate a large corpus of triplets that encode sufficiently fine-grained distinctions by ensuring the negative image is similar enough to the query image, but not too similar. Furthermore, since human raters need to be involved to provide the triplet ranking ground truth, collecting high quality image triplets for training is costly and labor-intensive. We instead propose moving from triplet learning to a classification framework that learns embeddings capable of associating an image to one of a large number of possible query strings.</p><p>Such an approach produces image embeddings that are predictive of queries that might lead to the image. In addition, we also obtain similarity data between the images themselves, encoding for example the fact that two images were both clicked in a particular setting. This relational data encodes important aspects of human image perception but is not easily encapsulated by labels (i.e., imagelabel pairs for training). To incorporate image-image similarity into the training we employ neural graph learning, in which the model is trained by minimizing the supervised loss combined with a graph regularizer that drives the model to reduce embedding distance between similar image pairs.</p><p>To the best of our knowledge, this work brings the following contributions:</p><p>• Effective embedding learning via large scale classification. We formulate the problem of image embedding learning as an image classification task at an unprecedented scale, with label space (i.e., total number of classes) in O(40M) and the number of images in O(260M). This is the largest scale in terms of number of classes, and one of the largest in terms of images used for learning image embeddings. Furthermore, the proposed model is one of the largest vision models in terms of the number of parameters (see Section 5.1 and Section 5.2). No previous literature has demonstrated the effectiveness of such a large-scale image classification for learning image representation. • Neural graph learning on image representation. We propose a neural graph learning framework that leverages graph structure to regularize the training of deep neural networks. This is the first work deploying large-scale neural graph learning for image representation. We will describe below two techniques to construct image-image graphs based on "co-click" rate and "similar-image click" rate, designed to capture ultra-fine-grained notions of similarity that emerge from human perception of result sets. • Graph-RISE for instance-level semantics. We present Graph-RISE, an image embedding that captures ultra-finegrained, instance-level semantics. Graph-RISE outperforms the state-of-the-art algorithms for learning image embeddings on several evaluations based on k-Nearest-Neighbor (kNN) search and triplet ranking. Experimental results show that Graph-RISE improves the Top-1 accuracy of the kNN evaluation by approximately 2X on the ImageNet dataset and by more than 5X on the iNaturalist dataset. Case studies also show that, qualitatively, Graph-RISE outperforms the state of the art and captures instance-level semantics.</p><p>The remainder of this paper is organized as follows. Section 2 provides related work on learning image embeddings. Section 3 formulates the problem and provides the details of training datasets. Section 4 explains the proposed learning algorithms, followed by Section 5 with the details of network architecture and training infrastructure. Section 6 shows the experimental results and Section 7 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There are several prior works on learning image similarity <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. Most of them focus on category-level image similarity, in which two images are considered to be similar if they belong to the same category. In general, visual and semantic similarities tend to be consistent with each other across category boundaries <ref type="bibr" target="#b5">[6]</ref>. Visual variability within a semantically-defined category still exists, especially for broadly defined categories such as "animal, " or "plant, " as a result of the broad semantic distinctions within such classes. As classes become finer grained, however, the visual distinctions within a class due to natural variations in image capture (angle, lighting, background, etc) become larger relative to the fine distinctions between classes that are semantically closer; hence, new techniques are required.</p><p>For learning fine-grained image similarity, local distance learning <ref type="bibr" target="#b6">[7]</ref> and OASIS <ref type="bibr" target="#b4">[5]</ref> developed ranking models based on hand-crafted features, trained with triplets wherein each entry contains {query image, positive image, negative image} that characterizes the ranking orders based on relative similarity. In <ref type="bibr" target="#b28">[29]</ref>, a DeepRanking model that integrates the deep learning and ranking model is proposed to learn a fine-grained image similarity ranking model directly from images, rather than from hand-crafted features. As discussed above, while these ranking models have been widely used for learning image embeddings, the model performance relies heavily on the quality of triplet samples, which involves pair-wise comparisons that can be costly and labor-intensive to collect. As we will show later in Section 3 and Section 6, Graph-RISE does not require models to be trained by triplets and outperforms the state-of-the-art on capturing image semantics for several evaluation tasks.</p><p>There has also been a significant amount of work on improving image classification to near-human levels <ref type="bibr" target="#b17">[18]</ref> by increasing the representational capacity and the depth of network architectures. See, e.g., VGG-19 <ref type="bibr" target="#b19">[20]</ref>, Inception <ref type="bibr" target="#b24">[25]</ref>, and ResNet <ref type="bibr" target="#b9">[10]</ref>. To support learning such deep networks with millions of parameters, largescale datasets such as ImageNet <ref type="bibr" target="#b12">[13]</ref>, iNaturalist <ref type="bibr" target="#b26">[27]</ref> and YouTube-8M <ref type="bibr" target="#b1">[2]</ref> have played a crucial role. For example, the authors of <ref type="bibr" target="#b14">[15]</ref> demonstrate that the rich mid-level image features learned by Convolutional Neural Networks (CNNs) on ImageNet can be efficiently transferred to other visual recognition tasks with limited amount of training data. Their study suggests that the number of images and the coverage of classes for training in the source task are important for the performance in the target task. In <ref type="bibr" target="#b21">[22]</ref>, the authors reveal a logarithmic relationship between the performance on vision tasks and the amount of training data used for representation learning. In this paper, we share the same observation and further show that when increasing the number of classes to O(40M) with sufficient amount of training data, the purposed Graph-RISE is able to capture instance-level, ultra-fine-grained semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>In this section, we formulate the task of learning an image embedding model, and then provide the details of training dataset used for this task.</p><p>Problem Formulation. Given the following inputs:</p><p>• A labeled set D L that contains image-label pairs (x, y), where label y provides ultra-fine-grained semantics to the corresponding image x. • An unlabeled set D U that contains images without labels. The objective is to find an image embedding model that achieves instance-level semantic understanding. Specifically, let ϕ(·) represent a function that projects an image to a dense vector representing an embedding; given two images x 1 and x 2 , the similarity in the image embedding space is defined according to a distance metric: d(ϕ(x 1 ), ϕ(x 2 )), where d(·) is a distance function (e.g., Euclidean  distance or cosine distance). If x 1 and x 2 belong to the same class, an ideal ϕ(·) minimizes the distance between ϕ(x 1 ) and ϕ(x 2 ), indicating these two images to be semantically similar. <ref type="table" target="#tab_0">Table 1</ref> provides the symbols and the corresponding definitions used throughout this paper.</p><p>Dataset. In order to achieve instance-level semantic understanding, the classes should be ultra-fine-grained. Thus, we created a training dataset D L derived from Google Image Search. It contains approximately 260 million images; selection data are used to characterize how frequently anonymous users selected an image when that image appeared in the results of a particular textual search query. After the characterization, a search query is then treated as the "label" annotated to the corresponding image to provide semantics as labeled samples. Each image is labeled with one or more queries (2.55 queries per image on average), and the total number of unique queries (used as classes) is around forty million. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates six potential image-query pairs 1 . To the best of our knowledge, this is the largest scale of training data for learning image embedding in terms of the number of classes 2 , and one of the largest in terms of the number of training images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Unlabeled dataset D U contains approximately 20 million images, without any annotation or labeling. Unlabeled dataset is mainly for constructing similarity graphs (see Section 4.2) and for evaluation purposes (see Section 6.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LEARNING ALGORITHMS</head><p>In this section, we first introduce the algorithm that takes imagequery pairs as training data in a supervised learning manner. Then we elaborate on neural graph learning, a methodology incorporating graph signals into the learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Discriminative Embedding Learning</head><p>From triplet loss to softmax loss. In order to train image embedding models, metric learning objectives (such as contrastive loss <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref> and triplet loss <ref type="bibr" target="#b28">[29]</ref>), and classification objectives (such as logistic loss and softmax loss) have been widely explored. When using metric learning objectives, collecting high quality samples (e.g., triplets) is often challenging. Furthermore, optimizing metric learning objectives suffers from slow convergence or poor local optima if sampling techniques are inappropriately applied <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>. In order to achieve instance-level semantic understanding, we employ softmax loss for training the image embedding model. When the size of classes is sufficiently large, O(40M) in our case, classification training (with softmax loss) works better than triplet loss <ref type="bibr" target="#b28">[29]</ref>.</p><p>For each training example x, the probability of each label k ∈ {1, . . . , K} in our model is computed via softmax:</p><formula xml:id="formula_0">p(k |x) = exp{z k } K i=1 exp{z i }<label>(1)</label></formula><p>where z i are the logits or unnormalized log probabilities. Here, the z i are computed by adding a fully connected layer on top of the image embeddings, i.e., z i = W T i ϕ(x) + b i , where W i and b i are weights and bias for target label, respectively. Let q(k |x) denote the ground-truth distribution over classes for this training example such that K i=k q(k |x) = 1. As one image may have multiple groundtruth labels, q(k |x) is uniformly distributed to the ground-truth labels. The cross-entropy loss for the example is computed as:</p><formula xml:id="formula_1">ℓ = − K k=1 loд(p(k |x))q(k |x)<label>(2)</label></formula><p>While softmax loss works well when the number of classes is not large (say 10K or 100K), several challenges arise if the number of classes is increased to millions or even billions. First, the computational complexity involved in computing the normalization constant of the target class probability p(k |x) is prohibitively expensive <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>. Second, as the training objective encourages the logits corresponding to the ground-truth labels to be larger than all other logits, the model may learn to assign full probability to the ground-truth labels for each training example. This would result in over-fitting and make the model fail to generalize <ref type="bibr" target="#b24">[25]</ref>.</p><p>Instead of computing the normalization constant for all the classes, we sample a small subset of the target vocabulary L ∈ {1, . . . , K} to compute the normalization constant in p(k |x) for each parameter update. Then, the target label probability can be computed as:</p><formula xml:id="formula_2">p ′ (k |x) = exp{z k } i ∈L exp{z i }<label>(3)</label></formula><p>which leads to much lower computational complexity and allows us to efficiently use Tensor Processing Units (TPUs) <ref type="bibr" target="#b11">[12]</ref> to train this deep image embedding model with sampled softmax. Furthermore, to discourage the model from assigning full probability to the ground-truth labels (and therefore becoming prone to over-fitting), we follow <ref type="bibr" target="#b24">[25]</ref> to "smooth" the label distribution by replacing the label distribution with a mixture of the original ground-truth distribution q(k |x) and the fixed distribution u(k):</p><formula xml:id="formula_3">q ′ (k |x) = (1 − ϵ)q(k |x) + ϵu(k)<label>(4)</label></formula><p>where u(k) is a uniform distribution over the sampled vocabulary u(k) = 1 |L | , and ϵ is a smoothing parameter. Finally, the discriminative objective for training the neural network can be defined as the cross-entropy of the target label probability on the sampled subset and the smoothed ground-truth distribution:</p><formula xml:id="formula_4">L(θ ) = − x i ∈ D L k ∈L i loд(p ′ (k |x i ))q ′ (k |x i )<label>(5)</label></formula><p>where θ denotes the neural network parameters. The ground-truth labels of x i are always selected within the sampled labels L i . In our experiments, we randomly sample 100K classes for each training instance and ϵ is selected to be 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Graph Learning</head><p>While the discriminative objective indeed paves the way to learning an image representation that captures fine-grained semantics, there is more information available in human interactions with images. Many of these additional data sources can be represented as graphs (such as image-image co-occurrence), and yet current vision models (e.g., ResNet) cannot consume such graphs as inputs. Thus, we propose to train the network using graph structure about the relationships among images. In particular, images that are more strongly connected in the graph should reflect stronger semantic similarity based on user feedback (see Section 4.3 for the details of graph construction), and should be closer in the embedding space. To achieve this goal, we deploy graph regularization <ref type="bibr" target="#b3">[4]</ref> to train the neural network for encouraging neighboring images (from the graph) to lie closer in the embedding space. The final graph-regularized objective is the sum of the discriminative loss and the graph regularization:</p><formula xml:id="formula_5">R(θ ) = L(θ ) + α (u,v)∈ E w u,v d ϕ(x u ), ϕ(x v ) Ω(θ )<label>(6)</label></formula><p>where Ω(θ ) denotes the graph regularizer, E represents a set of edges between images, w u,v represents the edge weight between image u and v, ϕ(·) is the representation extracted from the embedding layer <ref type="bibr" target="#b2">3</ref> , d(·) is the distance metric function, and α ≥ 0 is the multiplier (applied on regularization) that controls the trade-off between the discriminative loss and the regularization induced from the graph structure. An illustration of the graph-regularized neural network is given in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>The multiplier α (applied on regularization) controls the balance between the discriminative information (i.e., predictive power) and the contributions of the graph structure (i.e., encoding power). In other words, the neural network is trained to both (a) make accurate classification (or prediction), and (b) encode the graph structure. When α = 0, the proposed objective ignores the graph regularization and degenerates to a neural network with only supervised objective in Eq. <ref type="bibr" target="#b4">(5)</ref>. On the other hand, when p ′ (x) = ϕ(x), where p ′ (x) is the predicted label distribution, we have a label propagation objective as in <ref type="bibr" target="#b16">[17]</ref> by training with the objective using p ′ (x) directly without parameters θ (i.e., no neural network involved), and letting the distance function d(·) and the loss function ℓ(·) to be mean squared errors (MSE). The label propagation objective encourages the learned label distribution p ′ (x) of similar images to be close. Furthermore, the label distribution of a sample is aggregated from its neighbors to adjust its own distribution. Thus, the proposed objective in Eq. (6) could be viewed as a "graph-regularized version of neural network objective" or as a "non-linear version of label propagation. "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Graph Construction</head><p>In this section, we provide the details for constructing graphs used by the regularizer Ω(θ ) in Eq. <ref type="bibr" target="#b5">(6)</ref>. In addition to image-query pairs (described in Section 3) where images are annotated by queries for obtaining semantics, we propose to find "image-image" pairs where the semantics shared by these two images are closer to human perception and beyond textual search queries. Each image is treated as a "vertex" and an image-image pair is treated as an "edge, " together forming a graph that can be included as ancillary training data.</p><p>Specifically, each image-image pair contains one source vertex x u ∈ D L and one target vertex x u ∈ {D L D U }. In this work, we introduce two methods to construct edges: (a) based on co-click rate of the image pair, and (b) based on similar-image click rate of the image pair. The co-click rate of the image pair characterizes how often users select both the source image x u and the target image x v in response to both x u and x v being concurrently identified by search results from a textual search query. This type of imageimage relationship sheds light on the question: "Given that one image is selected from the resulting images, what other images that are sufficiently similar will also be selected?" If the co-click rate between x u and x v is higher than a pre-defined threshold, x u and x v are considered to be sufficiently similar and an edge between them is constructed; the edge weight w u,v is calculated based on the co-click rate. Then x u and x v will be used for calculating the graph regularization Ω(θ ) in Eq. <ref type="bibr" target="#b5">(6)</ref>.</p><p>Different from the co-click rate, the similar-image click rate of the image pair characterizes how often users select the source image x u in response to x u being identified by a search result</p><formula xml:id="formula_6">y u CNN CNN φ(x u ) φ(x v )</formula><p>x v The training objective consists of both the supervised loss L and the graph regularization Ω; minimizing Ω drives the distance between the embeddings of similar images-ϕ(x u ) and ϕ(x v )-to be minimized, which means the neural network is trained to encode the local structure of a graph.</p><p>for a search query using the target image x v (instead of a textual query). This type of image-image relationship sheds the light upon the question: "Given an image issued as the query, what other images that are sufficiently similar will be selected in response to the query image?" Similar with how edges are constructed based on the co-click rate, if the similar-image click between x u and x v is higher than a pre-defined threshold, x u and x v are considered to be sufficiently similar and an edge between them is constructed. Edge weight w u,v is calculated based on the similar-image click rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TRAINING FRAMEWORK</head><p>In this section, we provide the details of network architecture and training infrastructure along with the configurations we used in this work. <ref type="figure">Figure 4</ref> illustrates the proposed network architecture. The main model is the 101-layer ResNet (referred as ResNet-101) <ref type="bibr" target="#b9">[10]</ref>. Compared to Inception <ref type="bibr" target="#b23">[24]</ref>, ResNet-101 has larger model capacity, which yields more than 2% of performance improvement on our internal metric for embedding evaluation. While the major architecture of ResNet-101 remains unchanged, several detailed configurations have been modified. The input layer is modified to take enlarged input images from 224×224 to 289×289 pixels. The output 10×10×2K feature map is first avg pooled to 4×4×2K using a 4×4 kernel of stride 2, and then flattened and projected to 64dimensional layer representing image embeddings. The activation function is ReLU-6 4 . Finally, a softmax layer is added to produce a multinomial distribution across 40 million classes (e.g., queries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Network Architecture</head><p>During training, both training samples and their neighbors provided from graphs (described in Section 4.3) are fed into the model for enabling graph regularization; the 64-dimensional embedding <ref type="bibr" target="#b3">4</ref> ReLU-6 computes Rectified Linear Unit as: min(max(x, 0), 6). layer is selected as the target layer to be regularized as described in Eq. <ref type="bibr" target="#b5">(6)</ref>. Furthermore, 100K out of 40 millions labels are sampled via an important sampling technique <ref type="bibr" target="#b2">[3]</ref> for each parameter update Eq. (3). Finally, batch normalization is applied during training.</p><p>During the inference phase, a 64-dimensional L2 normalized embedding 5 is generated for each input image as a new, semanticallymeaningful representation. Note that neighbors and graph regularization is not required when making inference (see the flow in red in <ref type="figure">Figure 4</ref>). In addition to the embedding, the queries with the top-k predicted probabilities are also outputted. Since the focus of this paper is image embedding, the output queries will largely be ignored in the rest of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Infrastructure</head><p>We implement the network architecture described in Section 5.1 using TensorFlow <ref type="bibr" target="#b0">[1]</ref>. The details of training configurations are as follows. We select the batch size to be 24, and the momentum <ref type="bibr" target="#b15">[16]</ref> as the optimizer; the initial learning rate is 0.001 and will be decayed with an exponential rate of 0.9 every 100,000 steps. The label smoothing ϵ is 0.1. The multiplier for applying L2 regularization (a.k.a. "weight decay") is 0.00004. For configurations related to graph regularization, the multiplier for applying graph regularization α is 1.0, and the distance function d(·) in Eq. (6) is selected to be cosine distance <ref type="bibr" target="#b5">6</ref> . For constructing graphs, the threshold is set to 0.1, and we combine the edges (approximately 50 million edges, built from co-clicks and similar-image clicks) into one graph for calculating regularizer Ω(θ ) in Eq. 6.</p><p>Since our model is one of the largest vision models in terms of number of parameters (40M ×64 plus the parameters of ResNet-101   <ref type="figure">Figure 4</ref>: An illustration of the Graph-RISE framework. Flow in red is added to enable graph regularization and required only during training. In the input layer, a labeled image is associated with one of its neighbor images, which can be either labeled or unlabeled, and then fed into the ResNet together with its neighbor image. Then, the image embeddings generated from ResNet are used to both (a) compute the cross-entropy loss and (b) graph regularization.</p><formula xml:id="formula_7">φ(x u ) φ(x v ) Input Layer p ′ (x)</formula><p>architecture), together with the scale of the training dataset, using TPUs <ref type="bibr" target="#b11">[12]</ref> to train the model is the only feasible solution. The training is distributed to 8×8 TPU cores, and takes two weeks to converge from scratch after 5M steps. Training with graph regularization costs additional computation that grows with the number of neighbors used (in this work, approximately 40% training time increase due to the use of neighbors). To reduce the computation, we compare the performance of two models: one is trained without graph regularization from scratch; the other is trained with graph regularization with α set to zero until it is converged, and then set α = 1 to fine-tune the model. We find that the performance of the first model (w/o graph regularization) is slightly better in the beginning of the training, and two models achieves almost the same performance after 4M steps. With these setting, the fine-tuning model (the one with graph regularization) takes approximately 2 additional days to train for 500K extra steps until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we explain the details of evaluation setup, and then show the experimental results for performance evaluation. We also provide the case studies for the qualitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Setup</head><p>We are interested in providing image embeddings with instancelevel semantics, such that the similarity between embeddings approximates the relationships (of images) in terms of semantics. To evaluate the performance of the proposed image embedding model, we conduct both k-Nearest-Neighbor (kNN) search and triplet evaluation as metrics; these two are the most popular methods used for evaluating embedding models.</p><p>For kNN evaluation, we conduct experiments on ImageNet <ref type="bibr" target="#b12">[13]</ref> and iNaturalist <ref type="bibr" target="#b26">[27]</ref> datasets, and then report two metrics in the experiments: Top-1 and Top-5 accuracy, where Top-k accuracy calculates the percentage of query images that find at least 1 imagefrom the top k searched image results-carrying the exact same labels as the query images. For the ImageNet dataset, the images in the validation set are used as the query images, and the training set is used as the index set to search for top-k results. For the iNaturalist dataset, for each class two images are randomly sampled to construct the query set, and the remainder of the images in that class are used as the index set.</p><p>For triplet evaluation, we follow the evaluation strategy in <ref type="bibr" target="#b28">[29]</ref> to sample triplets (A, P, N )-representing Anchor, Positive, Negative images-from Google Image Search and ask human raters to verify if P is more semantically closer to A than N. We sample the triplets in a way such that A and P have the same or a very similar instancelevel concept, and N is slightly less relevant ("hard-negative"). Each triplet is independently ranked by three raters and only the triplets receiving unanimous scores are used for evaluation. Assume positive image P is rated to be more similar to anchor image A than negative image N, the prediction of a model is considered to be accurate if the following condition holds:</p><formula xml:id="formula_8">η + d(ϕ(A), ϕ(P)) − d(ϕ(A), ϕ(N )) &lt; 0<label>(7)</label></formula><p>where η is the hyper-parameter that controls the margin between the distance of two image projections. Two triplet datasets are created for calculating triplet evaluation metrics: (a) Product-Instance-Triplets (PIT) is a dataset designed to focus on evaluating the semantic concepts of images in the commercial product vertical, which consists of 10,000 triplets; (b) Generic-Instance-Triplets (GIT) is a dataset focusing on evaluating ImageNet <ref type="bibr" target="#b12">[13]</ref> iNaturalist <ref type="bibr" target="#b26">[27]</ref> Top-1 Top-  <ref type="table">Table 3</ref>: Performance comparisons (in %) via triplet accuracy (η = 0) on the internal evaluation datasets.</p><p>the semantic concepts of general images, including all possible image verticals from Google Image Search, consisting of 14,000 triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Model Comparisons</head><p>We compare the proposed method with the following state-of-theart models:</p><p>• DeepRanking model <ref type="bibr" target="#b28">[29]</ref> that employs triplet loss on multiscale Inception network architecture <ref type="bibr" target="#b24">[25]</ref> with an online triplet sampling algorithm. • Inception network architecture <ref type="bibr" target="#b24">[25]</ref> that employs sampled softmax loss over 8 millions labels. • ResNet-101 network architecture <ref type="bibr" target="#b9">[10]</ref> that employs sampled softmax loss over 8 millions and 40 millions labels (referred as ResNet (8M) and ResNet (40M) in <ref type="table">Table 2</ref>, respectively). • Graph-RISE model based on ResNet-101 network architecture proposed in Section 4.2. The input layers in DeepRanking model and Inception model both use 224×224 image pixels, while the ResNet-101 and Graph-RISE use 289×289. Label smoothing is applied to all the classification-based models. When the graph regularization multiplier α = 0, Graph-RISE is equivalent to the ResNet-101 model. In all the experiments, the Euclidean (L2) distance of the embedding vectors extracted from the penultimate layer-the layer before the final softmax or ranking layer-is used as similarity measure. To evaluate the effectiveness of image embeddings, no individual fine-tuning is performed for each dataset, and all the experiments are conducted directly based on the learned embeddings of the input images. <ref type="table">Table 2</ref> provides the performance comparisons (in terms of percentage) on kNN evaluations, and <ref type="table">Table 3</ref> shows the triplet evaluations (also in terms of percentage). From these results, we have several observations. First, Graph-RISE significantly outperforms the previous state-of-the-art <ref type="bibr" target="#b28">[29]</ref> and other models without graph regularization in all the evaluation criteria. We attribute this to the fact that Graph-RISE leverages the graph structure via neural graph learning to drive the embeddings of similar images to be as close as possible. Notice that, compared to the previous state-of-the-art <ref type="bibr" target="#b28">[29]</ref>, Graph-RISE improves the Top-1 accuracy by almost 2X (from 35.2% to 68.29%) on ImageNet dataset, and by more than 5X (from 6.03% to 31.12%) on the iNaturalist dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance Evaluation</head><p>Second, compared to the Inception network architecture, training image embeddings with ResNet-101 improves the performance for most datasets. This confirms the observation from <ref type="bibr" target="#b21">[22]</ref> that to fully exploit O(300M) images, a higher capacity model is required to achieve better performance. Furthermore, we confirm that sampled softmax is an effective technique to train image embedding model with datasets that have extremely-large label space; by comparing Inception (8M) and DeepRanking <ref type="bibr" target="#b28">[29]</ref> (both based on Inception network), we observe that sampled softmax helps achieve better performance in triplet loss, even if DeepRanking directly aims at optimizing the triplet accuracy.</p><p>Moreover, increasing the number of labels (from 8M to 40M) significantly improves the kNN accuracy. We conjecture that the labels in 40M are more fine-grained than 8M, and therefore the learned image embeddings also need to capture fine-grained semantics in order to distinguish these 40M labels. In addition, we find that training ResNet-101 using larger input size (289×289) instead of smaller input size (224×224) also helps improve the model performance   <ref type="figure">Figure 7</ref>: Retrieval results for 6 randomly-chosen query images. For each query image, we provide the Top-4 images retrieved (from 1 billion images) by DeepRanking, ResNet (40M), and Graph-RISE. Each retrieval result is rated and color-coded by human raters: retrieved images colored by green are rated to be strongly similar with the query image, whereas images colored by red are rated to be not (or somewhat) similar. Notice that images retrieved by Graph-RISE generally conform to experts' ratings.</p><p>(from 85.13% to 86.7%, in terms of accuracy on PIT triplet evaluation), since larger input size encapsulates more detailed information from training images. <ref type="figure">Figure 5</ref> and <ref type="figure">Figure 6</ref> depict the comparisons of triplet evaluation among four models: DeepRanking, Inception (8M), ResNet (8M) and Graph-RISE, on PIT dataset and GIT dataset respectively. Note that ResNet (40M) is ignored in the figures since the curves of ResNet (40M) and Graph-RISE are visually difficult to distinguish. In these two figures, x-axis is "Margin" and y-axis is "Racall" rate (the higher the better). "Margin" is the η in Eq. 7 representing the margin between the distance of "Anchor-Negative pair" and the distance of "Anchor-Positive pair. " A large margin means "the Negative image is further away from the Anchor image than the Positive image. " "Recall" rate represents the percentage of triplets that satisfy η + d(ϕ(A), ϕ(P)) &lt; d(ϕ(A), ϕ(N )). From these two figures, the performance of Graph-RISE is consistently better the other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Qualitative Analysis</head><p>Next, we evaluate the quality of images retrieved by DeepRanking <ref type="bibr" target="#b28">[29]</ref>, ResNet (40M), and Graph-RISE models. Given a randomlyselected query image, each method retrieves the most semantically similar images from an index containing one billion images. The top-ranked results are sent out to be rated by human experts. <ref type="figure">Figure 7</ref> illustrates the retrieval results for 6 randomly-selected query images; for each query image we provide the Top-4 images retrieved. The images colored by green are rated to be strongly similar with the query image, whereas the images colored by red are rated to be not (or somewhat) similar. Compared to other models, images retrieved by Graph-RISE generally conform to experts' ratings, meaning that Graph-RISE captures the semantic meaning of images more effectively as judged by human raters. For example, given a query image of "scroll with ribbon" (top-left in <ref type="figure">Figure 7)</ref>, the top three images retrieved by Graph-RISE are also "scroll with ribbon" (with similar colors, textures and shapes), and are rated as strongly similar by human experts. Another example is a query image of a landmark (top-right in <ref type="figure">Figure 7</ref>); Graph-RISE is able to retrieve images of the exact same landmark, while the other methods are only able to retrieve images of somewhat similar buildings.</p><p>In addition, we observe that the images retrieved by DeepRanking tend to be only visually similar to the query images, rather than semantically similar. This is probably because generating triplets that reflect the semantic concepts is very difficult, especially when the classes are ultra-fine-grained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we present Graph-RISE to answer the motivational question: "Is it possible to learn image content descriptors (a.k.a., embeddings) that capture image semantics and similarity close to human perception?" Graph-RISE confirms that ultra-fine-grained, instancelevel semantics can be captured by image embeddings extracted from training a sophisticated image classification model with largescale data: O(40M) classes and O(260M) images. Graph-RISE is also the first image embedding model based on neural graph learning that leverages graph structures of similar images to capture semantics close to human image perception. We conduct extensive experiments on several evaluation tasks based on both kNN search and triplet ranking, and experimental results confirm that Graph-RISE consistently and significantly outperforms the state-ofthe-art methods. Qualitative analysis of image retrieval tasks also demonstrates that Graph-RISE effectively captures instance-level semantics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Woodstock ' 18 ,</head><label>18</label><figDesc>June 03-05, 2018, Woodstock, NY © 2018 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9999-9/18/06. https://doi.org/10.1145/1122445.1122456 Category-level (coarse-grained) Fine-grained level Instance level (ultra fine-grained)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Six potential samples of image-query pairs. Each image is labeled with the corresponding textual search query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>An illustration of a graph-regularized neural network. The image similarity subgraph of a training image x u (with the ground-truth labels y u ) is factorized into image-image pairs, where the neighbor image x v is semantically similar to x u .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>PIT triplet evaluation on Recall v.s. Margin. GIT triplet evaluation on Recall v.s. Margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>ResNet</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>List of symbols.</figDesc><table><row><cell>Symbol</cell><cell cols="2">Definition and description</cell></row><row><cell>(x, y)</cell><cell>Image-label pair</cell></row><row><cell cols="3">D L , D U Labeled dataset, unlabeled dataset</cell></row><row><cell>K</cell><cell cols="2">Total number of classes</cell></row><row><cell>E</cell><cell>Set of edges</cell></row><row><cell>w u,v</cell><cell>Edge weight</cell></row><row><cell>η</cell><cell cols="2">Margin for triplet distance</cell></row><row><cell>ϕ(·)</cell><cell cols="2">Embedding mapping function</cell></row><row><cell>d (·)</cell><cell>Distance function</cell></row><row><cell cols="3">p(·), q(·) Probability and ground-truth distribution</cell></row><row><cell>L(·)</cell><cell>Loss function</cell></row><row><cell>Ω(·)</cell><cell>Graph regularizer</cell></row><row><cell>θ</cell><cell>Model parameters</cell></row><row><cell cols="2">white husky puppy</cell><cell>flowers mauritius</cell></row><row><cell cols="2">golden gate bridge</cell><cell>green salad with prawn</cell></row><row><cell cols="2">lamborghini aventador red</cell><cell>taipei 101 at night</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, "image-query pairs" and "image-label pairs" are used interchangeably since queries are used as labels.<ref type="bibr" target="#b1">2</ref> In prior arts (such as<ref type="bibr" target="#b28">[29]</ref>), the training dataset contains up to O(15M ) samples with O(100K ) classes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In general, the embedding layer refers to the layer right before the softmax layer.Graph-RISE: Graph-Regularized Image Semantic EmbeddingWoodstock '18, June 03-05, 2018, Woodstock, NY</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">L2 normalization is not applied on embedding during training as it makes the training hard to converge.<ref type="bibr" target="#b5">6</ref> We have also experimented with using the Euclidean (L2) distance as d (·); when using Euclidean distance with α = 0.01, it achieves almost the same performance as using cosine distance with α = 1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank Dr. Sergey Ioffe and Dr. Thomas Leung for the reviews and suggestions. We also thank Expander, Image Understanding and several related teams for the technical support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="713" to="722" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Graph Learning: Training Neural Networks Using Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramavajjala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Search and Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual and semantic similarity in imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image retrieval and classification using local distance functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 12th International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE 12th International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.100</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2006.100" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2007</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Computer Architecture</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean O&amp;apos;</forename><surname>Malley</surname></persName>
		</author>
		<title level="m">Visual content retrieval. US Patent 8,983</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">941</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large scale distributed semi-supervised learning using streaming approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning invariance through imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Spiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2729" to="2736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning image similarity from flickr groups using stochastic intersection kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 12th International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE 12th International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="428" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>ceedings of the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
