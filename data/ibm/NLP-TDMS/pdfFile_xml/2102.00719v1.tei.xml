<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Transformer Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
							<email>danieln@theator.io</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><forename type="middle">Zohar</forename><surname>Dotan</surname></persName>
							<email>dotan@theator.io</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asselmann</forename><surname>Theator</surname></persName>
						</author>
						<title level="a" type="main">Video Transformer Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents VTN, a transformer-based framework for video recognition. Inspired by recent developments in vision transformers, we ditch the standard approach in video action recognition that relies on 3D ConvNets and introduce a method that classifies actions by attending to the entire video sequence information. Our approach is generic and builds on top of any given 2D spatial network. In terms of wall runtime, it trains 16.1× faster and runs 5.1× faster during inference while maintaining competitive accuracy compared to other state-of-the-art methods. It enables whole video analysis, via a single end-to-end pass, while requiring 1.5× fewer GFLOPs. We report competitive results on Kinetics-400 and present an ablation study of VTN properties and the trade-off between accuracy and inference speed. We hope our approach will serve as a new baseline and start a fresh line of research in the video recognition domain. Code and models will be available soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Attention matters. For almost a decade, ConvNets have ruled the computer vision field <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6]</ref>. Applying deep ConvNets produced state-of-the-art results in many visual recognition tasks, i.e., image classification <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26]</ref>, object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22]</ref>, semantic segmentation <ref type="bibr" target="#b19">[20]</ref>, object instance segmentation <ref type="bibr" target="#b13">[14]</ref>, face recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23]</ref> and video action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref>. But, recently this domination is starting to crack as transformer-based models are showing promising results in many of these tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Video recognition tasks also rely heavily on ConvNets. In order to handle the temporal dimension, the fundamental approach is to use 3D ConvNets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. In contrast to other studies that add the temporal dimension straight from the input clip level, we aim to move apart from 3D networks. We use state-of-the-art 2D architectures to learn the spatial feature representations and add the temporal information later in the data flow by using attention mechanisms on top of the resulting features. Our approach input only <ref type="figure">Figure 1</ref>: Video Transformer Network architecture. Connecting three modules: A 2D spatial backbone (f (x)), used for feature extraction. Followed by a temporal attentionbased encoder (Longformer in this work), that uses the feature vectors (φ i ) combined with a position encoding. The [CLS] token is processed by a classification MLP head to get the final class prediction.</p><p>RGB video frames and without any bells and whistles (e.g., optical flow, streams lateral connections, multi-scale inference, multi-view inference, longer clips fine-tuning, etc.) achieves comparable results to other state-of-the-art models.</p><p>Video recognition is a perfect candidate for Transformers. Similar to language modeling, in which the input words or characters are represented as a sequence of tokens <ref type="bibr" target="#b28">[29]</ref>, videos are represented as a sequence of images (frames). However, this similarly is also a limitation when it comes to processing long sequences. Like long documents, long videos are hard to process. Even a 10 seconds video, such as those in the Kinetics-400 benchmark <ref type="bibr" target="#b16">[17]</ref>, are processed in recent studies as short, 2 seconds, clips.</p><p>But how does this clip-based inference would work on much longer videos (i.e., movie films, sports events, or sur- <ref type="figure">Figure 2</ref>: Extracting 16 frames evenly from a video of the abseiling category in the Kinetics-400 dataset <ref type="bibr" target="#b16">[17]</ref>. Analyzing the video's full context and attending to the relevant parts is much more intuitive than analyzing several clips built around specific frames, as many of these frames might lead to false predictions. gical procedures)? It seems counterintuitive that the information in a video of hours, or even a few minutes, can be grasped using only a snippet clip of a few seconds. Nevertheless, current networks are not designed to share longterm information across the full video.</p><p>VTN's temporal processing component is based on a Longformer <ref type="bibr" target="#b0">[1]</ref>. This type of transformer-based model can process a long sequence of thousands of tokens. The attention mechanism proposed by the Longformer makes it feasible to go beyond short clip processing and maintain global attention, which attends to all tokens in the input sequence.</p><p>In addition to long sequence processing, we also explore an important trade-off in machine learning -speed vs. accuracy. Our framework demonstrates a superior balance of this trade-off, both during training and also at inference time. In training, even though wall runtime per epoch is either equal or greater, compared to other networks, our approach requires much fewer passes of the training dataset to reach its maximum performance; end-to-end, compared to state-or-the-art networks, this results in a 16.1× faster training. At inference time, our approach can handle both multi-view and full video analysis while maintaining similar accuracy. In contrast, other networks' performance significantly decreases when analyzing the full video in a single pass. In terms of GFLOPS x Views, their inference cost is considerably higher than those of VTN, which concludes to a 1.5× fewer GFLOPS and a 5.1× faster validation wall runtime.</p><p>Our framework's structure components are modular ( <ref type="figure">Fig. 1)</ref>. First, the 2D spatial backbone can be replaced with any given network. The attention-based module can stack up more layers, more heads or can be set to a different Transformers model that can process long sequences. Finally, the classification head can be modified to facilitate different video-based tasks, like temporal action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Spatial-temporal networks. Most recent studies in video recognition suggested architectures that are based on 3D ConvNets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, a two-stream architecture was used, one stream for RGB inputs and another for Optical Flow (OF) inputs. Residual connections are inserted into the two-stream architecture to allow a direct link between RGB and OF layers. The idea of inflating 2D ConvNets into their 3D counterpart (I3D) was introduced in <ref type="bibr" target="#b2">[3]</ref>. I3D takes 2D ConvNets and expands its layers into 3D. Therefore it allows to leverage pre-trained state-of-the-art image recognition architectures in the spatial-temporal domain and apply them for video-based tasks.</p><p>Non-local Neural Networks (NLN) <ref type="bibr" target="#b29">[30]</ref> introduced a non-local operation, a type of self-attention, that computes responses based on relationships between different locations in the input signal. NLN demonstrated that the core attention mechanism in Transformers can produce good results on video tasks, however it is confined to processing only short clips. In order to extract long temporal context, <ref type="bibr" target="#b31">[32]</ref> introduced a long-term feature bank that acts as the entire video memory and a Feature Bank Operator (FBO) that computes interactions between short-term and longterm features. However, it requires precomputed features, and it is not efficient enough to support end-to-end training of the feature extraction backbone.</p><p>SlowFast <ref type="bibr" target="#b10">[11]</ref> explored a network architecture that operates in two pathways and different frame rates. Lateral connections fuse the information between the slow pathway, focused on the spatial information, and the fast pathway focused on temporal information.</p><p>The X3D study <ref type="bibr" target="#b9">[10]</ref> builds on top of SlowFast. It argues that in contrast to image classification architectures, which have been developed via a rigorous evolution, the video architectures have not been explored in detail, and historically are based on expending image-based networks to fit the temporal domain. X3D introduces a set of networks that progressively expand in different axes, e.g., temporal, frame rate, spatial, width, bottleneck width, and depth. Compared to SlowFast, it offers a lightweight network (in terms of GFLOPS and parameters) with similar performance.</p><p>Transformers in computer vision. The Transformers architecture <ref type="bibr" target="#b28">[29]</ref> reached state-of-the-art results in many NLP tasks, making it the de-facto standard. Recently, Transform-ers are starting to disrupt the field of computer vision, which traditionally depends on deep ConvNets. Studies like ViT and DeiT for image classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>, DETR for object detection and panoptic segmentation <ref type="bibr" target="#b1">[2]</ref>, and VisTR for video instance segmentation <ref type="bibr" target="#b30">[31]</ref> are some examples showing promising results when using Transformers in the computer vision field. Binding these results with the sequential nature of video makes it a perfect match for Transformers.</p><p>Applying Transformers on long sequences. BERT <ref type="bibr" target="#b6">[7]</ref> and its optimized version RoBERTa <ref type="bibr" target="#b18">[19]</ref> are transformerbased language representation models. They are pre-trained on large unlabeled text and later fine-tuned on a given target task. With minimal modification, they achieve state-of-theart results on a variety of NLP tasks.</p><p>One significant limitation of these models, and Transformers in general, is their ability to process long sequences. This is due to the self-attention operation, which has a complexity of O(n 2 ) per layer (n is sequence length) <ref type="bibr" target="#b28">[29]</ref>.</p><p>Longformer <ref type="bibr" target="#b0">[1]</ref> addresses this problem and enables lengthy document processing by introducing an attention mechanism with a complexity of O(n). This attention mechanism combines a local-context self-attention, performed by a sliding window, and task-specific global attention.</p><p>Similar to ConvNets, stacking up multiple windowed attention layers results in a larger receptive field. This property of Longformer gives it the ability to integrate information across the entire sequence. The global attention part focuses on pre-selected tokens (like the [CLS] token) and can attend to all other tokens across the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Transformer Network</head><p>Video Transformer Network (VTN) is a generic framework for video recognition. It operates with a single stream of data, from the frames level up to the objective task head.</p><p>In the scope of this study, we demonstrate our approach using the action recognition task by classifying an input video to the correct action category.</p><p>The architecture of VTN is modular and composed of three consecutive parts. A 2D spatial feature extraction model (spatial backbone), a temporal attention-based encoder, and a classification MLP head. <ref type="figure">Fig. 1</ref> demonstrates our architecture layout.</p><p>VTN is scalable in terms of video length during inference, and enables the processing of very long sequences. Due to memory limitation, we suggest several types of inference methods. (1) Processing the entire video in an endto-end manner. (2) Processing the video frames in chunks, extracting features first, and then applying them to the temporal attention-based encoder. <ref type="formula">(3)</ref>  features in advance and then feed them to the temporal encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatial backbone</head><p>The spatial backbone operates as a learned feature extraction module. It can be any network that works on 2D images, either deep or shallow, pre-trained or not, convolutional-or transformers-based. And its weights can be either fixed (if pre-trained) or trained during the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal attention-based encoder</head><p>As suggested by <ref type="bibr" target="#b28">[29]</ref>, we use a Transformer model architecture that applies attention mechanisms to make global dependencies in a sequence data. However, Transformers are limited by the number of tokens they can process at the same time. This limits their ability to process long inputs, such as videos, and incorporate connections between distant information.</p><p>In this work, we propose to process the entire video at once during inference. We use an efficient variant of selfattention, that is not all-pairwise, called Longformer <ref type="bibr" target="#b0">[1]</ref>. Longformer operates using sliding window attention that enables a linear computation complexity. The sequence of feature vectors of dimension d backbone (Sec. 3.1) is fed to the Longformer encoder. These vectors act as the 1D tokens embedding in the standard Transformer setup.</p><p>Like in BERT <ref type="bibr" target="#b6">[7]</ref> we add a special classification token ([CLS]) in front of the features sequence. After propagating the sequence through the Longformer layers, we use the final state of the features related to this classification token as the final representation of the video and apply it to the given classification task head. Longformer also maintains global attention on that special [CLS] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification MLP head</head><p>Similar to <ref type="bibr" target="#b7">[8]</ref>, the classification token (Sec. 3.2) is processed with an MLP head to provide a final predicted cat-egory. The MLP head contains two linear layers with a GELU non-linearity and Dropout between them. The input token representation is first processed with a Layer normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Looking beyond a short clip context</head><p>The common approach in recent studies for video action recognition uses 3D-based networks. During inference, due to the addition of a temporal dimension, these networks are limited by memory and runtime to clips of a small spatial scale and a low number of frames. In <ref type="bibr" target="#b2">[3]</ref>, the authors use the whole video during inference, averaging predictions temporally. More recent studies that achieved state-of-the-art results processed numerous, but relatively short, clips during inference. In <ref type="bibr" target="#b29">[30]</ref>, inference is done by sampling ten clips evenly from the full-length video and average the softmax scores to achieve the final prediction. SlowFast <ref type="bibr" target="#b10">[11]</ref> follows the same practice and introduces the term "view" -a temporal clip with a spatial crop. SlowFast uses ten temporal clips with three spatial crops at inference time; thus, 30 different views are averaged for the final prediction. X3D <ref type="bibr" target="#b9">[10]</ref> follows the same practice, but in addition, it uses larger spatial scales to achieve its best results on 30 different views.</p><p>This common practice of multi-view inference is somewhat counterintuitive, especially when handling long videos. A more intuitive way is to "look" at the entire video context before deciding on the action, rather than viewing only small portions of it. <ref type="figure">Fig. 2</ref> shows 16 frames extracted evenly from a video of the abseiling category. The actual action is obscured or not visible in several parts of the video; this might lead to a false action prediction in many views. The potential in focusing on the segments in the video that are most relevant is a powerful ability. However, full video inference produces poor performance in methods that were trained using short clips ( <ref type="table">Table 6 and</ref>  <ref type="bibr" target="#b6">7)</ref>. In addition, it is also limited in practice due to hardware, memory, and runtime aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Video Action Recognition with VTN</head><p>In order to evaluate our approach and the impact of context attention on video action recognition, we use several spatial backbones pre-trained on 2D images.</p><p>ViT-B-VTN. Combining the state-of-the-art image classification model, ViT-Base <ref type="bibr" target="#b7">[8]</ref>, as the backbone in VTN. We use a ViT-Base network that was pre-trained on ImageNet-21K. Using ViT as the backbone for VTN produces an endto-end transformers-based network that uses attention both for the spatial and temporal domains.</p><p>R50/101-VTN. As a comparison, we also use a standard 2D ResNet-50 and ResNet-101 networks <ref type="bibr" target="#b14">[15]</ref>, pre-trained on ImageNet.</p><p>DeiT-B/BD/Ti-VTN. Since ViT-Base was trained on ImageNet-21K we also want to compare VTN by using similar networks trained on ImageNet. We use the recent work of <ref type="bibr" target="#b26">[27]</ref> and apply DeiT-Tiny, DeiT-Base, and DeiT-Base-Distilled as the backbone for VTN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Training. The spatial backbones we use were pre-trained on either ImageNet or ImageNet-21k. The Longformer and the MLP classification head were randomly initialized from a normal distribution with zero mean and 0.02 std. We train the model end-to-end using video clips. These clips are formed by choosing a random frame as the starting point, then sampling 2.56 or 5.12 seconds as the video's temporal footprint. The final clip frames are subsampled uniformly to a fixed number of frames N (N = 16, 32), depending on the setup.</p><p>For the spatial domain, we randomly resize the shorter side of all the frames in the clip to a [256, 320] scale and randomly crop all frames to 224 × 224. Horizontal flip is also applied randomly on the entire clip.</p><p>The ablation experiments were done on a 4-GPU machine. Using a batch size of 16 for the ViT-VTN (on 16 frames per clip input) and a batch size of 32 for the R50/101-VTN. We use an SGD optimizer with an initial learning rate of 10 −3 and a different learning rate reduction policy, steps-based for the ViT-VTN versions and cosine schedule decay for the R50/101-VTN versions. In order to report the wall runtime, we use an 8-V100-GPU machine.</p><p>Since we use 2D models as the spatial backbone, we can manipulate the input clip shape x clip ∈ R B×C×T ×H×W by stacking all frames from all clips within a batch to create a single frames batch of shape x ∈ R (B·T )×C×H×W . Thus, during training, we propagate all batch frames in a single forward-backward pass.</p><p>For the Longformer, we use an effective attention window of size 32, which was applied for each layer. Two other hyperparameters are the dimensions set for the Hidden size and the FFN inner hidden size. These are a direct derivative of the spatial backbone. Therefore, in R50/101-VTN we use 2048 and 4096, respectively, and for ViT-B-VTN we use 768 and 3072, respectively. In addition, we apply Attention Dropout with a probability of 0.1. We also explore the impact of the number of Longformer layers.</p><p>The positional embedding information is only relevant for the temporal attention-based encoder <ref type="figure">(Fig. 1)</ref>. We explore three positional embedding approaches <ref type="table" target="#tab_3">(Table 3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>): (1)</head><p>Learned positional embedding -since a clip is represented using frames taken from the full video sequence, we can learn an embedding that uses as input the frame location (index) in the original video, giving the Transformer informa-   tion regarding the position of the clip in the entire sequence;</p><p>(2) Fixed absolute encoding -we use a similar method to the one in DETR <ref type="bibr" target="#b1">[2]</ref>, and modified it to work on the temporal axis only; and (3) No positional embedding -no information is added in the temporal dimension, but we still use the global position to mark the special [CLS] token position.</p><p>Inference. In order to show a comparison between different models, we use both the common practice of inference in multi-views and a full video inference approach (Sec. 3.4).</p><p>In the multi-view approach, we sample 10 clips evenly from the video. For each clip, we first resize the shorter side to 256, then take three crops of size 224 × 224 from the left, center, and right. The result is 30 views per video, and the final prediction is an average of all views' softmax scores.</p><p>In the full video inference approach, we read all the frames in the video. Then, we align them for batching purposes, by either sub-or up-sampling, to 250 frames uniformly. In the spatial domain, we resize the shorter side to 256 and take a center crop of size 224 × 224.   In the validation set, we are missing one video. To test our data's validity and compare it to previous studies, we evaluated the SlowFast-8X8-R50 model, published in PyS-lowFast <ref type="bibr" target="#b8">[9]</ref>, on our validation data. We got 76.45% top1accuracy vs. the reported 77%, thus a drop of 0.55%. This drop might be related to different FFmpeg encoding and rescaling of the videos. From this point forward, when comparing to other networks, we report results taken from the original studies except when we evaluate them on the full video inference in which we use our validation set. All our approach results are reported based on our validation set. Spatial backbone variations. We start by examining how different spatial backbone architectures impact VTN performance. <ref type="table" target="#tab_0">Table 1</ref> shows a comparison of different VTN variants and the pretrain dataset the backbone was first trained on. ViT-B-VTN is the best performing model and reaches 78.6% top-1 accuracy and 93.7% top-5 accuracy. The pretraining dataset is important. Using the same ViT backbone,  only changing between DeiT (pre-trained on ImageNet) and ViT (pre-trained on ImageNet-21K) we get an improvement in the results Longformer depth. Next, we explore how the number of attention layers impacts the performance. Each layer has 12 attention heads and the backbone is ViT-B. <ref type="table" target="#tab_2">Table 2</ref> shows the validation top-1 and top-5 accuracy for 1, 3, 6, and 12 attention layers. The comparison shows that the difference in performance is small. This is counterintuitive to the fact that deeper is better. It might be related to the fact that Kinetics-400 videos are relatively short, around 10 seconds. We believe that processing longer videos will benefit from a large receptive field obtained by using a deeper Longformer.</p><p>Longformer positional embedding. In <ref type="table" target="#tab_3">Table 3</ref> we compare three different positional embedding methods, focusing on learned, fixed, and no positional embedding. All ver-sions are done with a ViT-B-VTN, a temporal footprint of 5.12 seconds, and a clip size of 16 frames. Surprisingly, the one without any positional embedding achieved slightly better results than the fixed and learned versions.</p><p>As this is an interesting result, we also use the same trained models and evaluate them after randomly shuffling the input frames only in the validation set videos. This is done by first taking the unshuffled frame embeddings, then shuffle their order, and finally add the positional embedding. This raised another surprising finding, in which the shuffle version gives better results, reaching 78.9% top-1 accuracy on the no positional embedding version. Even in the case of learned embeddings it does not have a diminishing effect. Similar to the Longformer depth, we believe that this might be related to the relatively short videos in Kinetics-400, and longer sequences might benefit more from positional information. We also argue that this could mean that Kinetics-400 is primarily a static frame, appearance based classification problem rather than a motion problem.</p><p>Temporal footprint and number of frames in a clip. We also explore the effect of using longer clips in the temporal domain and compare a temporal footprint of 2.56 vs. 5.12 seconds. And also how the number of frames in the clip impact the network performance. The comparison is done on a ViT-B-VTN with one attention layer in the Longformer. <ref type="table" target="#tab_5">Table 4</ref> shows that top-1 and top-5 accuracy are similar, implying that VTN is agnostic to these hyperparameters.</p><p>Finetune the 2D spatial backbone. Instead of finetuning the spatial backbone, by continuing the backpropagation process, when training VTN, we can use a frozen 2D network solely for feature extraction. <ref type="table" target="#tab_6">Table 5</ref> shows the validation accuracy when training a ViT-B-VTN with three attention layers with and without also training the backbone. Fine-tuning the backbone improves the results by 7% in Kinetics-400 top-1 accuracy.  <ref type="table">Table 6</ref>: To measure the overall time needed to train each model, we observe how long it takes to train a single epoch and how many epochs are required to achieve the best performance. We compare these numbers to the validation top-1 and top-5 accuracy on Kinetics-400 and the number of parameters per model. To measure the training wall runtime, we ran a single epoch for each model, on the same 8-V100-GPU machine, with a 16GB memory per GPU. The models mark by (*) were taken from the PySlowFast GitHub repository <ref type="bibr" target="#b8">[9]</ref>. We report the accuracy as written in the Model Zoo, which was done using the 30 multi-view inference approach. To measure the wall runtime, we used the code base of PySlowFast. To calculate the SlowFast-16X8-R101 time on the same GPU machine, we used a batch size of 16. The number of epochs is reported, when possible, based on the original model paper. All other models, including the NL I3D, are trained using our codebase and evaluated with a full video inference approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does attention matter? A key component in our ap</head><p>VTN perceives the full video sequence. To convey this impact we train two VTN networks, using three layers in the Longformer, but with a single head for each layer. In one network the head is trained as usual, while in the second network instead of computing attention based on query/key dot products and softmax, we replace the attention matrix with a hard-coded uniform distribution that is not updated during back-propagation. <ref type="figure" target="#fig_1">Fig. 4</ref> shows the learning curves of these two networks. Although the training has a similar trend, the learned attention performs better. In contrast, the validation of the uniform attention collapses after a few epochs demonstrating poor generalization of that network. Further, we visualize the [CLS] token attention weights by processing the same video from <ref type="figure">Fig. 2</ref> with the single-head trained network and depicted, in <ref type="figure" target="#fig_0">Fig. 3</ref>, all the weights of the first attention layer aligned to the video's frames. Interestingly, the weights are much higher in segments related to the abseiling category. Training and validation runtime. An interesting observation we make concerns the training and validation wall runtime of our approach. Although our networks have more parameters, and therefore, are longer to train and test, they are actually much faster to converge and reach their best performance earlier. Since they are evaluated using a single view of all video frames, they are also faster during val- <ref type="figure">Figure 5</ref>: Kinetics-400 learning curves for our implementation of NL I3D (blue) vs. DeiT-B-VTN (red). We show the top-1 accuracy for the train set (solid line) and the validation set (dash line). Top-1 accuracy during training is calculated based on a single random clip, while during validation we use the full video inference approach. DeiT-B-VTN shows high performance in every step of the training and validation process. It reaches its best accuracy after only 25 epochs compared to the NL I3D that needs 50 epochs.</p><p>idation. <ref type="table">Table 6</ref>   <ref type="table">Table 7</ref>: Comparing the number of GFLOPs during inference. The models mark by (*) were taken from the PySlowFast GitHub repository <ref type="bibr" target="#b8">[9]</ref>. We reproduced the SlowFast-8X8-R50 results by using the repository and our Kinetics-400 validation set and got 76.45% compared to the reported value of 77%. When running this model using a full video inference approach, we get a significant drop in performance of about 8%. We did not run the SlowFast-16X8-R101 because it was not published. The inference GFLOPs is reported by multiplying the number of views with the GFLOPs calculated per view. ViT-B-VTN with one layer achieves 78.6% top-1 accuracy, a 0.3% drop compared to SlowFast-16X8-R101 while using 1.5× fewer GFLOPS.</p><p>SlowFast model, our ViT-B-VTN with one layer achieves almost the same results but completes an epoch faster while requiring fewer epochs. This accumulates to a 16.1× faster end-to-end training. The validation wall runtime is also 5.1× faster due to the full video inference approach.</p><p>To better demonstrate the fast convergence of our approach, we wanted to show an apples-to-apples comparison of different training and evaluating curves for various models. However, since other methods use the multi-view inference only post-training, but use a single view evaluation while training their models, this was hard to achieve. Thus, to show such comparison and give the reader additional visual information, we trained a NL I3D (pre-trained on ImageNet) with a full video inference protocol during validation (using our codebase and reproduced the original model results). We compare it to DeiT-B-VTN which was also pre-trained on ImageNet. <ref type="figure">Fig. 5</ref> shows that the VTNbased network converges to better results much faster than the NL I3D and enables a much faster training process compared to 3D-based networks.</p><p>Final inference computational complexity. Finally, we examine what is the final inference computational complexity for various models by measuring GFLOPs. Although other models need to evaluate multiple views to reach their highest performance, ViT-B-VTN performs almost the same for both inference protocols. <ref type="table">Table 7</ref> shows a significant drop of about 8% when evaluating the SlowFast-8X8-R50 model using the full video approach. In contrast, ViT-B-VTN maintains the same performance while requiring, end-to-end, fewer GFLOPs at inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a modular transformer-based framework for video recognition tasks. Our approach introduces an efficient way to evaluate videos at scale, both in terms of computational resources and wall runtime. It allows full video processing during test time, making it more suitable for dealing with long videos. Although current video classification benchmarks are not ideal for testing long-term video processing ability, hopefully, in the future, when such datasets become available, models like VTN will show even larger improvements compared to 3D ConvNets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustrating all the single-head first attention layer weights of the [CLS] token vs. 16 frames pulled evenly from a video. High weight values are represented by a warm color (yellow) while low values by a cold color (blue). The video's segments in which abseiling category properties are shown (e.g., shackle, rope) exhibit higher weight values compared to segments in which non-relevant information appears (e.g., shoes, people). The model prediction is abseiling for this video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Evaluating the influence of attention on the training (solid line) and validation (dashed line) curves for Kinetics-400. A similar ViT-B-VTN with three Longformer layers is trained for both cases, and we modify the attention heads between a learned one (red) and a fixed uniform version (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(In Appendix A. we show a few more examples).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Extracting all frames'</cell></row></table><note>VTN performance on Kinetics-400 validation set for different backbone variations. A full video inference is used. We show top-1 and top-5 accuracy. We report what pre-training was done for each backbone and the re- lated single-crop top-1 accuracy on ImageNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">Comparing different numbers of attention layers</cell></row><row><cell cols="4">in the Longformer. The results are top-1 and top-5 accuracy</cell></row><row><cell cols="4">on Kinetics-400 validation set using the full video inference</cell></row><row><cell>approach.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">positional embedding method shuffle top-1 top-5</cell></row><row><cell>learned positional embedding</cell><cell>-</cell><cell>78.4</cell><cell>93.5</cell></row><row><cell>learned positional embedding</cell><cell></cell><cell>78.8</cell><cell>93.6</cell></row><row><cell>fixed absolute encoding</cell><cell>-</cell><cell>78.3</cell><cell>93.7</cell></row><row><cell>fixed absolute encoding</cell><cell></cell><cell>78.5</cell><cell>93.7</cell></row><row><cell>no positional embedding</cell><cell>-</cell><cell>78.6</cell><cell>93.7</cell></row><row><cell>no positional embedding</cell><cell></cell><cell>78.9</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Evaluating the impact of different types of po- sitional embedding methods, with and without randomly shuffling the input frames. We conducted this experiment with an older experimental setup using a different learning rate scheduler, so the results of the learned positional em- bedding without shuffle are slightly different from what we report in other tables: 78.4% vs. 78.6%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparing the impact of temporal footprint size (in seconds) and the number of frames in a clip on Kinetics-400 validation set.</figDesc><table><row><cell cols="3">Finetune the backbone top-1 top-5</cell></row><row><cell>-</cell><cell>71.6</cell><cell>90.3</cell></row><row><cell></cell><cell>78.6</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Training a ViT-B-VTN with three attention layers with and without fine-tuning the 2D backbone. All other hyperparameters remain the same.</figDesc><table><row><cell>5. Experiments</cell></row><row><cell>5.1. Ablation Experiments on Kinetics-400</cell></row><row><cell>Kinetics-400 dataset. The original Kinetics-400 dataset</cell></row><row><cell>[17] consists of 246,535 training videos and 19,761 vali-</cell></row><row><cell>dation videos. Each video is labeled with one of 400 hu-</cell></row><row><cell>man action categories, curated from YouTube videos. Since</cell></row><row><cell>some YouTube links are expired, we could only download</cell></row><row><cell>234,584 of the original dataset, thus missing 11,951 videos</cell></row><row><cell>from the training set, which are about 5%. This leads to a</cell></row><row><cell>slight drop in performance of about 0.5% 1 .</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/ video-nonlocal-net/blob/master/DATASET.md</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Ross Girshick for providing valuable feedback on this manuscript and for helpful suggestions on several experiments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 3</ref><p>, we illustrate the [CLS] token weights of the first attention layer. We show some examples for successful predictions (a, b, c) and some of the failure modes of our approach (d, e).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyslowfast</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/slowfast" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">End-toend video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
