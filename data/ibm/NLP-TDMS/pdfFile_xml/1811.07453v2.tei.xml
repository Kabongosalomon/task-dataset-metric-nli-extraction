<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">THE PYTORCH-KALDI SPEECH RECOGNITION TOOLKIT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titouan</forename><surname>Parcollet</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Université d&apos;Avignon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CIFAR Fellow</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">THE PYTORCH-KALDI SPEECH RECOGNITION TOOLKIT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech recognition</term>
					<term>deep learning</term>
					<term>Kaldi</term>
					<term>PyTorch</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The availability of open-source software is playing a remarkable role in the popularization of speech recognition and deep learning. Kaldi, for instance, is nowadays an established framework used to develop state-of-the-art speech recognizers. PyTorch is used to build neural networks with the Python language and has recently spawn tremendous interest within the machine learning community thanks to its simplicity and flexibility.</p><p>The PyTorch-Kaldi project aims to bridge the gap between these popular toolkits, trying to inherit the efficiency of Kaldi and the flexibility of PyTorch. PyTorch-Kaldi is not only a simple interface between these software, but it embeds several useful features for developing modern speech recognizers. For instance, the code is specifically designed to naturally plug-in user-defined acoustic models. As an alternative, users can exploit several pre-implemented neural networks that can be customized using intuitive configuration files. PyTorch-Kaldi supports multiple feature and label streams as well as combinations of neural networks, enabling the use of complex neural architectures. The toolkit is publicly-released along with a rich documentation and is designed to properly work locally or on HPC clusters.</p><p>Experiments, that are conducted on several datasets and tasks, show that PyTorch-Kaldi can effectively be used to develop modern state-of-the-art speech recognizers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Over the last years, we witnessed a progressive improvement and maturation of Automatic Speech Recognition (ASR) technologies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, that have reached unprecedented performance levels and are nowadays used by millions of users worldwide.</p><p>A key role in this technological breakthrough is being played by deep learning <ref type="bibr" target="#b2">[3]</ref>, that contributed to overcoming previous speech recognizers based on Gaussian Mixture Models (GMMs). Beyond deep learning, other factors have played a role in the progress of the field. A number of speech-related projects such as AMI <ref type="bibr" target="#b3">[4]</ref> and DIRHA <ref type="bibr" target="#b4">[5]</ref> and speech recognition challenges such as CHiME <ref type="bibr" target="#b5">[6]</ref>, Babel, and Aspire, have remarkably fostered the progress in ASR. The public distribution of large datasets such as Librispeech <ref type="bibr" target="#b6">[7]</ref> has also played an important role to establish common evaluation frameworks and tasks.</p><p>Among the others factors, the development of open-source software such as HTK <ref type="bibr" target="#b7">[8]</ref>, Julius <ref type="bibr" target="#b8">[9]</ref>, CMU-Sphinx, RWTH-ASR <ref type="bibr" target="#b9">[10]</ref>, LIA-ASR <ref type="bibr" target="#b10">[11]</ref> and, more recently, the Kaldi toolkit <ref type="bibr" target="#b11">[12]</ref> have further helped popularize ASR, making both research and development of novel ASR applications significantly easier.</p><p>Kaldi currently represents the most popular ASR toolkit. It relies on finite-state transducers (FSTs) <ref type="bibr" target="#b12">[13]</ref> and provides a set of C++ libraries for efficiently implementing state-of-the-art speech recognition systems. Moreover, the toolkit includes a large set of recipes that cover all the most popular speech corpora. In parallel to the development of this ASR-specific software, several general-purpose deep learning frameworks, such as Theano <ref type="bibr" target="#b13">[14]</ref>, TensorFlow <ref type="bibr" target="#b14">[15]</ref>, and CNTK <ref type="bibr" target="#b15">[16]</ref>, have gained popularity in the machine learning community. These toolkits offer a huge flexibility in the neural network design and can be used for a variety of deep learning applications.</p><p>PyTorch <ref type="bibr" target="#b16">[17]</ref> is an emerging python package that implements efficient GPU-based tensor computations and facilitates the design of neural architectures, thanks to proper routines for automatic gradient computation. An interesting feature of PyTorch lies in its modern and flexible design, that naturally supports dynamic neural networks. In fact, the computational graph is dynamically constructed on-thefly at running time rather than being statically compiled.</p><p>The PyTorch-Kaldi project aims to bridge the gap between Kaldi and PyTorch 1 . Our toolkit implements acoustic models in PyTorch, while feature extraction, label/alignment computation, and decoding are performed with Kaldi, making it suitable to develop stateof-the-art DNN-HMM speech recognizers. PyTorch-Kaldi natively supports several DNNs, CNNs, and RNNs models. Combinations between deep learning models, acoustic features, and labels are also supported, enabling the use of complex neural architectures. For instance, users can employ a cascade between CNNs, LSTMs, and DNNs, or run in parallel several models that share some hidden layers. Users can also explore different acoustic features, context duration, neuron activations (e.g., ReLU, leaky ReLU), normalizations (e.g., batch <ref type="bibr" target="#b17">[18]</ref> and layer normalization <ref type="bibr" target="#b18">[19]</ref>), cost functions, regularization strategies (e.g, L2, dropout <ref type="bibr" target="#b19">[20]</ref>), optimization algorithms (e.g., Adam <ref type="bibr" target="#b20">[21]</ref>, RMSPROP), and many other hyper-parameters of an ASR system through simple edits of configuration files.</p><p>The toolkit is designed to make the integration of user-defined acoustic models as simple as possible. In practice, users can embed their deep learning model and conduct ASR experiments even without being fully familiar with the complex speech recognition pipeline. The toolkit can perform computations on both local machines and HPC cluster, and supports multi-gpu training, recovery strategy, and automatic data chunking.</p><p>The experiments, conducted on several datasets and tasks, have shown that PyTorch-Kaldi makes it possible to easily develop competitive state-of-the-art speech recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE PYTORCH-KALDI PROJECT</head><p>Some other speech recognition toolkits have been recently developed using the python language. PyKaldi <ref type="bibr" target="#b21">[22]</ref>, for instance, is an easy-to-use Python wrapper for the C++ code of Kaldi and OpenFst libraries. Differently from our toolkit, however, the current version of the PyKaldi does not provide several pre-implemented and readyto-use neural models. Another python project is ESPnet <ref type="bibr" target="#b22">[23]</ref>. ES-Pnet is an end-to-end speech processing toolkit, mainly focuses on end-to-end speech recognition and end-to-end text-to-speech. The main difference with our project is the current version of PyTorch-Kaldi implements hybrid DNN-HMM speech recognizers. An overview of the architecture adopted in PyTorch-Kaldi is reported in <ref type="figure" target="#fig_0">Fig. 1</ref>. The main script run exp.py is written in python and manages all the phases involved in an ASR system, including feature and label extraction, training, validation, decoding, and scoring. The toolkit is detailed in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Configuration file</head><p>The main script takes as input a configuration file in INI format 2 , that is composed of several sections. The section [Exp] specifies some high-level information such as the folder used for the experiment, the number of training epochs, the random seed. It also allows users to specify whether the experiments have to be conducted on a CPU, GPU, or on multiple GPUs. The configuration file continues with the [dataset * ] sections, that specify information on features and labels, including the paths where they are stored, the characteristics of the context window <ref type="bibr" target="#b23">[24]</ref>, and the number of chunks in which the speech dataset must be split. The neural models are described in the [architecture * ] sections, while the [model] section defines how these neural networks are combined. The latter section exploits a simple meta-language that is automatically interpreted by the run exp.py script. Finally, the configuration file defines the decoding parameters in the [decoding] section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Features</head><p>The feature extraction is performed with Kaldi, that natively provides c++ libraries (e.g., compute-mfcc-feats, compute-fbank-feats, compute-plp-feats) to efficiently extract the most popular speech <ref type="bibr" target="#b1">2</ref> The configuration file is fully described in the project documentation. recognition features. The computed coefficients are stored in binary archives (with extension .ark) and are later imported into the python environment using the kaldi-io utilities inherited from the kaldi-io-for-python project <ref type="bibr" target="#b2">3</ref> . The features are then processed by the function load-chunk, that performs context window composition, shuffling, as well as mean and variance normalization. As outlined before, PyTorch-Kaldi can manage multiple feature streams. For instance, users can define models that exploit combinations of MFCCs, FBANKs, PLP, and fMLLR <ref type="bibr" target="#b24">[25]</ref> coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Labels</head><p>The main labels used for training the acoustic model derive from a forced alignment procedure between the speech features and the sequence of context-dependent phone states computed by Kaldi with a phonetic decision tree. To enable multi-task learning, PyTorch-Kaldi supports multiple labels. For instance, it is possible to jointly load both context-dependent and context-independent targets and use the latter ones to perform monophone regularization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. It is also possible to employ models based on an ecosystem of neural networks performing different tasks, as done in the context of joint training between speech enhancement and speech recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> or in the context of the recently-proposed cooperative networks of deep neural networks <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Chunk and Mini-batch Composition</head><p>PyTorch-Kaldi automatically splits the full dataset into a number of chunks, which are composed of labels and features randomly sampled from the full corpus. Each chunk is then stored into the GPU or CPU memory and processed by the neural training algorithm run nn.py. The toolkit dynamically composes different chunks at each epoch. A set of mini-batches are then derived from them. Minibatches are composed of few training examples that are used for gradient computation and parameter optimization.</p><p>The way mini-batches are gathered strongly depends on the typology of the neural network. For feed-forward models, the minibatches are composed of randomly shuffled features and labels sampled from the chunk. For recurrent networks, the minibatches must be composed of full sentences. Different sentences, however, are likely to have different duration, making zero-padding necessary to form mini-batches of the same size. PyTorch-Kaldi sorts the speech sequences in ascending order according to their lengths (i.e., short sentences are processed first). This approach minimizes the need of zero-paddings and turned out to be helpful to avoid possible biases on batch normalization statistics. Moreover, it has been shown useful to slightly boost the performance and to improve the numerical stability of gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">DNN acoustic modeling</head><p>Each minibatch is processed by a neural network implemented with PyTorch, that takes as input the features and as outputs a set of posterior probabilities over the context-dependent phone states. The code is designed to easily plug-in customized models. As reported in the pseudo-code reported in <ref type="figure" target="#fig_1">Fig. 2</ref>, the new model can be simply defined by adding a new class into the neural nets.py. The class must be composed of an initialization method, that specifies the parameters with their initialization, and a forward method that defines the computations to perform. As an alternative, a number of pre-defined state-of-the-art neural models are natively implemented within the toolkit. The current version supports standard MLPs, CNNs, RNNs, LSTM, and GRU models. Moreover, it supports some advanced recurrent architectures, such as the recently-proposed Light GRU <ref type="bibr" target="#b30">[31]</ref> and twin-regularized RNNs <ref type="bibr" target="#b31">[32]</ref>. The SincNet model <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> is also implemented to perform speech recognition from raw waveform directly. The hyperparameters of the model (such as learning rate, number of neurons, number of layers, dropout factor, etc.) can be tuned using a utility that implements the random search algorithm <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Decoding and Scoring</head><p>The acoustic posterior probabilities generated by the neural network are normalized by their prior before feeding the HMM-based decoder of Kaldi. The decoder merges the acoustic scores with the language probabilities derived by an n-gram language model and tries to retrieve the sequence of words uttered in the speech signal using a beam-search algorithm. The final Word-Error-Rate (WER) score is computed with the NIST SCTK scoring toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP</head><p>In the following sub-sections, the corpora, and the DNN setting adopted for the experimental activity are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Corpora and Tasks</head><p>The first set of experiments was performed with the TIMIT corpus, considering the standard phoneme recognition task (aligned with the Kaldi s5 recipe <ref type="bibr" target="#b11">[12]</ref>).</p><p>To validate our model in a more challenging scenario, experiments were also conducted in distant-talking conditions with the DIRHA-English dataset <ref type="bibr" target="#b3">4</ref>  <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Training was based on the original WSJ-5k corpus (consisting of 7, 138 sentences uttered by 83 speakers) that was contaminated with a set of impulse responses measured in a domestic environment <ref type="bibr" target="#b36">[37]</ref>. The test phase was carried out with the real part of the dataset, consisting of 409 WSJ sentences uttered in the aforementioned environment by six native American speakers.</p><p>Additional experiments were conducted with the CHiME 4 dataset <ref type="bibr" target="#b5">[6]</ref>, that is based on speech data recorded in four noisy environments (on a bus, cafe, pedestrian area, and street junction). The training set is composed of 43, 690 noisy WSJ sentences recorded by five microphones (arranged on a tablet) and uttered by a total of 87 speakers. The test set ET-real considered in this work is based on 1, 320 real sentences uttered by four speakers, while the subset DT-real has been used for hyperparameter tuning. The CHiME experiments were based on the single channel setting <ref type="bibr" target="#b5">[6]</ref>.</p><p>Finally, experiments were performed with the LibriSpeech <ref type="bibr" target="#b6">[7]</ref> dataset. We used the training subset composed of 100 hours and the dev-clean set for the hyperparameter search. Test results are reported on the test-clean part using the fglarge decoding graph inherited from the Kaldi s5 recipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DNN setting</head><p>The experiments consider different acoustic features, i.e., 39 MFCCs (13 static+∆+∆∆), 40 log-mel filter-bank features (FBANKS), as well as 40 fMLLR features <ref type="bibr" target="#b24">[25]</ref> (extracted as reported in the s5 recipe of Kaldi), that were computed using windows of 25 ms with an overlap of 10 ms. The feed-forward models were initialized according to the Glorot's scheme <ref type="bibr" target="#b37">[38]</ref>, while recurrent weights were initialized with orthogonal matrices <ref type="bibr" target="#b38">[39]</ref>. Recurrent dropout was used as a regularization technique <ref type="bibr" target="#b39">[40]</ref>. Batch normalization was adopted for feed-forward connections only, as proposed in <ref type="bibr" target="#b40">[41]</ref>. The optimization was done using the RMSprop algorithm running for 24 epochs. The performance on the development set was monitored after each epoch and the learning rate was halved when the relative performance improvement went below 0.1%. The main hyperparameters of the model (i.e., learning rate, number of hidden layers, hidden neurons per layer, dropout factor, as well as the twin regularization term λ) were tuned on the development datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BASELINES</head><p>In this section, we discuss the baselines obtained with TIMIT, DIRHA, CHiME, and LibriSpeech datasets. As a showcase to illustrate the main functionalities of the PyTorch-Kaldi toolkit, we first report the experimental validation conducted on TIMIT. <ref type="table" target="#tab_0">Table 1</ref> shows the performance obtained with several feedforward and recurrent models using different features. To ensure a more accurate comparison between the architectures, five experiments varying the initialization seeds were conducted for each model and feature. The table thus reports the average phone error rates (PER) <ref type="bibr" target="#b4">5</ref> . Results show that, as expected, fMLLR features outperform MFCCs and FBANKs coefficients, thanks to the speaker adaptation process. Recurrent models significantly outperform the standard MLP one, especially when using LSTM, GRU, and Li-GRU architecture, that effectively address gradient vanishing through multiplicative gates. The best result (PER=14.2%) is obtained with the Li-GRU model <ref type="bibr" target="#b30">[31]</ref>, that is based on a single gate and thus saves 33% of the computations over a standard GRU. <ref type="table" target="#tab_1">Table 2</ref> details the impact of some popular techniques implemented in PyTorch-Kaldi for improving the ASR performance. The   first row (Baseline) reports the performance achieved with a basic recurrent model, where powerful techniques such as dropout and batch normalization are not adopted. The second row highlights the performance gain that is achieved when progressively increasing the sequence length during training. In this case, we started the training by truncating the speech sentence at 100 steps (i.e, approximately 1 second of speech) and we progressively double the maximum sequence duration at every epoch. This simple strategy generally improves the system performance since it encourages the model to first focus on short-term dependencies and learn longer-term ones only at a later stage. The third row shows the improvement achieved when adding recurrent dropout. Similarly to <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, we applied the same dropout mask for all the time steps to avoid gradient vanishing problems. The fourth line, instead, shows the benefits derived from batch normalization <ref type="bibr" target="#b17">[18]</ref>. Finally, the last line shows the performance achieved when also applying monophone regularization <ref type="bibr" target="#b26">[27]</ref>. In this case, we employ a multi-task learning strategy by means of two softmax classifiers: the first one estimates context-dependent states, while the second one predicts monophone targets. As observed in <ref type="bibr" target="#b26">[27]</ref>, our results confirm that this technique can successfully be used as an effective regularizer. The experiments discussed so far are based on single neural models. In <ref type="table" target="#tab_2">Table 3</ref> we compare our best Li-GRU system with a more complex architecture based on a combination of feed-forward and recurrent models fed by a concatenation of features. To the best of our knowledge, the PER=13.8% achieved by the latter system yields the best-published performance on the TIMIT test-set.</p><p>Previous achievements were based on features computed with Kaldi. However, within PyTorch-Kaldi users can employ their own features. <ref type="table" target="#tab_3">Table 4</ref> shows the results achieved with convolutional models fed by standard FBANKs coefficients or by the raw waveform directly. The standard CNN based on raw samples performs similarly to the one fed by FBANK features. A performance improvement is observed with SincNet <ref type="bibr" target="#b32">[33]</ref>, whose effectiveness in speech recognition is here highlighted for the first time. We now extend our experimental validation to other datasets. With this regard, <ref type="table" target="#tab_4">Table 5</ref> shows the performance achieved on DIRHA, CHiME, and Librispeech (100h) datasets. The <ref type="table">Table  consistently</ref> shows better performance with the Li-GRU model, confirming our previous achievements on TIMIT. The results on DIRHA and CHiME show the effectiveness of the proposed toolkit also in noisy condition. To give a comparison, the best Kaldi baseline proposed in egs/chime4/s5 1ch has a WER(%)=18.1%. An endto-end system trained with ESPnet reaches a WER(%)=44.99%, confirming how critical is end-to-end speech recognition is challenging acoustic conditions. DIRHA represents another very challenging task, that is characterized by the presence of considerable levels of noise and reverberation. The WER=23.9% obtained on this dataset represents the best performance published so-far on the single-microphone task. Finally, the performance obtained with Librispeech outperforms the corresponding p-norm Kaldi baseline (W ER = 6.5%) on the considered 100 hours subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>This paper described the PyTorch-Kaldi project, a new initiative that aims to bridge the gap between Kaldi and PyTorch. The toolkit is designed to make the development of an ASR system simpler and more flexible, allowing users to easily plug-in their customized acoustic models. PyTorch-Kaldi also supports combinations of neural architectures, features, and labels, allowing users to possibly employ complex ASR pipelines. The experiments have confirmed that PyTorch-Kaldi can achieve state-of-the-art results in some popular speech recognition tasks and datasets.</p><p>The current version of the PyTorch-Kaldi is already publiclyavailable along with a detailed documentation. The project is still in its initial phase and we invite all potential contributors to participate in it. We hope to build a community of developers larger enough to progressively maintain, improve, and expand the functionalities of our current toolkit. In the future, we plan to increase the number of pre-implemented models, support neural language model training/rescoring, sequence discriminative training, online speech recognition, as well end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENT</head><p>We would like to thank Maurizio Omologo, Enzo Telk, and Antonio Mazzaldi for their helpful comments. This research was enabled in part by support provided by Calcul Québec and Compute Canada.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>An overview of the PyTorch-Kaldi architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Adding a user model into PyTorch-Kaldi. c l a s s my NN ( nn . Module ) : d e f i n i t ( s e l f , o p t i o n s ) : s u p e r ( my NN , s e l f ) . i n i t ( ) # D e f i n i t i o n o f Model P a r a m e t e r s # P a r a m e t e r I n i t i a l i z a t i o n d e f f o r w a r d ( s e l f , m i n i b a t c h ) : # D e f i n i t i o n o f Model C o m p u t a t i o n s r e t u r n [ o u t p u t p r o b ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PER(%) obtained for the test set of TIMIT with various neural architectures.</figDesc><table><row><cell></cell><cell cols="3">MFCC FBANK fMLLR</cell></row><row><cell>MLP</cell><cell>18.2</cell><cell>18.7</cell><cell>16.7</cell></row><row><cell>RNN</cell><cell>17.7</cell><cell>17.2</cell><cell>15.9</cell></row><row><cell>LSTM</cell><cell>15.1</cell><cell>14.3</cell><cell>14.5</cell></row><row><cell>GRU</cell><cell>16.0</cell><cell>15.2</cell><cell>14.9</cell></row><row><cell>Li-GRU</cell><cell>15.3</cell><cell>14.9</cell><cell>14.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>PER(%) obtained on TIMIT when progressively applying some techniques implemented within PyTorch-Kaldi.</figDesc><table><row><cell></cell><cell cols="4">RNN LSTM GRU Li-GRU</cell></row><row><cell>Baseline</cell><cell>16.5</cell><cell>16.0</cell><cell>16.6</cell><cell>16.3</cell></row><row><cell>+ Incr. Seq. length</cell><cell>16.6</cell><cell>15.3</cell><cell>16.1</cell><cell>15.4</cell></row><row><cell>+ Recurrent Dropout</cell><cell>16.4</cell><cell>15.1</cell><cell>15.4</cell><cell>14.5</cell></row><row><cell>+ Batch Normalization</cell><cell>16.0</cell><cell>14.8</cell><cell>15.3</cell><cell>14.4</cell></row><row><cell>+ Monophone Reg.</cell><cell>15.9</cell><cell>14.5</cell><cell>14.9</cell><cell>14.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>PER(%) obtained by combining multiple neural networks and acoustic features.</figDesc><table><row><cell>Architecture</cell><cell>Features</cell><cell>PER (%)</cell></row><row><cell>Li-GRU</cell><cell>fMLLR</cell><cell>14.2</cell></row><row><cell cols="2">MLP+Li-GRU+MLP MFCC+FBANK+fMLLR</cell><cell>13.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>PER(%) obtained with standard convolutional and with the SincNet architectures.</figDesc><table><row><cell>Model</cell><cell>Features</cell><cell>PER (%)</cell></row><row><cell>CNN</cell><cell>FBANK</cell><cell>18.3</cell></row><row><cell>CNN</cell><cell>Raw waveform</cell><cell>18.1</cell></row><row><cell cols="2">SincNet Raw waveform</cell><cell>17.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>WER(%) obtained for the DIRHA, CHiME, and Lib-riSpeech (100h) datasets with various neural architectures.</figDesc><table><row><cell></cell><cell cols="3">DIRHA CHiME LibriSpeech</cell></row><row><cell>MLP</cell><cell>26.1</cell><cell>18.7</cell><cell>6.5</cell></row><row><cell>LSTM</cell><cell>24.8</cell><cell>15.5</cell><cell>6.4</cell></row><row><cell>GRU</cell><cell>24.8</cell><cell>15.2</cell><cell>6.3</cell></row><row><cell>Li-GRU</cell><cell>23.9</cell><cell>14.6</cell><cell>6.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">github.com/mravanelli/pytorch-kaldi/. arXiv:1811.07453v2 [eess.AS] 15 Feb 2019</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">github.com/vesis84/kaldi-io-for-python</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This dataset is distributed by the Linguistic Data Consortium (LDC).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Standard deviations range between 0.15 and 0.2 for all the experiments.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Automatic Speech Recognition -A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning for Distant Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Unitn</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interpretation of Multiparty Meetings the AMI and Amida Projects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HSCMA</title>
		<meeting>of HSCMA</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="115" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The DIRHA simulated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cristoforetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagmueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2629" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The third CHiME Speech Separation and Recognition Challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<title level="m">HTK -Hidden Markov Model Toolkit</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recent development of open-source speech recognition engine julius</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of APSIPA-ASC</title>
		<meeting>of APSIPA-ASC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RASR -The RWTH Aachen University Open Source Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lehnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nolden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiesler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The lia speech recognition system: From 10xrt to 1xrt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Linarès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nocera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Massonié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
		<editor>Text, Speech and Dialogue, Václav Matoušek and Pavel Mautner</editor>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="302" to="308" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Kaldi Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finite-state transducers in language and speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="311" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX-OSDI Symposium</title>
		<meeting>of USENIX-OSDI Symposium</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CNTK: Microsoft&apos;s Open-Source Deep-Learning Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2135" to="2135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pykaldi: A python wrapper for kaldi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hybrid ctc/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic context window composition for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="34" to="44" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum Likelihood Linear Transformations for HMM-Based Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="75" to="98" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contaminated speech training methods for robust DNN-HMM distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="756" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multitask learning of context-dependent targets in deep neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="238" to="247" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Batchnormalized joint training for dnn-based distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
		<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint noise adaptive training for robust automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4380" to="4384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A network of deep neural networks for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4880" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Light gated recurrent units for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="102" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Twin regularization for online speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Speaker Recognition from raw waveform with SincNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
		<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interpretable Convolutional Filters with SincNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS@IRASL</title>
		<meeting>of NIPS@IRASL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Random search for hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The DIRHA-English corpus and related tasks for distant-speech recognition in domestic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cristoforetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gretter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pellin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Realistic multimicrophone data simulation for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svaizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2786" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RNNDROP: A novel dropout for RNNS in ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving speech recognition by revising gated recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
