<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 4-8, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
							<email>hongweiw@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
							<email>zhangfuzheng@meituan.com</email>
							<affiliation key="aff1">
								<orgName type="department">Meituan-Dianping Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
							<email>zhangmengdi02@meituan.com</email>
							<affiliation key="aff2">
								<orgName type="department">Meituan-Dianping Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
							<email>csmiaozhao@comp.polyu.edu.hk</email>
							<affiliation key="aff4">
								<orgName type="institution">Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
							<email>wangzhongyuan02@meituan.com</email>
							<affiliation key="aff5">
								<orgName type="department">Meituan-Dianping Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff6">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems</title>
					</analytic>
					<monogr>
						<title level="m">The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD &apos;19)</title>
						<meeting> <address><addrLine>Anchorage, AK, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">August 4-8, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330836</idno>
					<note>. ACM, New York, NY, USA, 10 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge-aware recommendation</term>
					<term>graph neural networks</term>
					<term>label propagation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems are widely used in Internet applications to meet user's personalized interests and alleviate the information overload <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. Traditional recommender systems that are <ref type="bibr">KDD '19, August 4-8, 2019</ref>, Anchorage, AK, USA 2019. ACM ISBN 978-1-4503-6201-6/19/08. . . $15.00 https://doi.org/10.1145/3292500.3330836 based on collaborative filtering <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> usually suffer from the coldstart problem and have trouble recommending brand new items that have not yet been heavily explored by the users. The sparsity issue can be addressed by introducing additional sources of information such as user/item profiles <ref type="bibr" target="#b22">[23]</ref> or social networks <ref type="bibr" target="#b21">[22]</ref>.</p><p>Knowledge graphs (KGs) capture structured information and relations between a set of entities <ref type="bibr">[8, 9, 18, 24-28, 33, 34, 36]</ref>. KGs are heterogeneous graphs in which nodes correspond to entities (e.g., items or products, as well as their properties and characteristics) and edges correspond to relations. KGs provide connectivity information between items via different types of relations and thus capture semantic relatedness between the items.</p><p>The core challenge in utilizing KGs in recommender systems is to learn how to capture user-specific item-item relatedness captured by the KG. Existing KG-aware recommender systems can be classified into path-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>, embedding-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, and hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. However, these approaches rely on manual feature engineering, are unable to perform end-to-end training, and have poor scalability. Graph Neural Networks (GNNs), which aggregate node feature information from node's local network neighborhood using neural networks, represent a promising advancement in graph-based representation learning <ref type="bibr">[3, 5-7, 11, 15]</ref>. Recently, several works developed GNNs architecture for recommender systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, but these approaches are mostly designed for homogeneous bipartite useritem interaction graphs or user-/item-similarity graphs. It remains an open question how to extend GNNs architecture to heterogeneous knowledge graphs.</p><p>In this paper, we develop Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) that extends GNNs architecture to knowledge graphs to simultaneously capture semantic relationships between the items as well as personalized user preferences and interests. To account for the relational heterogeneity in KGs, similar to <ref type="bibr" target="#b27">[28]</ref>, we use a trainable and personalized relation scoring function that transforms the KG into a user-specific weighted graph, which characterizes both the semantic information of the KG as well as user's personalized interests. For example, in the movie recommendation setting the relation scoring function could learn that a given user really cares about "director" relation between movies and persons, while somebody else may care more about the "lead actor" relation. Using this personalized weighted graph, we then apply a graph neural network that for every item node computes its embedding by aggregating node feature information over the local network neighborhood of the item node. This way the embedding of each item captures it's local KG structure in a user-personalized way.</p><p>A significant difference between our approach and traditional GNNs is that the edge weights in the graph are not given as input. We set them using user-specific relation scoring function that is trained in a supervised fashion. However, the added flexibility of edge weights makes the learning process prone to overfitting, since the only source of supervised signal for the relation scoring function is coming from user-item interactions (which are sparse in general). To remedy this problem, we develop a technique for regularization of edge weights during the learning process, which leads to better generalization. We develop an approach based on label smoothness <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38]</ref>, which assumes that adjacent entities in the KG are likely to have similar user relevancy labels/scores. In our context this assumption means that users tend to have similar preferences to items that are nearby in the KG. We prove that label smoothness regularization is equivalent to label propagation and we design a leave-one-out loss function for label propagation to provide extra supervised signal for learning the edge scoring function. We show that the knowledge-aware graph neural networks and label smoothness regularization can be unified under the same framework, where label smoothness can be seen as a natural choice of regularization on knowledge-aware graph neural networks.</p><p>We apply the proposed method to four real-world datasets of movie, book, music, and restaurant recommendations, in which the first three datasets are public datasets and the last is from Meituan-Dianping Group. Experiments show that our method achieves significant gains over state-of-the-art methods in recommendation accuracy. We also show that our method maintains strong recommendation performance in the cold-start scenarios where user-item interactions are sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Graph Neural Networks</head><p>Graph Neural Networks (or Graph Convolutional Neural Networks, GCNs) aim to generalize convolutional neural networks to non-Euclidean domains (such as graphs) for robust feature learning. Bruna et al. <ref type="bibr" target="#b2">[3]</ref> define the convolution in Fourier domain and calculate the eigendecomposition of the graph Laplacian, Defferrard et al. <ref type="bibr" target="#b4">[5]</ref> approximate the convolutional filters by Chebyshev expansion of the graph Laplacian, and Kipf et al. <ref type="bibr" target="#b10">[11]</ref> propose a convolutional architecture via a first-order approximation. In contrast to these spectral GCNs, non-spectral GCNs operate on the graph directly and apply "convolution" (i.e., weighted average) to local neighbors of a node <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Recently, researchers also deployed GNNs in recommender systems: PinSage <ref type="bibr" target="#b31">[32]</ref> applies GNNs to the pin-board bipartite graph in Pinterest. Monti et al. <ref type="bibr" target="#b13">[14]</ref> and Berg et al. <ref type="bibr" target="#b18">[19]</ref> model recommender systems as matrix completion and design GNNs for representation learning on user-item bipartite graphs. Wu et al. <ref type="bibr" target="#b30">[31]</ref> use GNNs on user/item structure graphs to learn user/item representations. The difference between these works and ours is that they are all designed for homogeneous bipartite graphs or user/item-similarity graphs where GNNs can be used directly, while here we investigate GNNs for heterogeneous KGs. Wang et al. <ref type="bibr" target="#b27">[28]</ref> use GCNs in KGs for recommendation, but simply applying GCNs to KGs without proper regularization is prone to overfitting and leads to performance degradation as we will show later. <ref type="bibr">Schlichtkrull et al. also</ref> propose using GNNs to model KGs <ref type="bibr" target="#b16">[17]</ref>, but not for the purpose of recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-supervised Learning on Graphs</head><p>The goal of graph-based semi-supervised learning is to correctly label all nodes in a graph given that only a few nodes are labeled. Prior work often makes assumptions on the distribution of labels over the graph, and one common assumption is smooth variation of labels of nodes across the graph. Based on different settings of edge weights in the input graph, these methods are classified as: (1) Edge weights are assumed to be given as input and therefore fixed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>; (2) Edge weights are parameterized and therefore learnable <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>. Inspired by these methods, we design a module of label smoothness regularization in our proposed model. The major distinction of our work is that the label smoothness constraint is not used for semi-supervised learning on graphs, but serves as regularization to assist the learning of edge weights and achieves better generalization for recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Recommendations with Knowledge Graphs</head><p>In general, existing KG-aware recommender systems can be classified into three categories: (1) Embedding-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> pre-process a KG with knowledge graph embedding (KGE) <ref type="bibr" target="#b29">[30]</ref> algorithms, then incorporate learned entity embeddings into recommendation. Embedding-based methods are highly flexible in utilizing KGs to assist recommender systems, but the KGE algorithms focus more on modeling rigorous semantic relatedness (e.g., TransE <ref type="bibr" target="#b1">[2]</ref> assumes head +relation = tail), which are more suitable for graph applications such as link prediction rather than recommendations. In addition, embedding-based methods usually lack an end-to-end way of training. (2) Path-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> explore various patterns of connections among items in a KG (a.k.a meta-path or meta-graph) to provide additional guidance for recommendations. Path-based methods make use of KGs in a more intuitive way, but they rely heavily on manually designed metapaths/meta-graphs, which are hard to tune in practice. (3) Hybrid methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref> combine the above two categories and learn user/item embeddings by exploiting the structure of KGs. Our proposed model can be seen as an instance of hybrid methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>We begin by describing the KG-aware recommendations problem and introducing notation. In a typical recommendation scenario, we have a set of users U and a set of items V. The user-item interaction matrix Y is defined according to users' implicit feedback, where y uv = 1 indicates that user u has engaged with item v, such as clicking, watching, or purchasing. We also have a knowledge graph G = {(h, r , t)} available, in which h ∈ E, r ∈ R, and t ∈ E denote the head, relation, and tail of a knowledge triple, E and R are the set of entities and relations in the knowledge graph, respectively. For example, the triple (The Silence of the Lambs, film.film.star, Anthony Hopkins) states the fact that Anthony Hopkins is the leading actor in film "The Silence of the Lambs". In many recommendation scenarios, an item v ∈ V corresponds to an entity e ∈ E (e.g., item "The Silence of the Lambs" in MovieLens also appears in the knowledge graph as an entity). The set of entities E is composed from items V (V ⊆ E) as well as non-items E\V (e.g. nodes corresponding to item/product properties). Given user-item interaction matrix Y and knowledge graph G, our task is to predict whether user u has potential interest in item v with which he/she has not engaged before. Specifically, we aim to learn a prediction functionŷ uv = F (u, v |Θ, Y, G), whereŷ uv denotes the probability that user u will engage with item v, and Θ are model parameters of function F . We list the key symbols used in this paper in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR APPROACH</head><p>In this section, we first introduce knowledge-aware graph neural networks and label smoothness regularization, respectively, then we present the unified model.</p><formula xml:id="formula_0">Symbol Meaning U = {u 1 , · · · } Set of users V = {v 1 , · · · } Set of items Y User-item interaction matrix G = (E, R) Knowledge graph E = {e 1 , · · · } Set of entities R = {r 1 , · · · } Set of relations E\V Set of non-item entities s u (r ) User-specific relation scoring function A u Adjacency matrix of G w.r.t. user u D u Diagonal degree matrix of A u E</formula><p>Raw entity feature H (l ) , l = 0, ..., L − 1 Entity representation in the l-th layer W (l ) , l = 0, ..., L − 1 Transformation matrix in the l-th layer l u (e), e ∈ E Item relevancy labeling function l * u (e), e ∈ E Minimum-energy labeling function</p><formula xml:id="formula_1">l u (v), v ∈ V Predicted relevancy label for item v R(A u )</formula><p>Label smoothness regularization on A u <ref type="table">Table 1</ref>: List of key symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries: Knowledge-aware Graph Neural Networks</head><p>The first step of our approach is to transform a heterogeneous KG into a user-personalized weighted graph, which characterizes user's preferences. To this end, similar to <ref type="bibr" target="#b27">[28]</ref>, we use a user-specific relation scoring function s u (r ) that provides the importance of relation r for user u: s u (r ) = д(u, r), where u and r are feature vectors of user u and relation type r , respectively, and д is a differentiable function such as inner product. Intuitively, s u (r ) characterizes the importance of relation r to user u. For example, a user may be more interested in movies that have the same director with the movies he/she watched before, but another user may care more about the leading actor of movies. Given user-specific relation scoring function s u (·) of user u, knowledge graph G can therefore be transformed into a userspecific adjacency matrix A u ∈ R | E |×| E | , in which the (i, j)-entry A i j u = s u (r e i ,e j ), and r e i ,e j is the relation between entities e i and e j in G. 1 A i j u = 0 if there is no relation between e i and e j . See the left two subfigures in <ref type="figure" target="#fig_0">Figure 1</ref> for illustration. We also denote the raw feature matrix of entities as E ∈ R | E |×d 0 , where d 0 is the dimension of raw entity features. Then we use multiple feed forward layers to update the entity representation matrix by aggregating representations of neighboring entities. Specifically, the layer-wise forward propagation can be expressed as</p><formula xml:id="formula_2">H l +1 = σ D −1/2 u A u D −1/2 u H l W l , l = 0, 1, · · · , L − 1. (1)</formula><p>In Eq. (1), H l is the matrix of hidden representations of entities in layer l, and H 0 = E. A u is to aggregate representation vectors of neighboring entities. In this paper, we set A u ← A u + I, i.e., adding self-connection to each entity, to ensure that old representation vector of the entity itself is taken into consideration when updating entity representations. D u is a diagonal degree matrix with entries</p><formula xml:id="formula_3">D ii u = j A i j u , therefore, D −1/2 u</formula><p>is used to normalize A u and keep the entity representation matrix H l stable. W l ∈ R d l ×d l +1 is the layer-specific trainable weight matrix, σ is a non-linear activation function, and L is the number of layers.</p><p>A single GNN layer computes the representation of an entity via a transformed mixture of itself and its immediate neighbors in the KG. We can therefore naturally extend the model to multiple layers to explore users' potential interests in a broader and deeper way. The final output is H L ∈ R | E |×d L , which is the entity representations that mix the initial features of themselves and their neighbors up to L hops away. Finally, the predicted engagement probability of</p><formula xml:id="formula_4">user u with item v is calculated byŷ uv = f (u, v u ), where v u (i.e.,</formula><p>the v-th row of H L ) is the final representation vector of item v, and f is a differentiable prediction function, for example, inner product or a multilayer perceptron. Note that v u is user-specific since the adjacency matrix A u is user-specific. Furthermore, note that the system is end-to-end trainable where the gradients flow from f (·) via GNN (parameter matrix W) to д(·) and eventually to representations of users u and items v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Label Smoothness Regularization</head><p>It is worth noticing a significant difference between our model and GNNs: In traditional GNNs, edge weights of the input graph are fixed; but in our model, edge weights D</p><formula xml:id="formula_5">−1/2 u A u D −1/2 u in Eq.</formula><p>(1) are learnable (including possible parameters of function д and feature vectors of users and relations) and also requires supervised training like W. Though enhancing the fitting ability of the model, this will inevitably make the optimization process prone to overfitting, since the only source of supervised signal is from user-item interactions outside GNN layers. Moreover, edge weights do play an essential role in representation learning on graphs, as highlighted by a large amount of prior works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>. Therefore, more regularization on edge weights is needed to assist the learning of entity representations and to help generalize to unobserved interactions more efficiently.</p><p>Let's see how an ideal set of edge weights should be like. Consider a real-valued label function l u : E → R on G, which is constrained to take a specific value l u (v) = y uv at node v ∈ V ⊆ E. In our context, l u (v) = 1 if user u finds the item v relevant and has engaged with it, otherwise l u (v) = 0. Intuitively, we hope that adjacent entities in the KG are likely to have similar relevancy labels, which is known as label smoothness assumption. This motivates our choice of energy function E:</p><formula xml:id="formula_6">E(l u , A u ) = 1 2 e i ∈ E,e j ∈ E A i j u l u (e i ) − l u (e j ) 2 .<label>(2)</label></formula><p>We show that the minimum-energy label function is harmonic by the following theorem:</p><formula xml:id="formula_7">Theorem 1. The minimum-energy label function l * u = arg min l u :l u (v)=y uv ,∀v ∈V E(l u , A u ) (3) w.r.t. Eq. (2) is harmonic, i.e., l * u satisfies l * u (e i ) = 1 D ii u e j ∈ E A i j u l * u (e j ), ∀e i ∈ E\V.<label>(4)</label></formula><p>Proof. Taking the derivative of the following equation</p><formula xml:id="formula_8">E(l u , A u ) = 1 2 i, j A i j u l u (e i ) − l u (e j ) 2 with respect to l u (e i ) where e i ∈ E\V, we have ∂E(l u , A u ) ∂l u (e i ) = j A i j u l u (e i ) − l u (e j ) .</formula><p>The minimum-energy label function l * u should satisfy that</p><formula xml:id="formula_9">∂E(l u , A u ) ∂l u (e i ) l u =l * u = 0.</formula><p>Therefore, we have</p><formula xml:id="formula_10">l * u (e i ) = 1 j A i j u j A i j u l * u (e j ) = 1 D ii u j A i j u l * u (e j ), ∀e i ∈ E\V. □</formula><p>The harmonic property indicates that the value of l * u at each non-item entity e i ∈ E\V is the average of its neighboring entities, which leads to the following label propagation scheme <ref type="bibr" target="#b38">[39]</ref>: Theorem 2. Repeating the following two steps: (1) Propagate labels for all entities:</p><formula xml:id="formula_11">l u (E) ← D −1 u A u l u (E), where l u (E)</formula><p>is the vector of labels for all entities;</p><p>(2) Reset labels of all items to initial labels:</p><formula xml:id="formula_12">l u (V) ← Y[u, V] ⊤ ,</formula><p>where l u (V) is the vector of labels for all items and Y[u, V] = [y uv 1 , y uv 2 , · · · ] are initial labels;</p><p>will lead to l u → l * u .</p><formula xml:id="formula_13">Proof. Let l u (E) = l u (V) l u (E\V) . Since l u (V) is fixed on Y[u, V],</formula><p>we are only interested in l u (E\V). We denote P = D −1 u A u (the subscript u is omitted from P for ease of notation), and partition matrix P into sub-matrices according to the partition of l u :</p><formula xml:id="formula_14">P = P V V P V E P EV P EE .</formula><p>Then the label propagation scheme is equivalent to</p><formula xml:id="formula_15">l u (E\V) ← P EV Y[u, V] ⊤ + P EE l u (E\V).<label>(5)</label></formula><p>Repeat the above procedure, we have</p><formula xml:id="formula_16">l u (E\V) = lim n→∞ (P EE ) n l (0) u (E\V) + n i=1 (P EE ) i−1 P EV Y[u, V] ⊤ ,<label>(6)</label></formula><p>where l (0) u (E\V) is the initial value for l u (E\V). Now we show that lim n→∞ (P EE ) n l (0) u (E\V) = 0. Since P is row-normalized and P EE is a sub-matrix of P, we have for all possible row index i. Therefore,</p><formula xml:id="formula_17">∃ϵ &lt; 1, j P EE [i, j] ≤ ϵ,</formula><formula xml:id="formula_18">j (P EE ) n [i, j] = j (P EE ) (n−1) P EE [i, j] = j k (P EE ) (n−1) [i, k] P EE [k, j] = k (P EE ) (n−1) [i, k] j P EE [k, j] ≤ k (P EE ) (n−1) [i, k] ϵ ≤ · · · ≤ ϵ n .</formula><p>As n goes infinity, the row sum of (P EE ) n converges to zero, which implies that (P EE ) n l </p><formula xml:id="formula_19">l u (E\V) = lim n→∞ n i=1 (P EE ) i−1 P EV Y[u, V] ⊤ . Denote T = lim n→∞ n i=1 (P EE ) i−1 = ∞ i=1 (P EE ) i−1 ,</formula><p>and we have</p><formula xml:id="formula_20">T − TP EE = ∞ i=1 (P EE ) i−1 − ∞ i=1 (P EE ) i = I.</formula><p>Therefore, we derive that</p><formula xml:id="formula_21">T = (I − P EE ) −1 , and l u (E\V) = (I − P EE ) −1 P EV Y[u, V] ⊤ .</formula><p>This is the unique fixed point and therefore the unique solution to Eq. (5). Repeating the steps in Theorem 2 leads to</p><formula xml:id="formula_22">l u (E) → l * u (E) = Y[u, V] ⊤ (I − P EE ) −1 P EV Y[u, V] ⊤ .</formula><p>□ Theorem 2 provides a way for reaching the minimum-energy of relevancy label function E. However, l * u does not provide any signal for updating the edge weights matrix A u , since the labeled part of l * u , i.e., l * u (V), equals their true relevancy labels Y[u, V];</p><p>Moreover, we do not know true relevancy labels for the unlabeled nodes l * u (E\V). To solve the issue, we propose minimizing the leave-one-out loss <ref type="bibr" target="#b34">[35]</ref>. Suppose we hold out a single item v and treat it unlabeled. Then we predict its label by using the rest of (labeled) items and (unlabeled) non-item entities. The prediction process is identical to label propagation in Theorem 2, except that the label of item v is hidden and needs to be calculated. This way, the difference between the true relevancy label of v (i.e., y uv ) and the predicted labell u (v) serves as a supervised signal for regularizing edge weights:</p><formula xml:id="formula_23">R(A) = u R(A u ) = u v J y uv ,l u (v) ,<label>(7)</label></formula><p>where J is the cross-entropy loss function. Given the regularization in Eq. <ref type="formula" target="#formula_23">(7)</ref>, an ideal edge weight matrix A should reproduce the true relevancy label of each held-out item while also satisfying the smoothness of relevancy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Unified Loss Function</head><p>Combining knowledge-aware graph neural networks and LS regularization, we reach the following complete loss function:</p><formula xml:id="formula_24">min W,A L = min W,A u,v J (y uv ,ŷ uv ) + λR(A) + γ ∥F ∥ 2 2 ,<label>(8)</label></formula><p>where ∥F ∥ 2 2 is the L2-regularizer, λ and γ are balancing hyperparameters. In Eq. (8), the first term corresponds to the part of GNN that learns the transformation matrix W and edge weights A simultaneously, while the second term R(·) corresponds to the part of label smoothness that can be seen as adding constraint on edge weights A. Therefore, R(·) serves as regularization on A to assist GNN in learning edge weights.</p><p>It is also worth noticing that the first term can be seen as feature propagation on the KG while the second term R(·) can be seen as label propagation on the KG. A recommender for a specific user u is actually a mapping from item features to user-item interaction labels, i.e., F u : E v → y uv where E v is the feature vector of item v. Therefore, Eq. (8) utilizes the structural information of the KG on both the feature side and the label side of F u to capture users' higher-order preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>How can the knowledge graph help find users' interests? To intuitively understand the role of the KG, we make an analogy with a  physical equilibrium model as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Each entity/item is seen as a particle, while the supervised positive user-relevancy signal acts as the force pulling the observed positive items up from the decision boundary and the negative items signal acts as the force pushing the unobserved items down. Without the KG <ref type="figure" target="#fig_1">(Figure  2a</ref>), these items are only loosely connected with each other through the collaborative filtering effect (which is not drawn here for clarity). In contrast, edges in the KG serve as the rubber bands that impose explicit constraints on connected entities. When number of layers is L = 1 <ref type="figure" target="#fig_1">(Figure 2b)</ref>, representation of each entity is a mixture of itself and its immediate neighbors, therefore, optimizing on the positive items will simultaneously pull their immediate neighbors up together. The upward force goes deeper in the KG with the increase of L <ref type="figure" target="#fig_1">(Figure 2c</ref>), which helps explore users' long-distance interests and pull up more positive items. It is also interesting to note that the proximity constraint exerted by the KG is personalized since the strength of the rubber band (i.e., s u (r )) is user-specific and relation-specific: One user may prefer relation r 1 <ref type="figure" target="#fig_1">(Figure 2b</ref>) while another user (with same observed items but different unobserved items) may prefer relation r 2 <ref type="figure" target="#fig_1">(Figure 2d</ref>). Despite the force exerted by edges in the KG, edge weights may be set inappropriately, for example, too small to pull up the unobserved items (i.e., rubber bands are too weak). Next, we show by <ref type="figure" target="#fig_1">Figure 2e</ref> that how the label smoothness assumption helps regularizing the learning of edge weights. Suppose we hold out the positive sample in the upper left and we intend to reproduce its label by the rest of items. Since the true relevancy label of the held-out sample is 1 and the upper right sample has the largest label value, the LS regularization term R(A) would enforce the edges with arrows to be large so that the label can "flow" from the blue one to the striped one as much as possible. As a result, this will tighten the rubber bands (denoted by arrows) and encourage the model to pull up the two upper pink items to a greater extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we evaluate the proposed KGNN-LS model, and present its performance on four real-world scenarios: movie, book, music, and restaurant recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We utilize the following four datasets in our experiments for movie, book, music, and restaurant recommendations, respectively, in which the first three are public datasets and the last one is from  <ref type="figure">Figure 3</ref>: Probability distribution of the shortest path distance between two randomly sampled items in the KG under the circumstance that (1) they have no common user in the dataset; (2) they have common user(s) in the dataset. The statistics of the four datasets are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare the proposed KGNN-LS model with the following baselines for recommender systems, in which the first two baselines are KG-free while the rest are all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.</p><p>• SVD [12] is a classic CF-based model using inner product to model user-item interactions. We use the unbiased version (i.e., the predicted engaging probability is modeled as y uv = u ⊤ v).   treats the KG as heterogeneous information networks and extracts meta-path based features to represent the connectivity between users and items. We use manually designed "useritem-attribute-item" as meta-paths, i.e., "user-movie-directormovie", "user-movie-genre-movie", and "user-movie-star-movie" for MovieLens-20M; "user-book-author-book" and "user-bookgenre-book" for Book-Crossing, "user-musician-date_of_birthmusician" (date of birth is discretized), "user-musician-countrymusician", and "user-musician-genre-musician" for Last.FM; "user-restaurant-dish-restaurant", "user-restaurant-business_arearestaurant", "user-restaurant-tag-restaurant" for Dianping-Food. The settings of dimension and learning rate are the same as SVD. • CKE [34] is a representative of embedding-based methods, which combines CF with structural, textual, and visual knowledge in a unified framework. We implement CKE as CF plus a structural knowledge module in this paper. The dimension of embedding for the four datasets are 64, 128, 64, 64. The training weight for KG part is 0.1 for all datasets. The learning rate are the same as in SVD. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Validating the Connection between G and Y</head><p>To validate the connection between the knowledge graph G and user-item interaction Y, we conduct an empirical study where we investigate the correlation between the shortest path distance of two randomly sampled items in the KG and whether they have common user(s) in the dataset, that is there exist user(s) that interacted with both items. For MovieLens-20M and Last.FM, we randomly sample ten thousand item pairs that have no common users and have at least one common user, respectively, then count the distribution of their shortest path distances in the KG. The results are presented in <ref type="figure">Figure 3</ref>, which clearly show that if two items have common user(s) in the dataset, they are likely to be more close in the KG. For example, if two movies have common user(s) in MovieLens-20M, there is a probability of 0.92 that they will be within 2 hops in the KG, while the probability is 0.80 if they have no common user. This finding empirically demonstrates that exploiting the proximity structure of the KG can assist making recommendations. This also justifies our motivation to use label smoothness regularization to help learn entity representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Comparison with Baselines.</head><p>We evaluate our method in two experiment scenarios: (1) In top-K recommendation, we use the trained model to select K items with highest predicted click probability for each user in the test set, and choose Recall@K to evaluate the recommended sets. (2) In click-through rate (CTR) prediction, we apply the trained model to predict each piece of user-item pair in the test set (including positive items and randomly selected negative items). We use AUC as the evaluation metric in CTR prediction. The results of top-K recommendation and CTR prediction are presented in <ref type="table" target="#tab_3">Tables 3 and 4</ref>, respectively, which show that KGNN-LS outperforms baselines by a significant margin. For example, the AUC of KGNN-LS surpasses baselines by 5.1%, 6.9%, 8.3%, and 4.3% on average in MovieLens-20M, Book-Crossing, Last.FM, and Dianping-Food datasets, respectively.</p><p>We also show daily performance of KGNN-LS and baselines on Dianping-Food to investigate performance stability. <ref type="figure">Figure 4</ref> shows their AUC score from September 1, 2018 to <ref type="bibr">September 30, 2018</ref>. We notice that the curve of KGNN-LS is consistently above baselines over the test period; Moreover, the performance of KGNN-LS is also   with low variance, which suggests that KGNN-LS is also robust and stable in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Effectiveness of LS Regularization.</head><p>Is the proposed LS regularization helpful in improving the performance of GNN? To study the effectiveness of LS regularization, we fix the dimension of hidden layers as 4, 8, and 16, then vary λ from 0 to 5 to see how performance changes. The results of R@10 in Last.FM dataset are plotted in <ref type="figure">Figure 5</ref>. It is clear that the performance of KGNN-LS with a non-zero λ is better than λ = 0 (the case of Wang et al. <ref type="bibr" target="#b27">[28]</ref>), which justifies our claim that LS regularization can assist learning the edge weights in a KG and achieve better generalization in recommender systems. But note that a too large λ is less favorable, since it overwhelms the overall loss and misleads the direction of gradients. According to the experiment results, we find that a λ between 0.1 and 1.0 is preferable in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.3</head><p>Results in cold-start scenarios. One major goal of using KGs in recommender systems is to alleviate the sparsity issue. To investigate the performance of KGNN-LS in cold-start scenarios, we vary the size of training set of MovieLens-20M from r = 100% to r = 20% (while the validation and test set are kept fixed), and report the results of AU C in <ref type="table" target="#tab_6">Table 5</ref>. When r = 20%, AUC decreases by 8.4%, 5.9%, 5.4%, 3.6%, 2.8%, and 4.1% for the six baselines compared to the model trained on full training data (r = 100%), but the performance decrease of KGNN-LS is only 1.8%. This demonstrates that KGNN-LS still maintains predictive performance even when user-item interactions are sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.4</head><p>Hyper-parameters Sensitivity. We first analyze the sensitivity of KGNN-LS to the number of GNN layers L. We vary L from 1 to 4 while keeping other hyper-parameters fixed. The results are shown in <ref type="table" target="#tab_8">Table 6</ref>. We find that the model performs poorly when L = 4, which is because a larger L will mix too many entity embeddings   in a given entity, which over-smoothes the representation learning on KGs. KGNN-LS achieves the best performance when L = 1 or 2 in the four datasets.</p><p>We also examine the impact of the dimension of hidden layers d on the performance of KGNN-LS. The result in shown in <ref type="table" target="#tab_9">Table 7</ref>. We observe that the performance is boosted with the increase of d at the beginning, because more bits in hidden layers can improve the model capacity. However, the performance drops when d further increases, since a too large dimension may overfit datasets. The best performance is achieved when d = 8 ∼ 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Running Time Analysis</head><p>We also investigate the running time of our method with respect to the size of KG. We run experiments on a Microsoft Azure virtual machine with 1 NVIDIA Tesla M60 GPU, 12 Intel Xeon CPUs (E5-2690 v3 @2.60GHz), and 128GB of RAM. The size of the KG is increased by up to five times the original one by extracting more triples from Satori, and the running times of all methods on MovieLens-20M are reported in <ref type="figure">Figure 6</ref>. Note that the trend of a curve matters more than the real values, since the values are largely dependent on the minibatch size and the number of epochs (yet we did try to align the configurations of all methods). The result show that KGNN-LS exhibits strong scalability even when the KG is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose knowledge-aware graph neural networks with label smoothness regularization for recommendation. KGNN-LS applies GNN architecture to KGs by using user-specific relation scoring functions and aggregating neighborhood information with different weights. In addition, the proposed label smoothness constraint and leave-one-out loss provide strong regularization for learning the edge weights in KGs. We also discuss how KGs benefit recommender systems and how label smoothness can assist learning the edge weights. Experiment results show that KGNN-LS outperforms state-of-the-art baselines in four recommendation scenarios and achieves desirable scalability with respect to KG size.</p><p>In this paper, LS regularization is proposed for recommendation task with KGs. It is interesting to examine the LS assumption on other graph tasks such as link prediction and node classification. Investigating the theoretical relationship between feature propagation and label propagation is also a promising direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A Additional Details on Datasets</head><p>MovieLens-20M, Book-Crossing, and Last.FM dataset contain explicit feedbacks data (Last.FM provides the listening count as weight for each user-item interaction). Therefore, we transform them into implicit feedback, where each entry is marked with 1 indicating that the user has rated the item positively. The threshold of positive rating is 4 for MovieLens-20M, while no threshold is set for Book-Crossing and Last.FM due to their sparsity. Additionally, we randomly sample an unwatched set of items and mark them as 0 for each user, the number of which equals his/her positively-rated ones.</p><p>We use Microsoft Satori to construct the KGs for MovieLens-20M, Book-Crossing, and Last.FM dataset. In one triple in Satori KG, the head and tail are either IDs or textual content, and the relation is with the form "domain.head_category.tail_category" (e.g., "book.book.author"). We first select a subset of triples from the whole Satori KG with a confidence level greater than 0.9. Given the sub-KG, we collect Satori IDs of all valid movies/books/musicians by matching their names with tail of triples (head, film.film.name, tail), (head, book.book.title, tail), or (head, type.object.name, tail), for the three datasets. Items with multiple matched or no matched entities are excluded for simplicity. After having the set of item IDs, we match these item IDs with the head of all triples in Satori sub-KG, and select all well-matched triples as the final KG for each dataset.</p><p>Dianping-Food dataset is collected from Dianping.com, a Chinese group buying website hosting consumer reviews of restaurants similar to Yelp. We select approximately 10 million interactions between users and restaurants in Dianping.com from May 1, 2015 to December 12, 2018. The types of positive interactions include clicking, buying, and adding to favorites, and we sample negative interactions for each user. The KG for Dianping-Food is collected from Meituan Brain, an internal knowledge graph built for dining and entertainment by Meituan-Dianping Group. The types of entities include POI (restaurant), city, first-level and second-level category, star, business area, dish, and tag; The types of relations correspond to the types of entities (e.g., "organization.POI.has_dish").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Details on Hyper-parameter Searching</head><p>In KGNN-LS, we set functions д and f as inner product, σ as ReLU for non-last-layers and tanh for the last-layer. Note that the size of neighbors of an entity in a KG may vary significantly over the KG. To keep the computation more efficient, we uniformly sample a fixed-size set of neighbors for each entity instead of using its full set of neighbors. The number of sampled neighbors for each entity is denoted by S. Hyper-parameter settings are given in <ref type="table">Table  8</ref>, which are determined by optimizing R@10 on a validation set. The search spaces for hyper-parameters are as follows: 1.0 0.5 0.1 0.5 γ 10 −7 2 × 10 −5 10 −4 10 −7 η 2 × 10 −2 2 × 10 −4 5 × 10 −4 2 × 10 −2 <ref type="table">Table 8</ref>: Hyper-parameter settings for the four datasets (S: number of sampled neighbors for each entity; d: dimension of hidden layers, L: number of layers, λ: label smoothness regularizer weight, γ : L2 regularizer weight, η: learning rate).</p><formula xml:id="formula_25">• S = {2,</formula><p>• γ = {10 −9 , 10 −8 , 10 −7 , 2 × 10 −7 , 5 × 10 −7 , 10 −6 , 2 × 10 −6 , 5 × 10 −6 , 10 −5 , 2 × 10 −5 , 5 × 10 −5 , 10 −4 , 2 × 10 −4 , 5 × 10 −4 , 10 −3 }; • η = {10 −5 , 2 × 10 −5 , 5 × 10 −5 , 10 −4 , 2 × 10 −4 , 5 × 10 −4 , 10 −3 , 2 × 10 −3 , 5 × 10 −3 , 10 −2 , 2 × 10 −2 , 5 × 10 −2 , 10 −1 }. For each dataset, the ratio of training, validation, and test set is 6 : 2 : 2. Each experiment is repeated 5 times, and the average performance is reported. All trainable parameters are optimized by Adam algorithm. The code of KGNN-LS is implemented with Python 3.6, TensorFlow 1.12.0, and NumPy 1.14.3.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our proposed KGNN-LS model. The original KG is first transformed into a user-specific weighted graph, on which we then perform feature propagation using a graph neural network with the label smoothness regularization. The two modules constitute the complete loss function L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Analogy of a physical equilibrium model for recommender systems; (b)-(d) Illustration of the effect of the KG; (e) Illustration of the effect of label smoothness regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) → 0. It's clear that the choice of initial value l (0) u (E\V) does not affect the convergence. Since lim n→∞ (P EE ) n l (0) u (E\V) = 0, Eq. (6) becomes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The dimension and learning rate for the four datasets are set as: d = 8, η = 0.5 for MovieLens-20M, Book-Crossing; d = 8, η = 0.1 for Last.FM; d = 32, η = 0.1 for Dianping-Food. R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 SVD 0.036 0.124 0.277 0.401 0.027 0.046 0.077 0.109 0.029 0.098 0.240 0.332 0.039 0.152 0.329 0.451 LibFM 0.039 0.121 0.271 0.388 0.033 0.062 0.092 0.124 0.030 0.103 0.263 0.330 0.043 0.156 0.332 0.448 LibFM + TransE 0.041 0.125 0.280 0.396 0.037 0.064 0.097 0.130 0.032 0.102 0.259 0.326 0.044 0.161 0.343 0.455 PER 0.022 0.077 0.160 0.243 0.022 0.041 0.064 0.070 0.014 0.052 0.116 0.176 0.023 0.102 0.256 0.354 CKE 0.034 0.107 0.244 0.322 0.028 0.051 0.079 0.112 0.023 0.070 0.180 0.296 0.034 0.138 0.305 0.437 RippleNet 0.045 0.130 0.278 0.447 0.036 0.074 0.107 0.127 0.032 0.101 0.242 0.336 0.040 0.155 0.328 0.440 KGNN-LS 0.043 0.155 0.321 0.458 0.045 0.082 0.117 0.149 0.044 0.122 0.277 0.370 0.047 0.170 0.340 0.487</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>•</head><label></label><figDesc>RippleNet<ref type="bibr" target="#b23">[24]</ref> is a representative of hybrid methods, which is a memory-network-like approach that propagates users' preferences on the KG for recommendation. For RippleNet, d = 8, H = 2, λ 1 = 10 −6 , λ 2 = 0.01, η = 0.01 for MovieLens-20M; d = 16, H = 3, λ 1 = 10 −5 , λ 2 = 0.02, η = 0.005 for Last.FM; d = 32, H = 2, λ 1 = 10 −7 , λ 2 = 0.02, η = 0.01 for Dianping-Food.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Daily AU C of all methods on Dianping-Food in September 2018. Effectiveness of LS regularization on Last.FM. Running time of all methods w.r.t. KG size on MovieLens-20M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Statistics of the four datasets: MovieLens-20M</cell></row><row><cell>(movie), Book-Crossing (book), Last.FM (music), and</cell></row><row><cell>Dianping-Food (restaurant).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The results of Recall@K in top-K recommendation.</figDesc><table><row><cell>Model</cell><cell cols="4">Movie Book Music Restaurant</cell></row><row><cell>SVD</cell><cell>0.963</cell><cell>0.672</cell><cell>0.769</cell><cell>0.838</cell></row><row><cell>LibFM</cell><cell>0.959</cell><cell>0.691</cell><cell>0.778</cell><cell>0.837</cell></row><row><cell>LibFM + TransE</cell><cell>0.966</cell><cell>0.698</cell><cell>0.777</cell><cell>0.839</cell></row><row><cell>PER</cell><cell>0.832</cell><cell>0.617</cell><cell>0.633</cell><cell>0.746</cell></row><row><cell>CKE</cell><cell>0.924</cell><cell>0.677</cell><cell>0.744</cell><cell>0.802</cell></row><row><cell>RippleNet</cell><cell>0.960</cell><cell>0.727</cell><cell>0.770</cell><cell>0.833</cell></row><row><cell>KGNN-LS</cell><cell cols="3">0.979 0.744 0.803</cell><cell>0.850</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The results of AUC in CTR prediction. 8} and the number of training epochs is 50 for all datasets. • LibFM + TransE extends LibFM by attaching an entity representation learned by TransE [2] to each user-item pair. The dimension of TransE is 32 for all datasets. • PER [33] is a representative of path-based methods, which</figDesc><table /><note>• LibFM [16] is a widely used feature-based factorization model for CTR prediction. We concatenate user ID and item ID as in- put for LibFM. The dimension is set as {1, 1,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>AU C of all methods w.r.t. the ratio of training set r .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>R@10 w.r.t. the number of layers L.</figDesc><table><row><cell>d</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell></row><row><cell cols="7">MovieLens-20M 0.134 0.141 0.143 0.155 0.155 0.151</cell></row><row><cell cols="7">Book-Crossing 0.065 0.073 0.077 0.081 0.082 0.080</cell></row><row><cell>Last.FM</cell><cell cols="6">0.111 0.116 0.122 0.109 0.102 0.107</cell></row><row><cell cols="7">Dianping-Food 0.155 0.170 0.167 0.166 0.163 0.161</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>R@10 w.r.t. the dimension of hidden layers d.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>4, 8, 16, 32}; • d = {4, 8, 16, 32, 64, 128}; • L = {1, 2, 3, 4}; • λ = {0, 0.01, 0.1, 0.5, 1, 5};</figDesc><table><row><cell></cell><cell>Movie</cell><cell>Book</cell><cell>Music</cell><cell>Restaurant</cell></row><row><cell>S</cell><cell>16</cell><cell>8</cell><cell>8</cell><cell>4</cell></row><row><cell>d</cell><cell>32</cell><cell>64</cell><cell>16</cell><cell>8</cell></row><row><cell>L</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>2</cell></row><row><cell>λ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this work we treat G an undirected graph, so A u is a symmetric matrix. If both triples (h, r 1 , t ) and (t, r 2 , h) exist, we only consider one of r 1 and r 2 . This is due to the fact that: (1) r 1 and r 2 are the inverse of each other and semantically related; (2) Treating A u symmetric will greatly increase the matrix density.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://searchengineland.com/library/bing/bing-satori 3 https://grouplens.org/datasets/movielens/ 4 http://www2.informatik.uni-freiburg.de/~cziegler/BX/ 5 https://grouplens.org/datasets/hetrec-2011/ 6 https://www.dianping.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research has been supported in part by NSF OAC-1835598, DARPA MCS, ARO MURI, Boeing, Docomo, Hitachi, Huawei, JD, Siemens, and Stanford Data Science Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video suggestion and discovery for youtube: taking random walks through the view graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumeet</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="895" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging metapath based context for top-n recommendation with a neural co-attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1531" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Manifold-based similarity adaptation for label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1547" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent knowledge graph embedding for effective recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bozzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems</title>
		<meeting>the 12th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conferences on Learning Representations</title>
		<meeting>the 6th International Conferences on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Label propagation through linear neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint topic-semantic-aware social recommendation for online voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiannong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shine: Signed heterogeneous information network embedding for sentiment link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="592" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring High-Order User Preference on the Knowledge Graph for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DKN: Deep Knowledge-Aware Network for News Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1835" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2019 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge graph convolutional networks for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2019 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dik Lun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion for Bipartite Edge Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 10th International Joint Conference on Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Knowledge Engineering and Knowledge Management</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Personalized entity recommendation: A heterogeneous information network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 7th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hyperparameter learning for graph based semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1585" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Metagraph based recommendation fusion over heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dik Lun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning</title>
		<meeting>the 20th International conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with graphs. Ph.D. Dissertation. Carnegie Mellon University, language technologies institute, school of computer science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
