<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Character Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Character Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress has been made on developing a unified framework for joint text detection and recognition in natural images, but existing joint models were mostly built on two-stage framework by involving ROI pooling, which can degrade the performance on recognition task. In this work, we propose convolutional character networks, referred as CharNet, which is an one-stage model that can process two tasks simultaneously in one pass. CharNet directly outputs bounding boxes of words and characters, with corresponding character labels. We utilize character as basic element, allowing us to overcome the main difficulty of existing approaches that attempted to optimize text detection jointly with a RNN-based recognition branch. In addition, we develop an iterative character detection approach able to transform the ability of character detection learned from synthetic data to real-world images. These technical improvements result in a simple, compact, yet powerful onestage model that works reliably on multi-orientation and curved text. We evaluate CharNet on three standard benchmarks, where it consistently outperforms the state-of-theart approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref> by a large margin, e.g., with improvements of 65.33%→71.08% (with generic lexicon) on ICDAR 2015, and 54.0%→69.23% on Total-Text, on endto-end text recognition. Code is available at: https:// github.com/MalongTech/research-charnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Text reading in natural images has long been considered as two separate tasks: text detection and recognition, which are implemented sequentially. The two tasks have been advanced individually by the success of deep neural networks. Text detection aims to predict a bounding box for each text instance (e.g., typically a word) in natural images, and cur- * Corresponding author: whuang@malong.com. rent leading approaches are mainly extended from object detection or segmentation frameworks, such as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b23">24]</ref>. Built on text detection, the goal of text recognition is to recognize a sequence of character labels from an cropped image patch including a text instance. Generally, it can be cast into a sequence labeling problem, where various recurrent models with CNN-extracted features have been developed, with state-of-the-art performance achieved <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>However, the two-step pipeline often suffers from a number of limitations. First, learning the two tasks independently would result in a sub-optimization problem, making it difficult to fully explore the potential of text nature. For example, text detection and recognition can work collaboratively by providing strong context and complementary information to each other, which is critical to improving the performance, as substantiated by recent work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>. Second, it often requires to implement multiple sequential steps, resulting in a relatively complicated system, where the performance of text recognition is heavily relied on text detection results.</p><p>Recent effort has been devoted to developing a unified framework that implements text detection and recognition simultaneously <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. For example, in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b23">[24]</ref>, text detection models were extended to joint detection and recognition, by adding a new RNN-based branch for recognition, leading to the state-of-the-art performance on end-to-end (E2E) text recognition. These approaches can achieve joint detection and recognition using a single model, but they are in the family of two-stage framework and thus have the following limitations. Firstly, the recognition branch often explores a RNN-based sequential model, which is difficult to optimize jointly with the detection task, by requiring a significantly larger amount of training samples. Thus the performance is heavily depended on a welldesigned but complicated training scheme (e.g., <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b19">[20]</ref>). This is the central issue that impedes the development of a united framework. Secondly, current two-stage framework commonly involves RoI cropping and pooling, making it difficult to crop an accurate text region for feature pooling, where a large amount of background information may be included. This inevitably leads to significant performance degradation on recognition task, particularly for multi-orientation or curved text.</p><p>To overcome the limitations of RoI cropping and pooling for two-stage framework, He et al. <ref type="bibr" target="#b11">[12]</ref> proposed a text-alignment layer to precisely compute the convolutional features for a text instance of arbitrary orientation, which boosted the performance. In <ref type="bibr" target="#b23">[24]</ref>, multiple affinity transformations were applied to the convolutional features for enhancing text information in the RoI regions. However, these methods failed to work on curved text. In addition, many high-performance models consider words (for English) as detection units, but word-level detection often requires to cast text recognition into a sequence labelling problem, where a RNN model with additional modules, such as CTC <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref> or attention mechanism <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12]</ref>, was applied. Unlike English, words are not clearly distinguishable in some languages such as Chinese, where text instances can be defined and separated more clearly by characters. Therefore, characters are more clearly-defined elements that generalize better over various languages. Importantly, character recognition is straightforward, and can be implemented with a simple CNN model, rather than using a RNN-based sequential model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>In this work, we present Convolotional Character Networks (referred as CharNet) for joint text detection and recognition, by leveraging character as basic unit. Moreover, for the first time, we provide an one-stage CNN model for the joint tasks, with significant performance improvements over the state-of-the-art results achieved by a more complex two-stage framework, such as <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b24">[25]</ref> and <ref type="bibr" target="#b23">[24]</ref>. The proposed CharNet implements direct character detection and recognition, jointly with text instance (e.g., word) detection. This allows it to avoid the RNN-based word recognition, resulting in a simple, compact, yet powerful model that directly outputs bounding boxes of words and characters, as well as the corresponding character labels, as shown in <ref type="figure" target="#fig_0">Fig.1</ref>. Our main contributions are summarized as follows.</p><p>Firstly, we propose an one-stage CharNet for joint text detection and recognition, where a new branch for direct character detection and recognition is introduced, and can be integrated seamlessly into existing text detection framework. We explore character as basic unit, which allows us to overcome the main limitations of current two-stage framework using RoI pooling with RNN-based recognition.</p><p>Secondly, we develop an iterative character detection method which allows CharNet to transform the character detection capability learned from synthetic data to realworld images. This makes it possible for training CharNet on real-world images, without providing additional charlevel bounding boxes.</p><p>Thirdly, CharNet consistently outperforms recent twostage approaches such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref> by a large margin, with improvements of 65.33%→71.08% (generic lexicon) on ICDAR 2015, and 54.0%→69.23% (E2E) on Total-Text. Particularly, it can achieve comparable results, e.g., 67.24% on ICDAR 2015, even by completely removing a lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traditional approaches often regard text detection and recognition as two separate tasks that process sequentially <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32]</ref>. Recent progress has been made on developing a unified framework for joint text detection and recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. We briefly review the related studies on text detection, recognition and join of two tasks.</p><p>Text detection. Recent approaches for text detection were mainly built on general object detectors with various text-specific modifications. For instance, by building on Region Proposal Networks <ref type="bibr" target="#b28">[29]</ref>, Tian et al. <ref type="bibr" target="#b35">[36]</ref> proposed a Connectionist Text Proposal Network (CTPN) to explore the sequence nature of text, and detect a text instance in a sequence of fine-scale text proposals. Similarly, Shi et al. <ref type="bibr" target="#b29">[30]</ref> developed a method with linking segment which also localizes a text instance in a sequence, with the capability for detecting multi-oriented text. In <ref type="bibr" target="#b40">[41]</ref>, EAST was introduced by exploring IOU loss <ref type="bibr" target="#b38">[39]</ref> to detect multioriented text instances (e.g., words), with impressive results achieved. Recently, a single-shot text detector (SSTD) <ref type="bibr" target="#b8">[9]</ref> was proposed by extending SSD object detector <ref type="bibr" target="#b21">[22]</ref> to text detection. SSTD encodes text regional attention into convolutional features to enhance text information.</p><p>Text recognition. Inspired from speech recognition, recent work on text recognition commonly cast it into a sequence-to-sequence recognition problem, where recurrent neural networks (RNNs) were employed. For exam- <ref type="figure">Figure 2</ref>: Overview of the proposed CharNet, which contains two branches working in parallel: a character branch for direct character detection and recognition, and a detection branch for text instance detection. ple, He et al. <ref type="bibr" target="#b9">[10]</ref> exploited convolution neural networks (CNNs) to encode a raw input image into a sequence of deep features, and then a RNN is applied to the sequential features for decoding and yielding confidence maps, where connectionist temporal classification CTC <ref type="bibr" target="#b5">[6]</ref> is applied to generate final results. Shi et al. <ref type="bibr" target="#b30">[31]</ref> improved such CNN+RNN+CTC framework by making it end-toend trainable, with significant performance gain obtained. Recently, the framework was further improved by introducing various attention mechanisms, which are able to encode more character information explicitly or implicitly <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>End-to-end (E2E) text recognition. Recent work attempted to integrate text detection and recognition into a unified framework for E2E text recognition. Li et al. <ref type="bibr" target="#b19">[20]</ref> drew inspiration from Faster R-CNN <ref type="bibr" target="#b28">[29]</ref> and employed RoI pooling to obtain text features from a detection framework for further recognition. In <ref type="bibr" target="#b11">[12]</ref>, He et al. proposed an E2E framework by introducing a new text-alignment layer with character attention mechanism, leading to significant performance improvements by jointly training two tasks. Similar framework has been developed by Liu et al. in <ref type="bibr" target="#b23">[24]</ref>. Both works have achieved strong performance on E2E text recognition, but they were built on two-stage models implementing ROI cropping and pooling operations, which may reduce the performance, particularly on the recognition task for multi-orientation or curved text.</p><p>Our work is related to character-based approaches for text detection or recognition. Hu et al. proposed a Word-Sup able to detect text instances at the character level <ref type="bibr" target="#b13">[14]</ref>, while Liu et al. <ref type="bibr" target="#b22">[23]</ref> developed a character-aware neural network for distorted scene text recognition. However, they did not provide a full solution for E2E text recognition. The most closely related work is that of Mask TextSpotter <ref type="bibr" target="#b24">[25]</ref> which is a two-stage character-based framework for E2E recognition, built on recent Mask R-CNN <ref type="bibr" target="#b8">[9]</ref>. However, our CharNet has a number of clear distinctions: (1) CharNet is the first one-stage model for E2E text recognition, which is different from the two-stage Mask TextSpotter, where RoI cropping and pooling operations are required; (2) CharNet has a character branch that directly outputs accurate charlevel bounding boxes. This enables it to automatically identify characters, allowing it to work in a weakly-supervised manner by using the proposed iterative character detection;</p><p>(3) This results in a distinct capability for training CharNet without additional char-level bounding boxes in real-world images, while Mask TextSpotter requires full char-level annotations which are often highly expensive; (4) CharNet achieved consistent and significant performance improvements over Mask TextSpotter, as shown in <ref type="table">Table 4</ref> and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Convolutional Character Networks</head><p>In this section, we describe the proposed CharNet in details. Then an iterative character detection method is introduced for automatically identifying characters with bounding boxes from real-world images, by leveraging synthetic data. In this work, we use "text instance" as a higher level concept for text, which can be a word or a text-line, with multi-orientation or curved shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>As discussed, existing approaches for E2E text recognition are commonly limited by using RoI cropping and pooling, with a RNN-based sequential model for word recognition. The proposed CharNet is an one-stage convolutional architecture consisting of two branches: (1) a character branch designed for direct character detection and recognition, and (2) a text detection branch predicting a bounding box for each text instance in an image. The two branches are implemented in parallel, which form an one-stage model for joint text detection and recognition, as shown in <ref type="figure">Fig. 2</ref>. The character branch can be integrated seamlessly into an onestage text detection framework, resulting in an end-to-end trainable model. Training the model requires both instancelevel and char-level bounding boxes with character labels as supervised information. In inference, CharNet can directly output both instance-level and char-level bounding boxes with corresponding character labels in one pass.</p><p>Many existing text databases often do not include charlevel annotations which are highly expensive to obtain. We develop an iterative learning approach for automatic character detection, which allows us to learn a character detector from synthetic data where full char-level annotations can be generated unlimitedly. Then the learned character detection capability can be transformed and adapted gradually to real-word images. This enables the model with ability to automatically identify characters in real-world images, providing a weakly-supervision learning manner for CharNet.</p><p>Backbone networks. We employ ResNet-50 <ref type="bibr" target="#b7">[8]</ref> and Hourglass <ref type="bibr" target="#b18">[19]</ref> networks as backbone for our CharNet framework. For ResNet-50, we follow <ref type="bibr" target="#b40">[41]</ref>, and make use of the convolutional feature maps with 4× down-sampling ratio as the final convolutional maps to implement text detection and recognition. This results in high-resolution feature maps that enable CharNet to identify extremely smallscale text instances. For Hourglass networks, we stack two hourglass modules, as shown in <ref type="figure">Fig. 2</ref>, and the final feature maps are up-sampled to <ref type="bibr">1 4</ref> resolution of the input image. In this work, we use two variants of Hourglass networks, Hourglass-88 and Hourglass-57. Hourglass-88 is modified from Hourglass-104 in <ref type="bibr" target="#b18">[19]</ref> by removing two downsampling stages and reducing the number of layers in the last stage of each hourglass module by half. Hourglass-57 is constructed by further removing half number of layers in each stage of hourglass modules. Notice that, for both variants, we do not employ the intermediate supervision as did in CornerNet <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Character Branch</head><p>Existing RNN-based recognition methods were commonly built on word-level optimization with a sequential model, which has a significantly larger search space than direct character classification. This inevitably makes the models more complicated and difficult to train by requiring a significantly longer training time with a large amount of training samples. Recent work, such as <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>, had shown that the performance of RNN-based methods can be improved considerably by introducing char-level attention mechanism which is able to encode strong character information implicitly or explicitly. This enables the models to have the ability to identify characters more accurately, and essentially adds additional constraints to the models which in turn reduce the search space, leading to performance boost. This suggests that precise identification of characters is of great importance to RNN-based text recognition, which inspired the current work to simplify it into direct character recognition with an automatic character localization mechanism, resulting in a simple yet powerful one-stage fully convolutional model for E2E text recognition.</p><p>To this end, we introduce a new character branch which has the functions of direct character detection and recognition. The character branch uses character as basic unit for detection and recognition, and outputs char-level bounding boxes as well as the corresponding character labels. Specifically, the character branch is a stack of convolutional layers, which move densely over the final feature maps of the backbone. It has the input features maps with 1 4 spatial resolution of the input image. This branch contains three sub-branches, for text instance segmentation, character detection and character recognition, respectively. The text instance segmentation sub-branch and character detection sub-branch have three convolutional layers with filter sizes of 3×3, 3×3 and 1×1, respectively. The character recognition sub-branch has four convolutional layers with one more 3 × 3 convolutional layer.</p><p>Text instance segmentation sub-branch exploits a binary mask as supervision, and outputs 2-channel feature maps indicating text or non-text probability at each spatial location. Character detection sub-branch outputs 5-channel feature maps, estimating a character bounding box at each spatial location. By following EAST <ref type="bibr" target="#b40">[41]</ref>, each character bounding box is parameterized by five parameters, indicating the distances of current location to the top, bottom, left and right sides of the bounding box, as well as the orientation of bounding-box. In character recognition sub-branch, character labels are predicted densely over the input feature maps, generating 68-channel probability maps. Each channel is a probability map for a specific character class among 68 character classes, including 26 English characters, 10 digital numbers and 32 special symbols. All of the output feature maps from three sub-branches have the same spatial resolution, which is exactly the same as that of the input feature maps ( 1 4 of the input image). Finally, the char-level bounding boxes are generated by keeping the bounding boxes having a confident value over 0.95. Each generated bounding box has a corresponding character label, which is computed at the corresponding spatial location from the 68-channel classification maps -by using the maximum of the computed softmax scores.</p><p>Training character branch requires char-level bounding boxes with the corresponding character labels. Compared to word-level annotations, acquiring char-level labels with bounding boxes is much more expensive and would significantly increase labor cost. To avoid such additional cost, we develop an iterative character detection mechanism which is described in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Text Detection Branch</head><p>Text detection branch is designed to identify text instances at a higher level concept, such as words or textlines. It provides strong context information which is used to group the detected characters into text instances. Because directly grouping characters by using characters information (e.g., character locations or geometric features) is heuristic and complicated when multiple text instances are located closely within a region, particularly for text instances with multiple orientations or in a curved shape. Our text detection branch can be defined in different forms subjected to the type of text instances, and existing instancelevel text detectors can be adapted with minimum modification. We take text detectors for multi-orientation words or curved text-lines as examples.</p><p>Multi-Orientation Text. We simply modify EAST detector <ref type="bibr" target="#b40">[41]</ref> as our text detection branch, which contains two sub-branches for text instance segmentation and instancelevel bounding box regression using IoU loss. The predicted bounding boxes are parameterized by five parameters including 4 scalars for a bounding box with an orientation angle. We compute dense prediction at each spatial location of the feature maps by using two 3 × 3 convolutional layers, followed by another 1 × 1 convolutional layer. Finally, the text detection branch outputs 2-channel feature maps indicating text or non-text probability, and 5-channel detection maps for bounding boxes with orientation angles. We keep the bounding boxes having a confident value over 0.95.</p><p>Curved Text. For curved text, we modify Textfield in <ref type="bibr" target="#b37">[38]</ref> by using a direction field, which encodes the direction information that points away from text boundary. The direction field is used to separate adjacent text instances, and can be predicted by a new branch in parallel with text detection branch and character branch. This branch is composed of two 3 × 3 convolutional layers, followed by another 1 × 1 convolutional layer.</p><p>Generation of Final Results. The predicted instancelevel bounding boxes are applied to group the generated characters into text instances. We make use of a simple rule, by assigning a character to a text instance if the character bounding box have an overlap (e.g., with &gt; 0 IoU) with an instance-level bounding box. The final outputs of our CharNet are bounding boxes of both text instances and characters, with the corresponding character labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Iterative Character Detection</head><p>Training our model requires both char-level and wordlevel bounding boxes as well as the corresponding character labels. However, char-level bounding boxes are expensive to obtain and are not available in many existing benchmark datasets such as ICDAR 2015 <ref type="bibr" target="#b16">[17]</ref> and Total-Text <ref type="bibr" target="#b4">[5]</ref>. We develop an iterative character detection method that enables our model to have capability for identifying charac- ters by leveraging synthetic data, such as Synth800k <ref type="bibr" target="#b6">[7]</ref>, where multi-level supervised information can be generated unlimitedly. This allows us to train CharNet in a weaklysupervised manner by just using instance-level annotations from real-world images. A straightforward approach is to train our model directly with synthetic images, and then run inference on real-world images. However, it has a large domain gap between the synthetic images and real ones, and therefore the model trained from synthetic images is difficult to work directly on the real-world ones, as shown in <ref type="table">Table 1</ref>, where low performance is obtained on both text detection and E2E recognition. We observed that a text detector has relatively stronger generalization capability than a text recognizer. As shown in <ref type="bibr" target="#b35">[36]</ref>, a text detector trained solely on English and Chinese data can work reasonably on other languages, which inspired us to explore the generalization ability of a character detector to bridge the gap between the two domains.</p><p>Our intuition is to gradually improve the generalization capability of model which is initially trained from synthetic images where full char-level annotations are provided, and the key is to transform the capability of character detection learned from the synthetic data to real-world images. We develop an iterative process by gradually identifying the "correct" char-level bounding boxes from real-world images by the model itself. We make use of a simple rule that identifies a group of char-level bounding boxes as "correct" if the number of character bounding boxes in a text instance is exactly equal to the number of character labels in the provided instance-level transcript. Note that instancelevel transcripts (e.g., words) are often provided in existing datasets for E2E text recognition. The proposed iterative character detection are described as follows.</p><p>-(i) We first train an initial model on synthetic data, where both char-level and instance-level annotations are available to our CharNet. Then we apply the trained model to the training images from a real-world dataset, where char-level bounding boxes are predicted by the learned model.</p><p>-(ii) We explore the aforementioned rule to collect the "correct" char-level bounding boxes detected in realworld images, which are used to further train the model with the corresponding transcripts provided. Note that we do not use the predicted character labels, which are not fully correct and would reduce the performance in our experiments.</p><p>-(iii) This process is implemented iteratively to enhance the model capability gradually for character detection, which in turn continuously improves the quality of the identified characters, with an increasing number of the "correct" char-level bounding boxes generated, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref> and <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments, Results and Comparisons</head><p>Our CharNet is evaluated on three standard benchmarks: ICDAR 2015 <ref type="bibr" target="#b16">[17]</ref>, Total-Text <ref type="bibr" target="#b4">[5]</ref>, and ICDAR MLT 2017 <ref type="bibr" target="#b26">[27]</ref>. ICDAR 2015 includes 1,500 images collected by using Google Glasses. The training set has 1,000 images, and the remaining 500 images are used for evaluation. This dataset is challenging due to the presence of multiorientated and very small-scale text instances. Total-Text consists of 1,555 images with a variety of text types including horizontal, multi-oriented, and curved text instances. The training split and testing split have 1,255 images and 300 images, respectively. ICDAR MLT 2017 is a largescale multi-lingual text dataset, which contains 7,200 training images, 1,800 validation images, and 9,000 testing images. 9 languages are included in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Similar to recent work in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>, our CharNet is trained on both synthetic data and real-world data. The proposed iterative character detection is implemented by using 4 iterative steps. At the first step, CharNet is trained on synthetic data, Synth800k <ref type="bibr" target="#b6">[7]</ref>, for 5 epochs, where both char-level and word-level annotations are available. We use a mini-batch  <ref type="table">Table 2</ref>: 4-step iterative character detection with CharNet. "# Words" is the number of words identified as "correct" at each step iterative learning. "Ratio" denotes the ratio of the "correct" words to all words in the training images from Total-Text. "# Epochs" indicates the number of training epochs for each iterative step. At the Step 0, CharNet is trained on synthetic data for 5 epochs, while Step 1-3 are implemented on real-world images. "E2E" means "End-to-End Recognition with F-measure". of 32 images, with 4 images per GPU. On the synthetic data, we set a base learning rate of 0.0002, which is reduced according to lr base × (1 − iter max iter ) power with power = 0.9, by following <ref type="bibr" target="#b2">[3]</ref>. The remained three iterative steps are implemented on real-world data, by training CharNet for 100, 400 and 800 epochs respectively, on the training set of a benchmark provided, e.g., ICDAR 2015 <ref type="bibr" target="#b16">[17]</ref> or Total-Text <ref type="bibr" target="#b4">[5]</ref>. On the real-world data, we set a base learning rate of 0.002, and use the char-level bounding boxes generated by the model trained from the previous step. We make use of similar data augmentation as <ref type="bibr" target="#b23">[24]</ref> and OHEM <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">On Iterative Character Detection</head><p>Interactive character detection is an important function for CharNet that allows us to train the model on real-world images by only using text instance-level annotations. Thus accurate identification of characters is critical to the performance of CharNet. We evaluate the iterative character detection with CharNet by using various backbone networks on ICDAR 2015. Results are reported in <ref type="table">Table 1</ref>. As can be found, CharNet has low performance on both text detection and E2E recognition when we directly apply the model trained from synthetic data to testing images from ICDAR 2015, due to a large domain gap between the two data sets. The performance can be improved considerably by training CharNet on real-world data with iterative character detection, which demonstrates its efficiency.</p><p>We further investigate the capability of our model for identifying the "correct" characters in real-world images. Experiments were conducted on Total-Text. In this experiment, the "correct" characters are grouped into words, and we calculate the number of correctly-detected words at each iterative step. As shown in <ref type="table">Table 2</ref>, at the step 0, when CharNet is only trained on synthetic data, only 64.95% words are identified as "correct" from real-world training images. Interestingly, this number increases immediately from 64.95% to 88.94% at the step 1, when the proposed iterative character detection is applied. This also leads to a significant performance improvement, from 39.3% to 62.9% on E2E text recognition. The iterative training con- tinues until the number of the identified words dose not increase further. Finally, our method is able to collect 92.65% correct words from real-world images by implementing 4 iterative steps in total. We argue that this number of charlevel annotations learned automatically by model is enough to train our CharNet, as evidenced by the state-of-the-art performance obtained, which is shown next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Text Detection</head><p>We evaluate the performance of CharNet on text detection task. To make a fair comparison, we use the same backbone ResNet-50 as FOTS <ref type="bibr" target="#b23">[24]</ref>. As shown in <ref type="table">Table 3</ref>, our CharNet achieves comparable performance with FOTS when both methods are trained without recognition branch. By jointly optimizing the model with text recognition, Char-Net improves the detection performance by 4.13%, from a F-measure of 85.57% to 89.70%, which is more significant than 2.68% performance gain achieved by FOTS. It suggests that our one-stage model allows text detection and recognition to work more effectively and collaboratively. This enables CharNet with higher capability for identifying extremely challenging text instances with stronger robustness which also reduces false detections, as shown in <ref type="figure" target="#fig_2">Fig.  4</ref>. In addition, CharNet also has a performance improvement of 87.00% → 89.70% on F-measure over that of <ref type="bibr" target="#b11">[12]</ref> which uses a PVAnet <ref type="bibr" target="#b17">[18]</ref> as backbone with multi-scale implementation.</p><p>Moreover, our one-stage CharNet achieves new stageof-the-art performance on text detection on all three benchmarks, which improves recent strong baseline (e.g., He et al. <ref type="bibr" target="#b11">[12]</ref>, FOTS <ref type="bibr" target="#b23">[24]</ref> and TextFiled <ref type="bibr" target="#b37">[38]</ref>) by a large margin. For example, on single-scale case, the improvements on F-measure are: 87.99% → 90.97% on ICDAR 2015 (in <ref type="table">Table 4</ref>), 80.3% → 85.6% on the Total-Text for curved text (in <ref type="table">Table 5</ref>), and 67.25%→75.77% on ICDAR 2017 MLT (in <ref type="table">Table 6</ref>). Notice that CharNet is designed by using characters as basic unit. This natural property allows it to be easily adapted to curved text, where FOTS is difficult to work reliably. TextFiled was designed specifically for curved text but only has a F-measure of 82.4% on ICDAR 2015. Several examples for detecting challenging text instances are presented in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rec. R P F Gain He et al. <ref type="bibr" target="#b11">[12]</ref> 83.00 84.00 83.00 -He et al. <ref type="bibr" target="#b11">[12]</ref> 86.00 87.00 87.00 +4.00 FOTS <ref type="bibr" target="#b23">[24]</ref> 82.04 88.84 85.31 -FOTS <ref type="bibr" target="#b23">[24]</ref> 85. <ref type="bibr" target="#b16">17</ref>   <ref type="table">Table 3</ref>: Detection performance on ICDAR 2015. ResNet-50 was used by both FOTS and CharNet as backbone, while PVAnet <ref type="bibr" target="#b17">[18]</ref> was applied in <ref type="bibr" target="#b11">[12]</ref>. "Rec." denotes "Recognition". "Gain" is the performance gain obtained by joint optimization with text recognition. "R", "P", "F" indicate "Recall", "Precision", "F-measure".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on End-to-End Text Recognition</head><p>For E2E text recognition task, we compare our CharNet with recent state-of-the-art methods on ICDAR 2015 <ref type="bibr" target="#b16">[17]</ref> and Total-Text <ref type="bibr" target="#b4">[5]</ref>.</p><p>ICDAR 2015. As shown in <ref type="table">Table 4</ref>, by using a same backbone ResNet-50, our CharNet has comparable results with Mask TextSpotter <ref type="bibr" target="#b24">[25]</ref>. However, Mask TextSpotter has significant performance improvements by using additional char-level manual annotations on real-world images, with a weighted edit distance applied to a lexicon, e.g., 76.1% → 79.3% (S), 67.1% → 73.0% (W) and 56.7% → 62.4% (G) on E2E recognition. Furthermore, CharNet also outperforms FOTS by 1.38% in terms of generic lexicon. Unlike FOTS, which makes use of a heavy recognition branch with 6.31M parameters, our one-stage model only employs a light-weight CNN-based character branch with 1.19M parameters. Importantly, our model can work reliably without a lexicon, with performance of 60.72%, which is comparable to 60.72% of FOTS with a generic lexicon. These lexicon-free results demonstrate the strong capability of our CharNet, making it better applicable to real-world applications where a lexicon is not always available.</p><p>We further employ Hourglass-57 <ref type="bibr" target="#b18">[19]</ref> as backbone, which has the similar number of model parameters compared to FOTS (34.96M v.s. 34.98M). As shown in <ref type="table">Table  4</ref>, our CharNet outperforms FOTS by 6.12% with generic lexicon. With a more powerful Hourglass-88, we set a new state-of-the-art single-scale performance on the benchmark, and improve both Mask TextSpotter and FOTS considerably in all terms. Finally, with multi-scale inference, CharNet surpasses the previous best results <ref type="bibr" target="#b23">[24]</ref> by a large margin, e.g., from 65.33% to 71.08% with generic lexicon.</p><p>Total-Text. We conduct experiments on Total-text to show that the capability of our CharNet on curved text. We employ the protocol described in <ref type="bibr" target="#b4">[5]</ref> to evaluate the performance of text detection, and follow the evaluation protocol presented in <ref type="bibr" target="#b24">[25]</ref>   <ref type="table">Table 4</ref>: Results on ICDAR 2015. "R-*" and "H-*" denote "ResNet-*" and "Hourglass-*". "MS" means multi-scale inference. "R", "P", "R" are "Recall", "Precision", "F-measure". "S", "W", "G" and "N" mean F-measure using "Strong", "Week", "Generic" and "None" lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Detection E2E R P F Textboxes <ref type="bibr" target="#b20">[21]</ref> 45.5 62.1 52. <ref type="bibr" target="#b4">5</ref>   <ref type="table">Table 5</ref>: Results on Total-Text. "H-*" denotes "Hourglass-*". "MS" indicates multi-scale inference. "R", "P", "R" are "Recall", "Precision", "F-measure". "E2E" is "End-to-End Recognition using F-measure".  <ref type="table">Table 6</ref>: Text detection on ICDAR 2017 MLT. "R-*" and "H-*" denote "ResNet-*" and "Hourglass-*". "R", "P" and "F" represent "Recall", "Precision" and "F-measure". "MS" indicates multi-scale inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>in E2E recognition. As shown in <ref type="table">Table 5</ref>, CharNet outperforms current state-of-the-art methods by 5.9% F-measure on text detection, and 15.2% on E2E recognition. Compared to character-based method, Mask TextSpotter <ref type="bibr" target="#b24">[25]</ref>, our CharNet can obtain even larger performance improvements on curved text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented an one-stage CharNet for E2E text recognition. We introduce a new branch for direct character recognition, which can be integrated seamlessly into text detection framework. This results in the first one-stage fully convolutional model that implements two tasks jointly, setting it apart from existing RNN-integrated two-stage framework. We demonstrate that with CharNet, the two tasks can be trained more effectively and collaboratively, leading to significant performance improvements. Furthermore, we develop an iterative character detection able to transfer the character detection capability learned from synthetic data to real-world images. In addition, CharNet is compact with less parameters, and can work reliably on curved text. Extensive experiments were conducted on ICDAR 2015, MTL 2017 and Total-text, where CharNet consistently outperforms existing approaches by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed CharNet can directly output bounding boxes of words and characters, with corresponding character labels in one pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Character bounding boxes generated at 4 interactive steps from left to right. Red boxes indicate the identified "correct" ones by our rule, while blue boxes mean invalid ones, which are not collected for training in next step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>CharNet improves both recall and precision on text detection by jointly learning with character recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Full results by CharNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>for E2E recognition. No lexicon is used 89.99 91.98 90.97 CharNet H-88 83.10 79.15 69.14 65.73 Multi-Scale He et al. MS [12] -86.00 87.00 87.00 He et al. MS [12] 82.00 77.00 63.00 -FOTS R-50 MS [24] 34.98 M 87.92 91.85 89.84 FOTS R-50 MS [24] 83.55 79.11 65.33 -CharNet R-50 MS 26.48 M 90.90 89.44 90.16 CharNet R-50 MS 82.46 78.86 67.64 62.71 CharNet H-57 MS 34.96 M 91.43 88.74 90.06 CharNet H-57 MS 84.07 80.10 69.21 65.26 CharNet H-88 MS 89.21 M 90.47 92.65 91.55 CharNet H-88 MS 85.05 81.25 71.08 67.24</figDesc><table><row><cell>Method</cell><cell>Params</cell><cell>R</cell><cell>Detection P</cell><cell>F</cell><cell>Method</cell><cell>S</cell><cell cols="2">End-to-End Recognition W G N</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Single Scale</cell><cell></cell><cell></cell></row><row><cell>WordSup [14]</cell><cell>-</cell><cell cols="4">77.03 79.33 78.16 Neumann et al. [28]</cell><cell cols="2">35.00 20.00 16.00</cell><cell>-</cell></row><row><cell>EAST [41]</cell><cell>-</cell><cell cols="4">78.33 83.27 80.72 Deep text spotter [2]</cell><cell cols="2">54.00 51.00 47.00</cell><cell>-</cell></row><row><cell>R2CNN [16]</cell><cell>-</cell><cell cols="6">79.68 85.62 82.54 TextProp.+DictNet [13, 40] 53.30 49.61 47.18</cell><cell>-</cell></row><row><cell>Mask TextSpotter [25] *</cell><cell>-</cell><cell cols="4">81.00 91.60 86.00 Mask TextSpotter [25] *</cell><cell cols="2">79.30 73.00 62.40</cell><cell>-</cell></row><row><cell>FOTS R-50 [24]</cell><cell cols="5">34.98 M 85.17 91.00 87.99 FOTS R-50 [24]</cell><cell cols="2">81.09 75.90 60.80</cell><cell>-</cell></row><row><cell>CharNet R-50</cell><cell cols="5">26.48 M 88.30 91.15 89.70 CharNet R-50</cell><cell cols="3">80.14 74.45 62.18 60.72</cell></row><row><cell>CharNet H-57</cell><cell cols="5">34.96 M 88.88 90.45 89.66 CharNet H-57</cell><cell cols="3">81.43 77.62 66.92 62.79</cell></row><row><cell>CharNet H-88</cell><cell>89.21 M</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep textspotter: An end-to-end trainable scene text localization and recognition framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Ch&amp;apos;ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reading scene text in deep convolutional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reading scene text in deep convolutional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An end-to-end textspotter with explicit alignment and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5020" to="5029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text localization in natural images using stroke feature transform and text covariance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>13th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08588</idno>
		<title level="m">Pvanet: Lightweight deep neural networks for real-time object detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards end-to-end text spotting with convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Char-net: A character-aware neural network for distorted scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fots: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5676" to="5685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multilingual scene text detection and script identificationrrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IAPR International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
		<respStmt>
			<orgName>IC-DAR</orgName>
		</respStmt>
	</monogr>
	<note>Document Analysis and Recognition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Real-time lexicon-free scene text localization and recognition. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1872" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06520</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09900</idno>
		<title level="m">Textnet: Irregular text reading from images with an end-to-end trainable network</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced mser tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Symmetrybased text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2558" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
