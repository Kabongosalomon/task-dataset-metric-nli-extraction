<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaled-YOLOv4: Scaling Cross Stage Partial Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan</forename><forename type="middle">Mark</forename><surname>Liao</surname></persName>
							<email>liao@iis.sinica.edu.tw</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Alexey Bochkovskiy</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Science Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scaled-YOLOv4: Scaling Cross Stage Partial Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. We propose a network scaling approach that modifies not only the depth, width, resolution, but also structure of the network. YOLOv4large model achieves state-of-the-art results: 55.5% AP (73.4% AP 50 ) for the MS COCO dataset at a speed of ∼16 FPS on Tesla V100, while with the test time augmentation, YOLOv4-large achieves 56.0% AP (73.3 AP 50 ). To the best of our knowledge, this is currently the highest accuracy on the COCO dataset among any published work. The YOLOv4-tiny model achieves 22.0% AP (42.0% AP 50 ) at a speed of ∼443 FPS on RTX 2080Ti, while by using Ten-sorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774 FPS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The deep learning-based object detection technique has many applications in our daily life. For example, medical image analysis, self-driving vehicles, business analytics, and face identification all rely on object detection. The computing facilities required for the above applications maybe cloud computing facilities, general GPU, IoT clusters, or single embedded device. In order to design an effective object detector, model scaling technique is very important, because it can make object detector achieve high accuracy and real-time inference on various types of devices.</p><p>The most common model scaling technique is to change the depth (number of convolutional layers in a CNN) and width (number of convolutional filters in a convolutional layer) of the backbone, and then train CNNs suitable for different devices. For example among the ResNet <ref type="bibr" target="#b10">[11]</ref> series, ResNet-152 and ResNet-101 are often used in cloud server GPUs, ResNet-50 and ResNet-34 are often used in personal computer GPUs, and ResNet-18 and ResNet-10 can be used in low-end embedded systems. In <ref type="bibr" target="#b1">[2]</ref>, Cai et al. try to develop techniques that can be applied to various device network architectures with only training once. They use techniques such as decoupling training and search and knowledge distillation to decouple and train several sub-nets, so that the entire network and sub-nets are capable of processing target tasks. Tan et al. <ref type="bibr" target="#b33">[34]</ref> proposed using NAS technique to perform compound scaling, including the treatment of width, depth, and resolution on EfficientNet-B0. They use this initial network to search for the best CNN architecture for a given amount of computation and set it as EfficientNet-B1, and then use linear scaleup technique to obtain architectures such as EfficientNet-B2 to EfficientNet-B7. Radosavovic et al. <ref type="bibr" target="#b26">[27]</ref> summarized and added constraints from the vast parameter search space AnyNet, and then designed RegNet. In RegNet, they found that the optimal depth of CNN is about 60. They also found when the bottleneck ratio is set to 1 and the width increase rate of cross-stage is set to 2.5 will receive the best performance. In addition, recently there are NAS and model scaling methods specifically proposed for object detection, such as SpineNet <ref type="bibr" target="#b5">[6]</ref> and EfficientDet <ref type="bibr" target="#b34">[35]</ref>.</p><p>Through analysis of state-of-the-art object detectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>, we found that CSPDarknet53, which is the backbone of YOLOv4 <ref type="bibr" target="#b0">[1]</ref>, matches almost all optimal architecture features obtained by network architecture search technique. The depth of CSPDarknet53, bottleneck ratio, width growth ratio between stages are 65, 1, and 2, respectively. Therefore, we developed model scaling technique based on YOLOv4 and proposed scaled-YOLOv4. The proposed scaled-YOLOv4 turned out with excellent performance, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The design procedure of scaled-YOLOv4 is as follows. First, we re-design YOLOv4 and propose YOLOv4-CSP, and then based on YOLOv4-CSP we developed scaled-YOLOv4. In the proposed scaled-YOLOv4, we discussed the upper and lower bounds of linear scaling up/down models, and respectively analyzed the issues that need to be paid attention to in model scaling for small models and large models. Thus, we are able to systematically develop YOLOv4-large and YOLOv4-tiny models. Scaled-YOLOv4 can achieve the best trade-off between speed and accuracy, and is able to perform real-time object detection on 16 FPS, 30 FPS, and 60 FPS movies, as well as embedded systems.</p><p>We summarize the contributions of this paper : (1) design a powerful model scaling method for small model, which can systematically balance the computation cost and memory bandwidth of a shallow CNN; (2) design a simple yet effective strategy for scaling a large object detector; <ref type="bibr" target="#b2">(3)</ref> analyze the relations among all model scaling factors and then perform model scaling based on most advantageous group partitions; <ref type="bibr" target="#b3">(4)</ref> experiments have confirmed that the FPN structure is inherently a once-for-all structure; and (5) we make use of the above methods to develop YOLOv4-tiny and YOLO4v4-large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Real-time object detection</head><p>Object detectors is mainly divided into one-stage object detectors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref> and two-stage object detectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref>. The output of one-stage object detector can be obtained after only one CNN operation. As for twostage object detector, it usually feeds the high score region proposals obtained from the first-stage CNN to the secondstage CNN for final prediction. The inference time of onestage object detectors and two-stage object detectors can be expressed as T one = T 1 st and T two = T 1 st + mT 2 nd , where m is the number of region proposals whose confidence score is higher than a threshold. In other words, the inference time required for one-stage object detectors is constant, while the inference time required for two-stage object detectors is not fixed. So if we need real-time object detectors, they are almost necessarily one-stage object detectors. Today's popular one-stage object detectors mainly have two kinds: anchor-based <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref> and anchor-free <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref>. Among all anchor-free approaches, Cen-terNet <ref type="bibr" target="#b45">[46]</ref> is very popular because it does not require complicated post-processing, such as Non-Maximum Suppression (NMS). At present, the more accurate real-time onestage object detectors are anchor-based EfficientDet <ref type="bibr" target="#b34">[35]</ref>, YOLOv4 <ref type="bibr" target="#b0">[1]</ref>, and PP-YOLO <ref type="bibr" target="#b21">[22]</ref>. In this paper, we developed our model scaling methods based on YOLOv4 [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model scaling</head><p>Traditional model scaling method is to change the depth of a model, that is to add more convolutional layers. For example, the VGGNet <ref type="bibr" target="#b31">[32]</ref>   <ref type="bibr" target="#b42">[43]</ref> thought about the width of the network, and they changed the number of kernel of convolutional layer to realize scaling. They therefore design wide ResNet (WRN) , while maintaining the same accuracy. Although WRN has higher amount of parameters than ResNet, the inference speed is much faster. The subsequent DenseNet <ref type="bibr" target="#b11">[12]</ref> and ResNeXt <ref type="bibr" target="#b40">[41]</ref> also designed a compound scaling version that puts depth and width into consideration. As for image pyramid inference, it is a common way to perform augmentation at run time. It takes an input image and makes a variety of different resolution scaling, and then input these distinct pyramid combinations into a trained CNN. Finally, the network will integrate the multiple sets of outputs as its ultimate outcome. Redmon et al. <ref type="bibr" target="#b29">[30]</ref> use the above concept to execute input image size scaling. They use higher input image resolution to perform fine-tune on a trained Darknet53, and the purpose of executing this step is to get higher accuracy.</p><p>In recent years, network architecture search (NAS) related research has been developed vigorously, and NAS-FPN <ref type="bibr" target="#b7">[8]</ref> has searched for the combination path of feature pyramid. We can think of NAS-FPN as a model scaling technique which is mainly executed at the stage level. As for EfficientNet <ref type="bibr" target="#b33">[34]</ref>, it uses compound scaling search based on depth, width, and input size. The main design concept of EfficientDet <ref type="bibr" target="#b34">[35]</ref> is to disassemble the modules with different functions of object detector, and then perform scaling on the image size, width, #BiFPN layers, and #box/class layer. Another design that uses NAS concept is SpineNet <ref type="bibr" target="#b5">[6]</ref>, which is mainly aimed at the overall architecture of fish-shaped object detector for network architecture search. This design concept can ultimately produce a scale-permuted structure. Another network with NAS design is RegNet <ref type="bibr" target="#b26">[27]</ref>, which mainly fixes the number of stage and input resolution, and integrates all parameters such as depth, width, bottleneck ratio and group width of each stage into depth, initial width, slope, quantize, bottleneck ratio, and group width. Finally, they use these six parameters to perform compound model scaling search. The above methods are all great work, but few of them analyze the relation between different parameters. In this paper, we will try to find a method for synergistic compound scaling based on the design requirements of object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Principles of model scaling</head><p>After performing model scaling for the proposed object detector, the next step is to deal with the quantitative factors that will change, including the number of parameters with qualitative factors. These factors include model inference time, average precision, etc. The qualitative factors will have different gain effects depending on the equipment or database used. We will analyze and design for quantitative factors in 3.1. As for 3.2 and 3.3, we will design qualitative factors related to tiny object detector running on low-end device and high-end GPUs respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General principle of model scaling</head><p>When designing the efficient model scaling methods, our main principle is that when the scale is up/down, the lower/higher the quantitative cost we want to increase/decrease, the better. In this section, we will show and analyze various general CNN models, and try to understand their quantitative costs when facing changes in (1) image size, (2) number of layers, and (3) number of channels. The CNNs we chose are ResNet, ResNext, and Darknet.</p><p>For the k-layer CNNs with b base layer channels, the computations of ResNet layer is k *</p><formula xml:id="formula_0">[conv(1 × 1, b/4) → conv(3 × 3, b/4) → conv(1 × 1, b)], and that of ResNext layer is k * [conv(1 × 1, b/2) → gconv(3 × 3/32, b/2) → conv(1 × 1, b)].</formula><p>As for the Darknet layer, the amount of</p><formula xml:id="formula_1">computation is k * [conv(1 × 1, b/2) → conv(3 × 3, b)].</formula><p>Let the scaling factors that can be used to adjust the image size, the number of layers, and the number of channels be α, β, and γ, respectively. When these scaling factors vary, the corresponding changes on FLOPs are summarized in <ref type="table" target="#tab_1">Table  1</ref>. It can be seen from <ref type="table" target="#tab_1">Table 1</ref> that the scaling size, depth, and width cause increase in the computation cost. They respectively show square, linear, and square increase. The CSPNet <ref type="bibr" target="#b36">[37]</ref> proposed by Wang et al. can be applied to various CNN architectures, while reducing the amount of parameters and computations. In addition, it also improves accuracy and reduces inference time. We apply it to ResNet, ResNeXt, and Darknet and observe the changes in the amount of computations, as shown in <ref type="table" target="#tab_2">Table 2</ref>. From the figures shown in <ref type="table" target="#tab_2">Table 2</ref>, we observe that after converting the above CNNs to CSPNet, the new architecture can effectively reduce the amount of computations (FLOPs) on ResNet, ResNeXt, and Darknet by 23.5%, 46.7%, and 50.0%, respectively. Therefore, we use CSP-ized models as the best model for performing model scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scaling Tiny Models for Low-End Devices</head><p>For low-end devices, the inference speed of a designed model is not only affected by the amount of computation and model size, but more importantly, the limitation of peripheral hardware resources must be considered. Therefore, when performing tiny model scaling, we must also consider factors such as memory bandwidth, memory access cost (MACs), and DRAM traffic. In order to take into account the above factors, our design must comply with the following principles: Make the order of computations less than O(whkb 2 ): Lightweight models are different from large models in that their parameter utilization efficiency must be higher in order to achieve the required accuracy with a small amount of computations. When performing model scaling, we hope the order of computation can be as low as possible. In <ref type="table" target="#tab_3">Table  3</ref>, we analyze the network with efficient parameter utilization, such as the computation load of DenseNet and OS-ANet <ref type="bibr" target="#b14">[15]</ref>, where g means growth rate. For general CNNs, the relationship among g, b, and k listed in <ref type="table" target="#tab_3">Table 3</ref> is k &lt;&lt; g &lt; b. Therefore, the order of computation complexity of DenseNet is O(whgbk), and that of OSANet is O(max(whbg, whkg 2 )). The or-der of computation complexity of the above two is less than O(whkb 2 ) of the ResNet series. Therefore, we design our tiny model with the help of OSANet, which has a smaller computation complexity. Minimize/balance size of feature map: In order to get the best trade-off in terms of computing speed, we propose a new concept, which is to perform gradient truncation between computational block of the CSPOSANet. If we apply the original CSPNet design to the DenseNet or ResNet architectures, because the j th layer output of these two architectures is the integration of the 1 st to (j − 1) th layer outputs, we must treat the entire computational block as a whole. Because the computational block of OSANet belongs to the PlainNet architecture, making CSPNet from any layer of a computational block can achieve the effect of gradient truncation. We use this feature to re-plan the b channels of the base layer and the kg channels generated by computational block, and split them into two paths with equal channel numbers, as shown in <ref type="table" target="#tab_4">Table 4</ref>. layer ID original CSP partial in CB</p><formula xml:id="formula_2">1 b → g g → g g → g 2 g → g g → g g → g ... g → g g → g g → g k g → g g → g g → g T (b + kg) → (b + kg)/2 kg → kg (b + kg)/2 → (b + kg)/2</formula><p>When the number of channel is b + kg, if one wants to split these channels into two paths, the best partition is to divide it into two equal parts, i.e. (b + kg)/2. When we actually consider the bandwidth τ of the hardware, if software optimization is not considered, the best value is ceil((b + kg)/2τ ) × τ . The CSPOSANet we designed can dynamically adjust the channel allocation. Maintain the same number of channels after convolution: For evaluating the computation cost of low-end device, we must also consider power consumption, and the biggest factor affecting power consumption is memory access cost (MAC). Usually the MAC calculation method for a convolution operation is as follows:</p><formula xml:id="formula_3">M AC = hw(C in + C out ) + KC in C out<label>(1)</label></formula><p>where h, w, C in , C out , and K represent, respectively, the height and width of feature map, the channel number of input and output, and the kernel size of convolutional filter. By calculating geometric inequalities, we can derive the smallest MAC when C in = C out <ref type="bibr" target="#b22">[23]</ref>.</p><p>Minimize Convolutional Input/Output (CIO): CIO <ref type="bibr" target="#b3">[4]</ref> is an indicator that can measure the status of DRAM IO. <ref type="table" target="#tab_5">Table 5</ref> lists the CIO of OSA, CSP, and our designed CSPOSANet. When kg &gt; b/2, the proposed CSPOSANet can obtain the best CIO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scaling Large Models for High-End GPUs</head><p>Since we hope to improve the accuracy and maintain the real-time inference speed after scaling up the CNN model, we must find the best combination among the many scaling factors of object detector when performing compound scaling. Usually, we can adjust the scaling factors of an object detector's input, backbone, and neck. The potential scaling factors that can be adjusted are summarized as <ref type="table" target="#tab_6">Table 6</ref>. The biggest difference between image classification and object detection is that the former only needs to identify the category of the largest component in an image, while the latter needs to predict the position and size of each object in an image. In one-stage object detector, the feature vector corresponding to each location is used to predict the category and size of an object at that location. The ability to better predict the size of an object basically depends on the receptive field of the feature vector. In the CNN architecture, the thing that is most directly related to receptive field is the stage, and the feature pyramid network (FPN) architecture tells us that higher stages are more suitable for predicting large objects. In <ref type="table" target="#tab_7">Table 7</ref>, we illustrate the relations between receptive field and several parameters. Scaling factor Effect of receptive field size input no effect. width no effect. depth one more k × k conv layer, increases k − 1. #stage one more stage, receptive field doubled.</p><p>From <ref type="table" target="#tab_7">Table 7</ref>, it is apparent that width scaling can be independently operated. When the input image size is increased, if one wants to have a better prediction effect for large objects, he/she must increase the depth or number of stages of the network. Among the parameters listed in <ref type="table" target="#tab_7">Table  7</ref>, the compound of {size input , #stage} turns out with the best impact. Therefore, when performing scaling up, we first perform compound scaling on size input , #stage, and then according to real-time requirements, we further perform scaling on depth and width respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Scaled-YOLOv4</head><p>In this section, we put our emphasis on designing scaled YOLOv4 for general GPUs, low-end GPUs, and high-end GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CSP-ized YOLOv4</head><p>YOLOv4 is designed for real-time object detection on general GPU. In this sub-section, we re-design YOLOv4 to YOLOv4-CSP to get the best speed/accuracy trade-off. Backbone: In the design of CSPDarknet53, the computation of down-sampling convolution for cross-stage process is not included in a residual block. Therefore, we can deduce that the amount of computation of each CSPDarknet stage is whb 2 (9/4+3/4+5k/2). From the formula deduced above, we know that CSPDarknet stage will have a better computational advantage over Darknet stage only when k &gt; 1 is satisfied. The number of residual layer owned by each stage in CSPDarknet53 is 1-2-8-8-4 respectively. In order to get a better speed/accuracy trade-off, we convert the first CSP stage into original Darknet residual layer. Neck: In order to effectively reduce the amount of computation, we CSP-ize the PAN <ref type="bibr" target="#b19">[20]</ref> architecture in YOLOv4. The computation list of a PAN architecture is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(a). It mainly integrates the features coming from different feature pyramids, and then passes through two sets of reversed Darknet residual layer without shortcut connections. After CSP-ization, the architecture of the new computation list is shown in <ref type="figure" target="#fig_1">Figure 2(b)</ref>. This new update effectively cuts down 40% of computation. SPP: The SPP module was originally inserted in the middle position of the first computation list group of the neck. Therefore, we also inserted SPP module in the middle position of the first computation list group of the CSPPAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">YOLOv4-tiny</head><p>YOLOv4-tiny is designed for low-end GPU device, the design will follow principles mentioned in section 3.2. We will use the CSPOSANet with PCB architecture to form the backbone of YOLOv4. We set g = b/2 as the growth rate and make it grow to b/2 + kg = 2b at the end. Through calculation, we deduced k = 3, and its architecture is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. As for the number of channels of each stage and the part of neck, we follow the design of YOLOv3-tiny.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">YOLOv4-large</head><p>YOLOv4-large is designed for cloud GPU, the main purpose is to achieve high accuracy for object detection. We designed a fully CSP-ized model YOLOv4-P5 and scaling it up to YOLOv4-P6 and YOLOv4-P7. <ref type="figure" target="#fig_3">Figure 4</ref> shows the structure of YOLOv4-P5, YOLOv4-P6, and YOLOv4-P7. We designed to perform compound scaling on size input , #stage. We set the depth scale of each stage to 2 ds i , and d s to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b6">7]</ref>. Finally, we further use inference time as constraint to perform additional width scaling. Our experiments show that YOLOv4-P6 can reach real-time performance at 30 FPS video when the width scaling factor is equal to 1. For YOLOv4-P7, it can reach real-time performance at 16 FPS video when the width scaling factor is equal to 1.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We use MSCOCO 2017 object detection dataset to verify the proposed scaled-YOLOv4. We do not use Ima-geNet pre-trained models, and all scaled-YOLOv4 models are trained from scratch and the adopted tool is SGD optimizer. The time used for training YOLOv4-tiny is 600 epochs, and that used for training YOLOv4-CSP is 300 epochs. As for YOLOv4-large, we execute 300 epochs first and then followed by using stronger data augmentation method to train 150 epochs. As for the Lagrangian multiplier of hyper-parameters, such as anchors of learning rate, the degree of different data augmentation methods, we use </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation study on CSP-ized model</head><p>In this sub-section, we will CSP-ize different models and analyze the impact of CSP-ization on the amount of parameters, computations, throughput, and average precision. We use Darknet53 (D53) as backbone and choose FPN with SPP (FPNSPP) and PAN with SPP (PANSPP) as necks to design ablation studies. In <ref type="table" target="#tab_8">Table 8</ref> we list the AP val results after CSP-izing different DNN models. We use LeakyReLU (Leaky) and Mish activation function respectively to compare the amount of used parameters, computations, and throughput. Experiments are all conducted on COCO minval dataset and the resulting APs are shown in the last column of <ref type="table" target="#tab_8">Table 8</ref>. From the data listed in <ref type="table" target="#tab_8">Table 8</ref>, it can be seen that the CSP-ized models have greatly reduced the amount of parameters and computations by 32%, and brought improvements in both Batch 8 throughput and AP. If one wants to maintain the same frame rate, he/she can add more lay-  ers or more advanced activation functions to the models after CSP-ization. From the figures shown in <ref type="table" target="#tab_8">Table 8</ref>, we can see that both CD53s-CFPNSPP-Mish, and CD53s-CPANSPP-Leaky have the same batch 8 throughput with D53-FPNSPP-Leaky, but they respectively have 1% and 1.6% AP improvement with lower computing resources. From the above improvement figures, we can see the huge advantages brought by model CSP-ization. Therefore, we decided to use CD53s-CPANSPP-Mish, which results in the highest AP in <ref type="table" target="#tab_8">Table 8</ref> as the backbone of YOLOv4-CSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study on YOLOv4-tiny</head><p>In this sub-section, we design an experiment to show how flexible can be if one uses CSPNet with partial func- tions in computational blocks. We also compare with CSP-Darknet53, in which we perform linear scaling down on width and depth. The results are shown in <ref type="table" target="#tab_9">Table 9</ref>.</p><p>From the figures shown in <ref type="table" target="#tab_9">Table 9</ref>, we can see that the designed PCB technique can make the model more flexible, because such a design can be adjusted according to actual needs. From the above results, we also confirmed that linear scaling down does have its limitation. It is apparent that when under limited operating conditions, the residual addition of tinyCD53s becomes the bottleneck of inference speed, because its frame rate is much lower than the COSA architecture with the same amount of computations. Meanwhile, we also see that the proposed COSA can get a higher AP. Therefore, we finally chose COSA-2x2x which received the best speed/accuracy trade-off in our experiment as the YOLOv4-tiny architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation study on YOLOv4-large</head><p>In <ref type="table" target="#tab_1">Table 10</ref> we show the AP obtained by YOLOv4 models in training from scratch and fine-tune stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Scaled-YOLOv4 for object detection</head><p>We compare with other real-time object detectors, and the results are shown in <ref type="table" target="#tab_1">Table 11</ref>. The values marked in bold in the [AP, AP 50 , AP 75 , AP S , AP M , AP L ] items indicate that model is the best performer in the corresponding item. We can see that all scaled YOLOv4 models, including YOLOv4-CSP, YOLOv4-P5, YOLOv4-P6, YOLOv4-P7, are Pareto optimal on all indicators. When we compare YOLOv4-CSP with the same accuracy of EfficientDet-D3 (47.5% vs 47.5%), the inference speed is 1.9 times. When YOLOv4-P5 is compared with EfficientDet-D5 with the same accuracy (51.8% vs 51.5%), the inference speed is 2.9 times. The situation is similar to the comparisons between YOLOv4-P6 vs EfficientDet-D7 (54.5% vs 53.7%) and YOLOv4-P7 vs EfficientDet-D7x (55.5% vs 55.1%). In both cases, YOLOv4-P6 and YOLOv4-P7 are, respectively, 3.7 times and 2.5 times faster in terms of inference speed. All scaled-YOLOv4 models reached state-of-the-art results.</p><p>The results of test-time augmentation (TTA) experiments of YOLOv4-large models are shown in <ref type="table" target="#tab_1">Table 12</ref>. YOLOv4-P5, YOLOv4-P6, and YOLOv4-P7 gets 1.1%, 0.7%, and 0.5% higher AP, respectively, after TTA is applied. We then compare the performance of YOLOv4-tiny with that of other tiny object detectors, and the results are shown in <ref type="table" target="#tab_1">Table 13</ref>. It is apparent that YOLOv4-tiny achieves the best performance in comparison with other tiny models. Finally, we put YOLOv4-tiny on different embedded GPUs for testing, including Xavier AGX, Xavier NX, Jetson TX2, Jetson NANO. We also use TensorRT FP32 (FP16 if supported) for testing. All frame rates obtained by different models are listed in <ref type="table" target="#tab_1">Table 14</ref>. It is apparent that YOLOv4-tiny can achieve real-time performance no matter which device is used. If we adopt FP16 and batch size 4 to test Xavier AGX and Xavier NX, the frame rate can reach 380 FPS and 199 FPS respectively. In addition, if one uses TensorRT FP16 to run YOLOv4-tiny on general GPU RTX 2080ti, when the batch size respectively equals to 1 and 4, the respective frame rate can reach 773 FPS and 1774 FPS, which is extremely fast.   We can find that YOLOv4-P7 has the best AP at high resolution, while YOLOv4-P7\P7 and YOLOv4-P7\P7\P6 have the best AP at middle and low resolution, respectively. This means that we can use sub-nets of FPN-like models to execute the object detection task well. Moreover, we can perform compound scale-down the model architectures and input size of an object detector to get the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We show that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks. So we achieve the highest accuracy 56.0% AP on test-dev COCO dataset for the model YOLOv4-large, extremely high speed 1774 FPS for the small model YOLOv4-tiny on RTX 2080Ti by using TensorRT-FP16, and optimal speed and accuracy for other YOLOv4 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of the proposed YOLOv4 and other state-of-the-art object detectors. The dashed line means only latency of model inference, while the solid line include model inference and post-processing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Computaional blocks of reversed Dark layer (SPP) and reversed CSP dark layers (SPP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Computational block of YOLOv4-tiny.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Architecture of YOLOv4-large, including YOLOv4-P5, YOLOv4-P6, and YOLOv4-P7. The dashed arrow means replace the corresponding CSPUp block by CSPSPP block. k-means and genetic algorithms to determine. All details related to hyper-parameters are elaborated in Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5. 5 .</head><label>5</label><figDesc>TensorRT. FPS AGX FPS N X FPS T X2 FPS N AN O Scaled-YOLOv4 as naïve once-for-all modelIn this sub-section, we design experiments to show that an FPN-like architecture is a naïve once-for-all model. Here we remove some stages of top-down path and detection branch of YOLOv4-P7. YOLOv4-P7\P7 and YOLOv4-P7\P7\P6 represent the model which has removed {P7} and {P7, P6} stages from the trained YOLOv4-P7. Figure 5 shows the AP difference between pruned models and original YOLOv4-P7 with different input resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>YOLOv4-P7 as "once-for-all" model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>FLOPs of different computational layers with different model scalng factors.</figDesc><table><row><cell>Model</cell><cell>original</cell><cell cols="3">size α depth β width γ</cell></row><row><cell>Res layer</cell><cell>r = 17whkb 2 /16</cell><cell>α 2 r</cell><cell>βr</cell><cell>γ 2 r</cell></row><row><cell cols="3">ResX layer x = 137whkb 2 /128 α 2 x</cell><cell>βx</cell><cell>γ 2 x</cell></row><row><cell cols="2">Dark layer d = 5whkb 2</cell><cell>α 2 d</cell><cell>βd</cell><cell>γ 2 d</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FLOPs of different computational layers with/without CSP-ization.</figDesc><table><row><cell>Model</cell><cell>original</cell><cell>to CSP</cell></row><row><cell>Res layer</cell><cell>17whkb 2 /16</cell><cell>whb 2 (3/4 + 13k/16)</cell></row><row><cell cols="3">ResX layer 137whkb 2 /128 whb 2 (3/4 + 73k/128)</cell></row><row><cell>Dark layer</cell><cell>5whkb 2</cell><cell>whb 2 (3/4 + 5k/2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>FLOPs of Dense layer and OSA layer.</figDesc><table><row><cell>Model</cell><cell>FLOPs</cell></row><row><cell cols="2">Dense layer whgbk + whg 2 k(k − 1)/2</cell></row><row><cell>OSA layer</cell><cell>whbg + whg 2 (k − 1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Number of channel of OSANet, CSPOSANet, and CSPOSANet with partial in computational block (PCB).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>CIO of OSANet, CSPOSANet, and CSPOSANet with PCB. − 1)g 2 + (b + kg) 2 /2 kg 2 + (kg) 2 kg 2 + (b + kg) 2 /4</figDesc><table><row><cell>original</cell><cell>CSP</cell><cell>partial in CB</cell></row><row><cell>bg + (k</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Model scaling factors of different parts of object detectors.</figDesc><table><row><cell>Part</cell><cell>Scaling Factor</cell></row><row><cell>Input</cell><cell>size</cell></row></table><note>input Backbone width backbone , depth backbone , #stage backbone Neck width neck , depth neck , #stage neck</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Effect of receptive field caused by different model scaling factors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of CSP-ized models @608×608.</figDesc><table><row><cell cols="2">Backbone Neck</cell><cell cols="3">Act. #Param. FLOPs</cell><cell>Batch 8 FPS</cell><cell>AP val</cell></row><row><cell>D53</cell><cell>FPNSPP</cell><cell>Leaky</cell><cell>63M</cell><cell>142B</cell><cell cols="2">208 43.5%</cell></row><row><cell>D53</cell><cell>FPNSPP</cell><cell>Mish</cell><cell>63M</cell><cell>142B</cell><cell cols="2">196 45.3%</cell></row><row><cell>CD53s</cell><cell cols="2">CFPNSPP Leaky</cell><cell>43M</cell><cell>97B</cell><cell cols="2">222 45.7%</cell></row><row><cell>CD53s</cell><cell cols="2">CFPNSPP Mish</cell><cell>43M</cell><cell>97B</cell><cell cols="2">208 46.3%</cell></row><row><cell>D53</cell><cell>PANSPP</cell><cell>Leaky</cell><cell>78M</cell><cell>160B</cell><cell cols="2">196 46.5%</cell></row><row><cell>D53</cell><cell>PANSPP</cell><cell>Mish</cell><cell>78M</cell><cell>160B</cell><cell cols="2">185 46.9%</cell></row><row><cell>CD53s</cell><cell cols="2">CPANSPP Leaky</cell><cell>53M</cell><cell>109B</cell><cell cols="2">208 46.9%</cell></row><row><cell>CD53s</cell><cell cols="2">CPANSPP Mish</cell><cell>53M</cell><cell>109B</cell><cell cols="2">200 47.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Ablation study of partial at different position in computational block.</figDesc><table><row><cell>Backbone</cell><cell cols="3">Neck FLOPs FPS T X2 AP val</cell></row><row><cell cols="2">tinyCD53s tinyFPN 7.0B</cell><cell>30</cell><cell>22.2%</cell></row><row><cell cols="2">COSA-1x3x tinyFPN 7.6B</cell><cell>38</cell><cell>22.5%</cell></row><row><cell cols="2">COSA-2x2x tinyFPN 6.9B</cell><cell>42</cell><cell>22.0%</cell></row><row><cell cols="2">COSA-3x1x tinyFPN 6.3B</cell><cell>46</cell><cell>21.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ablation study of training schedule with/without fine-tuning.</figDesc><table><row><cell>Model</cell><cell cols="3">scratch finetune AP val AP val 50</cell><cell>AP val 75</cell></row><row><cell>YOLOv4-P5</cell><cell>300</cell><cell>-</cell><cell cols="2">50.5% 68.9% 55.2%</cell></row><row><cell>YOLOv4-P5</cell><cell>300</cell><cell>150</cell><cell cols="2">51.7% 70.3% 56.7%</cell></row><row><cell>YOLOv4-P6</cell><cell>300</cell><cell>-</cell><cell cols="2">53.4% 71.5% 58.5%</cell></row><row><cell>YOLOv4-P6</cell><cell>300</cell><cell>150</cell><cell cols="2">54.4% 72.7% 59.5%</cell></row><row><cell>YOLOv4-P7</cell><cell>300</cell><cell>-</cell><cell cols="2">54.6% 72.4% 59.7%</cell></row><row><cell>YOLOv4-P7</cell><cell>300</cell><cell>150</cell><cell cols="2">55.3% 73.3% 60.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Comparison of state-of-the-art object detectors.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Size</cell><cell>FPS</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell>EfficientDet-D0 [35]</cell><cell>EfficientNet-B0 [34]</cell><cell>512</cell><cell>97*</cell><cell cols="6">34.6% 53.0% 37.1% 12.4% 39.0% 52.7%</cell></row><row><cell>YOLOv4-CSP</cell><cell>CD53s</cell><cell>512</cell><cell>97/93*</cell><cell cols="6">46.2% 64.8% 50.2% 24.6% 50.4% 61.9%</cell></row><row><cell>EfficientDet-D1 [35]</cell><cell>EfficientNet-B1 [34]</cell><cell>640</cell><cell>74*</cell><cell>40.5%</cell><cell>59.1%</cell><cell>43.7%</cell><cell>18.3%</cell><cell>45.0%</cell><cell>57.5%</cell></row><row><cell>YOLOv4-CSP</cell><cell>CD53s</cell><cell>640</cell><cell>73/70*</cell><cell cols="6">47.5% 66.2% 51.7% 28.2% 51.2% 59.8%</cell></row><row><cell>YOLOv3-SPP [30]</cell><cell>D53 [30]</cell><cell>608</cell><cell>73</cell><cell>36.2%</cell><cell>60.6%</cell><cell>38.2%</cell><cell>20.6%</cell><cell>37.4%</cell><cell>46.1%</cell></row><row><cell>YOLOv3-SPP ours</cell><cell>D53 [30]</cell><cell>608</cell><cell>73</cell><cell>42.9%</cell><cell>62.4%</cell><cell>46.6%</cell><cell>25.9%</cell><cell>45.7%</cell><cell>52.4%</cell></row><row><cell>PP-YOLO [22]</cell><cell>R50-vd-DCN [22]</cell><cell>608</cell><cell>73</cell><cell>45.2%</cell><cell>65.2%</cell><cell>49.9%</cell><cell>26.3%</cell><cell>47.8%</cell><cell>57.2%</cell></row><row><cell>YOLOv4 [1]</cell><cell>CD53 [1]</cell><cell>608</cell><cell>62</cell><cell>43.5%</cell><cell>65.7%</cell><cell>47.3%</cell><cell>26.7%</cell><cell>46.7%</cell><cell>53.3%</cell></row><row><cell>YOLOv4 ours</cell><cell>CD53 [1]</cell><cell>608</cell><cell>62</cell><cell>45.5%</cell><cell>64.1%</cell><cell>49.5%</cell><cell>27.0%</cell><cell>49.0%</cell><cell>56.7%</cell></row><row><cell>EfficientDet-D2 [35]</cell><cell>EfficientNet-B2 [34]</cell><cell>768</cell><cell>57*</cell><cell>43.0%</cell><cell>62.3%</cell><cell>46.2%</cell><cell>22.5%</cell><cell>47.0%</cell><cell>58.4%</cell></row><row><cell>RetinaNet [18]</cell><cell>S49s [6]</cell><cell>640</cell><cell>53</cell><cell>41.5%</cell><cell>60.5%</cell><cell>44.6%</cell><cell>23.3%</cell><cell>45.0%</cell><cell>58.0%</cell></row><row><cell>ASFF [19]</cell><cell>D53 [30]</cell><cell>608*</cell><cell>46</cell><cell>42.4%</cell><cell>63.0%</cell><cell>47.4%</cell><cell>25.5%</cell><cell>45.7%</cell><cell>52.3%</cell></row><row><cell>YOLOv4-P5</cell><cell>CSP-P5</cell><cell>896</cell><cell>43/41*</cell><cell cols="6">51.8% 70.3% 56.6% 33.4% 55.7% 63.4%</cell></row><row><cell>RetinaNet [18]</cell><cell>S49 [6]</cell><cell>640</cell><cell>42</cell><cell>44.3%</cell><cell>63.8%</cell><cell>47.6%</cell><cell>25.9%</cell><cell>47.7%</cell><cell>61.1%</cell></row><row><cell>EfficientDet-D3 [35]</cell><cell>EfficientNet-B3 [34]</cell><cell>896</cell><cell>36*</cell><cell>47.5%</cell><cell>66.2%</cell><cell>51.5%</cell><cell>27.9%</cell><cell>51.4%</cell><cell>62.0%</cell></row><row><cell>YOLOv4-P6</cell><cell>CSP-P6</cell><cell>1280</cell><cell>32/30*</cell><cell cols="6">54.5% 72.6% 59.8% 36.8% 58.3% 65.9%</cell></row><row><cell>ASFF[19]</cell><cell>D53 [30]</cell><cell>800*</cell><cell>29</cell><cell>43.9%</cell><cell>64.1%</cell><cell>49.2%</cell><cell>27.0%</cell><cell>46.6%</cell><cell>53.4%</cell></row><row><cell>SM-NAS: E2 [42]</cell><cell>-</cell><cell>800*600</cell><cell>25</cell><cell>40.0%</cell><cell>58.2%</cell><cell>43.4%</cell><cell>21.1%</cell><cell>42.4%</cell><cell>51.7%</cell></row><row><cell>EfficientDet-D4 [35]</cell><cell>EfficientNet-B4 [34]</cell><cell>1024</cell><cell>23*</cell><cell>49.7%</cell><cell>68.4%</cell><cell>53.9%</cell><cell>30.7%</cell><cell>53.2%</cell><cell>63.2%</cell></row><row><cell>SM-NAS: E3 [42]</cell><cell>-</cell><cell>800*600</cell><cell>20</cell><cell>42.8%</cell><cell>61.2%</cell><cell>46.5%</cell><cell>23.5%</cell><cell>45.5%</cell><cell>55.6%</cell></row><row><cell>RetinaNet [18]</cell><cell>S96 [6]</cell><cell>1024</cell><cell>19</cell><cell>48.6%</cell><cell>68.4%</cell><cell>52.5%</cell><cell>32.0%</cell><cell>52.3%</cell><cell>62.0%</cell></row><row><cell>ATSS [45]</cell><cell>R101 [11]</cell><cell>800*</cell><cell>18</cell><cell>43.6%</cell><cell>62.1%</cell><cell>47.4%</cell><cell>26.1%</cell><cell>47.0%</cell><cell>53.6%</cell></row><row><cell>YOLOv4-P7</cell><cell>CSP-P7</cell><cell>1536</cell><cell>17/16*</cell><cell cols="6">55.5% 73.4% 60.8% 38.4% 59.4% 67.7%</cell></row><row><cell>RDSNet [39]</cell><cell>R101 [11]</cell><cell>600</cell><cell>17</cell><cell>36.0%</cell><cell>55.2%</cell><cell>38.7%</cell><cell>17.4%</cell><cell>39.6%</cell><cell>49.7%</cell></row><row><cell>CenterMask [16]</cell><cell>R101-FPN [17]</cell><cell>-</cell><cell>15</cell><cell>44.0%</cell><cell>-</cell><cell>-</cell><cell>25.8%</cell><cell>46.8%</cell><cell>54.9%</cell></row><row><cell>EfficientDet-D5 [35]</cell><cell>EfficientNet-B5 [34]</cell><cell>1280</cell><cell>14*</cell><cell>51.5%</cell><cell>70.5%</cell><cell>56.7%</cell><cell>33.9%</cell><cell>54.7%</cell><cell>64.1%</cell></row><row><cell>ATSS [45]</cell><cell>R101-DCN [5]</cell><cell>800*</cell><cell>14</cell><cell>46.3%</cell><cell>64.7%</cell><cell>50.4%</cell><cell>27.7%</cell><cell>49.8%</cell><cell>58.4%</cell></row><row><cell>SABL [38]</cell><cell>R101 [11]</cell><cell>-</cell><cell>13</cell><cell>43.2%</cell><cell>62.0%</cell><cell>46.6%</cell><cell>25.7%</cell><cell>47.4%</cell><cell>53.9%</cell></row><row><cell>CenterMask [16]</cell><cell>V99-FPN [16]</cell><cell>-</cell><cell>13</cell><cell>46.5%</cell><cell>-</cell><cell>-</cell><cell>28.7%</cell><cell>48.9%</cell><cell>57.2%</cell></row><row><cell>EfficientDet-D6 [35]</cell><cell>EfficientNet-B6 [34]</cell><cell>1408</cell><cell>11*</cell><cell>52.6%</cell><cell>71.5%</cell><cell>57.2%</cell><cell>34.9%</cell><cell>56.0%</cell><cell>65.4%</cell></row><row><cell>RDSNet [39]</cell><cell>R101 [11]</cell><cell>800</cell><cell>11</cell><cell>38.1%</cell><cell>58.5%</cell><cell>40.8%</cell><cell>21.2%</cell><cell>41.5%</cell><cell>48.2%</cell></row><row><cell>RetinaNet [18]</cell><cell>S143 [6]</cell><cell>1280</cell><cell>10</cell><cell>50.7%</cell><cell>70.4%</cell><cell>54.9%</cell><cell>33.6%</cell><cell>53.9%</cell><cell>62.1%</cell></row><row><cell>SM-NAS: E5 [42]</cell><cell>-</cell><cell cols="2">1333*800 9.3</cell><cell>45.9%</cell><cell>64.6%</cell><cell>49.6%</cell><cell>27.1%</cell><cell>49.0%</cell><cell>58.0%</cell></row><row><cell>EfficientDet-D7 [35]</cell><cell>EfficientNet-B6 [34]</cell><cell>1536</cell><cell>8.2*</cell><cell>53.7%</cell><cell>72.4%</cell><cell>58.4%</cell><cell>35.8%</cell><cell>57.0%</cell><cell>66.3%</cell></row><row><cell>ATSS [45]</cell><cell cols="2">X-32x8d-101-DCN [5] 800*</cell><cell>7.0</cell><cell>47.7%</cell><cell>66.6%</cell><cell>52.1%</cell><cell>29.3%</cell><cell>50.8%</cell><cell>59.7%</cell></row><row><cell>ATSS [45]</cell><cell cols="2">X-64x4d-101-DCN [5] 800*</cell><cell>6.9</cell><cell>47.7%</cell><cell>66.5%</cell><cell>51.9%</cell><cell>29.7%</cell><cell>50.8%</cell><cell>59.4%</cell></row><row><cell cols="2">EfficientDet-D7x [35] EfficientNet-B7 [34]</cell><cell>1536</cell><cell>6.5*</cell><cell>55.1%</cell><cell>74.3%</cell><cell>59.9%</cell><cell>37.2%</cell><cell>57.9%</cell><cell>68.0%</cell></row><row><cell>TSD [33]</cell><cell>R101 [11]</cell><cell>-</cell><cell>5.3*</cell><cell>43.2%</cell><cell>64.0%</cell><cell>46.9%</cell><cell>24.0%</cell><cell>46.3%</cell><cell>55.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Results of YOLOv4-large models with test-time augmentation (TTA).</figDesc><table><row><cell>Model</cell><cell>AP</cell><cell>AP 50 AP 75</cell></row><row><cell cols="3">YOLOv4-P5 with TTA 52.9% 70.7% 58.3%</cell></row><row><cell cols="3">YOLOv4-P6 with TTA 55.2% 72.9% 60.5%</cell></row><row><cell cols="3">YOLOv4-P7 with TTA 56.0% 73.3% 61.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Comparison of state-of-the-art tiny models.</figDesc><table><row><cell>Model</cell><cell cols="3">Size FPS 1080ti FPS T X2</cell><cell>AP</cell></row><row><cell>YOLOv4-tiny</cell><cell>416</cell><cell>371</cell><cell>42</cell><cell>21.7%</cell></row><row><cell cols="2">YOLOv4-tiny (3l) 320</cell><cell>252</cell><cell>41</cell><cell>28.7%</cell></row><row><cell cols="2">ThunderS146 [25] 320</cell><cell>248</cell><cell>-</cell><cell>23.6%</cell></row><row><cell cols="2">CSPPeleeRef [37] 320</cell><cell>205</cell><cell>41</cell><cell>23.5%</cell></row><row><cell cols="2">YOLOv3-tiny [30] 416</cell><cell>368</cell><cell>37</cell><cell>16.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>FPS of YOLOv4-tiny on embedded devices.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors wish to thank National Center for Highperformance Computing (NCHC) for providing computational and storage resources. A large part of the code is borrowed from https://github.com/AlexeyAB, https://github.com/WongKinYiu and https: //github.com/glenn-jocher. Thanks for their wonderful works.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">YOLOv4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10934</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">D2Det: Towards high quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11485" to="11494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HarDNet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yang</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Hsiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youn-Long</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05027</idno>
		<title level="m">SpineNet: Learning scale-permuted backbone for recognition and localization</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CenterNet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nas-Fpn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fast R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cornernet-Lite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08900</idno>
		<title level="m">Efficient keypoint based object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An energy and GPU-computation efficient backbone network for real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joong-Won</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangrok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop (CVPR Workshop)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshop (CVPR Workshop)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CenterMask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning spatial fusion for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09516</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">PP-YOLO: An effective and efficient implementation of object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanzhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12099</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ShuffleNetV2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Yuille</forename><surname>De-Tectors</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02334</idno>
		<title level="m">Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ThunderNet: Towards realtime generic object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BorderDet: Border feature for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="549" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11563" to="11572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">EfficientDet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CSPNet: A new backbone that can enhance learning capability of CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hau</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop (CVPR Workshop)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshop (CVPR Workshop)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Side-aware boundary localization for more precise object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="403" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">RDSNet: A new deep architecture for reciprocal object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05070</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scale-equalizing pyramid convolution for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13359" to="13368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SM-NAS: Structural-to-modular neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic R-CNN: Towards high quality object detection via dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="260" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. In arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
