<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlu</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Budapest University of Technology and Economics</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Márton</forename><surname>Szemenyei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Budapest University of Technology and Economics</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugen</forename><surname>Yi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jiangxi Normal University</orgName>
								<address>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenle</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jiangxi Normal University</orgName>
								<address>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buer</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Budapest University of Technology and Economics</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqi</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Budapest University of Technology and Economics</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SA-UNet: Spatial Attention U-Net for Retinal Vessel Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Segmentation</term>
					<term>retinal blood vessel</term>
					<term>SA-UNet</term>
					<term>U- Net</term>
					<term>spatial attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The precise segmentation of retinal blood vessels is of great significance for early diagnosis of eye-related diseases such as diabetes and hypertension. In this work, we propose a lightweight network named Spatial Attention U-Net (SA-UNet) that does not require thousands of annotated training samples and can be utilized in a data augmentation manner to use the available annotated samples more efficiently. SA-UNet introduces a spatial attention module which infers the attention map along the spatial dimension, and multiplies the attention map by the input feature map for adaptive feature refinement. In addition, the proposed network employs structured dropout convolutional blocks instead of the original convolutional blocks of U-Net to prevent the network from overfitting. We evaluate SA-UNet based on two benchmark retinal datasets: the Vascular Extraction (DRIVE) dataset and the Child Heart and Health Study (CHASE_DB1) dataset. The results show that the proposed SA-UNet achieves state-of-the-art performance on both datasets.  The implementation and the trained networks are available on Github 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Many diseases can be easily diagnosed and tracked by observing the fundus vascular system, because these diseases (such as diabetes and hypertension) can cause morphological changes in the blood vessels of the retina. Systemic microvascular and small vessel diseases are common pathological changes caused by diabetes, especially the fundus retinal vascular disease is the most vulnerable. Diabetic retinopathy (DR) is caused by diabetes <ref type="bibr" target="#b5">[1]</ref>. If swelling of the blood vessels in the retina of a diabetic patient is observed, special attention is required. Patients with long-term hypertension may observe blood vessel curvature due to increased arterial blood pressure or vascular stenosis, which is called hypertensive retinopathy (HR) <ref type="bibr" target="#b6">[2]</ref>. Retinal vessel segmentation is a key step in the quantitative analysis of fundus images. By segmenting the retinal blood vessels, we can obtain the relevant morphological information of the retinal blood vessel tree (such as the curvature, length, and width of the blood vessels) <ref type="bibr" target="#b7">[3]</ref>. Moreover, the vascular tree of retinal vessels has unique characteristics that can be applied to * Corresponding authors 1 https://github.com/clguo/SA-UNet biometric recognition <ref type="bibr" target="#b8">[4]</ref>, <ref type="bibr" target="#b9">[5]</ref> as well. Therefore, accurate segmentation of retinal blood vessels is of great significance.</p><p>However, retinal blood vessels have numorous small and fragile blood vessels, and the blood vessels are closely connected, so the retinal blood vessel tree structure is rather complex. In addition, the difference between the blood vessel area and the background is not obvious, and the fundus image is also susceptible to uneven lighting and noise. The above reasons cause retinal blood vessel segmentation to be a challenging task.</p><p>In the past few decades, a large number of retinal blood vessel segmentation methods have been proposed, mainly divided into manual and automatic segmentation methods. The former is time-consuming and labor-intensive and requires extremely high professional skills of practitioners. The latter can reduce the burden of manual segmentation, so the research on automatic segmentation algorithms is of great significance. With the advancement of deep learning in recent years, it has gradually become the mainstream technology of retinal segmentation.</p><p>In the field of medical image segmentation, U-Net <ref type="bibr" target="#b10">[6]</ref> is a common and well-known backbone network. Basically, U-Net consists of a typical downsampling encoder and upsampling decoder structure and a "skip connection" between them. It combines local and global context information through the encoding and decoding process. Due to the excellent performance of U-Net, many recent methods for retinal blood vessel segmentation are based on U-Net. Wang et al. <ref type="bibr" target="#b11">[7]</ref> reported the Dual Encoding U-Net (DEU-Net) that remarkably enhances network's capability of segmenting retinal vessels in an end-to-end and pixel-to-pixel way. Wu et al. <ref type="bibr" target="#b12">[8]</ref> proposed Vessel-Net, which first time uses a strategy that combines the advantages of the initial method and the residual method to perform retinal vessel segmentation. Zhang et al. <ref type="bibr" target="#b13">[9]</ref> proposed AG-Net, which designed an attention mechanism called "Attention Guide Filter" to better retain structural information. Although these U-Net variants perform well, they inevitably make the network more complex and less interpretable.</p><p>In order to address these problems, we introduce spatial attention in U-Net and propose a lightweight network model, which we named Spatial Attention U-Net (SA-UNet). As shown by SD-Unet <ref type="bibr" target="#b14">[10]</ref>, using DropBlock <ref type="bibr" target="#b15">[11]</ref> can effectively prevent overfitting of the network, so even small sample data- sets, such as retinal fundus images can be well trained. In addition, batch normalization (BN) can improve the convergence speed of the network <ref type="bibr" target="#b16">[12]</ref>. Therefore, SA-UNet first employs a variant of structured dropout convolutional block integrating DropBlock and batch normalization (BN) to replace the original U-Net convolutional block. More importantly, the difference between vascular and non-vascular features in the retinal fundus image is not obvious, especially the small and marginal vascular areas. With the introduction of a small number of additional parameters, spatial attention can enhance important features (such as vascular features) and suppress unimportant features, thereby improving the network's representation ability. We evaluate SA-UNet on two public retinal fundus image datasets: DRIVE and CHASE_DB1. We first evaluate the newly introduced part of the network through ablation experiments. The experimental results show that the structured dropout convolutional block and the spatial attention we introduced are effective, and compared with the original U-Net and AG-Net, the proposed SA-UNet is very lightweight. Finally, compared with other existing state-of-the-art methods for retinal vascular segmentation, our proposed SA-UNet achieves state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>A. Network Architecture <ref type="figure" target="#fig_0">Fig. 1</ref> shows the proposed SA-UNet with a U-shaped encoder (left side)-decoder (right side) structure. Every step of the encoder includes a structured dropout convolutional block and a 2×2 max pooling operation. The convolutional layer of each convolutional block is followed by a DropBlock, a batch normalization (BN) layer and a rectified linear unit (ReLU), and then the max pooling operation is utilized for downsampling with a stride size of 2. In each down-sampling step, we double the number of feature channels. Each step in the decoder includes a 2×2 transposed convolution operation for up-sampling and halves the number of feature channels, a concatenates with the corresponding feature map from the encoder, which then followed by a structured dropout convolutional block. The spatial attention module is added between the encoder and the decoder. At the final layer, a 1×1 convolution and Sigmoid activation function is used to get the output segmentation map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Structured Dropout Convolutional Block</head><p>Although data augmentation is performed for the original datasets, serious overfitting is still observed during original U-Net training, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (left). Therefore a lightweight U-Net with 18 convolutional layers is employed as our basic architecture, but it still has over-fitting problem, as shown in <ref type="figure" target="#fig_1">Fig. 2 (middle)</ref>. Motivated by the successful application of DropBlock in recent computer vision works <ref type="bibr" target="#b14">[10]</ref>, <ref type="bibr" target="#b15">[11]</ref>, <ref type="bibr" target="#b17">[13]</ref>, <ref type="bibr" target="#b23">[19]</ref>, we adopt DropBlock to regularize the network.</p><p>DropBlock, a structured form of dropout, can effectively prevent over-fitting problems in convolutional networks <ref type="bibr" target="#b14">[10]</ref>. Its primary difference from dropout is that it discards contiguous areas from a feature map of a layer instead of dropping independent random units. Based on this, we construct a structured dropout convolutional block, that is, each convolutional layer is followed by a DropBlock, a layer of batch normalization (BN) and a ReLU activation unit, as shown in the right side of <ref type="figure">Fig. 3</ref>. Unlike the convolutional blo- ock of SD-Unet (as shown in the middle of <ref type="figure">Fig. 3</ref>), the structured dropout convolutional block introduces batch normalization (BN) to accelerate network convergence. We employ this structured dropout convolutional block instead of the original convolutional block of U-Net to build a U-shaped network as our "Backbone". Compared to the 23 convolutional layers of the original U-Net, our Backbone has only 18 convolutional layers, and as shown in <ref type="figure" target="#fig_1">Fig. 2. (left)</ref>, the overfitting problem is perfectly solved and accelerates the convergence of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spatial Attention Module (SAM)</head><p>The Spatial Attention Module (SAM) was introduced as a part of the convolutional block attention module for classification and detection <ref type="bibr" target="#b18">[14]</ref>. SA uses the spatial relationship between features to produce a spatial attention map. To calculate spatial attention, SA first applies maxpooling and average-pooling operations along the channel axis and concatenate them to produce an efficient feature descriptor, as shown in <ref type="figure">Fig. 4</ref> </p><formula xml:id="formula_0">F F f F F AvgPool F MaxPool f F F M F F           (4) Where ) ( 7 7   f</formula><p>denotes a convolution operation with a kernel size of 7 and ) (  represents the Sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate our proposed SA-UNet on two public retinal fundus image datasets: DRIVE and CHASE DB1. The specific information on the two datasets is given in <ref type="table" target="#tab_1">Table I</ref>. It should be noted that the original size of the two datasets is not suitable for our network, so we adjusted its size by zero padding around it, but the size is cropped to the initial size during evaluation. To augment the data, we adopt four data augmentation methods shown in the last column of <ref type="table" target="#tab_1">Table I</ref> for both datasets, each of which generated three new images from an original image, that is, we augment the two original datasets from the original 20 training images to 256 images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Metrics</head><p>In order to evaluate our model, we compare the segmentation results with the corresponding ground truth and divide the results of each pixel comparison into true positive (TP), false positive (FP), false negative (FN), and true negative (TN). Then, the sensitivity (SE), specificity (SP), F1score (F1), and accuracy (ACC) are used to evaluate the performance of the model. In retinal vessel segmentation, only 9%-14% of the pixels belong to the blood vessel, while other pixels are considered background pixels. The Matthews Correlation Coefficient (MCC) is suitable for performance measurement of binary classifications for two categories with different sizes. Therefore, the MCC value can help find the optimal setting for the vessel segmentation algorithm. MCC is defined as:</p><formula xml:id="formula_1">) ( ) ( ) ( ) ( FN TN FP TN FN TP FP TP FN FP TN TP MCC           <label>(5)</label></formula><p>The area under the ROC curve (AUC) can be used to measure the performance of the segmentation. If the AUC value is 1, it means perfect segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>In order to monitor whether our network is overfitting, we randomly select 26 and 13 images in the DRIVE and CHASE DB1 augmented datasets as the validation set. As mentioned earlier, <ref type="figure" target="#fig_1">Fig. 2</ref> shows the case of training 100 epochs on the DRIVE dataset. SA-UNet is trained from scratch using the augmented training set. For both datasets, the Adam optimizer and the binary cross entropy loss function are employed, and in order to keep the number of parameters small, the number of channels after the first convolutional layer is set to 16. The number of epochs is 150 and the learning rate of the first 100 epochs is 0.001, the last 50 epochs is 0.0001. The size of the discard blocks of DropBlock is set to 7.</p><p>Respectively, for DRIVE dataset, the batch size of the training is set to 8 and the dropout rate of DropBlock is set to 0.18. For CHASE DB1, the batch size is set to 4 and the dropout rates is 0.13.</p><p>The implementation is based on the public Keras with Tensorflow as the backend and all experiments are run on an NVIDIA TITAN XP GPU, which has 12 Gigabyte memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Experiments</head><p>In order to prove that each component of the proposed SA-UNet can improve the performance of retinal vascular segmentation, ablation experiments were performed on DRIVE and CHASE_DB1 respectively. <ref type="table" target="#tab_1">Tables II and III</ref> show the segmentation performance of U-Net, U-Net + SA, SD-Unet (i.e. U-Net + DropBlock), Backbone (i.e. SD-Unet + BN), and SA-UNet (i.e. Backbone + SA) from top to bottom, respectively. In addition, <ref type="table" target="#tab_1">Table IV</ref> shows the parameter quantities of different models.</p><p>From the results, we could obtain several useful observations: (1) With only 98 parameters added, U-Net + SA has better performance compared with the U-Net, which proves the strategy of introducing spatial attention is effective.</p><p>(2) In the case of using structured dropout convolutional block based on U-Net, the ACC, AUC, F1 and MCC of the Backbone are 0.28% / 0.22%, 0.73% / 0.59%, 2.42% / 2.48%, and 2.48% / 2.64% higher than U-Net on DRIVE and CHASE_DB1 respectively, which demonstrates the effectiveness of adopting the newly constructed structured dropout convolutional block to build the Backbone. (3) Backbone has better performance compared to the original SD-Unet, although the number of parameters is increased slightly, which shows that adding the batch normalization (BN) can improve the network performance to a certain extent. (4) Finally, the proposed SA-UNet achieves the best performance on most metrics, and compared with AG-Net and the original U-Net with 23 convolutional layers, our SA-UNet has a much smaller amount of parameters, so for the task of retinal blood vessel segmentation, SA-UNet is a lightweight and effective network.</p><p>In <ref type="figure" target="#fig_3">Fig. 5</ref>, we show a test example on the DRIVE dataset, including the segmentation results obtained by U-Net, U-Net + SA, AG-Net, SD-Unet, Backbone and the proposed SA-UNet, and the corresponding ground truth. Compared with U-Net and U-Net + SA, AG-Net does have certain advantages in the segmentation of the edge structure, but at the intersection of small blood vessels, AG-Net is still not strong enough. SD-Unet ignores some edge and small vascular structures and there is even incorrect segmentation. The Backbone produces more accurate small vessel segmentation than the U-Net and SD-Unet, which proves the effectiveness of the Backbone constructed using structured dropout convolutional blocks. Compared with the Backbone, the SA-UNet proposed in this paper can produce more accurate segmentation results for small border blood vessels and retain more retinal blood vessel spatial structure, which proves that the spatial attention mechanism can highlight blood vessels and reduce the influence of background. In order to better observe the test results, we show more segmentation examples of U-Net, Backbone, and SA-UNet on DRIVE and CHASE_DB1 in <ref type="figure">Fig.  6</ref> and <ref type="figure">Fig. 7</ref>, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparisons with state-of-the-art methods</head><p>Finally, we compare the performance of SA-UNet with other state-of-the-art methods currently applied in retinal vessel segmentation task. In Tables V and VI, we summarize the release year of different methods and the performance on DRIVE and CHASE_DB1 datasets. From the results, it can be concluded that SA-UNet has achieved the best performance on both DRIVE and CHASE_DB1. It achieves the highest sensitivity of 0.8212 / 0.8573, the highest accuracy of 0.9698 / 0.9755, the highest AUC of 0.9864 / 0.9905, while the specificity is comparable with other methods. In addition, compared with the best performing AG-Net in the previous methods, SA-UNet has better segmentation performance at the intersection of small blood vessels, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Remarkablt, the parameter amount of SA-UNet is much smaller than that of AG-Net. The above results show that our proposed SA-UNet achieves state-of-the-art performance in the retinal vessel segmentation challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets CHASE_DB1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>Year SE SP ACC AUC</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Most retinal fundus image datasets are typical small sample datasets, which can make training deep neural networks problematic. To enable learning, data augmentation is applied in an ambitious way, then a lightweight U-Net is used, but overfitting is still observed. Inspired by the successful application of DropBlock and batch normalization in convolutional neural networks, we replace the convolutional block of U-Net with a structured dropout convolutional block that integrates DropBlock and batch normalization as our Backbone. In addition, in the retinal fundus images, the difference between the blood vessel area and the background is not obvious, especially the edges and small blood vessels. To help the network learn these, we add a spatial attention module between the encoder and decoder of the Backbone and propose Spatial Attention U-Net (SA-UNet). The spatial attention can help the network focus on important features and suppress unnecessary ones to improve the network's representation capability. We evaluate SA-UNet on two publicly available retinal fundus image data including DRIVE and CHASE_DB1. The experimental results demonstrate that using structured dropout of convolutional blocks and the introducing spatial attention are effective, and by comparing with other state-of-the-art methods for retinal vessel segmentation, our lightweight SA-UNet achieves stateof-the-art performance. Because the vascular structure characteristics of the retinal image are similar, we conclude that SA-UNet is a general network and can be applied to other retinal vessel segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Diagram of the proposed SA-UNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of different models training 100 epochs on DRIVE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Original U-Net block (left), SD-Unet block (middle), Structured dropout convolutional block (right) Diagram of the Spatial Attention Module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>(a) A test image from DRIVE dataset; (b) Segmentation result by U-Net; (c) Segmentation result by U-Net+SA; (d) Segmentation result by AG-Net; (e) Segmentation result by SD-Unet; (f) Segmentation result by Backbone; (g) Segmentation result by SA-UNet; (h) Corresponding ground truth segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Segmentation results on DRIVE Segmentation results on CHASE_DB1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I .</head><label>I</label><figDesc>THE SPECIFIC INFORMATION OF DRIVE AND CHASE_DB1</figDesc><table><row><cell></cell><cell>DATASETS</cell><cell></cell></row><row><cell>Datasets</cell><cell>DRIVE</cell><cell>CHASE_DB1</cell></row><row><cell>Obtained from</cell><cell>Dutch Diabetic Retinopathy Screening Program</cell><cell>Child Heart and Health Study</cell></row><row><cell>Total number</cell><cell>40</cell><cell>28</cell></row><row><cell>Train / Test number</cell><cell>20 / 20</cell><cell>20 / 8</cell></row><row><cell>Resolution (pixel)</cell><cell>584×565</cell><cell>999×960</cell></row><row><cell>Resize (pixel)</cell><cell>592×592</cell><cell>1008×1008</cell></row><row><cell>Augmentation</cell><cell cols="2">(1) Random rotation; (2) adding Gaussian noise; (3) color</cell></row><row><cell>methods</cell><cell cols="2">jittering; (4) horizontal, vertical and diagonal flips.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II .</head><label>II</label><figDesc>ABLATION STUDIES ON DRIVE DATASET.</figDesc><table><row><cell>Methods</cell><cell></cell><cell>SE</cell><cell>SP</cell><cell>ACC</cell><cell>AUC</cell><cell>F1</cell><cell>MCC</cell></row><row><cell>U-Net</cell><cell cols="7">0.7677 0.9857 0.9666 0.9789 0.8012 0.7839</cell></row><row><cell cols="8">U-Net + SA 0.7883 0.9845 0.9673 0.9809 0.8085 0.7909</cell></row><row><cell>SD-Unet</cell><cell cols="7">0.7978 0.9860 0.9695 0.9858 0.8208 0.8045</cell></row><row><cell cols="8">Backbone 0.8246 0.9832 0.9694 0.9862 0.8254 0.8087</cell></row><row><cell cols="8">SA-UNet 0.8212 0.9840 0.9698 0.9864 0.8263 0.8097</cell></row><row><cell cols="2">TABLE III.</cell><cell cols="6">ABLATION STUDIES ON CHASE_DB1 DATASET.</cell></row><row><cell>Methods</cell><cell cols="2">SE</cell><cell>SP</cell><cell>ACC</cell><cell>AUC</cell><cell>F1</cell><cell>MCC</cell></row><row><cell>U-Net</cell><cell cols="7">0.7842 0.9861 0.9733 0.9838 0.7875 0.7733</cell></row><row><cell cols="8">U-Net + SA 0.7840 0.9865 0.9738 0.9852 0.7902 0.7763</cell></row><row><cell cols="8">SD-Unet 0.8297 0.9854 0.9756 0.9897 0.8109 0.7981</cell></row><row><cell cols="8">Backbone 0.8422 0.9844 0.9755 0.9897 0.8123 0.7997</cell></row><row><cell cols="8">SA-UNet 0.8573 0.9835 0.9755 0.9905 0.8153 0.8033</cell></row><row><cell>TABLE IV.</cell><cell></cell><cell cols="6">AMOUNT OF PARAMETERS ON DIFFERENT MODELS.</cell></row><row><cell>Models</cell><cell></cell><cell></cell><cell>Total</cell><cell cols="4">Trainable Non-trainable</cell></row><row><cell cols="2">AG-Net</cell><cell></cell><cell cols="3">9,335,340 9,335,340</cell><cell>0</cell></row><row><cell cols="6">23 Layers U-Net 2,158,705 2,158,705</cell><cell>0</cell></row><row><cell cols="3">18 Layers U-Net</cell><cell>535,793</cell><cell></cell><cell>535,793</cell><cell>0</cell></row><row><cell cols="2">U-Net + SA</cell><cell></cell><cell>535,891</cell><cell></cell><cell>535,891</cell><cell>0</cell></row><row><cell cols="2">SD-Unet</cell><cell></cell><cell>535,793</cell><cell></cell><cell>535,793</cell><cell>0</cell></row><row><cell cols="2">Backbone</cell><cell></cell><cell>538,609</cell><cell></cell><cell>537,201</cell><cell cols="2">1,408</cell></row><row><cell cols="2">SA-UNet</cell><cell></cell><cell>538,707</cell><cell></cell><cell>537,299</cell><cell cols="2">1,408</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V .</head><label>V</label><figDesc>RESULTS OF SA-UNET AND OTHER METHODS ON DRIVE</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE VI.</cell><cell>RESULTS</cell></row><row><cell></cell><cell cols="2">DATASETS.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell>DRIVE</cell><cell></cell><cell></cell></row><row><cell>Metrics</cell><cell>Year</cell><cell>SE</cell><cell>SP</cell><cell>ACC</cell><cell>AUC</cell></row></table><note>OF SA-UNET AND OTHER METHODS ON CHASE_DB1 DATASETS.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is supported by the China Scholarship Council, the Stipendium Hungaricum Scholarship, and the National Natural Science Foundation of China under Grants 62062040.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Orlando</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liskowski</surname></persName>
		</author>
		<idno>15] 2016 0.7816 0.9836 0.9628 0.9823</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Orlando</surname></persName>
		</author>
		<idno>16] 2017 0.7277 0.9712 0.9458 0.9524</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<imprint>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Microfluidic analysis of red blood cell deformability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Santoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomech</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1767" to="1776" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A review on the extraction of quantitative retinal microvascular image feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kipli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hoque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sapawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Math. Methods Med</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DUNet: A deformable network for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangguo</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Personal verification based on extraction and characterization of retinal feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rouco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="80" to="90" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new scientific method of identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New York State Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="901" to="906" />
			<date type="published" when="1935-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual Encoding U-Net for Retinal Vessel Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Shen D. et al.</editor>
		<imprint>
			<biblScope unit="volume">11764</biblScope>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vessel-Net: Retinal Vessel Segmentation Under Multi-path Supervision. Medical Image Computing and Computer Assisted Intervention -MICCAI 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">11764</biblScope>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention Guided Network for Retinal Image Segmentation. Medical Image Computing and Computer Assisted Intervention -MICCAI 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">11764</biblScope>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SD-Unet: A Structured Dropout U-Net for Retinal Vessel Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szemenyei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE)</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DropBlock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nas-Fpn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmenting retinal blood vessels with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liskowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krawiec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2369" to="2380" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A discriminatively trained fully connected conditional random field model for blood vessel segmentation in fundus images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Prokofyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="27" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint segment-Level and pixel-Wise losses for deep learning based retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1912" to="1923" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiscale network followed network model for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>Frangi, A.,Schnabel, J., Davatzikos, C., Alberola-Lopez, C., Fichtinger,G.</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense Residual Network for Retinal Vessel Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szemenyei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1374" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
