<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Schuster</surname></persName>
							<email>tals@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<country>MIT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Ram</surname></persName>
							<email>ori.ram@cs.tau.ac.il</email>
							<affiliation key="aff1">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<email>regina@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<country>MIT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their mapping to derive an alignment for the contextdependent spaces. This mapping readily supports processing of a target language, improving transfer by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our method consistently outperforms the previous state-of-the-art on 6 tested languages, yielding an improvement of 6.8 LAS points on average. 1 * Equal contribution 1 Code and models:</p><p>https://github.com/ TalSchuster/CrossLingualELMo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multilingual embedding spaces have been demonstrated to be a promising means for enabling crosslingual transfer in many natural language processing tasks (e.g. <ref type="bibr" target="#b3">Ammar et al. (2016)</ref>; <ref type="bibr">Lample et al. (2018)</ref>). Similar to how universal part-ofspeech tags enabled parsing transfer across languages <ref type="bibr" target="#b24">(Petrov et al., 2012)</ref>, multilingual word embeddings further improve transfer capacity by enriching models with lexical information. Since this lexical representation is learned in an unsupervised fashion and thus can leverage large amounts of raw data, it can capture a more nuanced representation of meaning than unlexicalized transfer. Naturally, this enrichment is trans-lated into improved transfer accuracy, especially in low-resource scenarios <ref type="bibr" target="#b11">(Guo et al., 2015)</ref>.</p><p>In this paper, we are moving further along this line and exploring the use of contextual word embeddings for multilingual transfer. By dynamically linking words to their various contexts, these embeddings provide a richer semantic and syntactic representation than traditional context-independent word embeddings . A straightforward way to utilize this richer representation is to directly apply existing transfer algorithms on the contextual embeddings instead of their static counterparts. In this case, however, each token pair is represented by many different vectors corresponding to its specific context. Even when supervision is available in the form of a dictionary, it is still unclear how to utilize this information for multiple contextual embeddings that correspond to a word translation pair.</p><p>In this paper, we propose a simple but effective mechanism for constructing a multilingual space of contextual embeddings. Instead of learning the alignment in the original, complex contextual space, we drive the mapping process using context-independent embedding anchors. We obtain these anchors by factorizing the contextual embedding space into context-independent and context-dependent parts. Operating at the anchor level not only compresses the space, but also enables us to utilize a word-level bilingual dictionary as a source of supervision, if available. Once the anchor-level alignment is learned, it can be readily applied to map the original spaces with contextual embeddings.</p><p>Clearly, the value of word embeddings depends on their quality, which is determined by the amount of raw data available for their training <ref type="bibr" target="#b15">(Jiang et al., 2018)</ref>. We are interested in expanding the above approach to the truly low-resource scenario, where a language not only lacks annotations, but also has limited amounts of raw data. In this case, we can also rely on a data rich language to stabilize monolingual embeddings of the resource-limited language. As above, contextindependent anchors are informing this process. Specifically, we introduce an alignment component to the loss function of the language model, pushing the anchors to be closer in the joint space. While this augmentation is performed on the static anchors, the benefit extends to the contextual embeddings space in which we operate.</p><p>We evaluate our aligned contextual embeddings on the task of zero-shot cross-lingual dependency parsing. Our model consistently outperforms previous transfer methods, yielding absolute improvement of 6.8 LAS points over the prior stateof-the-art <ref type="bibr" target="#b3">(Ammar et al., 2016)</ref>. We also perform comprehensive studies of simplified variants of our model. Even without POS tag labeling or a dictionary, our model performs on par with context-independent models that do use such information. Our results also demonstrate the benefits of this approach for few-shot learning, i.e. processing languages with limited data. Specifically, on the Kazakh tree-bank from the recent CoNLL 2018 shared task with only 38 trees for training, the model yields 5 LAS points gain over the top result <ref type="bibr" target="#b28">(Smith et al., 2018a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Multilingual Embeddings The topic of crosslingual embedding alignment is an active area of research <ref type="bibr" target="#b21">(Mikolov et al., 2013;</ref><ref type="bibr">Xing et al., 2015;</ref><ref type="bibr">Dinu and Baroni, 2014;</ref><ref type="bibr">Lazaridou et al., 2015;</ref><ref type="bibr">Zhang et al., 2017)</ref>. Our work most closely relates to <ref type="bibr">MUSE (Conneau et al., 2018a)</ref>, which constructs a multilingual space by aligning monolingual embedding spaces. When a bilingual dictionary is provided, their approach is similar to those of <ref type="bibr">(Smith et al., 2017;</ref><ref type="bibr" target="#b4">Artetxe et al., 2017)</ref>. MUSE extends these methods to the unsupervised case by constructing a synthetic dictionary. The resulting alignment achieves strong performance in a range of NLP tasks, from sequence labeling <ref type="bibr">(Lin et al., 2018)</ref> to natural language inference <ref type="bibr">(Conneau et al., 2018b)</ref> and machine translation <ref type="bibr">(Lample et al., 2018;</ref><ref type="bibr" target="#b25">Qi et al., 2018)</ref>. Recent work further improves the performance on both the supervised  and unsupervised <ref type="bibr" target="#b10">(Grave et al., 2018b;</ref><ref type="bibr" target="#b2">Alvarez-Melis and Jaakkola, 2018;</ref><ref type="bibr" target="#b13">Hoshen and Wolf, 2018)</ref> settings for context-independent embeddings.</p><p>While MUSE operates over token based embeddings, we are interested in aligning contextual embeddings, which have shown their benefits in several monolingual applications <ref type="bibr" target="#b18">McCann et al., 2017;</ref><ref type="bibr" target="#b14">Howard and Ruder, 2018;</ref><ref type="bibr" target="#b26">Radford et al., 2018;</ref><ref type="bibr">Devlin et al., 2018)</ref>. However, this expansion introduces new challenges which we address in this paper.</p><p>In a concurrent study, <ref type="bibr" target="#b1">Aldarmaki and Diab (2019)</ref> introduced an alignment that is based only on word pairs in the same context, using parallel sentences. Our method achieves better word translations without relying on such supervision.</p><p>Our work also relates to prior approaches that utilize bilingual dictionaries to improve embeddings that were trained on small datasets. For instance, Xiao and <ref type="bibr">Guo (2014)</ref> represent word pairs as a mutual vector, while <ref type="bibr" target="#b0">Adams et al. (2017)</ref> jointly train cross-lingual word embeddings by replacing the predicted word with its translation. To utilize a dictionary in the contextualized case, we include a soft constraint that pushes those translations to be similar in their context-independent representation. A similar style of regularization was shown to be effective for cross-domain transfer of word embeddings <ref type="bibr">(Yang et al., 2017)</ref>.</p><p>Multilingual Parsing In early work on multilingual parsing, transfer was commonly implemented using delexicalized representation such as part-ofspeech tags <ref type="bibr" target="#b20">(McDonald et al., 2011;</ref><ref type="bibr" target="#b24">Petrov et al., 2012;</ref><ref type="bibr" target="#b22">Naseem et al., 2012;</ref><ref type="bibr">Tiedemann, 2015)</ref>.</p><p>Another approach for cross-lingual parsing includes annotation projection and treebank translation (Xiao and <ref type="bibr" target="#b11">Guo, 2015;</ref><ref type="bibr">Wang and Eisner, 2016;</ref><ref type="bibr">Tiedemann, 2017)</ref>, which mostly require some source of supervision.</p><p>Advancements in multilingual word representations opened a possibility of lexicalized transfer. Some of these approaches start by aligning monolingual embedding spaces <ref type="bibr">(Zhang and Barzilay, 2015;</ref><ref type="bibr" target="#b11">Guo et al., 2015</ref><ref type="bibr" target="#b12">Guo et al., , 2016</ref><ref type="bibr" target="#b3">Ammar et al., 2016)</ref>, and using resulting word embeddings as word representations instead of universal tags. Other approaches are learning customized multilingual syntactic embeddings bootstrapping from universal POS tags <ref type="bibr">(Duong et al., 2015)</ref>. While some models also learn a language embedding <ref type="bibr" target="#b3">(Ammar et al., 2016;</ref><ref type="bibr">de Lhoneux et al., 2018)</ref>, it is unfeasible in a zero-shot scenario.</p><formula xml:id="formula_0">D cos (ē i ,ē j ) D cos (ē i , e i,c )</formula><p>ALL WORDS HOMONYMS 0.85 (±0.09) 0.18 (±0.04) 0.21 (±0.04) <ref type="table">Table 1</ref>: Average cosine distances between pairs of embedding anchors (left column) and between contextualized embeddings of words to their corresponding anchor. The right column includes these distances only for homonyms, whereas the center column is averaged across all words. Only alphabetic words with at least 100 occurrences were included.</p><p>In all of the above cases, token-level embeddings are used. Inspired by strong results of using contextualized embeddings in monolingual parsing <ref type="bibr">(Che et al., 2018;</ref><ref type="bibr">Wang et al., 2018;</ref><ref type="bibr" target="#b23">Clark et al., 2018)</ref>, we aim to utilize them in the multilingual transfer case. Our results demonstrate that richer representation of lexical space does lead to significant performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Aligning Contextual Word Embeddings</head><p>In this section we describe several approaches for aligning context-dependent embeddings from a source language s to a target language t. We address multiple scenarios, where different amounts of supervision and data are present. Our approach is motivated by interesting properties of contextdependent embeddings, which we discuss later. We begin with some notations:</p><p>• Context Dependent Embeddings: Given a context c and a token i, we denote the embedding of i in the context c by e i,c . We use e i,· to denote the point cloud of all contextual embeddings for token i.</p><p>• Embedding Anchor: Given a token i we denote the anchor of its context dependent embeddings byē i , where:</p><formula xml:id="formula_1">e i = E c e i,c .<label>(1)</label></formula><p>In practice, we calculate the average over a subset of the available unlabeled data.</p><p>• Shift From Mean: For any embedding e i,c we can therefore define the shiftê i,c from the average via: • Embedding Alignment: Given an embedding e s i,c in s, we want to generate an embedding e s→t i,c in the target language space, using a linear mapping W s→t . Formally, our alignment is always of the following form:</p><formula xml:id="formula_2">e i,c =ē i +ê i,c .<label>(2)</label></formula><formula xml:id="formula_3">e s→t i,c = W s→t e s i,c .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Geometry of Context-Dependent Embeddings</head><p>A given token i can generate multiple vectors e i,c , each corresponding to a different context c. A key question is how the point cloud e i,· is distributed. In what follows we explore this structure, and reach several conclusions that will motivate our alignment approach. The following experiments are performed on ELMo . Point Clouds are Well Separated A cloud e i,· corresponds to occurrences of the word i in different contexts. Intuitively, we would expect its points to be closer to each other than to points from e j,· for a different word j. Indeed, when measuring similarity between points e i,c and their anchorē i , we find that these are much more similar than anchors of different wordsē i andē j (see <ref type="table">Table 1</ref>). This observation supports our hypothesis that anchor-driven alignment can guide the construction of the alignment for the contextual space. A visualized example of the contextualized representations of four words is given in <ref type="figure">Figure</ref> 1, demonstrating the appropriateness of their anchors. Still, as previous studies have shown, and <ref type="figure">Figure 2</ref>: Contextual embeddings for the English word "bear" and its two possible translations in Spanish -"oso" (animal) in blue and "tener" (to have) in red. The figure shows a two dimensional PCA for the aligned space of the two languages. The symbols are the anchors, the clouds represent the distribution of the contextualized Spanish words, and the black dots are for contextualized embeddings of "bear". The gray colored triangles show the anchors of the English words "dog", "elephant", "cat", from left to right respectively. as our results point, the context component is very useful for downstream tasks.</p><p>Homonym Point Clouds are Multi-Modal When a word i has multiple distinct senses, we might expect the embeddings for i to reflect this by separating into multiple distinct clouds, one for each meaning. <ref type="figure">Figure 2</ref> demonstrates that this indeed happens for the English word "bear". Furthermore, it can be seen that after alignment (Section 3.3) with Spanish, the distinct point clouds are aligned with their corresponding distinct words in Spanish. See App. D for another example.</p><p>We examined the shift from mean for a list of 250 English homonyms from Wikipedia. 2 As Table 1 shows, the shift of these words is indeed slightly higher than it is for other words. However, they still remain relatively close to their per-token anchor. Therefore, these anchors can still serve as a good approximation for learning alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context-Independent Alignment</head><p>We begin by briefly reviewing previous approaches for aligning context-independent embeddings, as they are generalized in this work to the contextual case. We denote the embedding of a word i by e i . At first, assume we are given word pairs {(e s i , e t i )} from a source language s and a target language t, and we look for a mapping between those. <ref type="bibr" target="#b21">Mikolov et al. (2013)</ref> proposed to learn a linear transformation whereby e t i is approximated via W e s i , for a learned matrix W . We focus on methods that follow this linear alignment. The alignment matrix is found by solving:</p><formula xml:id="formula_4">W s→t = argmin W ∈O d (R) n i=1 W e s i − e t i 2 ,<label>(4)</label></formula><p>where O d (R) is the space of orthogonal matrices. This constraint was proposed by <ref type="bibr">Xing et al. (2015)</ref> in order to preserve inter-lingual relations. Under this constraint, Eq. 4 is an instance of the orthogonal Procrustes problem, which has a closed-form solution W s→t = U V T . The columns of U and V are the left and right singular vectors of the multiplication of the source and (transposed) target embedding matrices. For the unsupervised case (i.e. when a dictionary is absent), Conneau et al. (2018a) (MUSE) suggested to learn the alignment via adversarial training, such that a discriminator is trained to distinguish between target and aligned source embeddings. Thereafter, a refinement procedure is applied iteratively as follows. First, a dictionary is built dynamically using the current alignment such that only words with high confidence are considered. Using the dictionary, the alignment matrix is re-calculated as in the supervised case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Context-Dependent Alignment</head><p>We next turn our attention to the main task of this paper, which is aligning context-dependent embeddings. We now describe our generalization of the methods described in Section 3.2 for this case. The first two methods are based only on anchors while the third one uses the contextual vectors themselves. Altogether, we suggest three alignment procedures, one aimed for the supervised and two for the unsupervised cases.</p><p>Supervised Anchored Alignment As a first step, we are assuming access to a dictionary for the source and target domains. For each source word i denote by D(i) the corresponding word in the target language. <ref type="bibr">3</ref> In the context-dependent case, Eq. 4 is no longer well-defined, as there are many corresponding vectors to both the source and the target words. However, this challenge can be addressed by aligning the vectorsē i for which we do have one per word. This is motivated by our observations in Section 3.1 that context-dependent embeddings are well clustered around their centers.</p><p>Thus, in the case where a dictionary is available, we solve Eq. 4 with token anchors as inputs.</p><p>We emphasize that by constraining W s→t to be orthogonal, we also preserve relations between e i,c andê i,c that represent the contextual information.</p><p>Unsupervised Anchored Alignment In this setting, no dictionary is present. As in the supervised case, we can naturally extend a contextindependent alignment procedure to the contextual space by leveraging the anchor spaceē i . This can be done using the adversarial MUSE framework proposed by Conneau et al. (2018a) and described at the end of Section 3.2.</p><p>Unsupervised Context-based Alignment Alternatively, the alignment could be learned directly on the contextual space. To this end, we follow again the adversarial algorithm of MUSE, but for each word we use multiple embeddings induced by different contexts, rather than the word anchor.</p><p>This context-based alignment presents opportunities but also introduces certain challenges. On the one hand, it allows to directly handle homonyms during the training process. However, empirically we found that training in this setting is less stable than unsupervised anchored alignments.</p><p>Refinement As a final step, for both of the unsupervised methods, we perform the refinement procedure that is incorporated in MUSE (end of Section 3.2). In order to synthesize a dictionary, we use distance in the anchor space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning Anchored Language Models</head><p>Thus far we assumed that embeddings for both source and target languages are pretrained separately. Afterwards, the source is mapped to the target in a second step via a learned mapping. However, this approach may not work well when raw data for the source languages is scarce, resulting in deficient embeddings. In what follows, we show how to address this problem when a dictionary is available. We focus on embeddings that are learned using a language model objective but this can be easily generalized to other objectives as well.</p><p>Our key idea is to constrain the embeddings across languages such that word translations will be close to each other in the embedding space. This can serve as a regularizer for the resourcelimited language model. In this case, the anchors are the model representations prior to its contextaware components (e.g., the inputs to ELMo's LSTM).</p><p>Denote the anchor for word i in language s by v s i . Now, assume we have trained a model for the target language and similarly have embeddings v t i . We propose to train the source model with an added regularization term as follows:</p><formula xml:id="formula_5">λ anchor · i v s i − v t D(i) 2 2 ,<label>(5)</label></formula><p>where λ anchor is a hyperparamter. This regularization has two positive effects. First, it reduces overfitting by reducing the effective number of parameters the model fits (e.g., if the regularizer has large coefficient, these parameters are essentially fixed). Second, it provides a certain level of alignment between the source and target language since they are encouraged to use similar anchors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multilingual Dependency Parsing</head><p>Now that we presented our method for aligning contextual embeddings, we turn to evaluate it on the task of cross-lingual dependency parsing. We first describe our baseline model, and then show how our alignment can easily be incorporated into this architecture to obtain a multilingual parser.</p><p>Baseline Parser Most previous cross-lingual dependency parsing models used transition-based models <ref type="bibr" target="#b3">(Ammar et al., 2016;</ref><ref type="bibr" target="#b12">Guo et al., 2016</ref> </p><formula xml:id="formula_6">s arc ij = h arc−head i T U arc h arc−dep j + b arc .</formula><p>(6) Additionally, the score for predicting the dependency label r for an edge (i, j) is defined as</p><formula xml:id="formula_7">s rel (i,j),r = h rel−head i T U rel r h rel−dep j + u rel−head r T h rel−head i + u rel−dep r T h rel−dep j + b r .<label>(7)</label></formula><p>At test time, MST is calculated to ensure valid outputs.</p><p>Multilingual Parsing with Alignment We now extend this model, in order to effectively use it for transfer learning. First, we include contextualized word embeddings by replacing the static embeddings with a pre-trained ELMo  model (instead of e i ). Second, we share all model parameters across languages and use the contextual word embeddings after they are aligned to a joint space J. Formally, if s is a sentence of language , contextual word embeddings are obtained via:</p><formula xml:id="formula_8">e →J i,s = W →J e i,s ,<label>(8)</label></formula><p>where W →J is the alignment matrix from language to the joint space. 4 This alignment is learned apriori and kept fixed during parser training. This setup is applicable for both single and multiple training languages. For the tested language, training data could be available, sometimes limited (few-shot), or absent (zero-shot). The alignment methods are described in detail in Section 3. In their paper,  suggest to output a linear combination over the representations of each layer of ELMo, learning these weights jointly with a downstream task. Our alignment is learned separately for each layer. Therefore, we keep the weights of the combination fixed during the training to ensure that the parser's inputs are from the joint cross-lingual space. Alternatively, one can share the weights of the combination between the languages and learn them.</p><p>All the above modifications are at the word embedding level, making them applicable to any other NLP model that uses word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Contextual Embeddings We use the ELMo model  with its default parameters to generate embeddings of dimension 1024 for all languages. For each language, training data comprises Wikipedia dumps 5 that were tokenized using UDpipe <ref type="bibr">(Straka and Straková, 2017)</ref>. We randomly shuffle the sentences and, following the setting of ELMO, use 95% of them for training and 5% for evaluation.</p><p>Alignment We utilize the MUSE framework 6 (Conneau et al., 2018a) and the dictionary tables provided by them. Theē i (anchor) vectors for the alignment are generated by computing the average of representations on the evaluation set (except for the limited unlabeled data case). To evaluate our alignment, we use the anchors to produce word translations. For all experiments we use the 50k most common words in each language.</p><p>Dependency Parsing We used the biaffine parser implemented in AllenNLP , refactored to handle our modifications as described in Section 4. <ref type="bibr">7</ref> The parser is trained on trees from a single or multiple languages, as described in each setting (Section 6). For the multiple case, we randomly alternate between the available languages, i.e. at each iteration we randomly choose one language and sample a corresponding batch. Dropout <ref type="bibr">(Srivastava et al., 2014)</ref> is applied on ELMo representations, Bi-LSTM representations and outputs of MLP layers. We also apply early stopping, where validation accuracy is measured as average LAS score on the development set across all training languages. The parser hyperparameters are the same as Dozat et al. <ref type="formula" target="#formula_1">(2017)</ref>   <ref type="bibr" target="#b17">Liu et al. (2019)</ref>, showing that the first layer alone can perform better than a mixture. <ref type="table" target="#tab_2">DE  ES  FR  IT  PT  SV  AVERAGE   SUPERVISED ANCHORED  78  85  86  82  74  68  79   UNSUPERVISED ANCHORED  63  61  70  58  35  22  52  + REFINE  72  74  81  77  53  33  65  UNSUPERVISED CONTEXT-BASED 57  68  59  57  53  *  49  + REFINE  73  82  77  73</ref> 66 * 62  agrees with <ref type="bibr" target="#b5">Belinkov et al. (2017)</ref>, showing that lower layers capture more syntactic information. Therefore, we fix the weights over ELMo layers to [0, 1, 0], i.e. using only representations from the first LSTM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALIGNMENT METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Scenarios for Dependency Parsing</head><p>For a fair comparison, we use the same setting as used by previous models for each scenario. Our main model (which we refer to as OURS) is using a SUPERVISED ANCHORED ALIGNMENT (Section 3.3) to align the multilingual pretrained ELMo embeddings which are used by the parser. We compare against several variants of our model:</p><p>• ALIGNED FASTTEXT: instead of ELMo, we use FASTTEXT pretrained embeddings <ref type="bibr" target="#b9">(Grave et al., 2018a)</ref>, aligned to English using MUSE.</p><p>• ALIGNEDē: instead of contextualized embeddings, we use the anchors themselves as fixed embeddings, aligned to English.</p><p>• NO DICTIONARY: we assume the absence of a dictionary and use UNSUPERVISED AN-CHORED ALIGNMENT.</p><p>• NO POS: no use of part of speech tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Alignment As mentioned above, we use outputs of the first LSTM layer of ELMo in our parsing experiments. Therefore, we present the alignment accuracy for those in    <ref type="table" target="#tab_3">Table 3</ref> summarizes the results for our zero-shot, multi-source experiments on six languages from Google universal dependency treebank version 2.0. 9 For each tested language, the parser was trained on all treebanks in the five other languages and English. We align each of the six languages to English. We compare our model to the performance of previous methods in the same setting (referred to as L t ∩ L s = ∅ in <ref type="bibr" target="#b3">Ammar et al. (2016)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-Shot Parsing, Multiple Source Languages</head><p>The results show that our multilingual parser outperforms all previous parsers with a large margin of 6.8 LAS points. Even with an unsupervised alignment, our model consistently improves over previous models.</p><p>To make a fair comparison to previous models, we also use gold POS tags as inputs to our parser. However, for low-resource languages, we might not have access to such labels. Even without the use of POS tags at all, in five out of six languages the score is still higher than previous methods that do consider such annotations. An exception is the Portuguese language where it leads to a drop of 8.8 LAS points. While in the single language setting this good performance can be explained by the knowledge captured in the character level, contextual embeddings <ref type="bibr" target="#b29">(Smith et al., 2018b;</ref><ref type="bibr">Belinkov 9</ref> https://github.com/ryanmcd/ uni-dep-tb/ et al., 2017), the results suggest that this knowledge transfers across languages.</p><p>In order to assess the value of contextual embeddings, we also evaluate our model using non-contextual embeddings produced by <ref type="bibr">FAST-TEXT (Bojanowski et al., 2017)</ref>. While these improve over previous works, our context-aware model outperforms them for all six languages in UAS score and for 5 out of 6 languages in LAS score, obtaining an average higher by 3 points. To further examine the impact of introducing context, we run our model with precomputed anchors (ē). Unlike FASTTEXT embeddings of size 300, these anchors share the same dimension with contextual embeddings but lack the contextual information. Indeed, the context-aware model is consistently better.</p><p>Few-Shot Parsing, Small Treebanks In this scenario, we assume a very small tree-bank for the tested language and no POS tags. We use the Kazakh tree-bank from CoNLL 2018 shared task <ref type="bibr">(Zeman et al., 2018)</ref>. The training set consists of only 38 trees and no development set is provided. Segmentation and tokenization are applied using UDPipe. Similar to <ref type="bibr" target="#b27">Rosa and Mareček (2018)</ref>; <ref type="bibr" target="#b28">Smith et al. (2018a)</ref>, we utilize the available training data in Turkish as it is a related language. To align contextual embeddings, we use a dictionary generated and provided by <ref type="bibr" target="#b27">Rosa and Mareček (2018)</ref> and compute an alignment from Kazakh to Turkish. The dictionary was obtained using FastAlign <ref type="bibr" target="#b6">(Dyer et al., 2013)</ref> on the Open-Subtitles2018 (Lison and Tiedemann, 2016) parallel sentences dataset from OPUS (Tiedemann, 2012). 10 <ref type="table" target="#tab_6">Table 5</ref> summarizes the results, showing that our algorithm outperforms the best model from the shared task by 5.05 LAS points and improves by over 10 points over a FASTTEXT baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-Shot Parsing, Limited Unlabeled Data</head><p>To evaluate our anchored language model (Section 3.4), we simulate a low resource scenario by extracting only 10k random sentences out of the Spanish unlabeled data. We also extract 50k sentences for LM evaluation but perform all computations, such as anchor extraction, on the 10k training data. For a dictionary, we used the 5k training table from <ref type="bibr">Conneau et al. (2018a)</ref>. 11 Another table of size 1,500 was used to evaluate the alignment. In this scenario, we assume a single training language (English) and no usage of POS tags nor any labeled data for the tested language. <ref type="table" target="#tab_5">Table 4</ref> shows the results. Reducing the amount of unlabeled data drastically decreases the precision by around 20 points. The regularization introduced in our anchored LM significantly improves the validation perplexity, leading to a gain of 7 UAS points and 9 LAS points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings of different languages, pretrained in an unsupervised fashion. At the core of our methods, we suggest to use anchors for tokens, reducing this problem to context-independent alignment. Our methods are compatible both for cases where a dictionary is present and absent, as well as for lowresource languages. The acquired alignment can be used to improve cross-lingual transfer learning, gaining from the contextual nature of the embeddings. We show that these methods lead to good word translation results, and improve significantly upon state-of-the-art zero-shot and few-shot crosslingual dependency parsing models.</p><p>In addition, our analysis reveals interesting properties of the context-aware embeddings generated by the ELMo model. Those findings are another step towards understanding the nature of contextual word embeddings.</p><p>As our method is in its core task-independent, we conjecture that it can generalize to other tasks as well. A Alignment Results for All Layers <ref type="table" target="#tab_9">Table 6</ref> manifests word-to-word translation results when supervised alignment is performed over different layer outputs from ELMo's LSTM. Even though layer zero produces context independent representations, the anchors computed over the contextual representations achieved higher precision. We conjecture that this is due to the language model objective being applied to the output of the second layer. Hence, unlike token-based embeddings such as FASTTEXT that optimize them directly, the context-independent representations of ELMo are optimized to produce a good base for the contextual embeddings that are computed on top of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Parsing Results</head><p>In   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters</head><p>We now detail the hyperparameters used throughout our experiments. All alignment experiments were performed using the default hyperparameters of the MUSE framework (see their github repository). <ref type="table" target="#tab_12">Table 8</ref> depicts the values used in multilingual parsing experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Alignment Example</head><p>We provide an additional example of a homonym. <ref type="figure">Figure 3</ref> shows the contextual embeddings of the word "bank" in English and the words "banco" (a financial establishment) and "orilla" (shore) in Spanish. In this case, unlike the "bear" example ( <ref type="figure">Figure 2)</ref>, the embeddings do not form two obvious clusters in the reduced two dimensional space. A possible explanation is that here the two meanings have the same POS tag (Noun). Even so, as shown in <ref type="table">Table 9</ref>, the alignment succeeds to place the embeddings of words from each context close to the matching translation. The nearest-neighbors for the "bear" example are presented in <ref type="table">Table 10</ref>.  k "BANCO" ANCHOR 1 Unlike in primary succession , the species that dominate secondary succession , are usually present from the start of the process , often in the soil seed bank .</p><p>2 Canto XLV is a litany against Usura or usury , which Pound later defined as a charge on credit regardless of potential or actual production and the creation of wealth ex nihilo by a bank to the benefit of its shareholders .</p><p>3 This prompted some investigation , led by Sir Benjamin Hall , which quickly turned up the fact that O' Connor was registered as the owner of all the estates , and of the associated bank . 4 The commercial NaS battery bank offers : ( Japanese ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Both team leaders are given a mystery word , which along with their team -mates use gigantic foam blocks and place them on the clue bank ( similar to Boggle ) with only giving a clue to the word ...</p><formula xml:id="formula_9">k "ORILLA" ANCHOR 1</formula><p>The combined Protestant forces , now numbering 25,000 strong , positioned themselves on the western bank of the Rhine River . 2 The Romans had a small advance guard of auxiliaries and cavalry on the opposite bank of the river .</p><p>3</p><p>Between Vaugirard and the river Seine he had a considerable force of cavalry , the front of which was flanked by a battery advantageously posted near Auteuil on the right bank of the river . 4 Mallus therefore stood on the eastern bank of the river . 5 The Argentine squadron spent the night of February 7 anchored between Juncal Island and the west bank of the river .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head><p>After marching north from Tewkesbury , Sir William Waller tried to contain the cavalry forces of Maurice on the western bank of the Severn , cutting this substantial force off from the rest of the Royalist army . <ref type="table">Table 9</ref>: Nearest-neighbors (after alignment) of the Spanish anchors "banco" (a financial establishment) and "orilla" (shore) from the contextual embeddings of the word "bank" in English. The full sentence is presented for context. <ref type="figure">Figure 3</ref>: Contextual embeddings for the English word "bank" and its two possible translations in Spanish -"banco" (a financial establishment) in red and "orilla" (shore) in blue. The figure shows a two dimensional PCA for the aligned space of the two languages. The symbols are the anchors and the dots are the contextualized embeddings. (best viewed in color) k "TENER" ANCHOR 1 It may be difficult for the patient to bear the odour of the smoke at first , but once he gets used to such a smell , it does not really matter . 2 No matter what the better class of slave owners might do , they had to bear the stigma of cruelty with the worst of tyrants ... 3 Every new car will bear the lan name instead of Van Diemen , so the highly successful marque will gradually disappear . 4 had a sufficient economic stake to bear the litigation burden necessary to maintain a private suit for recovery under section 4 . 5 In this example , consumers bear the entire burden of the tax ; the tax incidence falls on consumers .</p><p>k "OSO" ANCHOR 1 In 2010 , the government of the NWT decided to update its version of the polar bear -shaped plate .</p><p>2</p><p>Salad Fingers appears to be masochistic , as he can be seen taking pleasure from impaling his finger on a nail , rubbing stinging nettles on himself or stepping onto a bear trap . 3 The old bear -hunter , on being toasted , made a speech to the Texians , replete with his usual dry humor . 4 Balto arrives , distracts the bear , saves Aleu , they both escape and the bear disappears . 5 Defeated , the polar bear shrinks and transforms into a plush toy . <ref type="table">Table 10</ref>: Nearest-neighbors (after alignment) of the Spanish anchors "tener" (carry, verb) and "oso" (animal, noun) from the contextual embeddings of the word "bear" in English. The full sentence is presented for context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A two dimensional PCA showing examples of contextual representations for four Spanish words. Their corresponding anchors are presented as a star in the same color. (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>except we reduce the POS tag embedding size from 100 to 50 and increase the head/dependent MLP dimension from 400 to 500. All hyperparameter values used are listed in App. C.From experiments on the English tree-bank, we found that using the outputs of the first LSTM layer is as good as learning a combination. 8 This</figDesc><table><row><cell>5 https://lindat.mff.cuni.cz/</cell></row><row><cell>repository/xmlui/handle/11234/1-1989</cell></row><row><cell>6 https://github.com/facebookresearch/</cell></row><row><cell>MUSE/</cell></row></table><note>7 https://github.com/TalSchuster/ allennlp-MultiLang8 This was concurrently justified by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Word translation toEnglish precision @5 using CSLS (Conneau et al., 2018a)  with a dictionary (supervised) and without (unsupervised) for German (DE), Spanish (ES), French (FR), Italian (IT), Portuguese (PT) and Swedish (SV). Each of the unsupervised results is followed by a line with the results post the anchor-based refinement steps.* stands for 'Failed to converge'.</figDesc><table><row><cell>MODEL</cell><cell>DE</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell>PT</cell><cell>SV</cell><cell>AVERAGE</cell></row><row><cell>Zhang and Barzilay (2015)</cell><cell>54.1</cell><cell>68.3</cell><cell>68.8</cell><cell>69.4</cell><cell>72.5</cell><cell>62.5</cell><cell>65.9</cell></row><row><cell>Guo et al. (2016)</cell><cell>55.9</cell><cell>73.1</cell><cell>71.0</cell><cell>71.2</cell><cell>78.6</cell><cell>69.5</cell><cell>69.9</cell></row><row><cell>Ammar et al. (2016)</cell><cell>57.1</cell><cell>74.6</cell><cell>73.9</cell><cell>72.5</cell><cell>77.0</cell><cell>68.1</cell><cell>70.5</cell></row><row><cell>ALIGNED FASTTEXT</cell><cell>61.5</cell><cell>78.2</cell><cell>76.9</cell><cell>76.5</cell><cell>83.0</cell><cell>70.1</cell><cell>74.4</cell></row><row><cell>ALIGNEDē</cell><cell>58.0</cell><cell>76.7</cell><cell>76.7</cell><cell>76.1</cell><cell>79.2</cell><cell>71.9</cell><cell>73.1</cell></row><row><cell>OURS</cell><cell>65.2</cell><cell>80.0</cell><cell>80.8</cell><cell>79.8</cell><cell>82.7</cell><cell>75.4</cell><cell>77.3</cell></row><row><cell>OURS, NO DICTIONARY</cell><cell>64.1</cell><cell>77.8</cell><cell>79.8</cell><cell>79.7</cell><cell>79.1</cell><cell>69.6</cell><cell>75.0</cell></row><row><cell>OURS, NO POS</cell><cell>61.4</cell><cell>77.5</cell><cell>77.0</cell><cell>77.6</cell><cell>73.9</cell><cell>71.0</cell><cell>73.1</cell></row><row><cell cols="2">OURS, NO DICTIONARY, NO POS 61.7</cell><cell>76.6</cell><cell>76.3</cell><cell>77.1</cell><cell>69.1</cell><cell>54.2</cell><cell>69.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Zero-shot cross lingual LAS scores compared to previous methods, for German (DE), Spanish (ES), French (FR), Italian (IT), Portuguese (PT) and Swedish (SV). Aligned FASTTEXT andē context-independent embeddings are also presented as baselines. The bottom three rows are models that don't use POS tags at all and/or use an unsupervised anchored alignment. Corresponding UAS results are provided in App. B.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, summarizing the</cell></row><row><cell>precision@5 word-translation from 6 languages</cell></row><row><cell>to English. Results for the other layers are pre-</cell></row><row><cell>sented in App. A. As expected, supervised align-</cell></row><row><cell>ments outperform unsupervised ones by a large</cell></row><row><cell>margin. Between the two unsupervised methods,</cell></row><row><cell>the context-based alignment achieved significantly</cell></row><row><cell>better results for Spanish and Portuguese but failed</cell></row><row><cell>to converge for Swedish. In both cases, the value</cell></row><row><cell>of anchors in the REFINE step is clear, substantially</cell></row><row><cell>improving the precision for all languages.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Zero-shot, single-source results for the Spanish limited unlabeled data experiments. The parsing results are UAS/LAS scores, the perplexity is of the ELMo model, and the alignment scores are precision@5 on the held-out set, based on CSLS. All embeddings were aligned to English using supervised anchored alignment.</figDesc><table><row><cell>MODEL</cell><cell>LAS-F1</cell></row><row><cell>Rosa and Mareček (2018)</cell><cell>26.31</cell></row><row><cell>Smith et al. (2018a)</cell><cell>31.93</cell></row><row><cell>ALIGNED FASTTEXT</cell><cell>26.77</cell></row><row><cell>OURS</cell><cell>36.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Results for the Kazakh dataset from CoNLL 2018 Shared Task on Multilingual Parsing, compared to the two leading models w.r.t. this treebank.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Piotr Bojanowski, EdouardGrave, Armand Joulin, and  Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135-146. Dong Wang, Chao Liu, and Yiye Lin. 2015. Normalized word embedding and orthogonal transform for bilingual word translation. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational</figDesc><table><row><cell cols="2">retrieval criterion. In Proceedings of the 2018 Con-In Proceedings of the 2018 Conference on Empiri-</cell><cell>Language Learning, pages 73-82. Association for</cell></row><row><cell cols="2">ference on Empirical Methods in Natural Language cal Methods in Natural Language Processing, pages</cell><cell>Computational Linguistics.</cell></row><row><cell cols="2">Wanxiang Che, Yijia Liu, Yuxuan Wang, Bo Zheng, and Ting Liu. 2018. Towards better UD parsing: Deep contextualized word embeddings, ensemble, and treebank concatenation. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages Processing, pages 2979-2984. Association for Com-putational Linguistics. Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-ple and accurate dependency parsing using bidirec-tional LSTM feature representations. Transactions 2711-2720. Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. 2017. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. In International Conference on Learning Representations. of the Association for Computational Linguistics, 4:313-327. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,</cell><cell>Chao Xing, Linguistics: Human Language Technologies, pages 1006-1011. Association for Computational Linguis-tics.</cell></row><row><cell cols="2">55-64. Association for Computational Linguistics. Kevin Clark, Minh-Thang Luong, Christopher D. Man-ning, and Quoc Le. 2018. Semi-supervised se-quence modeling with cross-view training. In Pro-ceedings of the 2018 Conference on Empirical Meth-Guillaume Lample, Alexis Conneau, Ludovic De-noyer, and Marc'Aurelio Ranzato. 2018. Unsuper-vised machine translation using monolingual cor-pora only. In International Conference on Learning Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Re-search, 15:1929-1958. Representations. Milan Straka and Jana Straková. 2017. Tokenizing,</cell><cell>Wei Yang, Wei Lu, and Vincent Zheng. 2017. A simple regularization-based algorithm for learning cross-domain word embeddings. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2898-2904.</cell></row><row><cell cols="2">ods in Natural Language Processing, pages 1914-1925. Association for Computational Linguistics. Alexis Conneau, Guillaume Lample, Marc'Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-roni. 2015. Hubness and pollution: Delving into cross-space mapping for zero-shot learning. In Pro-ceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th In-POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Univer-sal Dependencies, pages 88-99, Vancouver, Canada. Association for Computational Linguistics. 2018a. Word translation without parallel data. In International Conference on Learning Representa-tions. ternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 270-280. Association for Computational Linguistics. Jörg Tiedemann. 2012. Parallel data, tools and inter-faces in OPUS. In Proceedings of the Eighth In-ternational Conference on Language Resources and</cell><cell>Daniel Zeman, Jan Hajič, Martin Popel, Martin Pot-thast, Milan Straka, Filip Ginter, Joakim Nivre, and Slav Petrov. 2018. CoNLL 2018 Shared Task: Mul-tilingual parsing from raw text to universal depen-dencies. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Univer-sal Dependencies, pages 1-21, Brussels, Belgium. Association for Computational Linguistics.</cell></row><row><cell cols="2">Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-ina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018b. XNLI: Evaluating cross-lingual sentence representations. In Proceed-ings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475-2485. Association for Computational Linguistics. Miryam de Lhoneux, Johannes Bjerva, Isabelle Augen-Evaluation (LREC-2012). European Language Re-sources Association (ELRA). stein, and Anders Søgaard. 2018. Parameter sharing between dependency parsers for related languages. In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages 4992-4997. Association for Computational Linguis-tics. Jörg Tiedemann. 2015. Cross-lingual dependency parsing with universal dependencies and predicted POS labels. In Proceedings of the Third In-ternational Conference on Dependency Linguistics (Depling 2015), pages 340-349.</cell><cell>Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Adversarial training for unsupervised bilingual lexicon induction. In Proceedings of the 55th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages 1959-1970. Association for Computational Linguis-tics.</cell></row><row><cell cols="2">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language under-standing. CoRR, abs/1810.04805. Georgiana Dinu and Marco Baroni. 2014. Improving zero-shot learning by mitigating the hubness prob-Dingquan Wang and Jason Eisner. 2016. The galactic guistics. lem. CoRR, abs/1412.6568. Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng Ji. 2018. A multi-lingual multi-task architecture for low-resource sequence labeling. In Proceedings of the 56th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers), pages 799-809. Association for Computational Lin-Jörg Tiedemann. 2017. Cross-lingual dependency parsing for closely related languages -helsinki's submission to vardial 2017. In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages 131-136. Association for Computational Linguistics.</cell><cell>Yuan Zhang and Regina Barzilay. 2015. Hierarchical low-rank tensors for multilingual transfer parsing. In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages 1857-1867. Association for Computational Linguis-tics.</cell></row><row><cell cols="2">Timothy Dozat and Christopher D. Manning. 2017. Deep Biaffine Attention for Neural Dependency Parsing. In International Conference on Learning dependencies treebanks: Getting more data by syn-Pierre Lison and Jrg Tiedemann. 2016. Opensub-thesizing new languages. Transactions of the Asso-titles2016: Extracting large parallel corpora from ciation for Computational Linguistics, 4:491-505. movie and tv subtitles. In Proceedings of the Tenth Representations. Timothy Dozat, Peng Qi, and Christopher D. Manning. 2017. Stanford's graph-based neural dependency International Conference on Language Resources Wenhui Wang, Baobao Chang, and Mairgup Mansur. and Evaluation (LREC 2016), Paris, France. Euro-2018. Improved dependency parsing using implicit pean Language Resources Association (ELRA). word connections learned from unlabeled data. In</cell></row><row><cell cols="2">parser at the CoNLL 2017 Shared Task. In Proceed-Proceedings of the 2018 Conference on Empirical</cell></row><row><cell cols="2">ings of the CoNLL 2017 Shared Task: Multilingual Methods in Natural Language Processing, pages</cell></row><row><cell cols="2">Parsing from Raw Text to Universal Dependencies, 2857-2863. Association for Computational Linguis-</cell></row><row><cell cols="2">pages 20-30. Association for Computational Lin-tics.</cell></row><row><cell>guistics.</cell><cell></cell></row><row><cell cols="2">Min Xiao and Yuhong Guo. 2014. Distributed word</cell></row><row><cell cols="2">Long Duong, Trevor Cohn, Steven Bird, and Paul representation learning for cross-lingual dependency</cell></row><row><cell cols="2">Cook. 2015. Cross-lingual transfer for unsupervised parsing. In Proceedings of the Eighteenth Confer-</cell></row><row><cell cols="2">dependency parsing without parallel data. In Pro-ence on Computational Natural Language Learning,</cell></row><row><cell cols="2">ceedings of the Nineteenth Conference on Computa-pages 119-129. Association for Computational Lin-</cell></row><row><cell cols="2">tional Natural Language Learning, pages 113-122. guistics.</cell></row><row><cell cols="2">Association for Computational Linguistics.</cell></row><row><cell>Min Xiao and Yuhong Guo. 2015.</cell><cell>Annotation</cell></row><row><cell cols="2">projection-based representation learning for cross-</cell></row><row><cell cols="2">lingual dependency parsing. In Proceedings of the</cell></row><row><cell cols="2">Nineteenth Conference on Computational Natural</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>we provide complementary results to</cell></row><row><cell>those in zero-shot closs-lingual parsing.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Per ELMo layer word translation to English precision @1 / @5 using CSLS (Conneau et al.</figDesc><table><row><cell>, 2018a)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Zero-shot cross lingual results compared to previous methods, measured in UAS. Aligned fastText andē context-independent models are also presented as baselines. The bottom three rows are models that don't use POS tags at all and/or use an unsupervised anchored alignment. Note that<ref type="bibr" target="#b3">Ammar et al. (2016)</ref> did not publish UAS results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameters used in parsing experiments, shared across different settings.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://en.wikipedia.org/wiki/List_ of_true_homonyms</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In practice, we may have multiple target words for a single source word, and the extension is straight-forward.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the space of the training language as our joint space and align the tested language to it. In the multi-source scenario, we align all embeddings to English.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/CoNLL-UD-2018/ CUNI-x-ling</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the MIT NLP group and the reviewers for their helpful discussion and comments. 11  We filtered words with multiple translations to the most common one by Google Translate.</p><p>The first and third authors were supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p><p>This work was also supported in part by the US-Israel Binational Science Foundation (BSF, Grant  No. 2012330), and by the Yandex Initiative in Machine Learning.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual word embeddings for low-resource language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Makarucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="937" to="947" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contextaware crosslingual mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Aldarmaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03243</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gromov-Wasserstein alignment of word embedding spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1881" to="1890" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Many languages, one parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="431" to="444" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1042</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013</title>
		<meeting>the 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AllenNLP: A deep semantic natural language processing platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Grus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</title>
		<meeting>Workshop for NLP Open Source Software (NLP-OSS)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>LREC</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised alignment of embeddings with Wasserstein Procrustes. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Berthet</surname></persName>
		</author>
		<idno>abs/1805.11222</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing based on distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A representation learning framework for multi-source transfer parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2734" to="2740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-adversarial unsupervised word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning word embeddings for low-resource languages by PU learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1093</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<title level="m">Loss in translation: Learning bilingual word mapping with a</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic knowledge and transferability of contextual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<editor>I. Guyon, U. V</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6294" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-source transfer of delexicalized dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="62" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selective sharing for multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="629" to="637" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">When and why are pre-trained word embeddings useful for neural machine translation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Felix</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2084</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="529" to="535" />
		</imprint>
	</monogr>
	<note>Sarguna Padmanabhan, and Graham Neubig. Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CUNI x-ling: Parsing under-resourced languages in CoNLL 2018 UD Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mareček</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">82 treebanks, 34 models: Universal dependency parsing with multi-treebank models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Miryam De Lhoneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stymne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">An investigation of the interactions between pre-trained word embeddings, character models and pos tags in dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Miryam De Lhoneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Stymne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
