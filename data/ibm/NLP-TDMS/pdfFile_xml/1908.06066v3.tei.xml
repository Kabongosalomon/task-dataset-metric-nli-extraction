<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
							<email>ligen.li@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software &amp; Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<email>nanduan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Natural Language Computing</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
							<email>fangyj@ss.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software &amp; Microelectronics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">STCA NLP Group</orgName>
								<address>
									<settlement>Microsoft, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
							<email>djiang@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">STCA NLP Group</orgName>
								<address>
									<settlement>Microsoft, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<email>mingzhou@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Natural Language Computing</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in a pre-training manner. Borrow ideas from cross-lingual pretrained models, such as XLM <ref type="figure">(</ref>Lample and Conneau 2019) and Unicoder (Huang et al. 2019), both visual and linguistic contents are fed into a multi-layer Transformer (Vaswani et al. 2017) for the cross-modal pre-training, where three pre-trained tasks are employed, including Masked Language Modeling (MLM), Masked Object Classification (MOC) and</p><p>Visual-linguistic Matching (VLM). The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large-scale image-caption pairs, we transfer Unicoder-VL to caption-based image-text retrieval and visual commonsense reasoning, with just one additional output layer. We achieve state-of-the-art or comparable results on both two tasks and show the powerful ability of the crossmodal pre-training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, pre-trained models have made great progress in both computer vision (CV) and natural language processing (NLP) communities.</p><p>In CV, pre-trained models, such as VGG <ref type="bibr" target="#b24">(Simonyan and Zisserman 2014)</ref> and ResNet <ref type="bibr" target="#b10">(He et al. 2016)</ref>, are usually trained based on CNN using ImageNet <ref type="bibr" target="#b7">(Deng et al. 2009</ref>), whose training objective is to predict the categorical label of a given image. For downstream tasks, such as image classification, image retrieval <ref type="bibr" target="#b12">(Karpathy and Fei-Fei 2015)</ref> ( ) and object detection <ref type="bibr" target="#b21">(Ren et al. 2015)</ref>, the resulting models can extract feature representations for input images, which will be further used in following task-specific models.</p><p>In NLP, pre-trained models, such as BERT <ref type="bibr" target="#b8">(Devlin et al. 2018)</ref>, XLNet  and <ref type="bibr">RoBERTa (Liu et al. 2019)</ref>, have achieved state-of-the-art performances in many NLP tasks as well, such as natural language inference <ref type="bibr">(Bowman et al. 2015)</ref>, and machine reading comprehension <ref type="bibr">(Ra-jpurkar et al. 2016)</ref>. Pre-trained with language modeling, such models can learn general knowledge from large-scale corpus first, and then transfer them to downstream tasks with simple fine-tuning layers.</p><p>However, these two types of pre-trained models cannot well handle a cross-modal task directly, if its natural language inputs are long sequences (such as questions), rather than short phrases (such as tags). The reason is two-fold. On one hand, as ImageNet covers categorical labels only, the resulting models cannot deal with long sequences. This is why most such tasks, e.g. visual question answering (VQA) <ref type="bibr" target="#b2">(Antol et al. 2015)</ref>, visual commonsense reasoning (VCR) <ref type="bibr" target="#b33">(Zellers et al. 2019</ref>) and image retrieval <ref type="bibr" target="#b12">(Karpathy and Fei-Fei 2015)</ref>, still need additional fusion layers to model interaction between visual and linguistic contents. On the other hand, existing NLP pre-trained models can handle long natural language sequences very well. But none of them is trained with visual contents directly.</p><p>Motivated by these, we propose a Universal encoder for Vision and Language, short for Unicoder-VL, a universal encoder based on a multi-layer Transformer <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref>, which aims to learn joint representations of vision and language (especially for long sequences) in a pre-training manner. Inspired by BERT and some recent cross-lingual pre-trained models. such as XLM <ref type="bibr" target="#b14">(Lample and Conneau 2019)</ref> and Unicoder <ref type="bibr" target="#b11">(Huang et al., 2019)</ref>, a cross-modal pre-training framework is designed to model the relationships between visual and linguistic contents and learn their joint representations. We use large-scale imagecaption pairs in Unicoder-VL training, as such annotations are easy to collect from web, with relatively good quality. Three pre-trained tasks are employed, including Masked Language Modeling (MLM), Masked Object Classification (MOC) and Visual-linguistic Matching (VLM). The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other.</p><p>As the first step along this new pre-training direction, we evaluate Unicoder-VL on image-text retrieval tasks. From experiments we can see that, by adding a simple fine-tuning layer, Unicoder-VL achieves state-of-the-art results on both red truck driving on the road at sunset  Figure 1: Illustration of Unicoder-VL in the context of an object and text masked token prediction, or cloze, task. Unicoder-VL contains multiple Transformer encoders which are used to learn viusal and linguistic representation jointly.</p><formula xml:id="formula_0">T v(road) T v(MASK) V road V [MASK</formula><p>MSCOCO <ref type="bibr" target="#b5">(Chen et al. 2015)</ref> and Flicker30K <ref type="bibr" target="#b32">(Young et al. 2014</ref>), comparing to a bunch of strong baselines. Furthermore, it also shows good performance in a zero-shot setting, which indicates a generalization ability. In VCR, we achieve comparable results with concurrent state-of-the-art works. It shows that cross-modal pre-training improve the ability of visual commonsense reasoning. The main contributions of our work are summarized as follows. We leverage a multi-layer Transformer to model cross-modal semantic representations. Meanwhile, we propose three well-designed cross-modal pre-training tasks to learn high-level visual representations and capture rich relationships between visual and linguistic contents. We finetune our pre-trained model to image-text retrieval and visual commonsense reasoning task and achieve significant improvements, demonstrating the effectiveness of our proposed method. Note, this pre-training method is general and not limited to image-text retrieval tasks. We will move further to evaluate it on more cross-modal tasks, such as image captioning <ref type="bibr" target="#b1">(Anderson et al. 2018)</ref>, scene graph generation, video classification and video question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training for CV Tasks</head><p>Most existing pre-trained CV models are based on multilayer CNN, such as VGG <ref type="bibr" target="#b24">(Simonyan and Zisserman 2014)</ref> and ResNet <ref type="bibr" target="#b10">(He et al. 2016)</ref>, and trained using ImageNet. As ImageNet <ref type="bibr" target="#b7">(Deng et al. 2009</ref>) only contains image labels, the resulting pre-trained models cannot deal with cross-modal tasks with long natural language inputs, such as queries in image retrieval and VQA tasks. These tasks pay more atttention on visual relations and descriptions rather than what is the image. By contrast, Unicoder-VL is pre-trained using image-caption pairs. So it is more suitable to these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training for NLP Tasks</head><p>Latest pre-trained NLP models are based on multi-layer Transformer, such as GPT <ref type="bibr" target="#b19">(Radford et al. 2018)</ref>, BERT <ref type="bibr" target="#b8">(Devlin et al. 2018)</ref>, XLNet  and <ref type="bibr">RoBERTa(Liu et al. 2019</ref>). All of the works are trained using large-scale corpus by language modeling. Such models learn contextualized text representations by predicting word tokens based on their contexts, and can be adapted to downstream tasks by additional fine-tuning.</p><p>Since the image is not a sequential data, the autoencoding objective of BERT is very appropriate for visual content. The key question is how to include visual contents in pre-training as well. However, the cross-modal pre-training is not limited to transformer-based models like BERT or XL-Net. We leave more exploration in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training for Cross-modal Tasks</head><p>Very recently, several attempts have been made to pre-train models for cross-modal tasks.</p><p>VideoBERT <ref type="bibr" target="#b27">(Sun et al. 2019</ref>) is one such method, whose goal is to learn cross-modal representations from videos and their corresponding transcripts. However, instead of using visual features directly in pre-training, it generates a sequence of visual words from each video first, and then uses them with transcript words together in LM pre-training. While in Unicoder-VL, we present visual features of objects in the images jointly training with linguistic contents.</p><p>Concurrent to our work, several recent released works, such as ViLBERT , VisualBERT , VL-BERT <ref type="bibr" target="#b26">(Su et al. 2019</ref>) and UNITER ) are pre-training methods on vision-and-language tasks. The concurrent emergency of these research works indicates the importance of deriving a generic pre-trainable representation for cross-modal tasks.</p><p>The comparison of these models are: 1) The model of ViLBERT is a two single-modal network applied on input sentences and images respectively, followed by a cross-modal Transformer combining information from the two sources. They propose a co-attentional Transformer layer (Co-TRM) in their model and claim such structure has a better ability to model interactions between visual and linguistic contents. Then the third Transformer fuses them. On the other hand, VisualBERT, Unicoder-VL, VL-BERT and UNITER proposed a single-stream architecture (vanilla BERT structure), which fuses cross-modal information early and freely, 2) a) The masked language model pre-training task is used by all of the above models. b) VisualBERT does not apply the object prediction task. Our model predicts the object labels while the others calculate the KL divergence between the input and output distributions. VL-BERT masked the image before applying by Faster-RCNN. UNITER masks only one modality each time. c) The visual-linguistic matching is used by all of the above models except VL-BERT, which claims this task is of no use.</p><p>3) VisualBERT is pre-trained on MSCOCO Captions dataset. ViLBERT, and VL-BERT are all pre-trained on about 3 million Conceptual Captions <ref type="bibr" target="#b22">(Sharma et al. 2018)</ref> dataset and then transfer to down-stream tasks. UNITER add about 1 million image-caption pair besides the Conceptual Captions and in-domain MSCOCO Caption and Visual Genome Dense Captions <ref type="bibr" target="#b13">(Krishna et al. 2017)</ref> data.</p><p>Compare to recent works, we achieve the SOTA results on image-to-text and text-to-image retrieval and VCR, which proves Unicoder-VL's ability on these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>In this section, we first briefly summarize the original BERT model, and present our cross-modal pre-trained model Unicoder-VL, including details of image and text preprocessing and three cross-modal pre-training tasks we used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT</head><p>BERT <ref type="bibr" target="#b8">(Devlin et al. 2018</ref>) is a pre-trained model based on multi-layer Transformer <ref type="bibr" target="#b27">(Vaswani et al. 2017)</ref>. Two tasks are used in pre-training: masked language model and next sentence prediction. In masked language model, BERT tries to predict the identity of each masked word based on all context words. In next sentence prediction, BERT tries to predict whether the second half of the input follows the first half of the input in the corpus, or is a random paragraph. A special token, <ref type="bibr">[CLS]</ref>, is prepended to every input sequence, and its representation in final layer will be used for the next sentence prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unicoder-VL</head><p>The overview of Unicoder-VL is shown in <ref type="figure">Fig 1.</ref> Given a pair of image and sentence, Unicoder-VL takes the visual regions of the image and textual tokens of the sentence as the input and then encode the input to the linguistic embedding and image embedding. These embeddings are then fed into a multi-layer self-attention Transformer to learn a crossmodality contextualized embedding between visual regions and textual tokens.</p><p>Linguistic Embedding. Following the text preprocessing of BERT, We tokenize each input text w = {w 1 , ..., w T }. T is the length of the WordPiece <ref type="bibr" target="#b30">(Wu et al. 2016</ref>) linguistic input. Besides, as shown in <ref type="figure">Fig 1,</ref> we also add the special tokens [CLS] and <ref type="bibr">[SEP]</ref>. For the visual elements, a special [IMG] token is assigned for each one of them. The final representation for each sub-word token is obtained via summing up its word embedding and position embedding, followed by a layer normalization (LN) layer. These embeddings are all initialized from BERT.</p><p>Image Embedding. For each input image, we first use Faster R-CNN (weights are initialized from <ref type="bibr" target="#b25">(Singh et al. 2018)</ref>) to extract the visual features (pooled ROI features) for each region. We also encode the location features with a 5-</p><formula xml:id="formula_1">D vector, b = ( x1 W , y1 H , x2 W , y2 H , (y2−y1)(x2−x1) W ·H ),</formula><p>where (x 1 , y 1 ) and (x 2 , y 2 ) denote the coordinate of the bottomleft and top-right corner and the fraction of image area covered respectively, and W , H are of the width and height of the input image. Both visual and location features are then fed through a fully-connected (FC) layer, to be projected into the same embedding space. The final visual embedding for each region is obtained by summing up the two FC outputs and then passing through another LN layer. The final image regions are denotes as v = {v 1 , ..., v I }. I is the length of the objects extracted from this image.</p><p>We also keep the predicted label of each detected object, which will be used in the object label prediction task. Note that the whole Faster R-CNN model is fixed during training.</p><p>Pre-training Tasks. We propose three tasks when doing the cross-modal pre-training: Masked Language Modeling (MLM), Masked Object Classifation (MOC) and Visuallinguistic Matching (VLM).</p><p>Masked Language Modeling (MLM). We denote the linguistic input as w = {w 1 , ..., w T } and object regions as v = {v 1 , ..., v I }, and the mask indices as m ∈ N M . In MLM, we randomly mask out the input words with probability of 15%, and replace the masked ones w m with special token <ref type="bibr">[MASK]</ref>. The goal is to predict these masked words based on the observation of their surrounding words w \m and all image regions v, by minimizing the negative log-likelihood:</p><formula xml:id="formula_2">L MLM (θ) = −E (w,v)∼D log P θ (w m |w \m , v)<label>(1)</label></formula><p>where θ is the trainable parameters. Each pair (w, v) is sampled from the whole training set D.</p><p>Masked Object Classifation (MOC). Similar to MLM, we also sample image regions and mask their visual features with a probability of 15%. We replace the object feature vector with a zero-initialized vector v m 90% of the time, and keep the object feature unchanged in the left 10% time. We simply take the object category with the highest confidence score predicted by the same detection model as the ground-truth label. We first feed the Transformer output of the masked region v (i) m m into an FC layer to predict the scores of K object classes, which further goes through a softmax function to be transformed into a normalized distribution g θ (v (i) m ). The final objective is:</p><formula xml:id="formula_3">L MOC (θ) = −E (w,v)∼D M i=1 CE(c(v (i) m ), g θ (v (i) m )) (2) where c(v (i) m ) ∈ R K is the one-hot vector of the ground-truth label.</formula><p>Visual-linguistic Matching (VLM). we also learn an instance-level alignment (rather than token/region-level) between the whole image and the sentence via VLM. We take final hidden state of [CLS] to predict whether the linguistic sentence is semantically matched with the visual content, with an additional FC layer. The scoring function is denoted as s θ (w, v). During training, we sample both positive and negative image-sentence pairs and learn their matching scores (including negative image and negative sentence). We denote the label as y ∈ {0, 1}, indicating if the sampled pair is a match. Then</p><formula xml:id="formula_4">L VLM (θ) = − E (w,v)∼D [y log s θ (w, v) + (1 − y) log(1 − s θ (w, v))]<label>(3)</label></formula><p>Overall, we have three training regimes corresponding to the image-text inputs. Our final pre-training objective is the sum of the losses above:</p><formula xml:id="formula_5">L = (L MLM + L MOC ) · I[y = 1] + L VLM<label>(4)</label></formula><p>where I[y = 1] is an indicator for the label 1 being correct for the image and caption pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we describe how we pre-train our model and show the evaluation details on image-text retrieval task to which we transfer the pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Unicoder-VL</head><p>Conceptual Captions dataset <ref type="bibr" target="#b22">(Sharma et al. 2018</ref>) contains about 3.3M image and caption pairs harvested from the web, which are very suitable for our cross-modal pre-training. Due to some broken urls, the size of image-caption pairs of Conceptual Captions dataset is about 3M. Similar to Conceptual Captions, SBU Captions <ref type="bibr" target="#b18">(Ordonez, Kulkarni, and Berg 2011)</ref> dataset is also automatically collected from Web and contains 1M image-caption pairs. Due to some broken urls, the size of image-caption pairs of SBU dataset is about 0.8M.</p><p>Finally, we use 3.8M image-caption pairs to do pretraining.</p><p>Our model has 12 layers of Transformer blocks, where each block has 768 hidden units and 12 self-attention heads. The maximum sequence length is set as 144. We sample 1 negative image or 1 negative caption and then judge whether this image and caption is matching when do the VLM task. The parameters are initialized from BERT-base, which is pre-trained on text data only.</p><p>For the visual part, we use fixed 100 RoIs with detection scores higher than 0.2 are selected for each image. If eligible RoIs are less than 100, we simply select the top-100 RoIs, regardless of the detection score threshold.</p><p>During Pre-training, our experiments are running on 4 NVIDIA Tesla V100 GPU. Our best performing model is pre-trained for 10 epochs with three training tasks introduced above, using the ADAM optimizer with learning rate of 1e-4 with a batch size of 192 with gradient accumulation (every 4 steps). The model will warmup the first 10% of all training steps. We use float16 operations to speed up training and to reduce the memory usage of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tune on Downstream Tasks</head><p>The pre-trained Unicoder-VL model can be transferred to multiple downstream visual-linguistic tasks, with simple modifications on the input format, output prediction, loss function and training strategy.</p><p>Image-Text Retrieval. Image-text retrieval is the task of identifying an image from candidates given a caption describing its content, or vice versa. We use two datasets as follows. 1) MSCOCO consists of 123,287 images, and each image contains roughly five textual descriptions. It is split into 82,783 training images, 5,000 validation images and 5,000 testing images. We follow the data split in <ref type="bibr" target="#b9">(Faghri et al. 2017)</ref> to add 30,504 images that were originally in the validation set of MSCOCO. 2) Flickr30K contains 31,783 images collected from the Flickr website. Following (Karpathy and Fei-Fei 2015), we split the dataset into 29,783 training images, 1,000 validation images and 1,000 testing images. Besides, we use three evaluation metrics, i.e., R@K (K=1,5,10). R@K is the percentage of ground-truth matchings appearing in the top K-ranked results.</p><p>During fine-tuning on image-text retrieval, we formulate it as a ranking problem. we sample 3 negative cases in each matching tasks. Inputs of fine-tuning share the same data preprocessing procedures with pre-training, except that we do not mask word and object in the fine-tuning stage. Similar to the VLM task, we also denote the score function as s θ (w, v). We omit this trainable parameter θ below. We propose two image-text matching tasks: image-to-text, textto-image. We use triplet loss and maximize the margin of positive and negative samples after generating the similarity score between two input modalities.</p><p>In this study, we focus on the hardest negatives in every sampled examples, following <ref type="bibr" target="#b9">(Faghri et al. 2017)</ref>. For a positive pair (w, v), the hardest negatives are given by v − h = arg max vi =v s(w, v i ) and w − h = arg max wi =w s(w i , v). So the hardest triplet loss function is:</p><formula xml:id="formula_6">L hard (x, y) = y − ∈Ny {max[0, γ − s(x, y) + s(x, y − h )]}</formula><p>(5) where x and y are encodings of two modality, N y is the set of negative samples of y.</p><p>Finally, we merge these ranking constraints into one loss function:  Followed We use γ = 0.2, λ 1 = 1.0, λ 2 = 1.0 as the hyper-parameters of loss function. The optimizer is Adam and learning rate is set as 5e-5. The batch size is 192 with gradient accumulation (every 4 steps). We also use float16 operations to speed up training and to reduce the memory usage of our models.</p><formula xml:id="formula_7">L = λ 1 w,v L hard (w, v) + λ 2 w,v L hard (v, w) (6)</formula><p>Zero-shot Image-Text Retrieval. The previous tasks are all transferring tasks that include dataset specific fine-tuning. In this zero-shot task, we directly apply the pretrained the multi-modal alignment prediction mechanism to image-text retrieval without finetuning. The goal of this task is to demonstrate that the pretraining has developed the ability to ground text and that this can generalize to visual and linguistic variation without any task specific fine-tuning. We directly use the pre-trained Unicoder-VL model and the same alignment prediction objective as a scoring function and test on the same split as the image-text retrieval task described above.</p><p>Visual Commonsense Reasoning. Given an image, the VCR task presents two problems visual question answering (Q→ A) and answer justification (QA→ R) both being posed as multiple choice problems. The holistic setting (Q→AR) requires both the chosen answer and then the chosen rationale to be correct. The Visual Commonsense Reasoning (VCR) dataset consists of 290k multiple choice QA problems derived from 110k movie scenes. Different from the VQA dataset, VCR integrates object tags into the language providing direct grounding supervision and explicitly excludes referring expressions.</p><p>To finetune on this VCR, we concatenate the question and each possible response with semicolons to form four different linguistic inputs and pass each through the model along with the image. w = {q 1 , ..., q n , ; , a 1 , ..., a n } for Q→ A and w = {q 1 , ..., q n , ; , a * 1 , ..., a * n , ; , r 1 , ..., r n }. Here, q 0 ,... are all question tokens, a 0 ,... are answer tokens, a * 0 are answer tokens for the correct answer, and r 0 are rationale tokens.</p><p>VCR provides ground truth boxes. For each ground truth box, we select the visual feature with highest intersection over union(IoU) from 100 boxes we extract as the new features. Then we add other visual features left after the features with ground truth boxes until the number is 100.</p><p>Since some of objects are referenced in Q, A, R, we add visual feature v i to these tokens additionally. i is the object index referenced by the linguistic word.</p><p>We also add a projection layer to calculate the score for each pair and the final prediction is a softmax over these four scores. The model is trained under a cross-entropy loss. We trained over 20 epochs with a batch size of 48 and initial learning rate of 3e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis Evaluation Results</head><p>Results on Image-Text Retrieval. We compare Unicoder-VL with state-of-the-art methods on image retrieval and sentence retrieval tasks in three different settings:</p><p>• zero-shot, where Unicoder-VL is applied to test set directly, without fine-tuning; • task-specific train, where Unicoder-VL is trained on task-specific training data directly, without pre-training; • pre-train + fine-tune, where Unicoder-VL is further finetuned on specific tasks. Experimental results of both datasets are shown in Tab 1. From Tab 1, the results of the zero-shot setting show that Unicoder-VL can learn general cross-modal knowledge, which take effects in image retrieval and sentence retrieval tasks directly, without any task-specific fine-tuning. Because the difference between automatically collected Conceptual Captions and human-annotated MSCOCO/Flickr30k, this zero-shot result is lower than the finetuned result. Usually finetuning will help the pre-trained model adapt to a little different downstream dataset. The results of the task-specific train setting show that Unicoder-VL trained on task-specific training data without pre-training still perform better than most previous approaches. It demonstrates the effectiveness of the selfattention mechanism itself on the image-text retrieval tasks.</p><p>The results of the pre-train + fine-tune setting show that this setting can significantly outperform all baselines on all evaluation metrics, which proves the superiority of our cross-modal pre-training method.</p><p>Taking R@1 for example, our best result on MSCOCO 1K test set obtains 7.8% and 8.1% absolute improvements against the PFAN approach on sentence retrieval task and image retrieval task, respectively. For MSCOCO 5K test set, we can also significantly outperform all baselines on these two tasks. On the Flickr30k testing set, the experiments show similar achievement. Unicoder-VL achieves new state-of-the-art performance and yield a result of 86.2% and 71.5% on R@1 for sentence retrieval and image retrieval, respectively. Compared with PFAN, we achieve absolute boost of 16.2% on R@1 for sentence retrieval and 21.1% on R@1 for image retrieval. The higher improvement on Flickr30k proves that low-resource task can be improved better with pre-training.</p><p>We also compare Unicoder-VL with ViLBERT ) and UNITER in the image retrieval and sentence retrieval setting. 10.1 points improvements than ViLBERT show the superiority of Unicoder-VL. UNITER uses 1.8M more image-caption pairs than Unicoder-VL, including in-domain dataset like Visual Genome Caption dataset during pre-training, which may greatly boost the performance of the image-text retrieval. Our Unicoder-VL can still achieve comparable results.  Results on Visual Commonsense Reasoning (VCR) Our final results on the VCR task are shown in Tab 2. Pretraining Unicoder-VL only slightly improves the perfor-mance. This might be because the pre-training task of image captioning is at the perceptual level, while the VCR task is at the cognitive understanding level. There is a gap between these two data types. Compared with baseline, R2C, we do not use task-specific modules. Instead, we simply add a simple classification layer to Unicoder-VL and jointly train the whole model end-to-end. Unicoder-VL outperforms R2C by large margins, indicating the power of our simple cross-modal architecture. The results without pre-training are slightly lower than results of pre-trained Unicoder-VL. It proves that VCR benefits from cross-modal pre-training. However, due to the difference of VCR dataset and caption dataset, the pre-training will not help too much. Compared with other concurrent works, i.e. ViL-BERT, VisualBERT, B2T2 and VLBERT, our Unicoder-VL achieves comparable performance with state-of-the-art results. It proves that pre-train the tranformer-based model with large-scale dataset will yield improvement than previous task-specific methods on visual commonsense reasoning tasks. Note that UNITER proposes a two-stage pre-training for VCR. Here, we select the one-stage pre-training result of UNITER, and it shows similar performance with concurrent works. But two-stage pre-training may be helpful on some very different datasets, like VCR and the caption dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>For the pre-training tasks. Unlike VideoBERT <ref type="bibr" target="#b27">(Sun et al. 2019</ref>), we do not use image-only inputs since the model fails to converge. But the viusal inputs of VideoBERT is actually generated visual words and its objective is still LM pretraining. We assume the true visual inputs without the guidance of linguistic data will damage the pretrained weights of BERT, which is pre-trained on linguistic data only. For future works, we are curious about how we could extend Unicoder-VL to image-only tasks like image-caption, scene graph generation or visual saliency detection.</p><p>For image-text retrieval task, the results of Unicoder-VL outperform all the methods without jointly pre-training (acturally viusal features from ResNet and linguistic word embeddings are pre-trained separately). It demonstrates that this transferring learning can also achieve great performance in cross-modal tasks. However, for image RoI based methods like SCAN(Lee et al. 2018), Unicoder-VL and ViL-BERT , the backbone of Faster-RCNN is still not fine-tuned with the whole model during cross-modal training. We have no idea that whether the performance is better or not if the backbone of detection model is fine-tuned with the cross-modal training and how to do so. We would like to explore these in the future.</p><p>We notice that the zero-shot image-text retrieval result of UNITER ) is much higher than ours. The reason is that UNITER uses in-domain dataset incluing MSCOCO Caption and Visual Genome Caption dataset to pretrain. These datasets are very similar to Flickr30k and it may be not a zero-shot testing. We believe that it is inappropriate to use in-domain dataset as pre-training dataset unless as the second-stage pre-training dataset because this in-domain dataset is human-annotated (of high quality) but Conceptual Captions and SBU Captions are au-tomatically collected (sometimes not human-like or not related). However, we agree that the performance on these downstream tasks should be enhanced with more highquality pre-training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>In this section, we perform ablation experiments in order to better understand the effect of the model size and the pretrain dataset size.</p><p>Effect of Model Size. We compare the results of Unicoder-VL models when varying Transformer encoder layers. We test our model with 6-layer, 12-layer and 24layer Transformer encoders. If the number of the layers are less than 12, we simply load the first several layers of pretrained weights from BERT. As shown in Tab 3, we find that the image-text retrieval tasks benefit from larger models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Sentence Retrieval Image Retrieval R@1 R@5 R@10 R@1 R@5 R@10  <ref type="table">Table 3</ref>: Ablation study of the depth of Unicoder-VL with respect to the number of Transformer encoder layers. All of these experiments are fine-tuning on Flickr30k with pretrained Unicoder-VL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Training Sets Size</head><p>We also studied the impact of the size of the pretraining dataset. For this experiment, we take 75% from the full dataset, and pretrain and finetune Unicoder-VL using the same setup as above. We can see that the accuracy grows monotonically as the amount of data increases, which suggests that Unicoder-VL may benefit from even more pretraining data. The same experiment results can be observed in ViLBERT ) and UNITER   <ref type="table">Table 4</ref>: Ablation study of the Flickr30k retrieval results of Unicoder-VL with respect to the pre-training dataset size. The number in parentheses is the number of image-text pairs we used in pre-training. 0 means without pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we proposed Unicoder-VL for cross-modal tasks. We utilize large-scale image-caption pairs to pretrain Unicoder-VL. We introduce three different pre-training tasks to align the visual and linguistic modalities and learn better cross-modal representations. When fine-tuning on image and sentence retrieval tasks, our experiment results on Flickr30K and MSCOCO datasets demonstrate that our pre-trained Transformer model can boost retrieval performance significantly. The zero-shots experiments exhibit that Unicoder-VL can learn general cross-modal knowledge, which take effects in image retrieval and sentence retrieval tasks directly, without any task-specific fine-tuning. The VCR experiment shows that cross-modal pre-training improve the ability of visual commonsense reasoning. This pre-training method is general and not limited to these tasks. We do not see any reason preventing it from finding broader cross-modal applications, including video related tasks. Meanwhile, we still have interest on how Unicoder-VL learn from image-only inputs. We will try to extend to some image-only tasks like image-caption and scene graph generation in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results on MSCOCO and Flickr30k test set.</figDesc><table /><note>† means the concurrent work.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results compared to the state-of-the-art methods with single model on VCR dataset by the time of submission. † means concurrent works. * means that the UNITER's one-stage pre-training result, which is similar to the concurrent work's setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>).</figDesc><table><row><cell>Methods</cell><cell cols="6">Sentence Retrieval R@1 R@5 R@10 R@1 R@5 R@10 Image Retrieval</cell></row><row><cell>Unicoder-VL (0)</cell><cell>73.0</cell><cell>89.0</cell><cell>94.1</cell><cell>57.8</cell><cell>82.2</cell><cell>88.9</cell></row><row><cell>Unicoder-VL (3M)</cell><cell>82.3</cell><cell>95.1</cell><cell>97.8</cell><cell>68.3</cell><cell>90.3</cell><cell>94.6</cell></row><row><cell cols="2">Unicoder-VL (3.8M) 86.2</cell><cell>96.3</cell><cell>99.0</cell><cell>71.5</cell><cell>90.9</cell><cell>94.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their helpful comments and discussions. This research is supported by National Natural Science Foundation of China under Grant NO.61672062, NO.61232005.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fusion of detected objects in text for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reitter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05054</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Vse++: Improved visual-semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unicoder: A universal language encoder by pre-training with multiple cross-lingual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00964</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cross-lingual language model pretraining</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<idno>arXiv:1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<meeting><address><addrLine>Roberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge aware semantic concept expansion for imagetext matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5182" to="5189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pythia-a platform for vision &amp; language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SysML Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01766</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Position focused attention network for imagetext matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3792" to="3798" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
