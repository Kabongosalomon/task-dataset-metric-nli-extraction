<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-08-19">19 Aug 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
							<email>guohao.li@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>MÃ¼ller</surname></persName>
							<email>matthias.mueller.2@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
							<email>ali.thabet@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Visual Computing Center</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<region>Thuwal</region>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-08-19">19 Aug 2019</date>
						</imprint>
					</monogr>
					<note>DeepGCNs: Can GCNs Go as Deep as CNNs? https://sites.google.com/view/deep-gcns</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem (see <ref type="figure">Figure 1</ref>). As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>GCNs have been gaining a lot of momentum in the last few years. This increased interest is attributed to two main factors: the increasing proliferation of non-Euclidean data in real-world applications, and the limited performance of CNNs when dealing with such data. GCNs operate directly on non-Euclidean data and are very promising for applications that depend on this information modality. GCNs are currently used to predict individual relations in social networks <ref type="bibr" target="#b35">[36]</ref>, model proteins for drug discovery <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b39">40]</ref>, enhance predictions of recommendation engines <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">50]</ref>, efficiently segment large point clouds <ref type="bibr" target="#b41">[42]</ref>, among other fields. * equal contribution A key reason behind the success of CNNs is the ability to design and reliably train very deep CNN models. In contrast, it is not yet clear how to properly train deep GCN architectures, where several works have studied their limitations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53]</ref>. Stacking more layers into a GCN leads to the common vanishing gradient problem. This means that back-propagating through these networks causes oversmoothing, eventually leading to features of graph vertices converging to the same value <ref type="bibr" target="#b18">[19]</ref>. Due to these limitations, most state-of-the-art GCNs are no deeper than 4 layers <ref type="bibr" target="#b52">[53]</ref>.</p><p>Vanishing gradients is not a foreign phenomenon in the world of CNNs. It also posed limitations on the depth growth of these types of networks. ResNet <ref type="bibr" target="#b10">[11]</ref> provided a big step forward in the pursuit of very deep CNNs when it introduced residual connections between input and output layers. These connections massively alleviated the vanishing gradient problem. Today, ResNets can reach 152 layers and beyond. Further extension came with DenseNet <ref type="bibr" target="#b12">[13]</ref>, where more connections are introduced across layers. More layers could potentially mean more spatial information loss due to pooling. This issue was also addressed, with Dilated Convolutions <ref type="bibr" target="#b50">[51]</ref>. The introductions of these key concepts had substantial impact on the progress of CNNs, and we believe they can have a similar effect if well adapted to GCNs.</p><p>In this work, we present an extensive study of methodologies that allow for training very deep GCNs. We adapt concepts that were successful in training deep CNNs, mainly residual connections, dense connections, and dilated convolutions. We show how we can incorporate these layers into a graph framework, and present an extensive analysis of the effect of these additions to the accuracy and stability of deep GCNs. To showcase these layer adaptations, we apply them to the popular task of point cloud semantic segmentation. We show that adding a combination of residual and dense connections, and dilated convolutions, enables successful training of GCNs up to 56 layers deep (refer to <ref type="figure" target="#fig_0">Figure 1</ref>). This very deep GCN improves the state-of-the-art on the challenging S3DIS <ref type="bibr" target="#b0">[1]</ref> point cloud dataset by 3.7%.</p><p>Contributions. We summarize our contributions as three fold. <ref type="bibr" target="#b0">(1)</ref> We adapt residual/dense connections, and dilated convolutions to GCNs. <ref type="bibr" target="#b1">(2)</ref> We present extensive experiments on point cloud data, showing the effect of each of these new layers to the stability and performance of training deep GCNs. We use point cloud semantic segmentation as our experimental testbed. (3) We show how these new concepts help build a 56-layer GCN, the deepest GCN architecture by a large margin, and achieve close to 4% boost in state-of-the-art performance on the S3DIS dataset [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A large number of real-world applications deal with non-Euclidean data, which cannot be systematically and reliably processed by CNNs in general. To overcome the shortcomings of CNNs, GCNs provide well-suited solutions for non-Euclidean data processing, leading to greatly increasing interest in using GCNs for a variety of applications. In social networks <ref type="bibr" target="#b35">[36]</ref>, graphs represent connections between individuals based on mutual interests/relations. These connections are non-Euclidean and highly irregular. GCNs help better estimate edge strengths between the vertices of social network graphs, thus leading to more accurate connections between individuals. Graphs are also used to model chemical molecule structures <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b39">40]</ref>. Understanding the bioactivities of these molecules can have substantial impact on drug discovery. Another popular use of graphs is in recommendation engines <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b49">50]</ref>, where accurate modelling of user interactions leads to improved product recommendations. Graphs are also popular modes of representation in natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>, where they are used to represent complex relations between large text units.</p><p>GCNs also find many applications in computer vision. In scene graph generation, semantic relations between objects are modelled using a graph. This graph is used to detect and segment objects in images, and also to predict semantic relations between object pairs <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b19">20]</ref>. Scene graphs also facilitate the inverse process, where an image is reconstructed given a graph representation of the scene <ref type="bibr" target="#b16">[17]</ref>. Graphs are also used to model human joints for action recognition in video <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b15">16]</ref>. GCNs are a perfect candidate for 3D point cloud processing, especially since the unstructured nature of point clouds poses a representational challenge for systematic research. Several attempts in creating structure from 3D data exist by either representing it with multiple 2D views <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>, or by voxelization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. More recent work focuses on directly processing unordered point cloud representations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">49]</ref>. The recent EdgeConv method by Wang et al. <ref type="bibr" target="#b41">[42]</ref> applies GCNs to point clouds. In particular, they propose a dynamic edge convolution algorithm for semantic segmentation of point clouds. The algorithm dynamically computes node adjacency at each graph layer using the distance between point features. This work demonstrates the potential of GCNs for point cloud related applications and beats the state-of-the-art in the task of point cloud segmentation. Unlike most other works, EdgeConv does not rely on RNNs or complex point aggregation methods.</p><p>Current GCN algorithms including EdgeConv are limited to shallow depths. Recent works attempt to train deeper GCNs. For instance, Kipf et al. trained a semi-supervised GCN model for node classification and showed how performance degrades when using more than 3 layers <ref type="bibr" target="#b17">[18]</ref>. Pham et al. <ref type="bibr" target="#b25">[26]</ref> proposed Column Network (CLN) for collective classification in relational learning and showed peak performance with 10 layers with the performance degrading for deeper graphs. Rahimi et al. <ref type="bibr" target="#b30">[31]</ref> developed a Highway GCN for user geo-location in social media graphs, where they add "highway" gates between layers to facilitate gradient flow. Even with these gates, the authors demonstrate performance degradation after 6 layers of depth. Xu et al. <ref type="bibr" target="#b45">[46]</ref> developed a Jump Knowledge Network for representation learning and devised an alternative strategy to select graph neighbors for each node based on graph structure. As with other works, their network is limited to a small number of layers <ref type="bibr" target="#b5">(6)</ref>. Recently, Li et al. <ref type="bibr" target="#b18">[19]</ref> studied the depth limitations of GCNs and showed that deep GCNs can cause over-smoothing, which results in features at vertices within each connected component converging to the same value. Other works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b52">53]</ref> also show the limitations of stacking multiple GCN layers, which lead to highly complex backpropagation and the common vanishing gradient problem.</p><p>Many difficulties facing GCNs nowadays (e.g. vanishing gradients and limited receptive field) were also present in the early days of CNNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51]</ref>. We bridge this gap and show that the majority of these drawbacks can be remedied by borrowing several orthogonal tricks from CNNs. Deep CNNs achieved a huge boost in performance with the introduction of ResNet <ref type="bibr" target="#b10">[11]</ref>. By adding residual connections between inputs and outputs of layers, ResNet tends to alleviate the vanishing gradient problem. DenseNet <ref type="bibr" target="#b12">[13]</ref> takes this idea a step further and adds connections across layers as well. Dilated Convolutions <ref type="bibr" target="#b50">[51]</ref> are a more recent approach that has lead to significant performance gains, specifically in image-to-image translation tasks such as semantic segmentation <ref type="bibr" target="#b50">[51]</ref>, by increasing the receptive field without loss of resolution. In this work, we show how one can benefit from concepts introduced for CNNs, mainly residual/dense connections and dilated convolutions, to train very deep GCNs. We support our claim by extending the work of Wang et al. <ref type="bibr" target="#b41">[42]</ref> to a much deeper GCN, and therefore significantly increasing its performance. Extensive experiments on the task of point cloud semantic segmentation validate these ideas for general graph scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Representation Learning on Graphs</head><p>Graph Definition. A graph G is represented by a tuple G = (V, E) where V is the set of unordered vertices and E is the set of edges representing the connectivity between vertices v â V. If e i,j â E, then vertices v i and v j are connected to each other with an edge e i,j . Graph Convolution Networks. Inspired by CNNs, GCNs intend to extract richer features at a vertex by aggregating features of vertices from its neighborhood. GCNs represent vertices by associating each vertex v with a feature vector h v â R D , where D is the feature dimension. Therefore, the graph G as a whole can be represented by concatenating the features of all the unordered vertices, i.e.</p><formula xml:id="formula_0">h G = [h v1 , h v2 , ..., h v N ] â R N ÃD ,</formula><p>where N is the cardinality of set V. A general graph convolution operation F at the l-th layer can be formulated as the following aggregation and update operations,</p><formula xml:id="formula_1">G l+1 = F(G l , W l ) = U pdate(Aggregate(G l , W agg l ), W update l ).</formula><p>(1)</p><formula xml:id="formula_2">G l = (V l , E l ) and G l+1 = (V l+1 , E l+1 )</formula><p>are the input and output graphs at the l-th layer, respectively. W agg l and W update l are the learnable weights of the aggregation and update functions respectively, and they are the essential components of GCNs. In most GCN frameworks, aggregation functions are used to compile information from the neighborhood of vertices, while update functions perform a non-linear transform on the aggregated information to compute new vertex representations. There are different variants of those two functions. For example, the aggregation function can be a mean aggregator <ref type="bibr" target="#b17">[18]</ref>, a max-pooling aggregator <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42]</ref>, an attention aggregator <ref type="bibr" target="#b38">[39]</ref> or an LSTM aggregator <ref type="bibr" target="#b24">[25]</ref>. The update function can be a multi-layer perceptron <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7]</ref>, a gated network <ref type="bibr" target="#b20">[21]</ref>, etc. More concretely, the representation of vertices is computed at each layer by aggregating features of neighbor vertices for all v l+1 â V l+1 as follows,</p><formula xml:id="formula_3">h v l+1 = Ï (h v l , Ï({h u l |u l â N (v l )}, h v l , W Ï ), W Ï ), (2)</formula><p>where Ï is a vertex feature aggregation function and Ï is a vertex feature update function, h v l and h v l+1 are the vertex features at the l-th layer and l + 1-th layer respectively. N (v l ) is the set of neighbor vertices of v at the l-th layer, and h u l is the feature of those neighbor vertices parametrized by W Ï . W Ï contains the learnable parameters of these functions. For simplicity and without loss of generality, we use a max-pooling vertex feature aggregator, without learnable parameters, to pool the difference of features between vertex v l and all of its neighbors:</p><formula xml:id="formula_4">Ï(.) = max(h u l â h v l | u l â N (v l )</formula><p>). We then model the vertex feature updater Ï as a multi-layer perceptron (MLP) with batch normalization <ref type="bibr" target="#b14">[15]</ref> and a ReLU as an activation function. This MLP concatenates h v l with its aggregate features from Ï(.) to form its input.</p><p>Dynamic Edges. As mentioned earlier, most GCNs have fixed graph structures and only update the vertex features at each iteration. Recent work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b37">38]</ref> demonstrates that dynamic graph convolution, where the graph structure is allowed to change in each layer, can learn better graph representations compared to GCNs with fixed graph structure. For instance, ECC (Edge-Conditioned Convolution) <ref type="bibr" target="#b33">[34]</ref> uses dynamic edge-conditional filters to learn an edgespecific weight matrix. Moreover, EdgeConv <ref type="bibr" target="#b41">[42]</ref> finds the nearest neighbors in the current feature space to reconstruct the graph after every EdgeConv layer. In order to learn to generate point clouds, Graph-Convolution GAN (Generative Adversarial Network) <ref type="bibr" target="#b37">[38]</ref> also applies k-NN graphs to construct the neighbourhood of each vertex in every layer. We find that dynamically changing neighbors in GCNs helps alleviate the over-smoothing problem and results in an effectively larger receptive field, when deeper GCNs are considered. In our framework, we propose to recompute edges between vertices via a Dilated k-NN function in the feature space of each layer to further increase the receptive field. In what follows, we provide detailed description of three operations that can enable much deeper GCNs to be trained: residual connections, dense connections, and dilated aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual Learning for GCNs</head><p>Designing deep GCN architectures <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b52">53]</ref> is an open problem in the graph learning space. Recent work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53]</ref> suggests that GCNs do not scale well to deep architectures, since stacking multiple layers of graph convolutions leads to high complexity in back-propagation. As such, most state-of-the-art GCN models are usually no more than 3 layers deep <ref type="bibr" target="#b52">[53]</ref>. Inspired by the huge success of ResNet <ref type="bibr" target="#b10">[11]</ref>, DenseNet <ref type="bibr" target="#b12">[13]</ref> and Dilated Convolutions <ref type="bibr" target="#b50">[51]</ref>,  we transfer these ideas to GCNs to unleash their full potential. This enables much deeper GCNs that reliably converge in training and achieve superior performance in inference.</p><p>In the original graph learning framework, the underlying mapping F, which takes a graph as an input and outputs a new graph representation (see Equation <ref type="formula">(1)</ref>), is learned. Here, we propose a graph residual learning framework that learns an underlying mapping H by fitting another mapping F. After G l is transformed by F, vertex-wise addition is performed to obtain G l+1 . The residual mapping F learns to take a graph as input and outputs a residual graph representation G res l+1 for the next layer. W l is the set of learnable parameters at layer l. In our experiments, we refer to our residual model as ResGCN.</p><formula xml:id="formula_5">G l+1 = H(G l , W l ) = F(G l , W l ) + G l = G res l+1 + G l .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dense Connections in GCNs</head><p>DenseNet <ref type="bibr" target="#b12">[13]</ref> was proposed to exploit dense connectivity among layers, which improves information flow in the network and enables efficient reuse of features among layers. Inspired by DenseNet, we adapt a similar idea to GCNs so as to exploit information flow from different GCN layers. In particular, we have:</p><formula xml:id="formula_6">G l+1 = H(G l , W l ) = T (F(G l , W l ), G l ) = T (F(G l , W l ), ..., F(G 0 , W 0 ), G 0 ).<label>(4)</label></formula><p>The operator T is a vertex-wise concatenation function that densely fuses the input graph G 0 with all the intermediate GCN layer outputs. To this end, G l+1 consists of all the GCN transitions from previous layers. Since we fuse GCN representations densely, we refer to our dense model as DenseGCN. The growth rate of DenseGCN is equal to the dimension D of the output graph (similar to DenseNet for CNNs <ref type="bibr" target="#b12">[13]</ref>). For example, if F produces a D dimensional vertex feature, where the vertices of the input graph</p><formula xml:id="formula_7">G 0 are D 0 dimensional, the dimension of each vertex fea- ture of G l+1 is D 0 + D Ã (l + 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dilated Aggregation in GCNs</head><p>Dilated wavelet convolution is an algorithm originating from the wavelet processing domain <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>. To alleviate spatial information loss caused by pooling operations, Yu et al. <ref type="bibr" target="#b50">[51]</ref> propose dilated convolutions as an alternative to applying consecutive pooling layers for dense prediction tasks, e.g. semantic image segmentation. Their experiments demonstrate that aggregating multi-scale contextual information using dilated convolutions can significantly increase the accuracy of semantic segmentation tasks. The reason behind this is the fact that dilation enlarges the receptive field without loss of resolution. We believe that dilation can also help with the receptive fields of deep GCNs. Therefore, we introduce dilated aggregation to GCNs. There are many possible ways to construct a dilated neighborhood. We use a Dilated k-NN to find dilated neighbors after every GCN layer and construct a Dilated Graph. In particular, for an input graph G = (V, E) with Dilated k-NN and d as the dilation rate, the Dilated k-NN returns the k nearest neighbors within the k Ã d neighborhood region by skipping every d neighbors. The nearest neighbors are determined based on a pre-defined distance metric. In our experiments, we use the 2 distance in the feature space of the current layer.</p><p>Let <ref type="figure" target="#fig_2">Figure 3</ref>), i.e. Therefore, the edges E (d) of the output graph are defined on the set of d-dilated vertex neighbors N (d) (v). Specifically, there exists a directed edge e â E (d) from vertex v to every vertex u â N (d) (v). The GCN aggregation and update functions are applied, as in Equation <ref type="formula">(1)</ref>, by using the edges E (d) created by the Dilated k-NN, so as to generate the feature h</p><formula xml:id="formula_8">N (d) (v) denote the d-dilated neighborhood of vertex v. If (u 1 , u 2 , ..., u kÃd ) are the first sorted k Ã d nearest neighbors, vertices (u 1 , u 1+d , u 1+2d , ..., u 1+(kâ1)d ) are the d-dilated neighbors of vertex v (see</formula><formula xml:id="formula_9">N (d) (v) = {u 1 , u 1+d , u 1+2d , ..., u 1+(kâ1)d }.</formula><formula xml:id="formula_10">(d) v of each output vertex in V (d) .</formula><p>We denote this layer operation as a dilated graph convolution with dilation rate d, or more formally:</p><formula xml:id="formula_11">G (d) = (V (d) , E (d) ).</formula><p>To improve generalization, we use stochastic dilation in practice. During training, we perform the aforementioned dilated aggregations with a high probability (1 â ) leaving a small probability to perform random aggregation by uniformly sampling k neighbors from the set of kÃd neighbors {u 1 , u 2 , ..., u kÃd }. At inference time, we perform deterministic dilated aggregation without stochasticity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We propose ResGCN and DenseGCN to handle the vanishing gradient problem of GCNs. To enlarge the receptive field, we define a dilated graph convolution operator for GCNs. To evaluate our framework, we conduct extensive experiments on the task of large-scale point cloud segmentation and demonstrate that our methods significantly improve performance. In addition, we also perform a comprehensive ablation study to show the effect of different components of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Graph Learning on 3D Point Clouds</head><p>Point cloud segmentation is a challenging task because of the unordered and irregular structure of 3D point clouds. Normally, each point in a point cloud is represented by its 3D spatial coordinates and possibly auxiliary features such as color and surface normal. We treat each point as a vertex v in a directed graph G and we use k-NN to construct the directed dynamic edges between points at every GCN layer (refer to Section 3.1). In the first layer, we construct the input graph G 0 by executing a dilated k-NN search to find the nearest neighbor in 3D coordinate space. At subsequent layers, we dynamically build the edges using dilated k-NN in feature space. For the segmentation task, we predict the categories of all the vertices at the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>We use the overall accuracy (OA) and mean intersection over union (mIoU) across all classes as evaluation metrics. For each class, the IoU is computed as T P T P +T âP , where T P is the number of true positive points, T is the number of ground truth points of that class, and P is the number of predicted positive points. To motivate the use of deep GCNs, we do a thorough ablation study on area 5 to analyze each component and provide insights. We then evaluate our proposed reference model (backbone of 28 layers with residual graph connections and stochastic dilated graph convolutions) on all 6 areas and compare it to the shallow DGCNN baseline <ref type="bibr" target="#b41">[42]</ref> and other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network Architectures</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, all the network architectures in our experiments have three blocks: a GCN backbone block, a fusion block and an MLP prediction block. The GCN backbone block is the only part that differs between experiments. For example, the only difference between PlainGCN and ResGCN is the use of residual skip connections for all GCN layers in ResGCN. Both have the same number of parameters. We linearly increase the dilation rate d of dilated k-NN with network depth. For fair comparison, we keep the fusion and MLP prediction blocks the same for all architectures. In the S3DIS semantic segmentation task, the GCN backbone block takes as input a point cloud with 4096 points, extracts features by applying consecutive GCN layers to aggregate local information, and outputs a learned graph representation with 4096 vertices. The fusion and MLP prediction blocks follow a similar architecture as PointNet <ref type="bibr" target="#b26">[27]</ref> and DGCNN <ref type="bibr" target="#b41">[42]</ref>. The fusion block is used to fuse the global and multi-scale local features. It takes as input the extracted vertex features from the GCN backbone block at every GCN layer and concatenates those features, then passes them through a 1Ã1 convolution layer followed by max pooling. The latter layer aggregates the vertex features of the whole graph into a single global feature vector, which in return is concatenated with the feature of each vertex from all previous GCN layers (fusion of global and local information). The MLP prediction block applies three MLP layers to the fused features of each vertex/point to predict its category. In practice, these layers are 1Ã1 convolutions.</p><p>PlainGCN. This baseline model consists of a PlainGCN backbone block, a fusion block, and a MLP prediction block. The backbone stacks 28 EdgeConv <ref type="bibr" target="#b41">[42]</ref> layers with dynamic k-NN, each of which is similar to the one used in DGCNN <ref type="bibr" target="#b41">[42]</ref>. No skip connections are used here.</p><p>ResGCN. We construct ResGCN by adding dynamic dilated k-NN and residual graph connections to PlainGCN. These connections between all GCN layers in the GCN backbone block do not increase the number of parameters.</p><p>DenseGCN. Similarly, DenseGCN is built by adding dynamic dilated k-NN and dense graph connections to the PlainGCN. As described in Section 3.3, dense graph connections are created by concatenating all the intermediate graph representations from previous layers. The dilation rate schedule of our DenseGCN is the same as ResGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation</head><p>We implement all our models using Tensorflow. For fair comparison, we use the Adam optimizer with the same initial learning rate 0.001 and the same learning rate schedule; the learning rate decays 50% every 3 Ã 10 5 gradient decent steps. The networks are trained with two NVIDIA Tesla V100 GPUs using data parallelism. The batch size is set to 8 for each GPU. Batch Normalization is applied to every layer. Dropout with a rate of 0.3 is used at the second MLP layer of the MLP prediction block. As mentioned in Section 3.4, we use dilated k-NN with a random uniform sampling probability = 0.2 for GCNs with dilations. In order to isolate the effect of the proposed deep GCN architectures, we do not use any data augmentation or post processing techniques. We train our models end-to-end from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results</head><p>For convenient referencing, we use the naming convention BackboneBlock-#Layers to denote the key models in our analysis and we provide all names in <ref type="table" target="#tab_1">Table 1</ref>. We focus on residual graph connections for our analysis, since ResGCN-28 is easier and faster to train, but we expect that our observations also hold for dense graph connections.</p><p>We investigate the performance of different ResGCN architectures, e.g. with dynamic dilated k-NN, with regular dynamic k-NN (without dilation), and with fixed edges. We also study the effect of different parameters, e.g. number of k-NN neighbors <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32)</ref>, number of filters <ref type="bibr" target="#b31">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128)</ref>, and number of layers <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr">56)</ref>. Overall, we conduct 20 experiments and show their results in <ref type="table" target="#tab_1">Table 1</ref>. <ref type="table" target="#tab_1">Table 1</ref> (Reference) show that residual graph connections play an essential role in training deeper networks, as they tend to result in more stable gradients. This is analogous to the insight from CNNs <ref type="bibr" target="#b10">[11]</ref>. When the residual graph connections between layers are removed (i.e. in PlainGCN-28), performance dramatically degrades (-12% mIoU). In Appendices A and B, we show similar performance gains by combining residual graph connections and dilated graph convolutions with other types of GCN layers. Effect of dilation. Results in <ref type="table" target="#tab_1">Table 1</ref> (Dilation) <ref type="bibr" target="#b50">[51]</ref> show that dilated graph convolutions account for a 2.85% improvement in mean IoU (row 3), motivated primarily by the expansion of the network's receptive field. We find that adding stochasticity to the dilated k-NN does help performance but not to a significant extent. Interestingly, our results in <ref type="table" target="#tab_1">Table 1</ref> also indicate that dilation especially helps deep networks when combined with residual graph connections <ref type="figure" target="#fig_0">(rows 1,8)</ref>. Without such connections, performance can actually degrade with dilated graph convolutions. The reason for this is probably that these varying neighbors result in 'worse' gradients, which further hinder convergence when residual graph connections are not used. Effect of dynamic k-NN. While we observe an improvement when updating the k nearest neighbors after every layer, we would also like to point out that it comes at a relatively high computational cost. We show different variants without dynamic edges in <ref type="table" target="#tab_1">Table 1</ref> (Fixed k-NN). Effect of dense graph connections. We observe similar performance gains with dense graph connections (DenseGCN-28) in <ref type="table" target="#tab_1">Table 1</ref> (Connections). However, with a naive implementation, the memory cost is prohibitive. Hence, the largest model we can fit into GPU memory uses only 32 filters and 8 nearest neighbors, as compared to 64 filters and 16 neighbors in the case of its residual counterpart ResGCN-28. Since the performance of these two deep GCN variants is similar, residual connections are more practical for most use cases and, hence we focus on them in our ablation study. Yet, we do expect the same insights to transfer to the case of dense graph connections. Effect of nearest neighbors. Results in <ref type="table" target="#tab_1">Table 1</ref> (Neighbors) show that a larger number of neighbors helps in general. As the number of neighbors is decreased by a factor of 2 and 4, the performance drops by 2.5% and 3.3% respectively. However, a large number of neighbors only results in a performance boost, if the network capacity is sufficiently large. This becomes apparent when we increase the number of neighbors by a factor of 2 and decrease the number of filters by a factor of 2. Effect of network depth.  <ref type="table" target="#tab_1">Table 1</ref>. Ablation study on area 5 of S3DIS. We compare our reference network (ResGCN-28) with 28 layers, residual graph connections, and dilated graph convolutions to several ablated variants. All models were trained with the same hyper-parameters for 100 epochs on all areas except for area 5, which is used for evaluation. We denote residual and dense connections with the â and symbols respectively. We highlight the most important results in bold. âmIoU denotes the difference in mIoU with respect to the reference model ResGCN-28.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of residual graph connections. Our experiments in</head><p>Qualitative Results. <ref type="figure">Figure 4</ref> shows qualitative results on area 5 of S3DIS <ref type="bibr" target="#b0">[1]</ref>. As expected from the results in Table 1, our ResGCN-28 and DenseGCN-28 perform particularly well on difficult classes such as board, beam, bookcase and door. Rows 1-4 clearly show how ResGCN-28 and DenseGCN-28 are able to segment the board, beam, bookcase and door respectively, while PlainGCN-28 completely fails. Please refer to Appendices C, D and E for more qualitative results and other ablation studies.</p><p>Comparison to state-of-the-art. Finally, we compare our reference network (ResGCN-28), which incorporates the ideas put forward in the methodology, to several state-ofthe-art baselines in <ref type="table">Table 2</ref>. The results clearly show the effectiveness of deeper models with residual graph connections and dilated graph convolutions. ResGCN-28 outperforms DGCNN <ref type="bibr" target="#b41">[42]</ref> by 3.9% (absolute) in mean IoU, even though DGCNN has the same fusion and MLP prediction blocks as ResGCN-28 but with a shallower PlainGCN backbone block. Furthermore, we outperform all baselines in 9 out of 13 classes. We perform particularly well in the difficult object classes such as board, where we achieve 51.1%, and sofa, where we improve state-of-the-art by about 10%. This significant performance improvement on the difficult classes is probably due to the increased network capacity, which allows the network to learn subtle details necessary to distinguish between a board and a wall for example. The first row in <ref type="figure">Figure 4</ref> is a representative example for this occurrence. Our performance gains are solely due to our innovation in the network architecture, since we use the same hyper-parameters and even learning rate schedule as the baseline DGCNN <ref type="bibr" target="#b41">[42]</ref> and only decrease the number of nearest neighbors from 20 to 16 and the batch size from 24 to 16 due to memory constraints. We outperform state-of-the art methods by a significant margin and expect further improvement from tweaking the hyper-parameters, especially the learning schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this work, we investigate how to bring proven useful concepts (residual connections, dense connections and dilated convolutions) from CNNs to GCNs and answer the question: how can GCNs be made deeper? Extensive experiments show that by adding skip connections to GCNs, we can alleviate the difficulty of training, which is the primary problem impeding GCNs to go deeper. Moreover, dilated graph convolutions help to gain a larger receptive field without loss of resolution. Even with a small amount of nearest neighbors, deep GCNs can achieve high performance on point cloud semantic segmentation. ResGCN-56 performs very well on this task, although it uses only 8 nearest neighbors compared to 16 for ResGCN-28. We were also able to train ResGCN-151 for 80 epochs; the network converged very well and achieved similar results as ResGCN-28 and ResGCN-56 but with only 3 nearest neighbors. Due to com-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>PlainGCN-28 DenseGCN-28 ResGCN-28 <ref type="bibr">Figure 4</ref>. Qualitative Results on S3DIS Semantic Segmentation. We show here the effect of adding residual and dense graph connections to deep GCNs. PlainGCN-28, ResGCN-28, and DenseGCN-28 are identical except for the presence of residual graph connections in ResGCN-28 and dense graph connections in DenseGCN-28. We note how both residual and dense graph connections have a substantial effect on hard classes like board, bookcase, and sofa. These are lost in the results of PlainGCN-28.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>OA mIOU ceiling floor wall beam column window door  <ref type="table">Table 2</ref>. Comparison of ResGCN-28 with state-of-the-art on S3DIS Semantic Segmentation. We report average per-class results across all areas for our reference model ResGCN-28, which has 28 GCN layers, residual graph connections, and dilated graph convolutions, and state-of-the-art baselines. ResGCN-28 outperforms state-of-the-art by almost 4%. It also outperforms all baselines in 9 out of 13 classes. The metrics shown are overall point accuracy (OA) and mean IoU (mIoU). '-' denotes not reported and bold denotes best performance.</p><p>putational constraints, we were unable to investigate such deep architectures in detail and leave it for future work.</p><p>Our results show that after solving the vanishing gradient problem plaguing deep GCNs, we can either make GCNs deeper or wider (e.g. ResGCN-28W) to get better performance. We expect GCNs to become a powerful tool for processing non-Euclidean data in computer vision, natural language processing, and data mining. We show successful cases for adapting concepts from CNNs to GCNs. In the future, it will be worthwhile to explore how to transfer other operators, e.g. deformable convolutions <ref type="bibr" target="#b5">[6]</ref>, other architectures, e.g. feature pyramid architectures <ref type="bibr" target="#b51">[52]</ref>, etc. It will also be interesting to study different distance measures to compute dilated k-NN, constructing graphs with different k at each layer, better dilation rate schedules <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref> for GCNs, and combining residual and dense connections.</p><p>We also point out that, for the specific task of point cloud semantic segmentation, the common approach of processing the data in 1m Ã 1m columns is sub-optimal for graph representation. A more suitable sampling approach should lead to further performance gains on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep GCN Variants</head><p>In our experiments in the paper, we work with a GCN based on EdgeConv <ref type="bibr" target="#b41">[42]</ref> to show how very deep GCNs can be trained. However, it is straightforward to build other deep GCNs with the same concepts we proposed (e.g. residual/dense graph connections, dilated graph convolutions).</p><p>To show that these concepts are universal operators and can be used for general GCNs, we perform additional experiments. In particular, we build ResGCNs based on Graph-SAGE <ref type="bibr" target="#b9">[10]</ref>, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b44">[45]</ref> and MRGCN (Max-Relative GCN) which is a new GCN operation we proposed. In practice, we find that EdgeConv learns a better representation than the other implementations. However, it is less memory and computation efficient. Therefore, we propose a simple GCN combining the advantages of them all.</p><p>All of the ResGCNs have the same components (e.g. dynamic k â N N , residual connections, stochastic dilation) and parameters (e.g. #NNs, #filters and #layers) as ResGCN-28 in <ref type="table">Table Ablation</ref> Study of the paper except for the internal GCN operations. To simplify, we refer to these models as ResEdgeConv, ResGraphSAGE, ResGIN and NewResGCN respectively. Note that ResEdgeConv is an alias for ResGCN in our paper. We refer to it as ResEdge-Conv to distinguish it from the other GCN operations.</p><p>ResEdgeConv. Instead of aggregating neighborhood features directly, EdgeConv <ref type="bibr" target="#b41">[42]</ref> proposes to first get local neighborhood information for each neighbor by subtracting the feature of the central vertex from its own feature. In order to train deeper GCNs, we add residual/dense graph connections and dilated graph convolutions to EdgeConv:</p><formula xml:id="formula_12">h res v l+1 = max {mlp(concat(h v l , h u l â h v l ))|u l â N (d) (v l )} , h v l+1 = h res v l+1 + h v l .<label>(5)</label></formula><p>ResGraphSAGE. GraphSAGE <ref type="bibr" target="#b9">[10]</ref> proposes different types of aggregator functions including a Mean aggregator, LSTM aggregator and Pooling aggregator. Their experiments show that the Pooling aggregator outperforms the others. We adapt GraphSAGE with the max-pooling aggregator to obtain ResGraphSAGE:</p><formula xml:id="formula_13">h res N (d) (v l ) = max {mlp(h u l )|u l â N (d) (v l )} , h res v l+1 = mlp concat h v l , h res N (d) (v l )</formula><p>,</p><formula xml:id="formula_14">h v l+1 = h res v l+1 + h v l ,<label>(6)</label></formula><p>In the original GraphSAGE paper, the vertex features are normalized after aggregation. We implement two variants, one without normalization (see Equation <ref type="formula" target="#formula_14">(6)</ref>), the other one with normalization h res</p><formula xml:id="formula_15">v l+1 = h res v l+1 / h res v l+1 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResGIN. The main difference between GIN [45] and other</head><p>GCNs is that an is learned at each GCN layer to give the central vertex and aggregated neighborhood features different weights. Hence ResGIN is formulated as follows:</p><formula xml:id="formula_16">h res v l+1 = mlp (1 + ) Â· h v l + sum({h u l |u l â N (d) (v l )}) , h v l+1 = h res v l+1 + h v l .<label>(7)</label></formula><p>ResMRGCN. We find that first using a max aggregator to aggregate neighborhood relative features (h u l â h v l ), u l â N (v l ) is more efficient than aggregating raw neighborhood features h v l , u l â N (v l ) or aggregating features after nonlinear transforms. We refer to this simple GCN as MRGCN (Max-Relative GCN). The residual version of MRGCN is as such:</p><formula xml:id="formula_17">h res N (d) (v l ) = max {h u l â h v l |u l â N (d) (v l )} , h res v l+1 = mlp concat h v l , h res N (d) (v l )</formula><p>,</p><formula xml:id="formula_18">h v l+1 = h res v l+1 + h v l .<label>(8)</label></formula><p>Where h v l+1 and h v l are the hidden state of vertex v at l+1; h res v l+1 is the hidden state of the residual graph. All the mlp (multilayer perceptron) functions use a ReLU as activation function; all the max and sum functions above are vertexwise feature operators; concat functions concatenate features of two vertices into one feature vector. N (d) (v l ) denotes the neighborhood of vertex v l obtained from Dilated k-NN. <ref type="table">Table 3</ref> shows a comparison of different deep residual GCNs variants on the task of semantic segmentation; we report the mIOU for area 5 of S3DIS. All deep GCN variants are 28 layers deep and we denote them as ResEdgeConv-28, ResGraphSAGE-28, ResGraphSAGE-N-28, ResGIN--28 and ResMRGCN-28; ResGraphSAGE-28 is GraphSAGE without normalization, ResGraphSAGE-N-28 is the version with normalization. The results clearly show that different deep GCN variants with residual graph connections and dilated graph convolutions converge better than the PlainGCN. ResMRGCN-28 achieves almost the same performance as ResEdgeConv-28 while only using half of the GPU memory. ResGraphSAGE-28 and ResGraphSAGE-N-28 are slightly worse than ResEdgeConv-28 and ResMRGCN-28. The results also show that using normalization for ResGraphSAGE is not essential. Interestingly, we find that ResGIN--28 converges  <ref type="table">Table 3</ref>. Comparisons of Deep GCNs variants on area 5 of S3DIS. We compare our different types of ResGCN (ResEdgeConv, Res-GraphSAGE, ResGIN and ResMRGCN) with 28 layers. Residual graph connections and Dilated graph convolutions are added to all the GCN variants. All models were trained with the same hyper-parameters for 100 epochs on all areas except for area 5 which is used for evaluation. We denote residual with the â symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results for Deep GCN Variants</head><p>well during the training phase and has a high training accuracy. However, it fails to generalize to the test set. This phenomenon is also observed in the original paper <ref type="bibr" target="#b44">[45]</ref> in which they find setting to 0 can get the best performance. Therefore, we can draw the conclusion that the concepts we proposed (e.g. residual/dense graph connections and dilated graph convolutions) generalize well to different types of GCNs and enable training very deep GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results for the Ablation Study</head><p>We summarize the most important insights of the ablation study in <ref type="figure">Figure 5</ref>. <ref type="figure">Figures 6, 7</ref>  <ref type="figure">Figure 5</ref>. Ablation study on area 5 of S3DIS. We compare our reference network (ResGCN-28) with 28 layers, residual graph connections and dilated graph convolutions to several ablated variants. All models were trained for 100 epochs on all areas except for area 5 with the same hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Run-time Overhead of Dynamic k-NN</head><p>We conduct a run-time experiment comparing the inference time of the reference model (28 layers, k=16) with dynamic k-NN and fixed k-NN. The inference time with fixed k-NN is 45.63ms. Computing the dynamic k-NN increases the inference time by 150.88ms. It is possible to reduce computation by updating the k-NN less frequently (e.g. computing the dynamic k-NN every 3 layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison with DGCNN over All Classes</head><p>To showcase the consistent improvement of our framework over the baseline DGCNN <ref type="bibr" target="#b41">[42]</ref>, we reproduce the results of DGCNN 1 in <ref type="table" target="#tab_5">Table 4</ref> and find our method outperforms DGCNN in all classes.  <ref type="figure" target="#fig_0">Figure 10</ref>. Qualitative Results for S3DIS Semantic Segmentation. We show the benefit of a wider and deeper network even with only half the number of nearest neighbors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Training Deep GCNs. (left) We show the training loss for GCNs with 7, 14, 28, and 56 layers, with and without residual connections. We note how adding more layers without residual connections translates to substantially higher loss. (right) In contrast, training GCNs with residual connections results in consistent stability across all depths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Proposed GCN architecture for point cloud semantic segmentation. (left) Our framework consists of three blocks: a GCN Backbone Block (feature transformation of input point cloud), a Fusion Block (global feature generation and fusion), and an MLP Prediction Block (point-wise label prediction). (right) We study three types of GCN Backbone Block (PlainGCN, ResGCN and DenseGCN) and use two kinds of layer connection (vertex-wise addition used in ResGCN or vertex-wise concatenation used in DenseGCN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Dilated Convolution in GCNs. Visualization of dilated convolution on a structured graph arranged in a grid (e.g. 2D image) and on a general structured graph. (top) 2D convolution with kernel size 3 and dilation rate 1, 2, 4 (left to right). (bottom) Dynamic graph convolution with dilation rate 1, 2, 4 (left to right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 (</head><label>1</label><figDesc>Depth) shows that increasing the number of layers improves network performance, but only if residual graph connections and dilated graph convolutions are used, as in Table 1 (Connections). Effect of network width. Results in Table 1 (Width)show that increasing the number of filters leads to a similar increase in performance as increasing the number of layers. In general, a higher network capacity enables learning nuances necessary for succeeding in corner cases.</figDesc><table><row><cell>Ablation</cell><cell>Model</cell><cell>mIoU</cell><cell>âmIoU</cell><cell>dynamic</cell><cell>connection</cell><cell>dilation</cell><cell>stochastic</cell><cell># NNs</cell><cell># filters</cell><cell># layers</cell></row><row><cell>Reference</cell><cell>ResGCN-28</cell><cell>52.49</cell><cell>0.00</cell><cell></cell><cell>â</cell><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>28</cell></row><row><cell></cell><cell></cell><cell>51.98</cell><cell>-0.51</cell><cell></cell><cell>â</cell><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>28</cell></row><row><cell>Dilation</cell><cell></cell><cell>49.64</cell><cell>-2.85</cell><cell></cell><cell>â</cell><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>28</cell></row><row><cell></cell><cell>PlainGCN-28</cell><cell>40.31</cell><cell>-12.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>28</cell></row><row><cell>Fixed k-NN</cell><cell></cell><cell>48.38 43.43</cell><cell>-4.11 -9.06</cell><cell></cell><cell>â</cell><cell></cell><cell></cell><cell>16 16</cell><cell>64 64</cell><cell>28 28</cell></row><row><cell></cell><cell>DenseGCN-28</cell><cell>51.27</cell><cell>-1.22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>32</cell><cell>28</cell></row><row><cell></cell><cell></cell><cell>40.47</cell><cell>-12.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>28</cell></row><row><cell>Connections</cell><cell></cell><cell>38.79</cell><cell>-13.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>64</cell><cell>56</cell></row><row><cell></cell><cell></cell><cell>49.23</cell><cell>-3.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>14</cell></row><row><cell></cell><cell></cell><cell>47.92</cell><cell>-4.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>7</cell></row><row><cell>Neighbors</cell><cell></cell><cell>49.98 49.22</cell><cell>-2.51 -3.27</cell><cell></cell><cell>â â</cell><cell></cell><cell></cell><cell>8 4</cell><cell>64 64</cell><cell>28 28</cell></row><row><cell></cell><cell>ResGCN-56</cell><cell>53.64</cell><cell>1.15</cell><cell></cell><cell>â</cell><cell></cell><cell></cell><cell>8</cell><cell>64</cell><cell>56</cell></row><row><cell>Depth</cell><cell>ResGCN-14</cell><cell>49.90</cell><cell>-2.59</cell><cell></cell><cell>â</cell><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>14</cell></row><row><cell></cell><cell>ResGCN-7</cell><cell>48.95</cell><cell>-3.53</cell><cell></cell><cell>â</cell><cell></cell><cell></cell><cell>16</cell><cell>64</cell><cell>7</cell></row><row><cell></cell><cell>ResGCN-28W</cell><cell>53.78</cell><cell>1.29</cell><cell></cell><cell>â</cell><cell></cell><cell></cell><cell>8</cell><cell>128</cell><cell>28</cell></row><row><cell>Width</cell><cell></cell><cell>49.18 48.80</cell><cell>-3.31 -3.69</cell><cell></cell><cell>â â</cell><cell></cell><cell></cell><cell>32 16</cell><cell>32 32</cell><cell>28 28</cell></row><row><cell></cell><cell></cell><cell>45.62</cell><cell>-6.87</cell><cell></cell><cell>â</cell><cell></cell><cell></cell><cell>16</cell><cell>16</cell><cell>28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>table chair</head><label>chair</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">sofa bookcase board clutter</cell></row><row><cell>PointNet [27]</cell><cell>78.5</cell><cell>47.6</cell><cell>88.0</cell><cell cols="3">88.7 69.3 42.4</cell><cell>23.1</cell><cell>47.5</cell><cell cols="3">51.6 54.1 42.0</cell><cell>9.6</cell><cell>38.2</cell><cell>29.4</cell><cell>35.2</cell></row><row><cell>MS+CU [8]</cell><cell>79.2</cell><cell>47.8</cell><cell>88.6</cell><cell cols="3">95.8 67.3 36.9</cell><cell>24.9</cell><cell>48.6</cell><cell cols="4">52.3 51.9 45.1 10.6</cell><cell>36.8</cell><cell>24.7</cell><cell>37.5</cell></row><row><cell>G+RCU [8]</cell><cell>81.1</cell><cell>49.7</cell><cell>90.3</cell><cell cols="3">92.1 67.9 44.7</cell><cell>24.2</cell><cell>52.3</cell><cell cols="3">51.2 58.1 47.4</cell><cell>6.9</cell><cell>39.0</cell><cell>30.0</cell><cell>41.9</cell></row><row><cell>PointNet++ [29]</cell><cell>-</cell><cell>53.2</cell><cell>90.2</cell><cell cols="3">91.7 73.1 42.7</cell><cell>21.2</cell><cell>49.7</cell><cell cols="4">42.3 62.7 59.0 19.6</cell><cell>45.8</cell><cell>48.2</cell><cell>45.6</cell></row><row><cell>3DRNN+CF [49]</cell><cell>86.9</cell><cell>56.3</cell><cell>92.9</cell><cell cols="3">93.8 73.1 42.5</cell><cell>25.9</cell><cell>47.6</cell><cell cols="4">59.2 60.4 66.7 24.8</cell><cell>57.0</cell><cell>36.7</cell><cell>51.6</cell></row><row><cell>DGCNN [42]</cell><cell>84.1</cell><cell>56.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ResGCN-28 (Ours) 85.9</cell><cell>60.0</cell><cell>93.1</cell><cell cols="3">95.3 78.2 33.9</cell><cell>37.4</cell><cell>56.1</cell><cell cols="4">68.2 64.9 61.0 34.6</cell><cell>51.5</cell><cell>51.1</cell><cell>54.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, 8, 9, 10 show qualitative results for the ablation study presented in the paper. NNs 35.00 37.50 40.00 42.50 45.00 47.50 50.00 52.50 55.00</figDesc><table><row><cell>reference</cell></row><row><cell>w/o stochastic</cell></row><row><cell>w/o dilation</cell></row><row><cell>w/o residual</cell></row><row><cell>1/2x NNs</cell></row><row><cell>1/4x NNs</cell></row><row><cell>1/2x layers</cell></row><row><cell>1/4x layers</cell></row><row><cell>1/2x filters</cell></row><row><cell>1/4x filters</cell></row><row><cell>2x layers, 1/2x NNs</cell></row><row><cell>2x filters, 1/2x</cell></row><row><cell>mIoU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparison of ResGCN-28 with DGCNN. Average perclass results across all areas for our reference network with 28 layers, residual graph connections and dilated graph convolutions compared to DGCNN baseline. ResGCN-28 outperforms DGCNN across all the classes. Metric shown is IoU.</figDesc><table><row><cell>Class</cell><cell cols="2">DGCNN [42] ResGCN-28 (Ours)</cell></row><row><cell>ceiling</cell><cell>92.7</cell><cell>93.1</cell></row><row><cell>floor</cell><cell>93.6</cell><cell>95.3</cell></row><row><cell>wall</cell><cell>77.5</cell><cell>78.2</cell></row><row><cell>beam</cell><cell>32.0</cell><cell>33.9</cell></row><row><cell>column</cell><cell>36.3</cell><cell>37.4</cell></row><row><cell>window</cell><cell>52.5</cell><cell>56.1</cell></row><row><cell>door</cell><cell>63.7</cell><cell>68.2</cell></row><row><cell>table</cell><cell>61.1</cell><cell>64.9</cell></row><row><cell>chair</cell><cell>60.2</cell><cell>61.0</cell></row><row><cell>sofa</cell><cell>20.5</cell><cell>34.6</cell></row><row><cell>bookcase</cell><cell>47.7</cell><cell>51.5</cell></row><row><cell>board</cell><cell>42.7</cell><cell>51.1</cell></row><row><cell>clutter</cell><cell>51.5</cell><cell>54.4</cell></row><row><cell>mIOU</cell><cell>56.3</cell><cell>60.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The results over all classes were not provided in the original DGCNN paper</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 8. Qualitative Results for S3DIS Semantic Segmentation. We show the importance of network depth (number of layers).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 9. Qualitative Results for S3DIS Semantic Segmentation. We show the importance of network width (number of filters per layer).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. The authors thank Adel Bibi and Guocheng Qian for their help with the project. This work was supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research through the Visual Computing Center (VCC) funding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DOR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>NieÃner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring spatial context for 3d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, 3DRMS Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Snapnet-r: Consistent 3d multi-view semantic labeling for robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="669" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wavelets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structuralrnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lstmcf: Unifying context modeling and fusion with lstms for rgbd scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="541" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Column networks for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>NieÃner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semi-supervised user geolocation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08049</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The discrete wavelet transform: wedding the a trous and mallat algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Shensa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2464" to="2482" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning localized generative models for 3d point clouds via graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fracastoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>VeliÄkoviÄ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<title level="m">Representation learning on graphs with jumping knowledge networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph rcnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3d recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="415" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
