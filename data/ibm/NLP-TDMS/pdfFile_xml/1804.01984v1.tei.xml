<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Look into Person: Joint Body Parsing &amp; Pose Estimation Network and A New Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">Look into Person: Joint Body Parsing &amp; Pose Estimation Network and A New Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human Parsing</term>
					<term>Pose Estimation</term>
					<term>Context Modeling</term>
					<term>Convolutional Neural Networks !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human parsing and pose estimation have recently received considerable interest due to their substantial application potentials. However, the existing datasets have limited numbers of images and annotations and lack a variety of human appearances and coverage of challenging cases in unconstrained environments. In this paper, we introduce a new benchmark named "Look into Person (LIP)" that provides a significant advancement in terms of scalability, diversity, and difficulty, which are crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels and 16 body joints, which are captured from a broad range of viewpoints, occlusions, and background complexities. Using these rich annotations, we perform detailed analyses of the leading human parsing and pose estimation approaches, thereby obtaining insights into the successes and failures of these methods. To further explore and take advantage of the semantic correlation of these two tasks, we propose a novel joint human parsing and pose estimation network to explore efficient context modeling, which can simultaneously predict parsing and pose with extremely high quality. Furthermore, we simplify the network to solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into the parsing results without resorting to extra supervision. The datasets, code and models are available at http://www.sysu-hcp.net/lip/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C OMPREHENSIVE human visual understanding of scenarios in the wild, which is regarded as one of the most fundamental problems in computer vision, could have a crucial impact in many higher-level application domains, such as person re-identification <ref type="bibr" target="#b50">[51]</ref>, video surveillance <ref type="bibr" target="#b41">[42]</ref>, human behavior analysis <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref> and automatic product recommendation <ref type="bibr" target="#b20">[21]</ref>. Human parsing (also named semantic part segmentation) aims to segment a human image into multiple parts with fine-grained semantics (e.g., body parts and clothing) and provides a more detailed understanding of image contents, whereas human pose estimation focuses on determining the precise locations of important body joints. Human parsing and pose estimation are two critical and correlated tasks in analyzing images of humans by providing both pixel-wise understanding and high-level joint structures.</p><p>Recently, convolutional neural networks (CNNs) have achieved exciting success in human parsing <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> and pose estimation <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Nevertheless, as demonstrated in many other problems, such as object detection <ref type="bibr" target="#b23">[24]</ref> and semantic segmentation <ref type="bibr" target="#b51">[52]</ref>, the performance of such CNN-based approaches heavily relies on the availability of annotated images for training. To train a human parsing or pose network with potential practical value in realworld applications, it is highly desired to have a largescale dataset that is composed of representative instances with varied clothing appearances, strong articulation, partial (self-)occlusions, truncation at image borders, diverse  <ref type="bibr" target="#b7">[8]</ref>, where the left arm is incorrectly labeled as the right arm. (c) Our parsing results successfully incorporate the structure information to generate reasonable outputs.</p><p>viewpoints and background clutters. Although training sets exist for special scenarios, such as fashion pictures <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b46">[47]</ref> and people in constrained situations (e.g., upright) <ref type="bibr" target="#b8">[9]</ref>, these datasets are limited in their coverage and scalability, as shown in <ref type="figure">Fig. 2</ref>. The largest public human parsing dataset <ref type="bibr" target="#b25">[26]</ref> to date only contains 17,000 fashion images, while others only include thousands of images. The MPII Human Pose dataset <ref type="bibr" target="#b0">[1]</ref> is the most popular benchmark for evaluating articulated human pose estimation methods, and this dataset includes approximately 25K images that contain over 40K people with annotated body joints. However, all these datasets only focus on addressing different aspects of human analysis by defining discrepant annotation sets. There are no available unified datasets with both human parsing and pose annotations for holistic human understanding, until our work fills this gap.</p><p>Furthermore, to the best of our knowledge, no attempt has been made to establish a standard representative benchmark that aims to cover a wide range of challenges for the two human-centric tasks. The existing datasets do not provide an evaluation server with a secret test set to avoid potential dataset over-fitting, which hinders further devel-(c) LIP <ref type="figure">Fig. 2</ref>. Annotation examples for our "Look into Person (LIP)" dataset and existing datasets. (a) The images in the MPII dataset <ref type="bibr" target="#b0">[1]</ref> (left) and LSP dataset <ref type="bibr" target="#b18">[19]</ref> (right) with only body joint annotations. (b) The images in the ATR dataset <ref type="bibr" target="#b25">[26]</ref> (left) are fixed in size and only contain stand-up person instances in the outdoors. The images in the PASCAL-Person-Part dataset <ref type="bibr" target="#b8">[9]</ref> (right) also have lower scalability and only contain 6 coarse labels. (c) The images in our LIP dataset have high appearance variability and complexity, and they are annotated with both human parsing and pose labels. opment on this topic. With the new benchmark named "Look into Person (LIP)", we provide a public server for automatically reporting evaluation results. Our benchmark significantly advances the state-of-the-art in terms of appearance variability and complexity, and it includes 50,462 human images with pixel-wise annotations of 19 semantic parts and 16 body joints.</p><p>The recent progress in human parsing <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b46">[47]</ref> has been achieved by improving the feature representations using CNNs and recurrent neural networks. To capture rich structure information, these approaches combine CNNs and graphical models (e.g., conditional random fields (CRFs)), similar to the general object segmentation approaches <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b51">[52]</ref>. However, when evaluated on the new LIP dataset, the results of some existing methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[31]</ref> are unsatisfactory. Without imposing human body structure priors, these general approaches based on bottom-up appearance information occasionally tend to produce unreasonable results (e.g., right arm connected with left shoulder), as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Human body structural information has previously been well explored in human pose estimation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b48">[49]</ref>, where dense joint annotations are provided. However, since human pars-ing requires more extensive and detailed predictions than pose estimation, it is difficult to directly utilize joint-based pose estimation models in pixel-wise predictions to incorporate the complex structure constraints. We demonstrate that the human joint structure can facilitate the pixel-wise parsing prediction by incorporating higher-level semantic correlations between human parts.</p><p>For pose estimation, increasing research efforts <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref> have been devoted to learning the relationships between human body parts and joints. Some studies <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref> explored encoding part constraints and contextual information for guiding the network to focus on informative regions (human parts) to predict more precise locations of the body joints, which achieved state-of-the-art performance. Conversely, some pose-guided human parsing methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b45">[46]</ref> also sufficiently utilized the peculiarity and relationships of these two correlated tasks. However, previous works generally solve these two problems separately or alternatively.</p><p>In this work, we aim to seamlessly integrate human parsing and pose estimation under a unified framework. We use a shared deep residual network for feature extraction, after which there are two distinct small networks to encode and predict the contextual information and results. Then, a simple yet efficient refinement network tailored for both parsing and pose prediction is proposed to explore efficient context modeling, which makes human parsing and pose estimation mutually beneficial. In our unified framework, we propose a scheme to incorporate multi-scale feature combinations and iterative location refinement together, which are often posed as two different coarse-to-fine strategies that are widely investigated for human parsing and pose estimation separately. To highlight the merit of unifying the two highly correlated and complementary tasks within an end-to-end framework, we name our framework the joint human parsing and pose estimation network (JPPNet). However, note that annotating both pixel-wise labeling maps and pose joints is unrealized in previous humancentric datasets. Therefore, in this work, we also design a simplified network suited to general human parsing datasets and networks with no need for pose annotations. To explicitly enforce the produced parsing results to be semantically consistent with the human pose and joint structures, we propose a novel structure-sensitive learning approach for human parsing. In addition to using the traditional pixelwise part annotations as the supervision, we introduce a structure-sensitive loss to evaluate the quality of the predicted parsing results from a joint structure perspective. This means that a satisfactory parsing result should be able to preserve a reasonable joint structure (e.g., the spatial layouts of human parts). We generate approximated human joints directly from the parsing annotations and use them as the supervision signal for the structure-sensitive loss. This self-supervised structure-sensitive network is a simplified verson of our JPPNet, denoted as SS-JPPNet, which is appropriate for the general human parsing datasets without pose annotations Our contributions are summarized in the following three aspects. 1) We propose a new large-scale benchmark and an evaluation server to advance the human parsing and pose estimation research, in which 50,462 images with pixel-wise annotations on 19 semantic part labels and 16 body joints are provided. 2) By experimenting on our benchmark, we present detailed analyses of the existing human parsing and pose estimation approaches to obtain some insights into the successes and failures of these approaches and thoroughly explore the relationship between the two humancentric tasks. 3) We propose a novel joint human parsing and pose estimation network, which incorporates the multiscale feature connections and iterative location refinement in an end-to-end framework to investigate efficient context modeling and then enable parsing and pose tasks that are mutually beneficial to each other. This unified framework achieves state-of-the-art performance for both human parsing and pose estimation tasks. The simplified network for human parsing task with self-supervised structure-sensitive learning also significantly surpasses the previous methods on both the existing PASCAL-Person-Part dataset <ref type="bibr" target="#b8">[9]</ref> and our new LIP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Human parsing and pose datasets: The commonly used publicly available datasets for human parsing and pose are summarized in <ref type="table" target="#tab_0">Table 1</ref>. For human parsing, the previous datasets were labeled with a limited number of images or categories. The largest dataset <ref type="bibr" target="#b25">[26]</ref> to date only contains 17,000 fashion images with mostly upright fashion models. These small datasets are unsuitable for training models with complex appearance representations and multiple components <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b38">[39]</ref>, which could perform better. For human pose, the LSP dataset <ref type="bibr" target="#b18">[19]</ref> only contains sports people, and it fails to cover real-life challenges. The MPII dataset <ref type="bibr" target="#b0">[1]</ref> has more images and a wider coverage of activities that cover real-life challenges, such as truncation, occlusions, and variability of imaging conditions. However, this dataset only provides 2D pose annotations. J-HMDB <ref type="bibr" target="#b17">[18]</ref> provides densely annotated image sequences and a larger number of videos for 21 human actions. Although the puppet mask and human pose are annotated in the all 31838 frames, detailed part segmentations are not labeled.</p><p>Our proposed LIP benchmark dataset is the first effort that focuses on the two human-centric tasks. Containing 50,462 images annotated with 20 parsing categories and 16 body joints, our LIP dataset is the largest and most comprehensive human parsing and pose dataset to date. Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval <ref type="bibr" target="#b29">[30]</ref> and fashion modeling <ref type="bibr" target="#b37">[38]</ref>, whereas our LIP dataset only focuses on human parsing and pose estimation.</p><p>Human parsing approaches: Recently, many research efforts have been devoted to human parsing <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. For example, Liang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a novel Co-CNN architecture that integrates multiple levels of image contexts into a unified network. In addition to human parsing, there has also been increasing research interest in the part segmentation of other objects, such as animals or   cars <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b42">[43]</ref>. To capture the rich structure information based on the advanced CNN architecture, common solutions include the combination of CNNs and CRFs <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b51">[52]</ref> and adopting multi-scale feature representations <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Chen et al. <ref type="bibr" target="#b7">[8]</ref> proposed an attention mechanism that learns to weight the multi-scale features at each pixel location.</p><p>Pose estimation approaches: Articulated human poses are generally modeled using a combination of a unary term and pictorial structures <ref type="bibr" target="#b1">[2]</ref> or graph model, e.g., mixture of body parts <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b49">[50]</ref>. With the introduction of Deep-Pose <ref type="bibr" target="#b39">[40]</ref>, which formulates the pose estimation problem as a regression problem using a standard convolutional architecture, research on human pose estimation began to shift from classic approaches to deep networks. For example, Wei et al. <ref type="bibr" target="#b43">[44]</ref> incorporated the inference of the spatial correlations among body parts within ConvNets. Newell et al. proposed a stacked hourglass network <ref type="bibr" target="#b32">[33]</ref> using a repeated pooling down and upsampling process to learn the spatial distribution. Some previous works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b45">[46]</ref> explored human pose information to guide human parsing by generating "pose-guided" part segment proposals. Additionally, some works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref> generated attention maps of the body part to guide pose estimation. To further utilize the advantages of parsing and pose and their relationships, our focus is a joint human parsing and pose estimation network, which can simultaneously predict parsing and pose with extremely high quality. Additionally, to leverage the human joint structure more effortlessly and efficiently, we simplify the network and propose a self-supervised structure-sensitive learning approach.</p><p>The rest of this paper is organized as follows. We present the analysis of existing human parsing and pose estimation datasets and then introduce our new LIP benchmark in Section 3. In Section 4, we present the empirical study of current methods based on our LIP benchmark and discuss the limitations of these methods. Then, to address the challenges raised by LIP, we propose a unified framework for simultaneous human parsing and pose estimation in Section 5. At last, more detailed comparisons between our approach and state-of-the-art methods are exhibited in Section 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LOOK INTO PERSON BENCHMARK</head><p>In this section, we introduce our new "Look into Person (LIP)" dataset, which is a new large-scale dataset that focuses on semantic understanding of human bodies and that has several appealing properties. First, with 50,462 annotated images, LIP is an order of magnitude larger and more challenging than previous similar datasets <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Second, LIP is annotated with elaborate pixel-wise annotations with 19 semantic human part labels and one background label for human parsing and 16 body joint locations for pose estimation. Third, the images collected from the real-world scenarios contain people appearing with challenging poses and viewpoints, heavy occlusions, various appearances and in a wide range of resolutions. Furthermore, the background of the images in the LIP dataset is also more complex and diverse than that in previous datasets. Some examples are shown in <ref type="figure">Fig. 2</ref>. With the LIP dataset, we propose a new benchmark suite for human parsing and pose estimation together with a standard evaluation server where the test set will be kept secret to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Annotation</head><p>The images in the LIP dataset are cropped person instances from Microsoft COCO <ref type="bibr" target="#b27">[28]</ref> training and validation sets. We defined 19 human parts or clothes labels for annotation, which are hat, hair, sunglasses, upper clothes, dress, coat, socks, pants, gloves, scarf, skirt, jumpsuit, face, right arm, left arm, right leg, left leg, right shoe, and left shoe, as well as a background label. Similarly, we provide rich annotations for human poses, where the positions and visibility of 16 main body joints are annotated. Following <ref type="bibr" target="#b0">[1]</ref>, we annotate joints in a "person-centric" manner, which means that the left/right joints refer to the left/right limbs of the person. At test time, this requires pose estimation with both a correct localization of the limbs of a person along with the correct match to the left/right limb.</p><p>We implemented an annotation tool and generate multiscale superpixels of images to speed up the annotation. More than 100 students are trained well to accomplish annotation work which lasts for five months. We supervise the whole annotation process and check the results periodically to control the annotation quality. Finally, we conduct a second-round check for each annotated image and selecte 50,000 usable and well-annotated images strictly and carefully from over 60,000 submitted images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Splits</head><p>In total, there are 50,462 images in the LIP dataset, including 19,081 full-body images, 13,672 upper-body images, 403 lower-body images, 3,386 head-missing images, 2,778 backview images and 21,028 images with occlusions. We split the images into separate training, validation and test sets. Following random selection, we arrive at a unique split that consists of 30,462 training and 10,000 validation images with publicly available annotations, as well as 10,000 test images with annotations withheld for benchmarking purposes.</p><p>Furthermore, to stimulate the multiple-human parsing research, we collect the images with multiple person instances in the LIP dataset to establish the first standard and comprehensive benchmark for multiple-human parsing and pose estimation. Our LIP multiple-human parsing and pose dataset contains 4,192 training, 497 validation and 458 test images, in which there are 5,147 multiple-person images in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Statistics</head><p>In this section, we analyze the images and categories in the LIP dataset in detail. In general, face, arms, and legs are the most remarkable parts of a human body. However, human parsing aims to analyze every detailed region of a person, including different body parts and different categories of clothes. We therefore define 6 body parts and 13 clothes categories. Among these 6 body parts, we divide arms and legs into the left side and right side for a more precise analysis, which also increases the difficulty of the task. For clothes classes, we not only have common clothes such as upper clothes, pants, and shoes but also have infrequent categories, such as skirts and jumpsuits. Furthermore, smallscale accessories such as sunglasses, gloves, and socks are also taken into account. The numbers of images for each semantic part label are presented in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>In contrast to other human image datasets, the images in the LIP dataset contain diverse human appearances, viewpoints, and occlusions. Additionally, more than half of the images suffer from occlusions of different degrees. An occlusion is considered to have occurred if any of the semantic parts or body joints appear in the image but are occluded or invisible. In more challenging cases, the images contain person instances in a back view, which gives rise to more ambiguity in the left and right spatial layouts. The numbers of images of different appearances (i.e., occlusion, full body, upper body, head missing, back view and lower body) are summarized in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL STUDY OF STATE-OF-THE-ART</head><p>In this section, we analyze the performance of leading human parsing or semantic object segmentation and pose estimation approaches on our benchmark. We take advantage of our rich annotations and conduct a detailed analysis of the various factors that influence the results, such as appearance, foreshortening, and viewpoints. The goal of  this analysis is to evaluate the robustness of the current approaches in various challenges for human parsing and pose estimation and identify the existing limitations to stimulate further research advances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Human Parsing</head><p>In our analysis, we consider fully convolutional networks <ref type="bibr" target="#b30">[31]</ref> (FCN-8s), a deep convolutional encoder-decoder architecture <ref type="bibr" target="#b2">[3]</ref> (SegNet), deep convolutional nets with atrous convolution and multi-scale <ref type="bibr" target="#b6">[7]</ref> (DeepLab (VGG-16), DeepLab (ResNet-101)) and an attention mechanism <ref type="bibr" target="#b7">[8]</ref> (Attention), which all have achieved excellent performance on semantic image segmentations in different ways and have completely available codes. For a fair comparison, we train each method on our LIP training set until the validation performance saturates, and we perform evaluation on the validation set and the test set. For the DeepLab methods, we remove the post-processing, dense CRFs. Following <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b44">[45]</ref>, we use the standard intersection over union (IoU) criterion and pixel-wise accuracy for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Overall Performance Evaluation</head><p>We begin our analysis by reporting the overall human parsing performance of each approach, and the results are summarized in <ref type="table" target="#tab_2">Table 2</ref> and  <ref type="bibr" target="#b14">[15]</ref>. This result suggests that detailed human parsing due to the small parts and diverse fine-grained labels is more challenging than object-level segmentation, which deserves more attention in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Performance Evaluation under Different Challenges</head><p>We further analyze the performance of each approach with respect to the following five challenging factors: occlusion, full body, upper body, head missing and back view (see <ref type="figure" target="#fig_4">Fig. 5</ref>). We evaluate the above five approaches on the LIP validation set, which contains 4,277 images with occlusions, 5,452 full-body images, 793 upper-body images, 112 headmissing images and 661 back-view images. As expected, the performance varies when the approaches are affected by different factors. Back view is clearly the most challenging case. For example, the IoU of Attention <ref type="bibr" target="#b7">[8]</ref> decreases from 42.92% to 33.50%. The second most influential factor is the appearance of the head. The scores of all approaches are considerably lower on head-missing images than the average score on the entire set. The performance also greatly suffers from occlusions. The results of full-body images are the closest to the average level. By contrast, upper body is relatively the easiest case, where fewer semantic parts are present and the part regions are generally larger. From these results, we can conclude that the head (or face) is an important cue for the existing human parsing approaches. The probability of ambiguous results will increase if the head part disappears in the images or in the back view. Moreover, the parts or clothes on the lower body are more difficult than those on the upper body because of the existence of small labels, such as shoes or socks. In this case, the body joint structure can play an effective role in guiding human parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Per-class Performance Evaluation</head><p>To discuss and analyze each of the 20 labels in the LIP dataset in more detail, we further report the performance of per-class IoU on the LIP validation set, as shown in <ref type="table" target="#tab_5">Table 4</ref>. We observe that the results with respect to labels with larger regions such as face, upper clothes, coats, and pants are considerably better than those on the small-region labels, such as sunglasses, scarf, and skirt. DeepLab (ResNet-101) <ref type="bibr" target="#b6">[7]</ref> and Attention <ref type="bibr" target="#b7">[8]</ref> perform better on small labels thanks to the utilization of deep networks and multi-scale features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Visualization Comparison</head><p>The qualitative comparisons of the five approaches on our LIP validation set are visualized in <ref type="figure" target="#fig_5">Fig. 6</ref>. We present example parsing results of the five challenging factor scenarios.</p><p>For the upper-body image (a) with slight occlusion, the five approaches perform well with few errors. For the back-view image (b), all five methods mistakenly label the right arm as the left arm. The worst results occur for the head-missing image (c). SegNet <ref type="bibr" target="#b2">[3]</ref> and FCN-8s <ref type="bibr" target="#b30">[31]</ref> fail to recognize arms and legs, whereas DeepLab (VGG-16) <ref type="bibr" target="#b6">[7]</ref> and Attention <ref type="bibr" target="#b7">[8]</ref> have errors on the right and left arms, legs and shoes. Furthermore, severe occlusion (d) also greatly affects the performance. Moreover, as observed from (c) and (d), some of the results are unreasonable from the perspective of human body configuration (e.g., two shoes on one foot) because the existing approaches lack the consideration of body structures. In summary, human parsing is more difficult than general object segmentation. Particularly, human body structures should receive more attention to strengthen the ability to predict human parts and clothes with more reasonable configurations. Consequently, we consider connecting human parsing results and body joint structure to determine a better approach for human parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pose Estimation</head><p>Similarly, we consider three state-of-the-art methods for pose estimation, including a sequential convolutional architecture <ref type="bibr" target="#b43">[44]</ref> (CPM) and a repeated bottom-up, top-down network <ref type="bibr" target="#b32">[33]</ref> (Hourglass). ResNet-101 with atrous convolutions <ref type="bibr" target="#b6">[7]</ref> is also taken into account, for which we reserve the entire network and change the output layer to generate pose heatmaps. These approaches achieve top performance on the MPII <ref type="bibr" target="#b0">[1]</ref> and LSP <ref type="bibr" target="#b18">[19]</ref> datasets and can be trained on our LIP dataset using publicly available codes. Again, we train each method on our LIP training set and evaluate on the validation set and the test set. Following MPII <ref type="bibr" target="#b0">[1]</ref>, the evaluation metric that we used is the percentage of correct keypoints with respect to head (PCKh). PCKh considers a candidate keypoint to be localized correctly if it falls within the matching threshold which is 50% of the head segment length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Overall Performance Evaluation</head><p>We again begin our analysis by reporting the overall pose estimation performance of each approach, and the results are summarized in <ref type="table" target="#tab_6">Table 5 and Table 6</ref>. On the LIP validation set, Hourglass <ref type="bibr" target="#b32">[33]</ref> achieves the best result of 77.5% total PCKh, benefiting from their multiple hourglass modules and intermediate supervision. With a sequential composition of convolutional architectures to learn implicit spatial models, CPM <ref type="bibr" target="#b43">[44]</ref> also obtains comparable performance. Interestingly, the achieved performance is substantially lower than the current best results on other pose estimation benchmarks, such as MPII <ref type="bibr" target="#b0">[1]</ref>. This wide gap reflects the higher complexity and variability of our LIP dataset and the significant development potential of pose estimation research. Similar performance on the LIP test set is again consistent with our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Performance Evaluation under Different Challenges</head><p>We further analyze the performance of each approach with respect to the four challenging factors (see <ref type="figure" target="#fig_6">Fig. 7</ref>). We leave head-missing images out because the PCKh metric depends on the head size of the person. In general and as expected,    the performance decreases as the complexity increases. However, there are interesting differences. The back-view factor clearly influences the performance of all approaches the most, as the scores of all approaches decrease nearly 10% compared to the average score on the entire set. The second most influential factor is occlusion. For example, the PCKh of Hourglass <ref type="bibr" target="#b32">[33]</ref> is 4.60% lower. These two factors are related to the visibility and orientation of heads in the images, which indicates that similar to human parsing, the existing pose estimation methods strongly depend on the contextual information of the head or face. In this case, exploring and leveraging the correlation and complementation of human parsing and pose estimation is advantageous for reducing this type of dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Visualization Comparison</head><p>The qualitative comparisons of the pose estimation results on our LIP validation set are visualized in <ref type="figure" target="#fig_7">Fig. 8</ref>. We select some challenging images with unusual appearances, truncations and occlusions to analyze the failure cases and obtain some inspiration. First, for the persons standing or sitting sideways, the existing approaches typically failed to predict their occluded body joints, such as the right arm in Col 1, the right leg in Col 2, and the right leg in Col 7. Second, for the persons in back view or head missing, the left and right arms (legs) of the persons are always improperly located, as with those in Cols 3 and 4. Moreover, for some images with strange appearances where some limbs of the person are very close (Cols 5 and 6), ambiguous and irrational results will be generated by these methods. In particular, the performance of the gray images (Col 8) is also far from being satisfactory. Learning from these failure cases, we believe that pose estimation should fall back on more instructional contextual information, such as the guidance from human parts with reasonable configurations. Summarizing the analysis of human parsing and pose estimation, it is clear that despite the strong connection of these two human-centric tasks, the intrinsic consistency between them will benefit each other. Consequently, we present a unified framework for simultaneous human parsing and pose estimation to explore this intrinsic correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview</head><p>In this section, we first summarize some insights about the limitations of existing approaches and then illustrate our joint human parsing and pose estimation network in detail. From the above detailed analysis, we obtain some insights into the human parsing and pose estimation tasks. 1) A major limitation of the existing human parsing approaches is the lack of consideration of human body configuration, which is mainly investigated in the human pose estimation problem. Meanwhile, the part maps produced by the detection network contain multiple contextual information and structural part relationships, which can effectively guide the regression network to predict the locations of the body joints. Human parsing and pose estimation aim to label each image with different granularities, that is, pixel-wise semantic labeling versus joint-wise structure prediction. The pixel-wise labeling can address more detailed information, whereas joint-wise structure provides more high-level structure, which means that the two tasks are complementary. 2) As learned from the existing approaches, the coarse-to-fine scheme is widely used in both parsing and pose networks to improve accuracy. For coarse-to-fine, there are two different definitions for parsing and pose tasks. For parsing or segmentation, it means using the multi-scale segmentation or attention-to-zoom scheme <ref type="bibr" target="#b7">[8]</ref> for more precise pixelwise classification. Conversely, for the pose task, it indicates iterative displacement refinement, which is widely used in pose estimation <ref type="bibr" target="#b43">[44]</ref>. It is reasonable to incorporate these two distinct coarse-to-fine schemes together in a unified network to further improve the parsing and pose results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Joint Human Parsing and Pose Estimation Network</head><p>To utilize the coherent representation of human parsing and pose to promote each task, we propose a joint human parsing and pose estimation network, which also incorporates two distinct coarse-to-fine schemes, i.e., multi-scale features and iterative refinement, together. The framework architecture is illustrated in <ref type="figure">Fig. 9</ref> and the detailed configuration is presented in <ref type="table" target="#tab_8">Table 7</ref>. We denote our joint human parsing and pose estimation network as JPPNet.</p><p>In general, the basic network of the parsing framework is a deep residual network <ref type="bibr" target="#b16">[17]</ref>, while the pose framework prefers a stacked hourglass network <ref type="bibr" target="#b32">[33]</ref>. In our joint framework, we use a shared residual network to extract human image features, which is more efficient and concise. Then, we have two distinct networks to generate parsing and pose features and results. They are followed by a refinement network, which takes features and results as input to produce more accurate segmentation and joint localization.</p><p>Feature extraction. We employ convolution with upsampled filters, or "atrous convolution" <ref type="bibr" target="#b6">[7]</ref>, as a powerful tool to repurpose ResNet-101 <ref type="bibr" target="#b16">[17]</ref> in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within DCNNs. It also effectively enlarges the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. The first four stages of ResNet-101 (i.e., Res-1 to Res-4) are shared in our framework. The deeper convolutional layers are different to learn for distinct tasks.</p><p>Parsing and pose subnet. We use ResNet-101 with atrous convolution as the basic parsing subnet, which contains atrous spatial pyramid pooling (ASPP) as the output layer to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields of view, thus capturing objects and image context at multiple scales. Furthermore, to generate the context used in the refinement stage, there are two convolutions following Res-5. For the pose subnet, we simply add several 3 × 3 convolutional layers to the fourth stage (Res-4) of ResNet-101 to generate pose features and heatmaps.</p><p>Refinement network. We also design a simple but efficient refinement network, which is able to iteratively refine both parsing and pose results. We reintegrate the intermediate parsing and pose predictions back into the feature space by mapping them to a larger number of channels with an additional 1 × 1 convolution. Then, we have four convolutional layers with an incremental kernel size that varies from 3 to 9 to capture a sufficient local context and to increase the receptive field size, which is crucial for learning long-term relationships. Next is another 1×1 convolution to generate the features for the next refinement stage. To refine pose, we concatenate the remapped pose and parsing results  and the pose features from the last stage. For parsing, we concatenate the two remapped results and parsing features and use ASPP again to generate parsing predictions. The entire joint network with refinement can be trained endto-end, feeding the output of the former stage into the next. Following other pose estimation methods that have demonstrated strong performance with multiple iterative stages and intermediate supervision <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b43">[44]</ref>, we apply a loss upon the prediction of intermediate maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Self-supervised Structure-sensitive Learning</head><p>The joint human parsing and pose estimation network (JPPNet) leverages both pixel-wise supervision from human part annotations and high-level structural guidance from joint annotations. However, in some cases, e.g., in previous human parsing datasets, the joint annotations may not be available. In this section, we show that high-level human structure cues can still help the human parsing task even without explicit supervision from manual annotations. We simplify our JPPNet and propose a novel self-supervised structure-sensitive learning for human parsing, which introduces a self-supervised structure-sensitive loss to evaluate the quality of the predicted parsing results from a joint structure perspective, as illustrated in <ref type="figure" target="#fig_0">Fig. 10</ref>. Specifically, in addition to using the traditional pixelwise annotations as the supervision, we generate the approximated human joints directly from the parsing annotations, which can also guide human parsing training. For the purpose of explicitly enforcing the produced parsing results to be semantically consistent with the human joint structures, we treat the joint structure loss as a weight of segmentation loss, which becomes our structure-sensitive loss.</p><p>Self-supervised Structure-sensitive Loss: Generally, for the human parsing task, no other extensive information is provided except the pixel-wise annotations. This situation means that rather than using augmentative information, we have to find a structure-sensitive supervision from the parsing annotations. Because the human parsing results are semantic parts with pixel-level labels, we attempt to explore pose information contained in human parsing results. We Parsing maps 2 <ref type="figure">Fig. 9</ref>. The proposed JPPNet learns to incorporate the image-level context, body joint context, body part context and refined context into a unified network, which consists of shared feature extraction, pixel-wise label prediction, keypoint heatmap prediction and iterative refinement. Given an input image, we use ResNet-101 to extract the shared feature maps. Then, a part module and a joint module are appended to capture the part context and keypoint context while simultaneously generating parsing score maps and pose heatmaps. Finally, a refinement network is performed based on the predicted maps and generated context to produce better results. To clearly observe the correlation between the parsing and pose maps, we combine pose heatmaps and parsing score maps into one map separately. For better viewing of all figures in this paper, please see the original zoomed-in color pdf file.  <ref type="figure" target="#fig_0">Fig. 11</ref>. Some examples of self-supervised human joints generated from our parsing results for different bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parsing result</head><p>pants and skirt are merged for lower body. The remaining regions can also be obtained by the corresponding labels. Some examples of generated human joints for different humans are shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. Following <ref type="bibr" target="#b35">[36]</ref>, for each parsing result and corresponding ground truth, we compute the center points of regions to obtain joints represented as heatmaps for training more smoothly. Then, we use the Euclidean metric to evaluate the quality of the generated joint structures, which also reflects the structure consistency between the predicted parsing results and the ground truth. Finally, the pixel-wise segmentation loss is weighted by the joint structure loss, which becomes our structure-sensitive loss. Consequently, the overall human parsing networks become self-supervised with the structure-sensitive loss. Formally, given an image I, we define a list of joint configurations C P I = {c p i |i ∈ [1, N ]}, where c p i is the heatmap of the i-th joint computed according to the parsing result map. Similarly, C GT I = {c gt i |i ∈ [1, N ]}, which is obtained from the corresponding parsing ground truth. Here, N is a variable decided by the human bodies in the input images, and it is equal to 9 for a full-body image. For the joints missed in the image, we simply replace the heatmaps with maps filled with zeros. The joint structure loss is the Euclidean (L2) loss, which is calculated as follows:</p><formula xml:id="formula_0">L Joint = 1 2N N i=1 c p i − c gt i 2 2<label>(1)</label></formula><p>The final structure-sensitive loss, denoted as L structure , is the combination of the joint structure loss and the parsing segmentation loss, and it is calculated as follows:</p><formula xml:id="formula_1">L Structure = L Joint · L Parsing<label>(2)</label></formula><p>where L Parsing is the pixel-wise softmax loss calculated based on the parsing annotations. We name this learning strategy "self-supervised" because the above structure-sensitive loss can be generated from existing parsing results without any extra information. Our self-supervised structure-sensitive JPPNet (SS-JPPNet) thus has excellent adaptability and extensibility, which can be injected into any advanced network to help incorporate rich high-level knowledge about human joints from a global perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Settings</head><p>Network architecture: We utilize the publicly available model DeepLab (ResNet-101) <ref type="bibr" target="#b6">[7]</ref> as the basic architecture of our JPPNet, which employs atrous convolution, multiscale inputs with max-pooling to merge the results from all scales, and atrous spatial pyramid pooling. For SS-JPPNet, our basic network is Attention <ref type="bibr" target="#b7">[8]</ref> due to its leading accuracy and competitive efficiency.</p><p>Training: To train JPPNet, the input image is scaled to 384 × 384. We first train ResNet-101 on the human parsing task for 30 epochs using the pre-trained models and networks settings from <ref type="bibr" target="#b6">[7]</ref>. Then, we train the joint framework end-to-end for another 30 epochs. We apply data augmentation, including randomly scaling the input images (from 0.75 to 1.25), randomly cropping and randomly leftright flipping during training.</p><p>When training SS-JPPNet, we use the pre-trained models and network settings provided by DeepLab <ref type="bibr" target="#b6">[7]</ref>. The scale of the input images is fixed as 321 × 321 for training networks based on Attention <ref type="bibr" target="#b7">[8]</ref>. Two training steps are employed to train the networks. First, we train the basic network on our LIP dataset for 30 epochs. Then, we perform the "self-supervised" strategy to fine-tune our model with the structure-sensitive loss. We fine-tune the networks for approximately 20 epochs. We use both human parsing and pose annotations to train JPPNet and only parsing labels for SS-JPPNet.</p><p>Inference: To stabilize the predictions, we perform inference on multi-scale inputs (with scales = 0.75, 0.5, 1.25) and also left-right flipped images. In particular, we compute  as the final result the average probabilities from each scale and flipped images, which is the same for predicting both parsing and pose. The difference is that we utilize predictions of all stages for parsing, but for pose, we only use the results of the last stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results and Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Human Parsing</head><p>We compare our proposed approach with the strong baselines on the LIP dataset, and we further evaluate SS-JPPNet on another public human parsing dataset. LIP dataset: We report the results and the comparisons with five state-of-the-art methods on the LIP validation set and test set in <ref type="table" target="#tab_2">Table 2 and Table 3</ref>. On the validation set, the proposed JPPNet framework improves the best performance from 44.80% to 51.37%. The simplified architecture can also provide a substantial enhancement in average IoU: 3.09% better than DeepLab (VGG-16) [7] and 1.81% better than Attention <ref type="bibr" target="#b7">[8]</ref>. On the test set, the JPPNet also considerably outperforms the other baselines. This superior performance achieved by our methods demonstrates the effectiveness of our joint parsing and pose networks, which incorporate the body joint structure into the pixel-wise prediction.</p><p>In <ref type="figure" target="#fig_4">Fig. 5</ref>, we show the results with respect to the different challenging factors on our LIP validation set. With our unified framework that models the contextual information of body parts and joints, the performance of all kinds of types is improved, which demonstrates that human joint structure is conducive for the human parsing task.</p><p>We further report per-class IoU on the LIP validation set to verify the detailed effectiveness of our approach, as presented in <ref type="table" target="#tab_5">Table 4</ref>. With the consideration of human body joints, we achieved the best performance on almost all the classes. As observed from the reported results, the proposed JPPNet significantly improves the performance of the labels such as arms, legs, and shoes, which demonstrates its ability to refine the ambiguity of left and right. Furthermore, the labels covering small regions such as socks, and gloves are better predicted with higher IoUs. This improvement also demonstrates the effectiveness of the unified framework, particularly for small labels.  For a better understanding of our LIP dataset, we train all methods on LIP and evaluate them on ATR <ref type="bibr" target="#b25">[26]</ref>, as reported in <ref type="table" target="#tab_9">Table 8</ref> (left). As ATR contains 18 categories while LIP has 20, we test the models on the 16 common categories (hat, hair, sunglasses, upper clothes, dress, pants, scarf, skirt, face, right arm, left arm, right leg, left leg, right shoe, left shoe, and background). In general, the performance on ATR is better than those on LIP because the LIP dataset contains instances with more diverse poses, appearance patterns, occlusions and resolution issues, which is more consistent with real-world situations.</p><p>Following the MSCOCO dataset <ref type="bibr" target="#b27">[28]</ref>, we have conducted an empirical analysis on different object sizes, i.e., small (area &lt; 153 2 ), medium (153 2 ≤ area &lt; 321 2 ) and large (area ≥ 321 2 ). The results of the five baselines and the proposed methods are reported in <ref type="table" target="#tab_9">Table 8</ref> (right). As shown, our methods show substantially superior performance for different sizes of objects, thus further demonstrating the advantage of incorporating the human body structure into the parsing model. PASCAL-Person-Part dataset <ref type="bibr" target="#b8">[9]</ref>. The public PASCAL-Person-Part dataset with 1,716 images for training and 1,817 for testing focuses on the human part segmentation annotated by <ref type="bibr" target="#b8">[9]</ref>. Following <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b44">[45]</ref>, the annotations are merged to be six person part classes and one background class, which are head, torso, upper / lower arms and upper / lower legs. We train and evaluate all methods using the training and testing data in PASCAL-Person-Part dataset <ref type="bibr" target="#b8">[9]</ref>. <ref type="table" target="#tab_10">Table 9</ref> shows the performance of our model and comparisons with four state-of-the-art methods on the standard IoU criterion. Our SS-JPPNet can significantly outperform the four baselines. For example, our best model achieves 59.36% IoU, which is 7.58% better than DeepLab-LargeFOV [7] and 2.97% better than Attention <ref type="bibr" target="#b7">[8]</ref>. This large improvement demonstrates that our self-supervised strategy is significantly beneficial for the human parsing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Pose Estimation</head><p>LIP dataset: <ref type="table" target="#tab_6">Table 5</ref> and <ref type="table" target="#tab_7">Table 6</ref> report the comparison of the PCKh performance of our JPPNet and previous state-of-theart at a normalized distance of 0.5. On the LIP test set, our method achieves state-of-the-art PCKh scores of 82.7%. In particular, for the most challenging body parts, e.g., hip and ankle, our method achieves 5.0% and 5.5% improvements compared with the closest competitor, respectively. Similar improvements also occur on the validation set.</p><p>We present the results with respect to different challenging factors on our LIP validation set in <ref type="figure" target="#fig_6">Fig. 7</ref>. As expected, with our unified architecture, the results of all different appearances become better, thus demonstrating the positive effects of the human parsing to pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII Human Pose dataset [1]:</head><p>To be more convincing, we also perform evaluations on the MPII dataset. The MPII dataset contains approximately 25,000 images, where each person is annotated with 16 joints. The images are extracted from YouTube videos, where the contents are everyday human activities. There are 18079 images in the training set, including 11431 single person images. We evaluate the models trained on our LIP training set and test on these 11431 single person images from MPII, as presented in <ref type="table" target="#tab_0">Table 10</ref>. The distance between our approach and others provides evidence of the higher generalization ability of our proposed JPPNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Studies of JPPNet</head><p>We further evaluate the effectiveness of our two coarse-tofine schemes of JPPNet, including the multi-scale features and iterative refinement. "Joint" denotes the JPPNet without multi-scale features ("MSC") or refinement networks. "S1" means one stage refinement and "S2" is noted for two stages. The human parsing and pose estimation results are shown in <ref type="table" target="#tab_0">Table 12 and Table 11</ref>. From the comparisons, we can learn that multi-scale features greatly improve for human parsing but slightly for pose estimation. However, pose estimation considerably benefits from iterative refinement, which is not quite helpful for human parsing, as two stage refinements will decrease the parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Qualitative Comparison</head><p>Human parsing: The qualitative comparisons of the parsing results on the LIP validation set are visualized in <ref type="figure" target="#fig_5">Fig. 6</ref>. As can be observed from these visual comparisons, our methods output more semantically meaningful and precise predictions than the other five methods despite the existence of large appearance and position variations. Taking (b) and (c) for example, our approaches can also successfully handle the confusing labels, such as left arm versus right arm and left leg versus right leg. These regions with similar appearances can be recognized and separated by the guidance from joint structure information. For the most difficult headmissing image (c), the left shoe, right shoe and legs are excellently corrected by our JPPNet approach. In general, by effectively exploiting human body joint structure, our approaches output more reasonable results for confusing labels on the human parsing task. Pose estimation: The qualitative comparisons of pose results on the LIP validation set are presented in <ref type="figure" target="#fig_7">Fig. 8</ref>. In Section 4.2.3, we summarize some challenging cases that cause considerable trouble for the previous pose estimation approaches. In contrast, by jointly modeling human parsing and pose estimation, our model can effectively avoid the cumbersome obstacles such sideways, occlusion or other erratic postures, thus leading to more promising and reasonably remarkable results.</p><p>Finally, we want to emphasize that our goal is to explore the intrinsic correlation between human parsing and pose estimation. For this purpose, we propose JPPNet, which is a unified model built upon two distinct coarse-to-fine schemes. Separating our framework into different components leads to inferior results, as demonstrated in <ref type="table" target="#tab_0">Table 12</ref> and <ref type="table" target="#tab_0">Table 11</ref>. Although we use more annotations than methods for individual tasks, the promising results of our framework verify that human parsing and pose estimation are essentially complementary; thus, performing the two tasks simultaneously will enhance the performance of each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we presented "Look into Person (LIP)", a largescale human parsing and pose estimation dataset and a carefully designed benchmark to spark progress in humancentric tasks. LIP contains 50,462 images, which are richly labeled with 19 semantic part labels and 16 body joints. It surpasses existing human parsing and pose estimation datasets in terms of scale and richness of annotations. Moreover, we proposed a joint human parsing and pose estimation network to explore the intrinsic connection of the two tasks. The extensive results clearly demonstrate the effectiveness of the proposed approaches. The datasets, code and models are available at http://www.sysu-hcp.net/lip/. Xiaodan Liang is currently an Associate Professor of Sun Yat-sen University. She was a postdoc researcher in Machine Learning Department at the Carnegie Mellon University, working with Prof. Eric Xing, since 2016 to 2018. She received her PhD degree from Sun Yat-sen University in 2016, advised by Liang Lin. She has published several cutting-edge projects on the human-related analysis including the human parsing, pedestrian detection and instance segmentation, 2D/3D human pose estimation and activity recognition.</p><p>Ke Gong received his BE degree and is currently working toward the ME degree in the School of Data and Computer Science, Sun Yat-sen University, China. His research interests include semantic segmentation and humancentric tasks, particularly human parsing and pose estimation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•Fig. 1 .</head><label>1</label><figDesc>X. Liang, K. Gong and L. Lin are with the School of Data and Computer Science, Sun Yat-sen University, China. • X. Shen is with Adobe Research. X. Liang and K. Gong contribute equally to this paper. Corresponding author: Liang Lin (E-mail: linliang@ieee.org) This example shows that human body structural information is helpful for human parsing. (a) The original image. (b) The parsing results by attention-to-scale</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The data distribution on 19 semantic part labels in the LIP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The numbers of images that show diverse visibilities in the LIP dataset, including occlusion, full body, upper body, lower body, head missing and back view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Human parsing performance comparison evaluated on the LIP validation set with different appearances, including occlusion, full body, upper body, head missing and back view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visualized comparison of human parsing results on the LIP validation set. (a) The upper-body images. (b) The back-view images. (c) The head-missing images. (d) The images with occlusion. (e) The full-body images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Pose estimation performance comparison evaluated on the LIP validation set with different appearances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Visualized comparison of pose estimation results with three state-of-the-art methods, including ResNet-101, CPM and Hourglass on the LIP validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Illustration of our SS-JPPNet for human parsing. An input image goes through parsing networks, including several convolutional layers, to generate the parsing results. The generated joints and ground truths of joints represented as heatmaps are obtained by computing the center points of corresponding regions in parsing maps, including head (H), upper body (U), lower body (L), right arm (RA), left arm (LA), right leg (RL), left leg (LL), right shoe (RS), and left shoe (LS). The structuresensitive loss is generated by weighting segmentation loss with joint structure loss. For a clear observation, we combine nine heatmaps into one map here. define 9 joints to construct a pose structure, which are the centers of the regions of head, upper body, lower body, left arm, right arm, left leg, right leg, left shoe and right shoe. The head regions are generated by merging the parsing labels of hat, hair, sunglasses and face. Similarly, upper clothes, coat and scarf are merged to be upper body, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Xiaohui</head><label></label><figDesc>Shen is a senior research scientist at Adobe Research, San Jose, CA. He received the PhD degree from the Department of Electrical Engineering and Computer Science at Northwestern University in 2013. Before that, he received the BS and MS degrees from the Department of Automation at Tsinghua University in China. He mainly focuses on the research topics in the area of Computer Vision, particularly semantic segmentation, depth prediction, object detection and recognition, image editing and deep learning-related problems.Liang Lin (M09, SM15) is the Executive R&amp;D Director of SenseTime Group Limited and a Full Professor of Sun Yat-sen University. He is the Excellent Young Scientist of the National Natural Science Foundation of China. He has authorized and co-authorized on more than 100 papers in top-tier academic journals and conferences. He was the recipient of Best Paper Runners-Up Award in ACM NPAR 2010, Google Faculty Award in 2012, Best Paper Diamond Award in IEEE ICME 2017, and Hong Kong Scholars Award in 2014. He is a Fellow of IET.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Overview of the publicly available datasets for human parsing and pose estimation. For each dataset, we report the number of images in the training, validation and test sets; parsing categories, including background; and body joints.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Total #Training #Validation</cell><cell>#Test</cell><cell cols="2">Parsing Categories Body Joints</cell></row><row><cell>Fashionista [48] (Parsing)</cell><cell>685</cell><cell>456</cell><cell>-</cell><cell>229</cell><cell>56</cell><cell>-</cell></row><row><cell>PASCAL-Person-Part [9] (Parsing)</cell><cell>3533</cell><cell>1,716</cell><cell>-</cell><cell>1,817</cell><cell>7</cell><cell>-</cell></row><row><cell>ATR [26] (Parsing)</cell><cell>17,700</cell><cell>16,000</cell><cell>700</cell><cell>1,000</cell><cell>18</cell><cell>-</cell></row><row><cell>LSP [19] (Pose)</cell><cell>2,000</cell><cell>1,000</cell><cell>-</cell><cell>1,000</cell><cell>-</cell><cell>14</cell></row><row><cell>MPII [1] (Pose)</cell><cell>24987</cell><cell>18079</cell><cell>-</cell><cell>6908</cell><cell>-</cell><cell>16</cell></row><row><cell>J-HMDB [18] (Pose)</cell><cell>31838</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2</cell><cell>13</cell></row><row><cell>LIP</cell><cell>50,462</cell><cell>30,462</cell><cell>10,000</cell><cell>10,000</cell><cell>20</cell><cell>16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Comparison of human parsing performance with five state-of-the-art methods on the LIP validation set.</figDesc><table><row><cell>Method</cell><cell>Overall accuracy</cell><cell>Mean accuracy</cell><cell>Mean IoU</cell></row><row><cell>SegNet [3]</cell><cell>69.04</cell><cell>24.00</cell><cell>18.17</cell></row><row><cell>FCN-8s [31]</cell><cell>76.06</cell><cell>36.75</cell><cell>28.29</cell></row><row><cell>DeepLab (VGG-16) [7]</cell><cell>82.66</cell><cell>51.64</cell><cell>41.64</cell></row><row><cell>Attention [8]</cell><cell>83.43</cell><cell>54.39</cell><cell>42.92</cell></row><row><cell>DeepLab (ResNet-101) [7]</cell><cell>84.09</cell><cell>55.62</cell><cell>44.80</cell></row><row><cell>JPPNet (with pose info)</cell><cell>86.39</cell><cell>62.32</cell><cell>51.37</cell></row><row><cell>SS-JPPNet</cell><cell>84.36</cell><cell>54.94</cell><cell>44.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Comparison of human parsing performance with five state-of-the-art methods on the LIP test set.</figDesc><table><row><cell>Method</cell><cell>Overall accuracy</cell><cell>Mean accuracy</cell><cell>Mean IoU</cell></row><row><cell>SegNet [3]</cell><cell>69.10</cell><cell>24.26</cell><cell>18.37</cell></row><row><cell>FCN-8s [31]</cell><cell>76.28</cell><cell>37.18</cell><cell>28.69</cell></row><row><cell>DeepLab (VGG-16) [7]</cell><cell>82.89</cell><cell>51.53</cell><cell>41.56</cell></row><row><cell>Attention [8]</cell><cell>83.56</cell><cell>54.28</cell><cell>42.97</cell></row><row><cell>DeepLab (ResNet-101) [7]</cell><cell>84.25</cell><cell>55.64</cell><cell>44.96</cell></row><row><cell>JPPNet (with pose info)</cell><cell>86.48</cell><cell>62.25</cell><cell>51.36</cell></row><row><cell>SS-JPPNet</cell><cell>84.53</cell><cell>54.81</cell><cell>44.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>On the LIP validation set, among the five approaches, DeepLab (ResNet-101)<ref type="bibr" target="#b6">[7]</ref> with the deepest networks achieves the best result of 44.80% mean IoU. Benefitting from the attention model that softly weights the multi-scale features, Attention<ref type="bibr" target="#b7">[8]</ref> also performs well with 42.92% mean IoU, whereas both FCN-8s [31] (28.29%) and SegNet<ref type="bibr" target="#b2">[3]</ref> (18.17%) perform significantly worse. Similar performance is observed on the LIP test set. The interesting outcome of this comparison is that the achieved performance is substantially lower than the current best results on other segmentation benchmarks, such as PASCAL VOC</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Performance comparison in terms of per-class IoU with five state-of-the-art methods on the LIP validation set. .00 52.38 15.30 24.23 13.82 13.17 9.26 6.47 70.62 18.17 FCN-8s [31] 39.79 58.96 5.32 3.08 49.08 12.36 26.82 15.66 49.41 6.48 0.00 2.16 62.65 29.78 36.63 28.12 26.05 17.76 17.70 78.02 28.29 DeepLab (VGG-16) [7] 57.94 66.11 28.50 18.40 60.94 23.17 47.03 34.51 64.00 22.38 14.29 18.74 69.70 49.44 51.66 37.49 34.60 28.22 22.41 83.25 41.64 Attention [8] 58.87 66.78 23.32 19.48 63.20 29.63 49.70 35.23 66.04 24.73 12.84 20.41 70.58 50.17 54.03 38.35 37.70 26.20 27.09 84.00 42.92 DeepLab (ResNet-101) [7] 59.76 66.22 28.76 JPPNet 59.75 67.25 28.95 21.57 65.30 29.49 51.92 38.52 68.02 24.48 14.92 24.32 71.01 52.64 55.79 40.23 38.80 28.08 29.03 84.56 44.73</figDesc><table><row><cell>SegNet [3]</cell><cell>26.60 44.01 0.01</cell><cell>0.00</cell><cell>34.46 0.00 15.97 3.59 33.56</cell><cell>0.01</cell><cell>0.00 0</cell></row></table><note>Method hat hair gloves sunglasses u-clothes dress coat socks pants jumpsuit scarf skirt face l-arm r-arm l-leg r-leg l-shoe r-shoe Bkg Avg23.91 64.95 33.68 52.86 37.67 68.05 26.15 17.44 25.23 70.00 50.42 53.89 39.36 38.27 26.95 28.36 84.09 44.80 JPPNet (with pose info) 63.55 70.20 36.16 23.48 68.15 31.42 55.65 44.56 72.19 28.39 18.76 25.14 73.36 61.97 63.88 58.21 57.99 44.02 44.09 86.26 51.37 SS-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Comparison of human pose estimation performance with state-of-the-art methods on the LIP test set.</figDesc><table><row><cell>Method</cell><cell cols="8">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell>ResNet-101 [7]</cell><cell>91.2</cell><cell>84.4</cell><cell>78.5</cell><cell>75.7</cell><cell>62.8</cell><cell>70.1</cell><cell>70.6</cell><cell>76.8</cell></row><row><cell>CPM [44]</cell><cell>90.8</cell><cell>85.1</cell><cell>78.7</cell><cell>76.1</cell><cell>64.7</cell><cell>70.5</cell><cell>71.2</cell><cell>77.3</cell></row><row><cell>Hourglass [33]</cell><cell>91.1</cell><cell>85.3</cell><cell>78.9</cell><cell>76.2</cell><cell>65.0</cell><cell>70.2</cell><cell>72.2</cell><cell>77.6</cell></row><row><cell>JPPNet (with parsing info)</cell><cell>93.3</cell><cell>89.3</cell><cell>84.4</cell><cell>82.5</cell><cell>70.0</cell><cell>78.3</cell><cell>77.7</cell><cell>82.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Comparison of human pose estimation performance with state-of-the-art methods on the LIP validation set.</figDesc><table><row><cell>Method</cell><cell cols="8">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell>ResNet-101 [7]</cell><cell>91.2</cell><cell>84.3</cell><cell>78.0</cell><cell>74.9</cell><cell>62.3</cell><cell>69.5</cell><cell>71.1</cell><cell>76.5</cell></row><row><cell>CPM [44]</cell><cell>91.1</cell><cell>85.1</cell><cell>78.7</cell><cell>75.0</cell><cell>63.7</cell><cell>69.6</cell><cell>71.7</cell><cell>77.0</cell></row><row><cell>Hourglass [33]</cell><cell>91.2</cell><cell>85.7</cell><cell>78.7</cell><cell>75.5</cell><cell>64.8</cell><cell>70.5</cell><cell>72.1</cell><cell>77.5</cell></row><row><cell>JPPNet (with parsing info)</cell><cell>93.2</cell><cell>89.3</cell><cell>84.6</cell><cell>82.2</cell><cell>69.9</cell><cell>78.0</cell><cell>77.3</cell><cell>82.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7</head><label>7</label><figDesc>The detailed configuration of our JPPNet .</figDesc><table><row><cell>Component</cell><cell>Name</cell><cell>Input</cell><cell cols="2">Kernel size Channels</cell></row><row><cell>Part module</cell><cell>conv-1 conv-2</cell><cell>Res-5 conv-1</cell><cell>3 × 3 3 × 3</cell><cell>512 256</cell></row><row><cell></cell><cell>conv-1</cell><cell>Res-4</cell><cell>3 × 3</cell><cell>512</cell></row><row><cell></cell><cell>conv-2</cell><cell>conv-1</cell><cell>3 × 3</cell><cell>512</cell></row><row><cell></cell><cell>conv-3</cell><cell>conv-2</cell><cell>3 × 3</cell><cell>256</cell></row><row><cell>Joint module</cell><cell>conv-4 conv-5</cell><cell>conv-3 conv-4</cell><cell>3 × 3 3 × 3</cell><cell>256 256</cell></row><row><cell></cell><cell>conv-6</cell><cell>conv-5</cell><cell>3 × 3</cell><cell>256</cell></row><row><cell></cell><cell>conv-7</cell><cell>conv-6</cell><cell>1 × 1</cell><cell>512</cell></row><row><cell></cell><cell>conv-8</cell><cell>conv-7</cell><cell>1 × 1</cell><cell>16</cell></row><row><cell></cell><cell>remap-1</cell><cell>pose maps</cell><cell>1 × 1</cell><cell>128</cell></row><row><cell></cell><cell>remap-2</cell><cell>parsing maps</cell><cell>1 × 1</cell><cell>128</cell></row><row><cell></cell><cell></cell><cell>remap-1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>concat</cell><cell>remap-2</cell><cell>-</cell><cell>512</cell></row><row><cell>Pose refinement</cell><cell></cell><cell>pose context</cell><cell></cell><cell></cell></row><row><cell></cell><cell>conv-1</cell><cell>concat</cell><cell>3 × 3</cell><cell>512</cell></row><row><cell></cell><cell>conv-2</cell><cell>conv-1</cell><cell>5 × 5</cell><cell>256</cell></row><row><cell></cell><cell>conv-3</cell><cell>conv-2</cell><cell>7 × 7</cell><cell>256</cell></row><row><cell></cell><cell>conv-4</cell><cell>conv-3</cell><cell>9 × 9</cell><cell>256</cell></row><row><cell></cell><cell>conv-5</cell><cell>conv-4</cell><cell>1 × 1</cell><cell>256</cell></row><row><cell></cell><cell>conv-6</cell><cell>conv-5</cell><cell>1 × 1</cell><cell>16</cell></row><row><cell></cell><cell>remap-1</cell><cell>pose maps</cell><cell>1 × 1</cell><cell>128</cell></row><row><cell></cell><cell>remap-2</cell><cell>parsing maps</cell><cell>1 × 1</cell><cell>128</cell></row><row><cell></cell><cell></cell><cell>remap-1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>concat</cell><cell>remap-2</cell><cell>-</cell><cell>512</cell></row><row><cell>Parsing refinement</cell><cell></cell><cell>parsing context</cell><cell></cell><cell></cell></row><row><cell></cell><cell>conv-1</cell><cell>concat</cell><cell>3 × 3</cell><cell>512</cell></row><row><cell></cell><cell>conv-2</cell><cell>conv-1</cell><cell>5 × 5</cell><cell>256</cell></row><row><cell></cell><cell>conv-3</cell><cell>conv-2</cell><cell>7 × 7</cell><cell>256</cell></row><row><cell></cell><cell>conv-4</cell><cell>conv-3</cell><cell>9 × 9</cell><cell>256</cell></row><row><cell></cell><cell>conv-5</cell><cell>conv-4</cell><cell>1 × 1</cell><cell>256</cell></row><row><cell></cell><cell>ASPP</cell><cell>conv-5</cell><cell>-</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Human parsing performance comparison in terms of mean IoU. Left: different test sets. Right: different sizes of objects.</figDesc><table><row><cell>Method</cell><cell>ATR</cell><cell>LIP</cell><cell>small</cell><cell>medium</cell><cell>large</cell></row><row><cell>SegNet [3]</cell><cell>15.79</cell><cell>21.79</cell><cell>16.53</cell><cell>18.58</cell><cell>18.18</cell></row><row><cell>FCN-8s [31]</cell><cell>34.44</cell><cell>32.28</cell><cell>22.37</cell><cell>29.41</cell><cell>28.09</cell></row><row><cell>DeepLab (VGG-16) [7]</cell><cell>48.64</cell><cell>43.97</cell><cell>28.77</cell><cell>40.74</cell><cell>43.02</cell></row><row><cell>Attention [8]</cell><cell>49.35</cell><cell>45.38</cell><cell>31.71</cell><cell>41.61</cell><cell>44.90</cell></row><row><cell>DeepLab (ResNet-101) [7]</cell><cell>53.28</cell><cell>46.99</cell><cell>31.70</cell><cell>43.14</cell><cell>47.62</cell></row><row><cell>JPPNet (with pose info)</cell><cell>54.45</cell><cell>53.99</cell><cell>44.56</cell><cell>50.52</cell><cell>52.58</cell></row><row><cell>SS-JPPNet</cell><cell>52.69</cell><cell>46.85</cell><cell>33.48</cell><cell>43.12</cell><cell>46.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="2">Comparison of human parsing performance with four state-of-the-art</cell></row><row><cell cols="2">methods on the PASCAL-Person-Part dataset [9].</cell></row><row><cell>Method</cell><cell>head torso u-arms l-arms u-legs l-legs Bkg Avg</cell></row><row><cell cols="2">DeepLab-LargeFOV [7] 78.09 54.02 37.29 36.85 33.73 29.61 92.85 51.78</cell></row><row><cell>HAZN [45]</cell><cell>80.79 59.11 43.05 42.76 38.99 34.46 93.59 56.11</cell></row><row><cell>Attention [8]</cell><cell>81.47 59.06 44.15 42.50 38.28 35.62 93.65 56.39</cell></row><row><cell>LG-LSTM [25]</cell><cell>82.72 60.99 45.40 47.76 42.33 37.96 88.63 57.97</cell></row><row><cell>SS-JPPNet</cell><cell>83.26 62.40 47.80 45.58 42.32 39.48 94.68 59.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 10</head><label>10</label><figDesc>Comparison of human pose estimation performance of the models trained on the LIP training set and evaluated on the MPII training set (11431 single person images).</figDesc><table><row><cell>Method</cell><cell cols="8">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell>ResNet-101 [7]</cell><cell>89.2</cell><cell>86.7</cell><cell>79.5</cell><cell>77.7</cell><cell>75.5</cell><cell>66.7</cell><cell>61.8</cell><cell>77.9</cell></row><row><cell>CPM [44]</cell><cell>86.6</cell><cell>83.6</cell><cell>75.8</cell><cell>72.1</cell><cell>70.9</cell><cell>62.0</cell><cell>59.1</cell><cell>74.0</cell></row><row><cell>Hourglass [33]</cell><cell>86.4</cell><cell>84.7</cell><cell>77.5</cell><cell>73.9</cell><cell>74.0</cell><cell>63.3</cell><cell>58.4</cell><cell>75.2</cell></row><row><cell>JPPNet (with parsing info)</cell><cell>90.4</cell><cell>91.7</cell><cell>86.4</cell><cell>84.0</cell><cell>82.5</cell><cell>76.5</cell><cell>71.3</cell><cell>84.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 11</head><label>11</label><figDesc>Human pose estimation comparison between different variants of the proposed JPPNet on the LIP test set using the PCKh metric.</figDesc><table><row><cell>Method</cell><cell cols="8">Head Shoulder Elbow Wrist Hip Knee Ankle Total</cell></row><row><cell>Joint</cell><cell>93.1</cell><cell>87.2</cell><cell>81.1</cell><cell>79.3</cell><cell>66.8</cell><cell>72.6</cell><cell>72.9</cell><cell>79.6</cell></row><row><cell>Joint + MSC</cell><cell>92.9</cell><cell>88.1</cell><cell>82.8</cell><cell>81.0</cell><cell>67.8</cell><cell>74.3</cell><cell>75.7</cell><cell>80.9</cell></row><row><cell>Joint + S1</cell><cell>93.5</cell><cell>88.8</cell><cell>83.6</cell><cell>81.6</cell><cell>70.2</cell><cell>76.4</cell><cell>76.8</cell><cell>82.1</cell></row><row><cell>Joint + MSC + S1</cell><cell>93.4</cell><cell>88.9</cell><cell>84.2</cell><cell>82.1</cell><cell>70.8</cell><cell>77.2</cell><cell>77.3</cell><cell>82.5</cell></row><row><cell>Joint + MSC + S2</cell><cell>93.3</cell><cell>89.3</cell><cell>84.4</cell><cell>82.5</cell><cell>70.0</cell><cell>78.3</cell><cell>77.7</cell><cell>82.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 12</head><label>12</label><figDesc>Human parsing comparison between different variants of the proposed JPPNet on the LIP test set.</figDesc><table><row><cell>Method</cell><cell>Overall accuracy</cell><cell>Mean accuracy</cell><cell>Mean IoU</cell></row><row><cell>Joint</cell><cell>86.10</cell><cell>59.64</cell><cell>49.48</cell></row><row><cell>Joint + MSC</cell><cell>86.18</cell><cell>61.40</cell><cell>50.83</cell></row><row><cell>Joint + S1</cell><cell>86.09</cell><cell>57.95</cell><cell>49.58</cell></row><row><cell>Joint + MSC + S1</cell><cell>86.48</cell><cell>62.25</cell><cell>51.36</cell></row><row><cell>Joint + MSC + S2</cell><cell>86.42</cell><cell>61.12</cell><cell>50.64</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards unified human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A deformable mixture parsing model with parselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>The pascal visual object classes challenge 2010 (voc2010) results</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Concepts not alone: Exploring pairwise relationships for zero-shot video activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1465" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Getting the look: clothing recognition and segmentation for automatic product suggestions in everyday photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM conference on International conference on multimedia retrieval</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Proposalfree network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep human parsing with active template regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards computational baby learning: A weakly-supervised approach for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic object parsing with local-global long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human parsing with contextualized convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active self-paced learning for cost-effective and progressive face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching-CNN Meets KNN: Quasi-Parametric Human Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parsing semantic parts of cars using graphical models and segment appearance consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2329" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attribute and-or grammar for joint parsing of human pose, parts and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A High Performance CRF Model for Clothes Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neuroaesthetics in fashion: Modeling the perception of fashionability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic part segmentation using compositional model combining shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deformable part model based multiple pedestrian detection for video surveillance in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Theory and Applications (VISAPP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint object and part segmentation using deep learned potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Huamn part segmentation with auto zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pose-guided human parsing by an and/or graph using pose-context features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Paper doll parsing: Retrieving similar styles to parse clothing items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
