<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Lightweight Lane Detection CNNs by Self Attention Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
							<email>liuchunxiao@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Lightweight Lane Detection CNNs by Self Attention Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, e.g., severe occlusion, ambiguous lanes, and poor lighting conditions. In this paper, we present a novel knowledge distillation approach, i.e., Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of 'free' supervision for further representation learning through performing topdown and layer-wise attention distillation within the network itself. SAD can be easily incorporated in any feedforward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet-18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 × fewer parameters and runs 10 × faster compared to the state-of-the-art SCNN [16], while still achieving compelling performance in all benchmarks. Our code is available at https://github. com/cardwing/Codes-for-Lane-Detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Lane detection <ref type="bibr">[1]</ref> plays a pivotal role in autonomous driving as lanes could serve as significant cues for constraining the maneuver of vehicles on roads. Detecting lanes in-the-wild is challenging due to poor lighting conditions, occlusions caused by other vehicles, irrelevant road markings, and the inherent long and thin property of lanes.</p><p>Contemporary algorithms <ref type="bibr">[5,</ref><ref type="bibr">8,</ref><ref type="bibr">14,</ref><ref type="bibr">16]</ref> typically adopt †: Corresponding author. a dense prediction formulation, i.e., treat lane detection as a semantic segmentation task, where each pixel in an image is assigned with a binary label to indicate whether it belongs to a lane or not. These methods heavily rely on the segmentation maps of lanes as the supervisory signals. Since lanes are long and thin, the number of annotated lane pixels is far fewer than the background pixels. Learning from such subtle and sparse annotations becomes a major challenge in training deep models for the task. A plausible way is to increase the width of lane annotations. However, it may degrade the detection performance.</p><p>Several schemes have been proposed to relieve the reliance of deep models on the sparse annotations, e.g., multitask learning (MTL) and message passing (MP). For example, Lee et al. <ref type="bibr">[14]</ref> exploit vanishing points to guide the training of deep models and Pan et al. <ref type="bibr">[16]</ref> incorporate spatial MP in their lane detection models. MTL can indeed provide additional supervisory signals but it requires additional efforts, usually with human intervention, to prepare the annotations, e.g., scene segmentation maps, vanishing points, or drivable areas. MP can help propagate the information between neurons to counter the effect of sparse supervision and better capture the scene context. However, it increases the inference time significantly due to the overhead of MP. For instance, applying MP in a layer of SCNN <ref type="bibr">[16]</ref> contributes 35% of its total feed-forward time.</p><p>In this work, we present a simple yet novel approach that allows a lane detection network to reinforce representation learning of itself without the need of additional labels and external supervisions. In addition, it does not increase the inference time of the base model. Our approach is named Self-Attention Distillation (SAD). As the name implies, SAD allows a network to exploit attention maps derived from its own layers as the distillation targets for its lower layers. Such an attention distillation mechanism is used to complement the usual segmentation-based supervised learning.</p><p>SAD is motivated by an interesting observation -when a lane detection network is trained to a reasonable level, attention maps derived from different layers would capture diverse and rich contextual information that hints the lane  <ref type="figure">Figure 1</ref>. Attention maps of the ENet <ref type="bibr">[17]</ref> before and after applying self attention distillation. Here, we extract the attention maps from the four stages/blocks following the design of ENet model. Note that self attention distillation is added in the 40 K episodes. locations and a rough outline of the scene, as shown in <ref type="figure">Fig. 1</ref> (before SAD at 40K episodes). By adding SAD to the learning of this half-trained model, i.e., having the preceding block to mimic the attention maps of a deeper block, e.g., block 3 mimic − −− → block 4 and block 2 mimic − −− → block 3, the network can learn to strengthen its representations, as shown in <ref type="figure">Fig. 1</ref> (after SAD): (1) the attention maps of lower layers are refined, with richer scene contexts captured by the visual attention, and (2) the better representation learned at lower layers in turn benefits the deeper layers. For instance, although block 4 does not learn from any distillation targets, its representation is reinforced, as evident from the much distinct attention at the lane locations. By contrast, without using SAD, the visual attentions of different layers of the same network hardly improve despite continual training up to 60K episodes.</p><p>SAD opens a new possibility of training accurate lane detection networks apart from deploying existing techniques such as multi-task learning and message passing, which can be expensive. It allows us to train small networks with excellent visual attention that is on par with very deep networks. In our experiments, we successfully demonstrate the effectiveness of SAD on a few popular lightweight models, e.g., ENet <ref type="bibr">[17]</ref>, ResNet-18 <ref type="bibr">[10]</ref> and ResNet-34 <ref type="bibr">[10]</ref>.</p><p>In summary, our contributions are three-fold: (1) We propose a novel attention distillation approach, i.e., SAD, to enhance the representation learning of CNN-based lane detection models. SAD is only used in the training phase and brings no computational cost during the deployment. Our work is the first attempt of using a network's own attention maps as the distillation targets. (2) We carefully and systematically investigate the inner mechanism of SAD, the consideration of choosing among different layer-wise mimicking paths, and the timepoint of introducing SAD to the training process for improved gains. (3) We verify the usefulness of SAD on boosting the performance of small lane detection networks. We further present several architectural reformulations to ENet <ref type="bibr">[17]</ref> for improved performance. Our lightweight model, ENet-SAD, achieves stateof-the-art lane detection performance on TuSimple <ref type="bibr">[18]</ref>, CULane <ref type="bibr">[16]</ref> and BDD100K <ref type="bibr">[23]</ref>. It can serve as a strong backbone to facilitate future research on lane detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Lane detection. Lane detection is conventionally handled via using specialized and hand-crafted features to obtain lane segments. These segments are further grouped to get the final results <ref type="bibr">[2,</ref><ref type="bibr">6]</ref>. These methods have many shortcomings, e.g., requiring complex feature selection process, being lack of robustness and only applicable to relatively easy driving scenarios.</p><p>Recently, deep learning has been employed to omit handcrafted features altogether and learn to extract features in an end-to-end manner <ref type="bibr">[14,</ref><ref type="bibr">16,</ref><ref type="bibr">8,</ref><ref type="bibr">5]</ref>. These approaches usually adopt the dense prediction formulation, i.e., treat lane detection as a semantic segmentation task, where each pixel in an image is assigned with a label to indicate whether it belongs to a lane or not. For example, He et al. Several schemes have been proposed to complement the lane-based supervision and to capture richer scene context, e.g., multi-task learning and message passing. For example, Zhang et al. <ref type="bibr">[25]</ref> establish a framework that accomplishes lane boundary segmentation and road area segmentation simultaneously. Geometric constraints that lane boundaries and lane areas constitute the road are also included to further enhance the final performance. Mohsen et al. <ref type="bibr">[8]</ref> take lane labels as extra inputs and integrate generative adversarial network (GAN) into the original framework so that the segmentation maps resemble labels more. Pan et al. <ref type="bibr">[16]</ref> perform sequential massage passing between the outputs of top-level layers to better exploit the structural information. While the aforementioned methods do bring additional gains to the performance, multi-task learning requires extra annotations and message passing is not efficient since it propagates information in a sequential way. On the contrary, the proposed SAD is free from the requirement of extra annotations and it does not increase the inference time. Knowledge and attention distillation. Knowledge distillation was originally proposed by <ref type="bibr">[11]</ref> to transfer the knowledge from large networks to small networks. Commonly in knowledge distillation, a small student network mimics the intermediate outputs of large teacher networks as well as the labels. In <ref type="bibr">[7,</ref><ref type="bibr">21]</ref> the student and teacher networks share the same capacity and mimicking is performed between pairs of layers with same dimensionality. Hou et al. <ref type="bibr">[12]</ref> also investigate knowledge distillation performed between heterogeneous networks. Recent studies <ref type="bibr">[24,</ref><ref type="bibr">19]</ref> have expanded knowledge distillation to attention distillation. For instance, Sergey et al. <ref type="bibr">[24]</ref> introduce two types of attention distillation, i.e., activation-based attention distillation and gradient-based attention distillation. In both kinds of distillation, a student network is trained through learning attention maps derived from a teacher network. The proposed SAD differs to <ref type="bibr">[24]</ref> in that our method does not need a teacher network. Distillation is conducted in a layerwise and top-down manner, in which attention knowledge is propagated layer by layer. This is new in the literature. It is noteworthy that our focus is to investigate the possibility of distilling layer-wise attention for self-learning. This differs from existing studies on using visual attention for weighting features <ref type="bibr">[4,</ref><ref type="bibr">13,</ref><ref type="bibr">24</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Lane detection is commonly formulated as a semantic segmentation task. More specifically, given an input image X, the objective is to assign a label l ij (l ij = 1, ..., N c ) to each pixel (i, j) of X, comprising the segmentation map s. Here, N c is the number of classes. The objective is to learn a mapping F: X → s. Recent studies use CNN as F for endto-end prediction. The task of lane existence prediction is also introduced to facilitate the evaluation process. We use b to represent the binary labels that indicate the existence of lanes. Then, the function becomes F: X → (s, b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self Attention Distillation</head><p>Apart from training our lane detection network with the aforementioned semantic segmentation and lane existence prediction losses, we aim to perform layer-wise and top-down attention distillation to enhance the representation learning process. The proposed SAD does not require any external supervision or additional labels since attention maps are derived from the network itself.</p><p>In general, attention maps can be divided into two categories, i.e., activation-based attention maps <ref type="bibr">[24]</ref> and gradient-based attention maps <ref type="bibr">[24]</ref>. The activation-based attention maps are obtained via processing the activation output of a specific layer while the gradient-based ones are obtained via using the layer's gradient output. In the experiment, we empirically find that activation-based attention distillation yields considerable performance gains while gradient-based attention distillation barely works. Hence, in the following sections we only discuss the activation-based attention distillation. Activation-based attention distillation. We use A m ∈ R Cm×Hm×Wm to denote the activation output of the m-th layer of the network, where C m , H m and W m denote the channel, height and width, respectively. Let M denote the number of layers in the network. The generation of the attention map is equivalent to finding a mapping function G: R Cm×Hm×Wm → R Hm×Wm . The absolute value of each element in this map represents the importance of this element on the final output. Therefore, this mapping function can be constructed via computing statistics of these values across the channel dimension. More specifically, the following three operations <ref type="bibr">[24]</ref> can serve as the mapping function:</p><formula xml:id="formula_0">G sum (A m ) = Cm i=1 |A mi |, G p sum (A m ) = Cm i=1 |A mi | p and G p max (A m ) = max i=1,Cm |A mi | p .</formula><p>Here, p &gt; 1 and A mi denotes the i-th slice of A m in the channel dimension.</p><p>The differences between these mapping functions are depicted in <ref type="figure" target="#fig_2">Fig. 2</ref>. Compared with G sum (A m ), G p sum (A m ) puts more weights to areas with higher activations. The larger the p is, the more focus is placed on these highly activated areas. Compared with G p max (A m ), G p sum (A m ) is less biased since it calculates weights across multiple neurons instead of selecting the maximum value of these neuron activations as the weight. In the experiment, we empirically find that using G 2 sum (.) as the mapping function yields the most performance gains. AT -GEN <ref type="figure">Figure 3</ref>. An instantiation of using SAD. E1 ∼ E4 comprise the encoder of ENet <ref type="bibr">[17]</ref>, D1 and D2 comprise the decoder of ENet. Following <ref type="bibr">[16]</ref>, we add a small network to predict the existence of lanes, denoted as P1. AT-GEN is the attention generator.</p><formula xml:id="formula_1">mimic 1 1 1 0 detected lanes E1 E2 D1 D2 E3 E4 E1 E2 E3 E4 D1 D2 P1 AT -GEN AT -GEN AT -GEN</formula><p>Adding SAD to training. The intuition behind SAD is that the attention maps of previous layers can distil useful contextual information from those of successive layers. Following <ref type="bibr">[24]</ref>, we also perform spatial softmax operation Φ(.) on G 2 sum (A m ). Bilinear upsampling B(.) is added before the softmax operation if the size of original attention maps is different from that of targets. However, different from Sergey et al. <ref type="bibr">[24]</ref> who perform attention distillation within two networks, the proposed self attention distillation is performed within the network itself.</p><p>Adding SAD to an existing network is straight-forward. It is possible to introduce SAD at different timepoint of the training, which could affect the convergence time. We will show an evaluation in the experiment section. Here we assume an ENet half-trained to 40K episodes. As shown in <ref type="figure">Fig. 3</ref>, we add an attention generator, abbreviated as AT-GEN, after each E 2 , E 3 , and E 4 encoder block of ENet. Formally, AT-GEN is represented by a function Ψ(.) = Φ(B(G 2 sum (.))). A successive layer-wise distillation loss is formulated as follows:</p><formula xml:id="formula_2">L distill (A m , A m+1 ) = M −1 m=1 L d (Ψ(A m ), Ψ(A m+1 )), (1)</formula><p>where L d is typically defined as a L 2 loss and Ψ(A m+1 ) is the target of the distillation loss. In the example shown in <ref type="figure">Fig. 3</ref>, we have the number of layers M = 4. Note that we do not assign different weights to different SAD paths, although this is possible. We found that this uniform scheme works well in our experiments.</p><p>The total loss is comprised of four terms:</p><formula xml:id="formula_3">L = L seg (s,ŝ) + αL IoU (s,ŝ) segmentation loss + βL exist (b,b) existence loss + γL distill (A m , A m+1 ) distillation loss .<label>(2)</label></formula><p>Here, the first two terms are segmentation losses that comprise of the standard cross entropy loss L seg and the IoU . We will evaluate this possibility in our experiments. Visualization of attention maps with and without SAD. We investigate the influence of SAD by studying the attention maps of different blocks in ENet with and without SAD. More results will be reported in Section 4. Both networks with and without SAD are trained up to 60K episodes. We visualize the attention maps of four existing blocks in ENet. As can be observed in <ref type="figure" target="#fig_5">Fig. 4</ref>, after adding SAD, the attention maps of ENet become more concentrated on task-relevant objects, e.g., lanes, vehicles and road curbs. This would in turn improve the lane detection accuracy, as we will show in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Lane Prediction</head><p>The output of the model is not post-processed for TuSimple and BDD100K except CULane. For CULane, in the inference stage, we feed the image into the ENet model. Then the multi-channel probability maps and the lane existence vector are obtained. Following <ref type="bibr">[16]</ref>, the final output is obtained as follows: First, we use a 9 × 9 kernel to smooth the probability maps. Then, for each lane whose  existence probability is larger than 0.5, we search the corresponding probability map every 20 rows for the position with the highest probability value. In the end, we use cubic splines to connect these positions to get the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture Design</head><p>The original ENet model is an encoder-decoder structure comprised of E 1 ∼ E 4 , D 1 and D 2 . Following <ref type="bibr">[16]</ref>, we add a small network P 1 to predict the existence of lanes. The encoder module is shared to save memory space. Apart from this modification, we also observed some useful techniques to modify ENet for achieving better performance in the lane detection task. Dilated convolution <ref type="bibr">[22]</ref> is added to replace the original convolution layers in the lane existence prediction branch to increase the receptive field of the network without increasing the number of parameters. In the original design, the resolution of feature maps of E 4 is only 36 × 100 for CULane. This leads to severe loss of information. Hence, we use feature concatenation to fuse the output of E 4 with that of E 3 so that the output of the encoder can benefit from information encoded in previous layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. <ref type="figure" target="#fig_6">Figure 5</ref> shows several video frames of three datasets that we use in our experiments. They are TuSimple <ref type="bibr">[18]</ref>, CULane <ref type="bibr">[16]</ref> and BDD100K <ref type="bibr">[23]</ref>. TuSimple and CULane are widely used in the literature. Many algorithms <ref type="bibr">[16,</ref><ref type="bibr">15,</ref><ref type="bibr">8]</ref> have been tested in TuSimple since it was the largest lane detection dataset before 2018. As to CULane, it contains many challenging driving scenarios like crowded road conditions or roads under poor lighting (see <ref type="figure" target="#fig_6">Fig. 5</ref>). BDD100K is originally designed for lane instance classification. However, since there are typically multiple lanes in an image and these lanes are usually very close to each other, using instance segmentation algorithms will yield inferior performance. Therefore, we choose to only detect lanes without differentiating lane instances for BDD100K. We discuss the details of transforming the original ground truths for our task in the following section on implementation details. <ref type="table" target="#tab_0">Table 1</ref> summarizes their details. Note that the last column of <ref type="table" target="#tab_0">Table 1</ref> shows that TuSimple and CULane have no more than 5 lanes in a video frame while BDD100K typically contains more than 8 lanes in a video frame. Besides, TuSimple is relatively easy while CULane and BDD100K are more challenging considering the total number of video frames and road types. Note that the original BDD100K dataset provides 100K video frames, in which 70K are used for training, 10K for validation and 20K for testing. However, since the ground-truth labels of the testing partition are not publicly available, we keep the training set unchanged but use the original validation set for testing. A new validation set is allocated separately from the training set, as shown in <ref type="table" target="#tab_0">Table 1</ref>.  <ref type="bibr">[16]</ref> 133, 235 88, 880 9, 675 34, 680 1640 × 590 urban, rural and highway × BDD100K <ref type="bibr">[23]</ref> 80, 000 60, 000 10, 000 10, 000 1280 × 720 urban, rural and highway √ Evaluation metrics. To facilitate comparisons against previous studies, we follow the literature and use the corresponding evaluation metrics for each particular dataset. 1) TuSimple. We use the official metric (accuracy) as the evaluation criterion. Besides, false positive (F P ) and false negative (F N ) are also reported. Accuracy is computed as <ref type="bibr">[18]</ref>: Accuracy = N pred Ngt , where N pred is the number of correctly predicted lane points and N gt is the number of ground-truth lane points.</p><p>2) CULane. Following <ref type="bibr">[16]</ref>, to judge whether a lane is correctly detected, we treat each lane as a line with 30 pixel width and compute the intersection-over-union (IoU) between labels and predictions. Predictions whose IoUs are larger than 0.5 are considered as true positives (TP). Then, we use F 1 measure as the evaluation metric, which is defined as: F 1 = 2×P recision×Recall P recision+Recall , where P recision = T P T P +F P and Recall = T P T P +F N . 3) BDD100K. Since there are typically more than 8 lanes in an image, we decide to use pixel accuracy and IoU of lanes to evaluate the performance of different models. Implementation details. Following <ref type="bibr">[16]</ref>, we resize the images of TuSimple and CULane to 368×640 and 288×800, respectively. As to BDD100K, we resize the image to 360×640 to save memory usage. The lanes of BDD100K are labelled by two lines. Training the networks using the provided labels is tricky. Therefore, based on these two lines, we calculate the center lines as new targets. We dilate ground-truth lanes of the training set of BDD100K as 8 pixels to provide denser targets while keeping these of testing set unchanged (2 pixels). We use SGD <ref type="bibr">[3]</ref> to train our models and the learning rate is set to 0.01. Batch size is set as 12 and the total number of training episodes is set as 1800 for TuSimple and 60K for CULane and BDD100K. The cross entropy loss of background pixels is multiplied by 0.4. Loss coefficients α, β, and γ are set as 0.1. Since we select lane pixel accuracy and IoU as the evaluation criterion for BDD100K dataset, we alter the original segmentation branch to output binary segmentation maps to facilitate the evaluation on BDD100K. The lane existence prediction branch is also removed for the BDD100K evaluation.</p><p>We empirically found that several practical techniques, i.e., data augmentation and IoU loss, can considerably enhance the performance of CNN-based lane detection models. As to data augmentation, we use random rotation, random cropping and horizontal flipping to process the input images. In our experiments, we apply the same segmentation losses and augmentation strategy to our method, <ref type="table">Table 2</ref>. Performance of different algorithms on TuSimple testing set. Here "R-18-SAD " denotes ResNet-18 + SAD and we use the same abbreviation in the following sections.</p><p>Algorithm Accuracy FP FN ResNet-18 <ref type="bibr">[10]</ref> 92.69% 0.0948 0.0822 ResNet-34 <ref type="bibr">[10]</ref> 92.84% 0.0918 0.0796 ENet <ref type="bibr">[17]</ref> 93.02% 0.0886 0.0734 LaneNet <ref type="bibr">[15]</ref> 96.38% 0.0780 0.0244 EL-GAN <ref type="bibr">[8]</ref> 96.39% 0.0412 0.0336 SCNN <ref type="bibr">[16]</ref> 96  <ref type="bibr">[15]</ref> and EL-GAN <ref type="bibr">[8]</ref> are not available, we use their results reported in their papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Tables 2-4 summarize the performance of our methods, i.e., ResNet-18-SAD, ResNet-34-SAD, and ENet-SAD against state-of-the-art algorithms on the testing set of TuSimple, CULane and BDD100K datasets. We also report the runtime and parameter count of different algorithm in <ref type="table" target="#tab_2">Table 3</ref> so that we can compare the performance with the complexity of the model taken into account. The runtime is recorded using a single GPU (GeForce GTX TITAN X) and the final value of runtime is obtained after averaging the runtime of 100 samples.</p><p>It is observed that ENet-SAD outperforms all baselines in BDD100K while achieving compelling performance in TuSimple and CULane. Considering that ENet-SAD has 20 × fewer parameters and runs 10 × faster compared with SCNN on CULane testing set, the performance strongly suggests the effectiveness of SAD. It is observed that ResNet-18-SAD and ResNet-34-SAD achieve slightly inferior performance to ENet-SAD despite their larger model capacity. The is because ResNet-18 and ResNet-34 only use spatial upsampling as the decoder while ENet has a specially designed decoder for the task. It is noteworthy that SAD also helps given a deeper model. Specifically, we apply SAD to ResNet-101, and find that it increases the F 1 -measure from 70.8 to 71.8 in CULane and the accuracy increases from 34.45% to 35.56% in BDD100K.</p><p>We show some qualitative results of our algorithm and some baselines in these three benchmarks. As can be seen in <ref type="figure">Fig. 6</ref>, ENet-SAD can detect lanes more precisely than ENet <ref type="bibr">[17]</ref> in TuSimple and CUlane. As can be seen in <ref type="figure">Fig.  7</ref>, the output probability maps of ENet-SAD are more compact and contain less noise compared with those of vanilla ENet and SCNN in poor lighting conditions. However, since many images in BDD100K contain more than 8 lanes and are collected in challenging scenarios like severe occlusion and poor lighting conditions, the performance of all algorithms is unsatisfactory and needs further improvement. In general, SAD can improve the visual attention as well as the detection performance in challenging conditions like crowded roads and poor light conditions. We also perform experiments that apply SAD and remove the effect of the P1 branch by blocking the gradient of the P1 branch from the main branch. Results show that ENet-SAD (without supervision from P1 branch) can still achieve 96.61% on TuSimple, 70.8 on CULane and 36.54% on BDD100K, which means the performance gains come mainly from SAD itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We investigate the effects of different factors, e.g., the mimicking path, on the final performance. Besides, we also perform extensive experiments to investigate the timepoint to introduce SAD in the training process. Distillation paths of SAD. We summarize the performance of performing SAD between different blocks of ENet in <ref type="table" target="#tab_4">Table 5</ref>. We have a few observations. (1) SAD works well  <ref type="figure">Figure 7</ref>. Performance of different algorithms on BDD100K testing set. We visualize the probability maps to better showcase the effect of adding self attention distillation. The brightness of the pixel indicates the probability of this pixel belonging to lanes. The number below each image denotes the pixel accuracy of lanes. Ground-truth lanes are drawn on the input image.</p><p>in the middle and high-level layers. (2) Adding SAD in low level layers will degrade the performance. The reason why SAD does not work in low-level layers is that these layers are originally designated to detect low-level details of the scene. Making them to mimic the attention maps of later layers will inevitably harm their ability of detecting local features since later layers encode more global infor- mation. Besides, we also find that mimicking the attention maps of the neighbouring layer successively brings more performance gains compared with mimicking those of nonadjacent layers (P 23 + P 34 outperforms P 24 + P 34 ). We conjecture that attention maps of neighbouring layers are closer from the semantic perspective compared with those of non-neighbouring layers (see <ref type="figure">Fig. 1</ref>).</p><p>Backward distillation. We also tested another distillation scheme that makes higher layers to mimic lower layers. It decreases the performance of ENet from 93.02% to 91.26% in TuSimple dataset. This is not surprising as low-level attention maps contain more details and are more noisy. Having higher-level layers to mimic lower layers will inevitably interfere the global information captured in higher layers, hampering the crucial clues for the lane detection task. SAD v.s. Deep Supervision. We also compare SAD with deep supervision <ref type="bibr">[20]</ref>. Here, deep supervision denotes the algorithm that uses the labels directly as supervision for each layer in the network. More specifically, we use 1x1 convolution and bilinear upsampling to obtain the prediction of intermediate layers and use the cross entropy loss to train the intermediate outputs of the model. We empirically find that adding deep supervision in blocks 2 to 4 obtains the most significant performance gains. As can be seen in <ref type="table" target="#tab_5">Table 6</ref>, SAD brings more performance gains than deep supervision in all three benchmarks. We attribute this to the following reasons. Firstly, compared with labels that are considered sparse and rigid, SAD provides softer attention targets that capture more contextual information that indicate the scene structure. Distilling information from attention maps of later layers helps previous layers to grasp the contextual signals. Secondly, a SAD path offers a feedback connection from deeper layers to shallower layers. The connection helps facilitate reciprocal learning between successive layers through attention distillation. When to add SAD. Recall that we assume a half-trained model before we add SAD into the training. Here, we investigate the different timepoints to add SAD. As can be seen in <ref type="figure" target="#fig_7">Fig. 8</ref>, although different timepoints of introducing SAD achieve almost the same performance in the end, the time to add SAD has an effect on the convergence speed of the networks. We attribute the phenomenon to the quality of the target attention maps produced by later layers. In earlier training stage, deeper layers have not been trained well and therefore the distillation targets produced by these  layers are of low quality. Introducing SAD at these earlier stages is not as fruitful. Conversely, adding SAD in later training stage would benefit the representation learning of the previous layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We have proposed a simple yet effective attention distillation approach, i.e., SAD, to improve the representation learning of CNN-based lane detection models. SAD is validated in various models (i.e., ENet, ResNet-18, ResNet-34, and ResNet-101) and achieves consistent performance gains in three popular benchmarks (i.e., TuSimple, CULane and BDD100K), demonstrating the effectiveness of SAD. The results show that SAD can generally improve the visual attention of different layers in various networks. It would be interesting to extend this idea to other tasks that demands fine-grained attention to details, such as image saliency detection and image matting. <ref type="table">Table 7</ref> summarizes the architecture of the lane existence prediction branch for ENet-SAD, ResNet-18-SAD and ResNet-34-SAD. As to ResNet-18-SAD and ResNet-34-SAD, we also use dilated convolution <ref type="bibr">[22]</ref> to replace the original convolution layers in the last two blocks for ResNet-18 <ref type="bibr">[10]</ref> and ResNet-34 <ref type="bibr">[10]</ref>. <ref type="table">Table 7</ref>. The architecture of the the lane existence prediction branch. Assuming the input is 3 × 288 × 800. Note that the output size is c × h × w before "Flatten", where c, h and w denote channel, height and width, respectively. The number in the bracket besides the layer name is the parameter for that layer. For instance, the four numbers besides dilated convolution denote kernel size, stride, padding and dilated rate, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Details of Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Name</head><p>Output Size </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lane Post-processing in CULane</head><p>For CULane, in the inference stage, we feed the image into the ENet model. Then the multi-channel probability maps and the lane existence vector are obtained. Following <ref type="bibr">[16]</ref>, the final output is obtained as follows: First, we use a 9 × 9 kernel to smooth the probability maps. Then, for each lane whose existence probability is larger than 0.5, we search the corresponding probability map every 20 rows for the position with the highest probability value. Finally, we use cubic splines to connect these positions to get the final output. The process improves the final lane prediction results as it removes noises in the probability maps. The process is depicted in <ref type="figure">Figure 9</ref>. Here, we differentiate different lane instances with different colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Qualitative Results in Lane Detection</head><p>Figures 10 and 11 depict the qualitative results of different algorithms on TuSimple <ref type="bibr">[18]</ref>, CULane <ref type="bibr">[16]</ref> and BDD100K <ref type="bibr">[23]</ref>. As can be seen in <ref type="figure">Fig. 10</ref>, ENet-SAD can detect lanes more precisely than ENet <ref type="bibr">[17]</ref> in TuSimple and CUlane. Besides, the detection of ENet-SAD is less affected by the irrelevant objects on the road compared with SCNN <ref type="bibr">[16]</ref>. As can be seen in <ref type="figure">Fig. 11</ref>, the output probabil-ity maps of ENet-SAD are more compact and contain less noise compared with those of SCNN in poor light conditions.   <ref type="figure">Figure 11</ref>. Performance of different algorithms on BDD100K testing set. We visualize the probability maps to better showcase the effect of adding self attention distillation. The brightness of the pixel indicates the probability of this pixel belonging to lanes. The number below each image denotes the pixel accuracy of lanes. Ground-truth lanes are drawn on the input image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[9] propose Dual-View CNN (DVCNN) to handle lane detection. The method takes front-view and top-view images as inputs. Another popular paradigm performs lane detection from the perspective of instance segmentation. For instance, Neven et al. [15] divide lane detection into two stages. Specifically, they first perform binary segmentation that differentiates lane pixels and background pixels. These lane pixels are then classified into different lane instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Attention maps of the block 4 of the ENet model using different mapping functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>loss L IoU . The IoU loss aims at increasing the intersectionover-union between the predicted lane pixels and groundtruth lane pixels. It is formulated as L IoU = 1 − Np Np+Ng−No , where N p is the number of predicted lane pixels, N g is the number of ground-truth lane pixels and N o is the number of lane pixels in the overlapped areas between predicted lane areas and ground-truth lane areas. L exist is the binary cross entropy loss.ŝ is the segmentation map produced by the network andb is the prediction of the existence of lanes. The parameters α, β, and γ balance the influence of segmentation losses, existence loss, and distillation loss on the final task.It is noteworthy that the SAD paths can be generalized to dense connections beyond the example shown here.For instance, we can add block 1 mimic − −− → block 3, block 1 mimic − −− → block 4, and block 2 mimic − −− → block 4 in addition to the current paths. In general, the number of possible SAD paths for a network with a depth of M layers is M (M −1)2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Attention maps of ENet with and without self attention distillation. Both networks with and without SAD are trained up to 60K episodes. SAD is applied to ENet at 40K training episodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Typical video frames of TuSimple, CULane and BDD100K datasets. Ground-truth lanes are marked in green color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Performance of adding self attention distillation on the ENet model at different training episodes on the CULane validation set. The number in the legend denotes the episode when self attention distillation is added. "Baseline" denotes the ENet model without self attention distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>The process of obtaining lanes from probability maps on the CULane dataset. From left to right: original image, probability map, extracted lane points and final lane prediction. Performance of different algorithms on (a) TuSimple and (b) CULane testing sets. The number below each image denotes the accuracy. Ground-truth lanes are drawn on the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Basic information of three lane detection datasets.</figDesc><table><row><cell>Name</cell><cell># Frame</cell><cell>Train</cell><cell>Validation</cell><cell>Test</cell><cell>Resolution</cell><cell>Road Type</cell><cell># Lane &gt; 5 ?</cell></row><row><cell>TuSimple [18]</cell><cell>6, 408</cell><cell>3, 268</cell><cell>358</cell><cell>2, 782</cell><cell>1280 × 720</cell><cell>highway</cell><cell>×</cell></row><row><cell>CULane</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance (F1-measure) of different algorithms on CULane testing set. For crossroad, only FP is shown. The second column denotes the proportion of each scenario in the testing set.</figDesc><table><row><cell>Category</cell><cell cols="3">Proportion ENet-SAD</cell><cell>R-18-SAD</cell><cell>R-34-SAD</cell><cell>R-101-SAD</cell><cell cols="2">ResNet-101 [10] SCNN [16]</cell></row><row><cell>Normal</cell><cell>27.7%</cell><cell>90.1</cell><cell></cell><cell>89.8</cell><cell>89.9</cell><cell>90.7</cell><cell>90.2</cell><cell>90.6</cell></row><row><cell>Crowded</cell><cell>23.4%</cell><cell>68.8</cell><cell></cell><cell>68.1</cell><cell>68.5</cell><cell>70.0</cell><cell>68.2</cell><cell>69.7</cell></row><row><cell>Night</cell><cell>20.3%</cell><cell>66.0</cell><cell></cell><cell>64.2</cell><cell>64.6</cell><cell>66.3</cell><cell>65.9</cell><cell>66.1</cell></row><row><cell>No line</cell><cell>11.7%</cell><cell>41.6</cell><cell></cell><cell>42.5</cell><cell>42.2</cell><cell>43.5</cell><cell>41.7</cell><cell>43.4</cell></row><row><cell>Shadow</cell><cell>2.7%</cell><cell>65.9</cell><cell></cell><cell>67.5</cell><cell>67.7</cell><cell>67.0</cell><cell>64.6</cell><cell>66.9</cell></row><row><cell>Arrow</cell><cell>2.6%</cell><cell>84.0</cell><cell></cell><cell>83.9</cell><cell>83.8</cell><cell>84.4</cell><cell>84.0</cell><cell>84.1</cell></row><row><cell>Dazzle light</cell><cell>1.4%</cell><cell>60.2</cell><cell></cell><cell>59.8</cell><cell>59.9</cell><cell>59.9</cell><cell>59.8</cell><cell>58.5</cell></row><row><cell>Curve</cell><cell>1.2%</cell><cell>65.7</cell><cell></cell><cell>65.5</cell><cell>66.0</cell><cell>65.7</cell><cell>65.5</cell><cell>64.4</cell></row><row><cell>Crossroad</cell><cell>9.0%</cell><cell>1998</cell><cell></cell><cell>1995</cell><cell>1960</cell><cell>2052</cell><cell>2183</cell><cell>1990</cell></row><row><cell>Total</cell><cell>-</cell><cell>70.8</cell><cell></cell><cell>70.5</cell><cell>70.7</cell><cell>71.8</cell><cell>70.8</cell><cell>71.6</cell></row><row><cell>Runtime (ms)</cell><cell>-</cell><cell>13.4</cell><cell></cell><cell>25.3</cell><cell>50.5</cell><cell>171.2</cell><cell>171.2</cell><cell>133.5</cell></row><row><cell>Parameter (M)</cell><cell>-</cell><cell>0.98</cell><cell></cell><cell>12.41</cell><cell>22.72</cell><cell>52.53</cell><cell>52.53</cell><cell>20.72</cell></row><row><cell cols="5">Table 4. Comparative results on BDD100K test set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Algorithm</cell><cell>Accuracy</cell><cell>IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet-18 [10]</cell><cell>30.66%</cell><cell cols="2">11.07</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet-34 [10]</cell><cell>30.92%</cell><cell cols="2">12.24</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet-101 [10]</cell><cell>34.45%</cell><cell cols="2">15.02</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ENet [17]</cell><cell>34.12%</cell><cell cols="2">14.64</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SCNN [16]</cell><cell>35.79%</cell><cell cols="2">15.84</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">R-18-SAD (ours)</cell><cell>31.10%</cell><cell cols="2">13.29</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">R-34-SAD (ours)</cell><cell>32.68%</cell><cell cols="2">14.56</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">R-101-SAD (ours)</cell><cell>35.56%</cell><cell cols="2">15.96</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ENet-SAD (ours)</cell><cell>36.56%</cell><cell cols="2">16.02</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>CULane testing sets. The number below each image denotes the accuracy. Ground-truth lanes are drawn on the input image.</figDesc><table><row><cell>input</cell><cell>ENet-SAD</cell><cell>ENet</cell><cell>SCNN</cell></row><row><cell>(a)</cell><cell>97.2 %</cell><cell>94.8 %</cell><cell>95.2 %</cell></row><row><cell></cell><cell>98.4 %</cell><cell>97.8 %</cell><cell>97.4 %</cell></row><row><cell>(b)</cell><cell>100 %</cell><cell>66.6 %</cell><cell>66.6 %</cell></row><row><cell></cell><cell>100 %</cell><cell>33.3 %</cell><cell>66.6 %</cell></row><row><cell cols="4">Figure 6. Performance of different algorithms on (a) TuSimple and</cell></row><row><cell cols="3">(b) ENet ENet-SAD input</cell><cell>SCNN</cell></row><row><cell></cell><cell>38.53 %</cell><cell>34.62 %</cell><cell>36.49 %</cell></row><row><cell></cell><cell>37.82 %</cell><cell>33.75 %</cell><cell>35.57 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Performance of different variants of ENet-SAD onTuSimple testing set. Pij denotes that the output of the i-th block of ENet mimics the output of the j-th block.</figDesc><table><row><cell cols="2">Path Accuracy</cell><cell cols="2">Path Accuracy</cell><cell>Path</cell><cell>Accuracy</cell></row><row><cell>P12</cell><cell>91.22%</cell><cell>P23</cell><cell>94.72%</cell><cell>P23, P24</cell><cell>95.38%</cell></row><row><cell>P13</cell><cell>91.36%</cell><cell>P24</cell><cell>94.63%</cell><cell>P23, P34</cell><cell>96.64%</cell></row><row><cell>P14</cell><cell>91.47%</cell><cell>P34</cell><cell>95.29%</cell><cell>P24, P34</cell><cell>96.52%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Performance of SAD and deep supervision applied to different base models on TuSimple, CULane and BDD100K testing sets.</figDesc><table><row><cell cols="2">Algorithm</cell><cell cols="4">TuSimple CULane Accuracy Total</cell><cell cols="3">BDD100K Accuracy IoU</cell></row><row><cell cols="2">ENet</cell><cell cols="2">93.02%</cell><cell cols="2">68.4</cell><cell cols="2">34.12%</cell><cell>14.64</cell></row><row><cell cols="2">ENet-Deep</cell><cell cols="2">94.69%</cell><cell cols="2">69.6</cell><cell cols="2">35.61%</cell><cell>15.38</cell></row><row><cell cols="2">ENet-SAD</cell><cell cols="2">96.64%</cell><cell cols="2">70.8</cell><cell cols="2">36.56%</cell><cell>16.02</cell></row><row><cell cols="2">R-18</cell><cell cols="2">92.69%</cell><cell cols="2">67.9</cell><cell cols="2">30.66%</cell><cell>11.07</cell></row><row><cell cols="2">R-18-Deep</cell><cell cols="2">94.14%</cell><cell cols="2">68.8</cell><cell cols="2">30.95%</cell><cell>12.23</cell></row><row><cell cols="2">R-18-SAD</cell><cell cols="2">96.02%</cell><cell cols="2">70.5</cell><cell cols="2">31.10%</cell><cell>13.29</cell></row><row><cell cols="2">R-34</cell><cell cols="2">92.84%</cell><cell cols="2">68.6</cell><cell cols="2">30.92%</cell><cell>12.24</cell></row><row><cell cols="2">R-34-Deep</cell><cell cols="2">94.52%</cell><cell cols="2">69.2</cell><cell cols="2">31.72%</cell><cell>13.59</cell></row><row><cell cols="2">R-34-SAD</cell><cell cols="2">96.24%</cell><cell cols="2">70.7</cell><cell cols="2">32.68%</cell><cell>14.56</cell></row><row><cell></cell><cell>76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F1-measure</cell><cell>68 69 70 71 72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 K</cell></row><row><cell></cell><cell>66 65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20 K 30 K 40 K</cell></row><row><cell></cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50 K</cell></row><row><cell></cell><cell>63</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">baseline</cell></row><row><cell></cell><cell>62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10 k</cell><cell>20 k</cell><cell>30 k</cell><cell>40 k</cell><cell>50 k</cell><cell>60 k</cell><cell>70 k</cell><cell>80 k</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Episode</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gold: A parallel real-time stereo vision system for generic obstacle and lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="81" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A novel lane detection system with efficient ground truth generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="365" to="374" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention to scale: scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rbnet: A deep neural network for unified road and road boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A random finite set approach to multiple lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szczot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="270" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Born-again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1602" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">EL-GAN: embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="256" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate and robust lane detection based on dual-view convolutional neutral network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1041" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STAT</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to steer by mimicking features from heterogeneous auxiliary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1965" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial CNN for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<ptr target="http://benchmark.tusimple.ai/#/t/1.Accessed" />
		<title level="m">TuSimple</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">BDD100K: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geometric constrained joint lane segmentation and lane boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gold: A parallel real-time stereo vision system for generic obstacle and lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="81" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A novel lane detection system with efficient ground truth generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="365" to="374" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention to scale: scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rbnet: A deep neural network for unified road and road boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A random finite set approach to multiple lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szczot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="270" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Born-again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1602" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">EL-GAN: embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="256" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Accurate and robust lane detection based on dual-view convolutional neutral network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1041" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STAT</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to steer by mimicking features from heterogeneous auxiliary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1965" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial CNN for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<ptr target="http://benchmark.tusimple.ai/#/t/1.Accessed" />
		<title level="m">TuSimple</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">BDD100K: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Geometric constrained joint lane segmentation and lane boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="502" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
