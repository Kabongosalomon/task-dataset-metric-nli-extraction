<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Person Re-ID: Fantastic Techniques and Where to Find Them</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyank</forename><surname>Pathak</surname></persName>
							<email>ppriyank@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<postCode>10012</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Erfan Eshratifar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gormish</surname></persName>
							<email>michael.gormish@clarifai.com</email>
							<affiliation key="aff2">
								<address>
									<postCode>94105</postCode>
									<settlement>Clarifai, San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Person Re-ID: Fantastic Techniques and Where to Find Them</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to identify the same person from multiple camera views without the explicit use of facial recognition is receiving commercial and academic interest. The current status-quo solutions are based on attention neural models. In this paper, we propose Attention and CL loss, which is a hybrid of center and Online Soft Mining (OSM) loss added to the attention loss on top of a temporal attention-based neural network. The proposed loss function applied with bag-of-tricks for training surpasses the state of the art on the common person Re-ID datasets, MARS and PRID 2011. Our source code is publicly available on github 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Person Re-IDentification (Re-ID) aims to recognize the same individual in different pictures or image sequences caught by distinct cameras that may or may not be temporally aligned <ref type="figure">(Fig 1)</ref>. The goal of video-based person Re-ID is to find the same person in a set of gallery videos from a query video. One industrial use case is surveillance for security purposes. In contrast with image-based person Re-id, which can be consequently affected by several factors such as blurriness, lighting, pose, viewpoint and occlusion, videobased person Re-ID is more robust to these pitfalls as multiple frames distributed across the video are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Baseline (Base Temporal Attention): We use <ref type="bibr" target="#b1">Gao and Nevatia's (2018)</ref>   <ref type="bibr" target="#b4">(Zheng et al. 2016)</ref>. Same individual captured in 3 input video clips the last layer (richer feature space), using warm-up learning rate, random erasing of patches within frames (simulating occlusion), label smoothing, center loss in addition to triplet loss, cosine-metric based triplet loss (angular distance proven better than L-2 distance) and lastly, batch normalization before the classification layer. Our experiments reveal that batch normalized feature provides a more robust model compared to non-normalized features. Attention and CL loss: <ref type="bibr" target="#b3">Wang et al. (2018)</ref> proposed Online Soft Mining and Class-Aware Attention (OSM loss) as an alternative to triplet loss for training Re-ID tasks, a modified contrastive loss with attention to remove noisy frames. We propose CL Centers OSM loss, which uses the center vectors from center loss as the class label vector representations, for cropping out noisy frames as they have greater variance compared to the originally proposed classifier weights. In addition, we penalize the model for giving high attention scores to frames where we have randomly deleted a patch. Such randomly erased frames are labeled as 1 otherwise 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention loss</head><formula xml:id="formula_0">= 1 N N i=1 label(i) * Attention score (i) (1)</formula><p>where N is the number of total frames. Model mAP (re-rank) CMC-1 (re-rank) CMC-5 (re-rank) CMC-20 (re-rank) SOTA w/o re-ranking <ref type="bibr" target="#b0">(Fu et al., (2018))</ref> 81.2 (-) 86.2(-) 95.7(-) -(-) SOTA with re-ranking <ref type="bibr" target="#b0">(Fu et al., (2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation and Results</head><p>In our experiments, we use four frames of the video selected randomly, N = 4. <ref type="figure" target="#fig_1">Figure 2</ref> shows the proposed model architecture. <ref type="table">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> show a comparison of our model to the state-of-the-art results on MARS and PRID 2011 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we mixed and improved existing techniques to surpass the state-of-the-art accuracy on MARS and PRID 2011 datasets. We plan to evaluate our work on other datasets and other similar tasks like facial re-identification in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>temporal attention model as our base model, where a pre-trained ResNet-50 on ImageNet creates features for each frame of a video clip, and an attention model computes a weighted sum of the features across frames. Bag-of-Tricks: Luo et al. (2019) proposed a series of tricks to enhance the performance of a ResNet model in imagebased person Re-ID, which includes reducing the stride of * Work done at Clarifai Inc. Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 https://github.com/ppriyank/Video-Person-Re-ID-Fantastic-Techniques-and-Where-to-Find-Them Figure 1: Data Samples are taken from MARS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The proposed model architecture. ⊗ indicates pairwise multiplication and ⊕ indicate summation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>PRID 2011 Dataset Performance. '-' indicates the results were not reported. '*' refers to hyperparameter optimized.</figDesc><table><row><cell>The Attention loss combined with OSM loss and CL Centers</cell></row><row><cell>is denoted as Attention and CL loss.</cell></row><row><cell>Hyperparameter optimization: We also applied Face-</cell></row><row><cell>book's hyperparameter optimization tool 2 to do hyperpa-</cell></row><row><cell>rameter search.</cell></row><row><cell>Datasets: We focus on the MARS and PRID datasets con-</cell></row><row><cell>taining 1251 and 178 identities, respectively which are</cell></row><row><cell>equally split among training and testing sets.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/facebook/Ax</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">STA: spatial-temporal attention for large-scale video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1811.04129</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Revisiting temporal modeling for video-based person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno>abs/1805.02104</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bag of tricks and A strong baseline for deep person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CoRR abs/1903.07071</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person reidentification with hierarchical deep learning feature and efficient xqda metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1811.01459</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia, MM &apos;18</title>
		<meeting>the 26th ACM International Conference on Multimedia, MM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1838" to="1846" />
		</imprint>
	</monogr>
	<note>Deep metric learning by online soft mining and class-aware attention</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
