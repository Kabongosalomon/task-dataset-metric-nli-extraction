<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transductive Information Maximization For Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éts</forename><surname>Montreal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziko</forename><forename type="middle">Imtiaz</forename><surname>Masud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éts</forename><surname>Montreal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Rony</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éts</forename><surname>Montreal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Dolz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éts</forename><surname>Montreal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Piantanida</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><forename type="middle">Ben</forename><surname>Ayed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éts</forename><surname>Montreal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CentraleSupélec</orgName>
								<orgName type="institution">CNRS Université Paris-Saclay</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transductive Information Maximization For Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Transductive Infomation Maximization (TIM) for few-shot learning. Our method maximizes the mutual information between the query features and their label predictions for a given few-shot task, in conjunction with a supervision loss based on the support set. Furthermore, we propose a new alternating-direction solver for our mutual-information loss, which substantially speeds up transductiveinference convergence over gradient-based optimization, while yielding similar accuracy. TIM inference is modular: it can be used on top of any base-training feature extractor. Following standard transductive few-shot settings, our comprehensive experiments 2 demonstrate that TIM outperforms state-of-the-art methods significantly across various datasets and networks, while used on top of a fixed feature extractor trained with simple cross-entropy on the base classes, without resorting to complex meta-learning schemes. It consistently brings between 2% and 5% improvement in accuracy over the best performing method, not only on all the well-established few-shot benchmarks but also on more challenging scenarios, with domain shifts and larger numbers of classes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning models have achieved unprecedented success, approaching human-level performances when trained on large-scale labeled data. However, the generalization of such models might be seriously challenged when dealing with new (unseen) classes, with only a few labeled instances per class. Humans, however, can learn new tasks rapidly from a handful of instances, by leveraging context and prior knowledge. The few-shot learning (FSL) paradigm <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b44">45]</ref> attempts to bridge this gap, and has recently attracted substantial research interest, with a large body of very recent works, e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9]</ref>, among many others. In the few-shot setting, a model is first trained on labeled data with base classes. Then, model generalization is evaluated on few-shot tasks, composed of unlabeled samples from novel classes unseen during training (the query set), assuming only one or a few labeled samples (the support set) are given per novel class.</p><p>Most of the existing approaches within the FSL framework are based on the "learning to learn" paradigm or meta-learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b21">22]</ref>, where the training set is viewed as a series of balanced tasks (or episodes), so as to simulate test-time scenario. Popular works include prototypical networks <ref type="bibr" target="#b37">[38]</ref>, which describes each class with an embedding prototype and maximizes the log-probability of query samples via episodic training; matching network <ref type="bibr" target="#b44">[45]</ref>, which represents query predictions as linear combinations of support labels and employs episodic training along with memory architectures; MAML <ref type="bibr" target="#b8">[9]</ref>, a meta-learner, which trains a model to make it "easy" to fine-tune; and the LSTM meta-learner in <ref type="bibr" target="#b34">[35]</ref>, which suggests optimization as a model for few-shot learning. A large body of meta-learning works followed-up lately, to only cite a few <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Transductive inference: In a recent line of work, transductive inference has emerged as an appealing approach to tackling few-shot tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51]</ref>, showing performance improvements over inductive inference. In the transductive setting 3 , the model classifies the unlabeled query examples of a single few-shot task at once, instead of one sample at a time as in inductive methods. These recent experimental observations in few-shot learning are consistent with established facts in classical transductive inference <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b5">6]</ref>, which is well-known to outperform inductive methods on small training sets. While <ref type="bibr" target="#b31">[32]</ref> used information of unlabeled query samples via batch normalization, the authors of <ref type="bibr" target="#b27">[28]</ref> were the first to model explicitly transductive inference in few-shot learning. Inspired by popular label-propagation concepts <ref type="bibr" target="#b5">[6]</ref>, they built a meta-learning framework that learns to propagate labels from labeled to unlabeled instances via a graph. The meta-learning transductive method in <ref type="bibr" target="#b13">[14]</ref> used attention mechanisms to propagate labels to unlabeled query samples. More closely related to our work, the recent transductive inference of Dhillion et al. <ref type="bibr" target="#b6">[7]</ref> minimizes the entropy of the network softmax predictions at unlabeled query samples, reporting competitive fewshot performances, while using standard cross-entropy training on the base classes. The competitive performance of <ref type="bibr" target="#b6">[7]</ref> is in line with several recent inductive baselines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b40">41]</ref>, which reported that standard cross-entropy training for the base classes matches or exceeds the performances of more sophisticated meta-learning procedures. Also, the performance of <ref type="bibr" target="#b6">[7]</ref> is in line with established results in the context of semi-supervised learning, where entropy minimization is widely used <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b1">2]</ref>. It is worth noting that the inference runtimes of transductive methods are, typically, much higher than their inductive counterparts. For, instance, the authors of <ref type="bibr" target="#b6">[7]</ref> fine-tune all the parameters of a deep network during inference, which is several orders of magnitude slower than inductive methods such as ProtoNet <ref type="bibr" target="#b37">[38]</ref>. Also, based on matrix inversion, the transductive inference in <ref type="bibr" target="#b27">[28]</ref> has a complexity that is cubic in the number of query samples.</p><p>Info-max principle: While the semi-supervised and few-shot learning works in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7]</ref> build upon Barlow's principle of entropy minimization <ref type="bibr" target="#b0">[1]</ref>, our few-shot formulation is inspired by the general info-max principle enunciated by Linsker <ref type="bibr" target="#b24">[25]</ref>, which formally consists in maximizing the Mutual Information (MI) between the inputs and outputs of a system. In our case, the inputs are the query features and the outputs are their label predictions. The idea is also related to info-max in the context of clustering <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. More generally, info-max principles, well-established in the field of communications, were recently used in several deep-learning problems, e.g., representation learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>, metric learning <ref type="bibr" target="#b2">[3]</ref> or domain adaptation <ref type="bibr" target="#b23">[24]</ref>, among other problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>• We propose Transductive Information Maximization (TIM) for few-shot learning. Our method maximizes the MI between the query features and their label predictions for a few-shot task at inference, while minimizing the cross-entropy loss on the support set.</p><p>• We derive an alternating-direction solver for our loss, which substantially speeds up transductive inference over gradient-based optimization, while yielding competitive accuracy.</p><p>• Following standard transductive few-shot settings, our comprehensive evaluations show that TIM outperforms state-of-the-art methods substantially across various datasets and networks, while using a simple cross-entropy training on the base classes, without complex metalearning schemes. It consistently brings between 2% and 5% of improvement in accuracy over the best performing method, not only on all the well-established few-shot benchmarks but also on more challenging, recently introduced scenarios, with domain shifts and larger numbers of ways. Interestingly, our MI loss includes a label-marginal regularizer, which has a significant effect: it brings substantial improvements in accuracy, while facilitating optimization, reducing transductive runtimes by orders of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transductive Information Maximization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-shot setting</head><p>Assume we are given a labeled training set,</p><formula xml:id="formula_0">X base := {x i , y i } Nbase i=1</formula><p>, where x i denotes raw features of sample i and y i its associated one-hot encoded label. Such labeled set is often referred to as the meta-training or base dataset in the few-shot literature. Let Y base denote the set of classes for this base dataset. The few-shot scenario assumes that we are given a test dataset:</p><formula xml:id="formula_1">X test := {x i , y i } Ntest i=1 , with a completely new set of classes Y test such that Y base ∩ Y test = ∅,</formula><p>from which we create randomly sampled few-shot tasks, each with a few labeled examples. Specifically, each K-way N S -shot task involves sampling N S labeled examples from each of K different classes, also chosen at random. Let S denote the set of these labeled examples, referred to as the support set with size |S| = N S · K. Furthermore, each task has a query set denoted by Q composed of |Q| = N Q · K unlabeled (unseen) examples from each of the K classes. With models trained on the base set, few-shot techniques use the labeled support sets to adapt to the tasks at hand, and are evaluated based on their performances on the unlabeled query sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed formulation</head><p>We begin by introducing some basic notation and definitions before presenting our overall Transductive Information Maximization (TIM) loss and the different optimization strategies for tackling it. For a given few-shot task, with a support set S and a query set Q, let X denote the random variable associated with the raw features within S ∪ Q, and let Y ∈ Y = {1, . . . , K} be the random variable associated with the labels. Let f φ : X −→ Z ⊂ R d denote the encoder (i.e., feature-extractor) function of a deep neural network, where φ denotes the trainable parameters, and Z stands for the set of embedded features. The encoder is first trained from the base training set X base using the standard cross-entropy loss, without any meta training or specific sampling schemes. Then, for each specific few-shot task, we propose to minimize a mutual-information loss defined over the query samples.</p><p>Formally, we define a soft-classifier, parametrized by weight matrix W ∈ R K×d , whose posterior distribution over labels given features 4 , p ik := P(Y = k|X = x i ; W, φ), and marginal distribution over query labels, p k = P(Y Q = k; W, φ), are given by:</p><formula xml:id="formula_2">p ik ∝ exp − τ 2 w k − z i 2 , and p k = 1 |Q| i∈Q p ik<label>(1)</label></formula><p>where W := [w 1 , . . . , w K ] denotes classifier weights,</p><formula xml:id="formula_3">z i = f φ (xi) f φ (xi) 2</formula><p>the L2-normalized embedded features, and τ is a temperature parameter. Now, for each single few-shot task, we introduce our empirical weighted mutual information between the query samples and their latent labels, which integrates two terms: The first is an empirical (Monte-Carlo) estimate of the conditional entropy of labels given the query raw features, denoted H(Y Q |X Q ), while the second is the empirical label-marginal entropy, H(Y Q ).:</p><formula xml:id="formula_4">I α (X Q ; Y Q ) := − K k=1 p k log p k H(Y Q ): marginal entropy + α 1 |Q| i∈Q K k=1 p ik log(p ik ) − H(Y Q |X Q ): conditional entropy ,<label>(2)</label></formula><p>with α a non-negative hyper-parameter. Notice that setting α = 1 recovers the standard mutual information. Setting α &lt; 1 allows us to down-weight the conditional entropy term, whose gradients may dominate the marginal entropy's gradients as the predictions move towards the vertices of the simplex. The role of both terms in (2) will be discussed after introducing our overall transductive inference loss in the following, by embedding supervision from the task's support set.</p><p>We embed supervision information from support set S by integrating a standard cross-entropy loss CE with the information measure in Eq. <ref type="formula" target="#formula_4">(2)</ref>, which enables us to formulate our Transductive Information Maximization (TIM) loss as follows:</p><formula xml:id="formula_5">min W λ · CE − I α (X Q ; Y Q ) with CE := − 1 |S| i∈S K k=1 y ik log(p ik ),<label>(3)</label></formula><p>where {y ik } denotes the k th component of the one-hot encoded label y i associated to the i-th support sample. Non-negative hyper-parameters α and λ will be fixed to α = λ = 0.1 in all our experiments. It is worth to discuss in more details the role (importance) of the mutual information terms in (3):</p><p>• Conditional entropy H(Y Q |X Q ) aims at minimizing the uncertainty of the posteriors at unlabeled query samples, thereby encouraging the model to output confident predictions <ref type="bibr" target="#b4">5</ref> . This entropy loss is widely used in the context of semi-supervised learning (SSL) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b1">2]</ref>, as it models effectively the cluster assumption: The classifier's boundaries should not occur at dense regions of the unlabeled features <ref type="bibr" target="#b10">[11]</ref>. Recently, <ref type="bibr" target="#b6">[7]</ref> introduced this term for few-shot learning, showing that entropy fine-tuning on query samples achieves competitive performances. In fact, if we remove the marginal entropy H(Y Q ) in objective <ref type="formula" target="#formula_5">(3)</ref>, our TIM objective reduces to the loss in <ref type="bibr" target="#b6">[7]</ref>. The conditional entropy H(Y Q |X Q ) is of paramount importance but its optimization requires special care, as its optima may easily lead to degenerate (non-suitable) solutions on the simplex vertices, mapping all samples to a single class. Such care may consist in using small learning rates and fine-tuning the whole network (which itself often contains several layers of regularization) as done in <ref type="bibr" target="#b6">[7]</ref>, both of which significantly slow down transductive inference.</p><p>• The label-marginal entropy regularizer H(Y Q ) encourages the marginal distribution of labels to be uniform, thereby avoiding degenerate solutions obtained when solely minimizing conditional entropy. Hence, it is highly important as it removes the need for implicit regularization, as mentioned in the previous paragraph. In particular, high-accuracy results can be obtained even using higher learning rates and fine-tuning only a fraction of the network parameters (classifier weights W instead of the whole network), speeding up substantially transductive runtimes. As it will be observed from our experiments, this term brings substantial improvements in performances (e.g., up to 10% increase in accuracy over entropy fine-tuning on the standard few-shot benchmarks), while facilitating optimization, thereby reducing transductive runtimes by orders of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization</head><p>At this stage, we consider that the feature extractor has already been trained on base classes (using standard cross-entropy). We now propose two methods for minimizing our objective (3) for each test task. The first one is based on standard Gradient Descent (GD). The second is a novel way of optimizing mutual information, and is inspired by the Alternating Direction Method of Multipliers (ADMM). For both methods:</p><p>• The pre-trained feature extractor f φ is kept fixed. Only the weights W are optimized for each task. Such a choice is discussed in details in subsection 3.4. Overall, and interestingly, we found that fine-tuning only classifier weights W, while fixing feature-extractor parameters φ, yielded the best performances for our mutual-information loss.</p><p>• For each task, weights W are initialized as the class prototypes of the support set:</p><formula xml:id="formula_6">w (0) k = i∈S y ik z i i∈S y ik</formula><p>Gradient descent (TIM-GD): A straightforward way to minimize our loss in Eq. (3) is to perform gradient descent over W, which we update using all the samples of the few-shot task (both support and query) at once (i.e., no mini-batch sampling). This gradient approach yields our overall best results, while being one order of magnitude faster than the transductive entropy-based fine-tuning in <ref type="bibr" target="#b6">[7]</ref>. As will be shown later in our experiments, the method in <ref type="bibr" target="#b6">[7]</ref> needs to fine-tune the whole network (i.e., to update both φ and W), which provides implicit regularization, avoiding the degenerate solutions of entropy minimization. However, TIM-GD (with W-updates only) still remains two orders of magnitude slower than inductive closed-form solutions <ref type="bibr" target="#b37">[38]</ref>. In the following, we present a more efficient solver for our problem.</p><p>Alternating direction method (TIM-ADM): We derive an Alternating Direction Method (ADM) for minimizing our objective in <ref type="bibr" target="#b2">(3)</ref>. Such scheme yields substantial speedups in transductive learning (one order of magnitude), while maintaining the high levels of accuracy of TIM-GD. To do so, we introduce auxiliary variables representing latent assignments of query samples, and minimize a mixed-variable objective by alternating two sub-steps, one optimizing w.r.t classifier's weights W, and the other w.r.t the auxiliary variables q.</p><p>Proposition 1. The objective in <ref type="formula" target="#formula_5">(3)</ref> can be approximately minimized via the following constrained formulation of the problem:</p><formula xml:id="formula_7">min W,q − λ |S| i∈S K k=1 y ik log(p ik ) CE + K k=1 q k log q k ∼ H(Y Q ) − α |Q| i∈Q K k=1 q ik log(p ik ) ∼ H(Y Q |X Q ) + 1 |Q| i∈Q K k=1 q ik log q ik p ik Penalty ≡ DKL(q p) s.t K k=1 q ik = 1, q ik ≥ 0, i ∈ Q, k ∈ {1, . . . , K},<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">q = [q ik ] ∈ R |Q|×K are auxiliary variables, p = [p ik ] ∈ R |Q|×K and q k = 1 |Q| i∈Q q ik .</formula><p>Proof. It is straightforward to notice that, when equality constraints q ik = p ik are satisfied, the last term in objective <ref type="formula" target="#formula_7">(4)</ref>, which can be viewed as a soft penalty for enforcing those equality constraints, vanishes. Objectives <ref type="formula" target="#formula_5">(3)</ref> and <ref type="formula" target="#formula_7">(4)</ref> then become equivalent.</p><p>Splitting the problem into sub-problems on W and q as in Eq. <ref type="formula" target="#formula_7">(4)</ref> is closely related to the general principle of ADMM (Alternating Direction Method of Multipliers) <ref type="bibr" target="#b3">[4]</ref>, except that the KL divergence is not a typical penalty for imposing the equality constraints <ref type="bibr" target="#b5">6</ref> . The main idea is to decompose the original problem into two easier sub-problems, one over W and the other over q, which can be alternately solved, each in closed-form. Interestingly, this KL penalty is important as it completely removes the need for dual iterations for the simplex constraints in (4), yielding closed-form solutions:</p><p>Proposition 2. ADM formulation in Proposition 1 can be approximately solved by alternating the following closed-form updates w.r.t auxiliary variables q and classifier weights W (t is the iteration index):</p><formula xml:id="formula_9">q (t+1) ik ∝ p (t) ik 1+α i∈Q p (t) ik 1+α 1/2 (5) w (t+1) k ← λ 1 + α i∈S y ik z i + p (t) ik (w (t) k − z i ) + |S| |Q| i∈Q q (t+1) ik z i + p (t) ik (w (t) k − z i ) λ 1 + α i∈S y ik + |S| |Q| i∈Q q (t+1) ik<label>(6)</label></formula><p>Proof. A detailed proof is deferred to the supplementary material. Here, we summarize the main technical ingredients of the approximation. Keeping the auxiliary variables q fixed, we optimize a convex approximation of Eq. (4) w.r.t W. With W fixed, the objective is strictly convex w.r.t the auxiliary variables q whose updates come from a closed-form solution of the KKT (Karush-Kuhn-Tucker) conditions. Interestingly, the negative entropy of auxiliary variables, which appears in the penalty term, handles implicitly the simplex constraints, which removes the need for dual iterations to solve the KKT conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Hyperparameters: To keep our experiments as simple as possible, our hyperparameters are kept fixed across all the experiments and methods (TIM-GD and TIM-ADM). The conditional entropy weight α and the cross-entropy weights λ in Objective <ref type="formula" target="#formula_5">(3)</ref> are both set to 0.1. The temperature parameter τ in the classifier is set to 15. In our TIM-GD method, we use the ADAM optimizer with the recommended parameters <ref type="bibr" target="#b19">[20]</ref>, and run 1000 iterations for each task. For TIM-ADM, we run 150 iterations.</p><p>Base-training procedure: The feature extractors are trained following the same simple base-training procedure as in <ref type="bibr" target="#b50">[51]</ref> and using standard networks (ResNet-18 and WRN28-10), for all the experiments. Specifically, they are trained using the standard cross-entropy loss on the base classes, with label smoothing. The label-smoothing parameter is set to 0.1. We emphasize that base training does not involve any meta-learning or episodic training strategy. The models are trained for 90 epochs, with the learning rate initialized to 0.1, and divided by 10 at epochs 45 and 66. Batch size is set to 256 for ResNet-18, and to 128 for WRN28-10. During training, all the images are resized to 84 × 84, and we used the same data augmentation procedure as in <ref type="bibr" target="#b50">[51]</ref>, which includes random cropping, color jitter and random horizontal flipping.</p><p>Datasets: We resort to 3 few-shot learning datasets to benchmark the proposed models. As standard few-shot benchmarks, we use the mini-Imagenet <ref type="bibr" target="#b44">[45]</ref> dataset, with 100 classes split as in <ref type="bibr" target="#b34">[35]</ref>, the Caltech-UCSD Birds 200 <ref type="bibr" target="#b46">[47]</ref> (CUB) dataset, with 200 classes, split following <ref type="bibr" target="#b4">[5]</ref>, and finally the larger tiered-Imagenet dataset, with 608 classes split as in <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison to state-of-the-art</head><p>We first evaluate our methods TIM-GD and TIM-ADM on the widely adopted mini-ImageNet, tiered-ImageNet and CUB benchmark datasets, in the most common 1-shot 5-way and 5-shot 5-way scenarios, with 15 query shots for each class. Results are reported in <ref type="table" target="#tab_0">Table 1</ref>, and are averaged over 10,000 episodes, following <ref type="bibr" target="#b45">[46]</ref>. We can observe that both TIM-GD and TIM-ADM yield state-of-the-art performances, consistently across all standard datasets, scenarios and backbones, improving over both transductive and inductive methods, by significant margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Impact of domain-shift</head><p>Chen et al. <ref type="bibr" target="#b4">[5]</ref> recently showed that the performance of most meta-learning methods may drop drastically when a domain-shift exists between the base training data and test data. Surprisingly, the simplest discriminative baseline exhibited the best performance in this case. Therefore, we evaluate our methods in this challenging scenario. To this end, we simulate a domain shift by training the feature encoder on mini-Imagenet while evaluating the methods on CUB, similarly to the setting introduced in <ref type="bibr" target="#b4">[5]</ref>. TIM-GD and TIM-ADM beat previous methods by significant margins in the domain-shift scenario, consistently with our results in the standard few-shot benchmarks, thereby demonstrating an increased potential of applicability to real-world situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pushing the meta-testing stage</head><p>Most few-shot papers only evaluate their method in the usual 5-way scenario. Nevertheless, <ref type="bibr" target="#b4">[5]</ref> showed that meta-learning methods could be beaten by their discriminative baseline when more ways were introduced in each task. Therefore, we also provide results of our method in the more challenging 10-way and 20-way scenarios on mini-ImageNet. These results, which are presented in <ref type="table" target="#tab_2">Table 3</ref>, show that TIM-GD outperforms other methods by significant margins, in both settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation study</head><p>Influence of each term: We now assess the impact of each term 7 in our loss in Eq. (3) on the final performance of our methods. The results are reported in <ref type="table" target="#tab_3">Table 4</ref>. We observe that integrating the three terms in our loss consistently outperforms any other configuration. Interestingly, removing     <ref type="formula" target="#formula_5">(3)</ref>, which acts as a powerful regularizer to prevent such trivial solutions.</p><p>Fine-tuning the whole network vs only the classifier weights: While our TIM-GD and TIM-ADM optimize w.r.t W and keep base-trained encoder f φ fixed at inference, the authors of <ref type="bibr" target="#b6">[7]</ref> fine-tuned the whole network {W, φ} when performing their transductive entropy minimization. To assess both approaches, we add to <ref type="table" target="#tab_3">Table 4</ref> a variant of TIM-GD, in which we fine-tune the whole network {W, φ}, by using the same optimization procedure as in <ref type="bibr" target="#b6">[7]</ref>. We found that, besides being much slower, fine-tuning the whole network for our objective in Eq. 3 degrades the performances, as also conveyed by the convergence plots in <ref type="figure" target="#fig_0">Figure 1</ref>. Interestingly, when fine-tuning the whole network {W, φ}, the absence of H(Y Q ) in the entropy-based loss CE + H(Y Q |X Q ) does not cause the same drastic drop in performance as observed earlier when optimizing with respect to W only. We hypothesize that the network's intrinsic regularization (such as batch normalizations) and the use of small learning rates, as prescribed by <ref type="bibr" target="#b6">[7]</ref>, help the optimization process, preventing the predictions from approaching the vertices of the simplex, where entropy's gradients diverge.  : Inference run-time per few-shot task for a 5-shot 5-way task on mini-ImageNet with a WRN28-10 backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run-times</head><p>Method Parameters Transductive Inference/task (s) <ref type="bibr" target="#b6">[7]</ref> {φ, W} 2.1 × 10 +1</p><formula xml:id="formula_10">SimpleShot [46] {W} 9.0 × 10 −3 TIM-ADM {W} 1.2 × 10 −1 TIM-GD {W} 2.2 × 10 +0 Ent-min</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference run-times</head><p>Transductive methods are generally slower at inference than their inductive counterparts, with runtimes that are, typically, several orders of magnitude larger. In <ref type="table" target="#tab_4">Table 5</ref>, we measure the average adaptation time per few-shot task, defined as the time required by each method to build the final classifier, for a 5-shot 5-way task on mini-ImageNet using the WRN28-10 network. <ref type="table" target="#tab_4">Table 5</ref> conveys that our ADM optimization gains one order of magnitude in run-time over our gradient-based method, and more than two orders of magnitude in comparison to <ref type="bibr" target="#b6">[7]</ref>, which fine-tunes the whole network. Note that TIM-ADM still remains slower than the inductive baseline. Our methods were run on the same GTX 1080 Ti GPU, while the run-time of <ref type="bibr" target="#b6">[7]</ref> is directly reported from the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and future work</head><p>Our TIM inference establishes new state-of-the-art results on the standard few-shot benchmarks, as well as in more challenging scenarios, with larger numbers of classes and domain shifts. We used feature extractors based on a simple base-class training with the standard cross-entropy loss, without resorting to the complex meta-training schemes that are often used and advocated in the recent few-shot literature. TIM is modular: it could be plugged on top of any feature extractor and base training, regardless of how the training was conducted. Therefore, while we do not claim that the very challenging few-shot problem is solved, we believe that our model-agnostic TIM inference should be used as a strong baseline for future few-shot learning research. In future work, we target on giving a more theoretical ground for our proposed mutual-information objective, and on exploring further generalizations of the objective, e.g., via embedding domain-knowledge priors. Specifically, one of our theoretical goals will be to connect TIM's objective to the classifier's empirical risk on the query set, showing that the former could be viewed as a surrogate for the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgements</head><p>This research was supported by the National Science and Engineering Research Council of Canada (NSERC), via its Discovery Grant program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader impact</head><p>Due to the simplicity and efficiency of our method, we lower the barrier of entry to few-shot learning.</p><p>In turn, we think that it will make a wider breadth of real-world applications tractable. The impact (positive or negative) on society is similar to that of any other few-shot method: being only a tool, its impact is entirely dependent on the final applications, and on the intentions of the people and institutions deploying it.</p><p>In our strive towards finding simple and efficient formulations -for instance, we stick to a standard cross-entropy, which not only eases implementation, but also avoid the huge memory consumption of more complex methods -we believe our method can enable and empower persons and communities that are unable to afford the costly resources and infrastructures required. This may help level the playing field with larger and better funded entities. For instance, to be adapted to a new task, our TIM-ADM method requires a little more than a recent smartphone computational power. This could spawn a lot of fresh and new applications on edge devices, closer to the end-users, in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>Proof of Proposition 1</p><p>Proof. Let us start from the initial optimization problem:</p><formula xml:id="formula_11">min W K k=1 p k log p k − α |Q| i∈Q K k=1 p ik log p ik − λ |S| i∈S K k=1 y ik log p ik<label>(7)</label></formula><p>We can reformulate problem <ref type="bibr" target="#b6">(7)</ref> using the ADM approach, i.e., by introducing auxiliary variables q = [q ik ] ∈ R |Q|×K and enforcing equality constraint q = p, with p = [p ik ] ∈ R |Q|×K , in addition to pointwise simplex constraints:</p><formula xml:id="formula_12">min W,q K k=1 q k log q k − α |Q| i∈Q K k=1 q ik log p ik − λ |S| i∈S K k=1 y ik log p ik s.t. q ik = p ik , i ∈ Q, k ∈ {1, . . . , K} K k=1 q ik = 1, i ∈ Q q ik ≥ 0, i ∈ Q, k ∈ {1, . . . , K}<label>(8)</label></formula><p>We can slove constrained problem (8) with a penalty-based approach, which encourages auxiliary pointwise predictions q i = [q i1 , . . . , q iK ] to be close to our model's posteriors</p><formula xml:id="formula_13">p i = [p i1 , . . . , p iK ].</formula><p>To add a penalty encouraging equality constraints q i = p i , we use the Kullback-Leibler (KL) divergence, which is given by:</p><formula xml:id="formula_14">D KL (q i ||p i ) = K k=1 q ik log q ik p ik</formula><p>Thus, our constrained optimization problem becomes:</p><formula xml:id="formula_15">min W,q K k=1 q k log q k − α |Q| i∈Q K k=1 q ik log p ik − λ |S| i∈S K k=1 y ik log p ik + 1 |Q| i∈Q D KL (q i ||p i ) s.t. K k=1 q ik = 1, i ∈ Q q ik ≥ 0, i ∈ Q, k ∈ {1, . . . , K}<label>(9)</label></formula><p>Proof of Proposition 2</p><p>Proof. Recall that we consider a softmax classifier over distances to weights W = {w 1 , . . . , w K }.</p><p>To simplify the notations, we will omit the dependence upon φ in what follows, and write</p><formula xml:id="formula_16">z i = f φ (xi) f φ (xi)</formula><p>, such that:</p><formula xml:id="formula_17">p ik = e − τ 2 zi−w k 2 K j=1 e − τ 2 zi−wj 2<label>(10)</label></formula><p>Without loss of generality, we use τ = 1 in what follows. Plugging the expression of p ik into Eq. (4), and grouping terms together, we get:</p><formula xml:id="formula_18">(4) = K k=1 q k log q k − 1 + α |Q| i∈Q K k=1 q ik log p ik − λ |S| i∈S K k=1 y ik log p ik + 1 |Q| i∈Q K k=1 q ik log q ik = K k=1 q k log q k + 1 + α 2|Q| i∈Q K k=1 q ik z i − w k 2 + 1 + α |Q| i∈Q log   K j=1 e − 1 2 zi−wj 2   + λ 2|S| i∈S K k=1 y ik z i − w k 2 + λ |S| i∈S log   K j=1 e − 1 2 zi−wj 2   + 1 |Q| i∈Q K k=1 q ik log q ik<label>(11)</label></formula><p>Now, we can solve our problem approximately by alternating two sub-steps: one sub-step optimizes w.r.t classifier weights W while auxiliary variables q are fixed; another sub-step fixes W and update q.</p><p>• W-update: Omitting the terms that do not involve W, Eq. (11) reads:</p><formula xml:id="formula_19">λ 2|S| i∈S y ik z i − w k 2 + 1 + α 2|Q| i∈Q q ik z i − w k 2 C:convex + λ |S| i∈S log   K j=1 e − 1 2 zi−wj 2   + 1 + α |Q| i∈Q log   K j=1 e − 1 2 zi−wj 2   C :non-convex<label>(12)</label></formula><p>One can notice that objective <ref type="formula" target="#formula_2">(11)</ref> is not convex w.r.t w k . Actually, it can be split into convex and non-convex parts as in Eq. <ref type="bibr" target="#b11">(12)</ref>. Thus, we cannot simply set the gradients to 0 to get the optimal w k . The non-convex part can be linearized at current solution w (t) k as follows:</p><formula xml:id="formula_20">C(w k ) ≈C(w (t) k ) + ∂C ∂w k (w (t) k ) T (w k − w (t) k ) c = λ |S| i∈S p (t) ik (z i − w (t) k ) T w k + 1 + α |Q| i∈Q p (t) ik (z i − w (t) k ) T w k<label>(13)</label></formula><p>Where c = stands for "equal, up to an additive constant". By adding this linear term to the convex part C, we can obtain a strictly convex objective in w k , whose gradients w.r.t w k read:</p><formula xml:id="formula_21">∂(12) ∂w k ≈ λ |S| [ i∈S y ik (z i − w k ) + p (t) ik (z i − w (t) k )] + 1 + α |Q| [ i∈Q q ik (z i − w k ) + p (t) ik (z i − w (t) k )]<label>(14)</label></formula><p>Note that the approximation we do here is similar in spirit to concave-convex procedures, which are well known in optimization. Concave-convex techniques proceed as follows: for a function in the form of a sum of a concave term and a convex term, the concave part is replaced by its first-order approximation, while the convex part is kept as is. The difference here is that the part that we linearize in Eq. <ref type="formula" target="#formula_2">(12)</ref> is not concave. Setting the gradients above to 0 yields the optimal solution for the approximate objective.</p><p>Another solution to obtain a strictly convex objective would have been to discard the nonconvex partC. Very interestingly, in this case, one would recover w k updates that would very much resemble the prototype updates of the K-means clustering algorithm (slightly modified to take into account the fact that for support points in S have labels). Note that the link between regularized K-means and mutual information maximization has been extensively explored in <ref type="bibr" target="#b16">[17]</ref>. Of course, in this case, the approximation is not as good as the first-order approximation above, and we found that omitting the non-convex part might decrease the performances significantly.</p><p>• q-update: With weights W fixed, the objective is convex w.r.t auxiliary variables q i (sum of linear and convex functions) and the simplex constraints are affine. Therefore, one can minimize this constrained convex problem for each q i by solving the Karush-Kuhn-Tucker (KKT) conditions <ref type="bibr" target="#b7">8</ref> . The KKT conditions yield closed-form solutions for both primal variable q i and the dual variable (Lagrange multiplier) corresponding to simplex constraint K j=1 q ij = 1. Interestingly, the negative entropy of auxiliary variables, i.e., K k=1 q ik log q ik , which appears in the penalty term, handles implicitly non-negativity constraints q i ≥ 0. In fact, this negative entropy acts as a barrier function, restricting the domain of each q i to non-negative values, which avoids extra dual variables and Lagrangiandual inner iterations for constraints q i ≥ 0. As we will see, the closed-form solutions of the KKT conditions satisfy these non-negativity constraints, without explicitly imposing them. In addition to non-negativity, for each point i, we need to handle probability simplex constraints K k=1 q ik = 1. Let γ i ∈ R denote the Lagrangian multiplier corresponding to this constraint. The KKT conditions correspond to setting the following gradient of the Lagrangian function to zero, while enforcing the simplex constraints:</p><formula xml:id="formula_22">∂(4) ∂q ik = − 1 + α |Q| log p ik + 1 |Q| (log q k + 1) + 1 |Q| (log q ik + 1) + γ i (15) = 1 |Q| log( q ik q k p 1+α ik ) + 2 + γ i<label>(16)</label></formula><p>This yields:</p><formula xml:id="formula_23">q ik = p 1+α ik q k e −(γi|Q|+2)<label>(17)</label></formula><p>Applying simplex constraint </p><p>Hence, plugging <ref type="bibr" target="#b17">(18)</ref> in <ref type="formula" target="#formula_2">(17)</ref> yields:</p><formula xml:id="formula_25">q ik = p 1+α ik q k K j=1 p 1+α ij q j<label>(19)</label></formula><p>Using the definition of q k , we can decouple this equation:</p><formula xml:id="formula_26">q k = 1 |Q| i∈Q q ik ∝ i∈Q p 1+α ik q k<label>(20)</label></formula><p>which implies:</p><formula xml:id="formula_27">q k ∝ i∈Q p 1+α ik 1/2<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Summary figure</head><p>We hereby provide a summarizing figure of the training and inference stages used in TIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Outline of TIM framework (best viewed in color). First, the feature extractor is trained with the standard cross-entropy on the base classes. Then, it is kept fixed at inference and weights W are optimized for by minimizing the cross-entropy on the support set S, while maximizing the mutual information between features and predictions on the query set Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of ADM ablation</head><p>In <ref type="table">Table 6</ref>, we provide the W and q updates for each configuration of the TIM-ADM ablation study, whose results were presented in <ref type="table" target="#tab_3">Table 4</ref>. The proof for each of these updates is very similar to the proof of Proposition 2 detailed in Appendix A. Therefore, we do not detail it here. <ref type="table">Table 6</ref>: The W and q-updates for each case of the ablation study. "-" refers to the updates in Proposition 2. "NA" refers to non-applicable.</p><p>Loss</p><formula xml:id="formula_28">w k update q ik update CE i∈S y ik z i i∈S y ik N/A CE + H(Y Q |X Q ) - ∝ p 1+α ik CE − H(Y Q ) - ∝ p ik    i∈Q p ik    1/2 CE − H(Y Q ) + H(Y Q |X Q ) - -</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Convergence plots for our methods on mini-ImageNet with a ResNet-18. Solid lines are averages, while shadows are 95% confidence intervals. Time is in logarithmic scale. Left: Evolution of the test accuracy during transductive inference. Right: Evolution of the mutual information between query features and predictions I(X Q ; Y Q ), computed as in Eq. (2), with α = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>K j=1 q ij = 1</head><label>1</label><figDesc>to<ref type="bibr" target="#b16">(17)</ref>, Lagrange multiplier γ i verifies:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison to the state-of-the-art methods on mini-ImageNet, tiered-Imagenet and CUB. The methods are sub-grouped into transductive and inductive methods, as well as by backbone architecture. Our results (gray-shaded) are averaged over 10,000 episodes. "-" signifies the result is unavailable.</figDesc><table><row><cell>mini-ImageNet tiered-ImageNet</cell><cell>CUB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The results for the domain-shift setting mini-Imagenet → CUB. The results obtained by our models (gray-shaded) are averaged over 10,000 episodes.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>mini-ImageNet → CUB 5-shot</cell></row><row><cell>MatchNet [45]</cell><cell>ResNet-18</cell><cell>53.1</cell></row><row><cell>MAML [9]</cell><cell>ResNet-18</cell><cell>51.3</cell></row><row><cell>ProtoNet [38]</cell><cell>ResNet-18</cell><cell>62.0</cell></row><row><cell>RelatNet [40]</cell><cell>ResNet-18</cell><cell>57.7</cell></row><row><cell>SimpleShot [46]</cell><cell>ResNet-18</cell><cell>64.0</cell></row><row><cell>GNN [42]</cell><cell>ResNet-10</cell><cell>66.9</cell></row><row><cell>Neg-Cosine [26]</cell><cell>ResNet-18</cell><cell>67.0</cell></row><row><cell>Baseline [5]</cell><cell>ResNet-18</cell><cell>65.6</cell></row><row><cell cols="2">LaplacianShot [51] ResNet-18</cell><cell>66.3</cell></row><row><cell>TIM-ADM</cell><cell>ResNet-18</cell><cell>70.3</cell></row><row><cell>TIM-GD</cell><cell>ResNet-18</cell><cell>71.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results for increasing the number of classes on mini-ImageNet. The results obtained by our models (gray-shaded) are averaged over 10,000 episodes.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">10-way</cell><cell cols="2">20-way</cell></row><row><cell>Methods</cell><cell cols="5">Backbone 1-shot 5-shot 1-shot 5-shot</cell></row><row><cell>MatchNet [45]</cell><cell>ResNet-18</cell><cell>-</cell><cell>52.3</cell><cell>-</cell><cell>36.8</cell></row><row><cell>ProtoNet [38]</cell><cell>ResNet-18</cell><cell>-</cell><cell>59.2</cell><cell>-</cell><cell>45.0</cell></row><row><cell>RelatNet [40]</cell><cell>ResNet-18</cell><cell>-</cell><cell>53.9</cell><cell>-</cell><cell>39.2</cell></row><row><cell cols="2">SimpleShot [46] ResNet-18</cell><cell>45.1</cell><cell>68.1</cell><cell>32.4</cell><cell>55.4</cell></row><row><cell>Baseline [5]</cell><cell>ResNet-18</cell><cell>-</cell><cell>55.0</cell><cell>-</cell><cell>42.0</cell></row><row><cell>Baseline++ [5]</cell><cell>ResNet-18</cell><cell>-</cell><cell>63.4</cell><cell>-</cell><cell>50.9</cell></row><row><cell>TIM-ADM</cell><cell>ResNet-18</cell><cell>56.0</cell><cell>72.9</cell><cell>39.5</cell><cell>58.8</cell></row><row><cell>TIM-GD</cell><cell>ResNet-18</cell><cell>56.1</cell><cell>72.8</cell><cell>39.3</cell><cell>59.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the effect of each term in our loss in Eq. (3), when only the classifier weights are fine-tuned, i.e., updating only W, and when the whole network is fine-tuned, i.e., updating the label-marginal entropy, H(Y Q ), reduces significantly the performances in both TIM-GD and TIM-ADM, particularly when only classifier weights W are updated and feature extractor φ is fixed. Such a behavior could be explained by the following fact: the conditional entropy term, H(Y Q |X Q ), may yield degenerate solutions (assigning all query samples to a single class) on numerous tasks, when used alone. This emphasizes the importance of the label-marginal entropy term H(Y Q ) in our loss</figDesc><table><row><cell>mini-ImageNet tiered-ImageNet</cell><cell>CUB</cell></row></table><note>{φ, W}. The results are reported for ResNet-18 as backbone. The same term indexing as in Eq. (3) is used here: H(Y Q ): Marginal entropy, H(Y Q |X Q ): Conditional entropy, CE: Cross-entropy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Transductive few-shot inference is not to be confused with semi-supervised few-shot learning<ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b22">23]</ref>. The latter uses extra unlabeled data during meta-training. Transductive inference has access to exactly the same training/testing data as its inductive counterpart.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In order to simplify our notations, we deliberately omit the dependence of posteriors p ik on the network parameters (φ, W). Also, p ik takes the form of softmax predictions, but we omit the normalization constants.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The global minima of each pointwise entropy in the sum of H(YQ|XQ) are one-hot vectors at the vertices of the simplex.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Typically, ADMM methods use multiplier-based quadratic penalties for enforcing the equality constraint.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">The W and q updates of TIM-ADM associated to each configuration can be found in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Note that strong duality holds since the objective is convex and the simplex constraints are affine. This means that the solutions of the (KKT) conditions minimize the objective.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Plugging this back in Eq. <ref type="bibr" target="#b18">(19)</ref>, we get:</p><p>ik 1/2 <ref type="bibr" target="#b21">(22)</ref> Notice that q ik ≥ 0, hence the solution fulfils the positivity constraint of the original problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TIM algorithms</head><p>In this section, we provide the pseudo-code for TIM's inference stage (both TIM-GD and TIM-ADM).</p><p>Result: Query predictionsŷ i = arg max k p ik , i ∈ Q </p><p>Result: Query predictionsŷ i = arg max k p ik , i ∈ Q</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Comput</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Imtiaz Masud Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers. In Foundations and Trends® in Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Dengyong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attentive weights generation for few shot learning via information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiluan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross attention network for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Empirical bayes transductive meta-learning with synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">G</forename><surname>Shell Xu Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damianou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep clustering: On the link between discriminative models and k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Jabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Mitiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative clustering by regularized information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan G</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to self-train for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Negative margin matters: Understanding margin in few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prototype rectification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul A</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodríguez López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cross-domain few-shot classification via learned feature-wise transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An overview of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks (TNN)</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Few-shot learning with localization in realistic settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning embedding adaptation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Variational few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Laplacian regularized few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Imtiaz Masud Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
