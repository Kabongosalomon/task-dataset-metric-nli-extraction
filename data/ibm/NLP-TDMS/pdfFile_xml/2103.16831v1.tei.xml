<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Hough Matching Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">POSTECH CSE &amp; GSAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">POSTECH CSE &amp; GSAI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Hough Matching Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite advances in feature representation, leveraging geometric relations is crucial for establishing reliable visual correspondences under large variations of images. In this work we introduce a Hough transform perspective on convolutional matching and propose an effective geometric matching algorithm, dubbed Convolutional Hough Matching (CHM). The method distributes similarities of candidate matches over a geometric transformation space and evaluate them in a convolutional manner. We cast it into a trainable neural layer with a semi-isotropic high-dimensional kernel, which learns non-rigid matching with a small number of interpretable parameters. To validate the effect, we develop the neural network with CHM layers that perform convolutional matching in the space of translation and scaling. Our method sets a new state of the art on standard benchmarks for semantic visual correspondence, proving its strong robustness to challenging intra-class variations. CHM input: all possible candidate matches CHM output: filtered matches Regions with different centers and scales Matches of object regions with different centers and scales convolution Hough voting kernel</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual correspondence lies at the heart of image understanding, being used as a core component for numerous tasks such as object recognition, image retrieval, motion estimation, object tracking, and reconstruction <ref type="bibr" target="#b15">[16]</ref>. With recent advances in deep neural networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b56">57]</ref>, there has been substantial progress in learning robust feature representation for establishing correspondences. Despite the effectiveness of deep convolutional features, however, spatial matching with a geometric constraint is still essential to handle image pairs with large variations, e.g., viewpoint and illumination changes, blur, occlusion, lack of texture, etc. In particular, the presence of intra-class variations, i.e., scenes depicting different instances of the same categories, remains a critical challenge for correspondence <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. The process of geometric matching is the de facto solution of choice, which most recent methods adopt in their models.</p><p>Geometric matching commonly relies on exploiting a geometric consensus of candidate matches to verify rela- tive transformations. In computer vision, RANSAC <ref type="bibr" target="#b14">[15]</ref> and Hough transform <ref type="bibr" target="#b21">[22]</ref> have long been used as geometric verification for wide-baseline correspondence problems with rigid motion models, while graph matching <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b54">55]</ref> has played a main role in matching deformable objects with non-rigid motion. Recent work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> has advanced the idea of Hough transform to perform non-rigid image matching, showing that the Hough voting process incorporated in neural networks is effective for challenging correspondence problems with intra-class variations. However, their matching modules are neither fully differentiable nor learnable, and weak to background clutter due to the position-invariant global Hough space.</p><p>In this work we introduce Convolutional Hough Matching (CHM) that distributes similarities of candidate matches over a geometric transformation space and evaluates them in a convolutional manner. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the convolutional nature makes the output equivariant to translation in the transformation space and also attentive to each position with its surrounding contexts, thus bringing robustness to background clutter. We design CHM as a learnable layer with an semi-isotropic high-dimensional kernel that acts on top of a correlation tensor. The CHM layer is compatible with any neural networks that use correlation computation, allowing flexible non-rigid matching and even multiple matching surfaces or objects. It naturally generalizes existing 4D convolutions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b53">54]</ref> and provides a new perspective of Hough transform on convolutional matching. To demonstrate the effect, we propose the neural network with CHM layers that perform convolutional matching in the high-dimensional space of translation and scaling. Our method clearly outperforms state-of-the-art methods on standard benchmarks for semantic correspondence, proving its strong robustness to challenging intra-class variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Hough transformation. The Hough transform <ref type="bibr" target="#b21">[22]</ref> is a classic method developed to identify primitive shapes in an image via geometric voting in a parameter space. Ballard <ref type="bibr" target="#b0">[1]</ref> generalizes the idea to identify positions of arbitrary shapes with R-table. Early approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> in computer vision widely adopt Hough transform for its effectiveness in extracting features of a particular shape in an image. As a representative example, Leibe et al. <ref type="bibr" target="#b38">[39]</ref> introduce a Hough-based object segmentation and detection method by incorporating information about supporting patterns of parts for the target category. The idea of Hough voting has widely been adopted in diverse tasks including retrieval <ref type="bibr" target="#b23">[24]</ref>, object discovery <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref>, shape recovery <ref type="bibr" target="#b58">[59]</ref>, 3D vision <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, and pose estimation <ref type="bibr" target="#b28">[29]</ref> to name a few. In geometric matching, Cho et al. <ref type="bibr" target="#b5">[6]</ref> first extends it to the Probabilistic Hough Matching (PHM) algorithm for unsupervised object discovery. Recent methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b57">58]</ref> have demonstrated the efficacy of the Hough matching with good empirical performance. They, however, are all limited in the sense that the geometric voting is carried out to discover a global offset consensus rather than a local and individual consensus for a match, which makes it less accurate and weak to clutter. Semantic visual correspondence. Traditional approaches to the task of semantic correspondence <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63]</ref> typically use hand-crafted descriptors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43]</ref>. Although the classic methods work satisfactorily for some applications, they still suffer apparent disadvantages of such features, e.g., lack of semantic patterns. Recent approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref> build upon features from convolutional neural network (CNN) pretrained on classification task <ref type="bibr" target="#b11">[12]</ref>. Han et al. <ref type="bibr" target="#b19">[20]</ref> introduce a CNN-based matching model that learns to compute a correlation tensor. <ref type="bibr">Rocco et al. [51]</ref> propose to learn a CNN regressor that computes a series of 2D convolutions on a dense correlation matrix to predict global geometric transformation parameters, either affine or TPS <ref type="bibr" target="#b12">[13]</ref>. Seo et al. <ref type="bibr" target="#b55">[56]</ref> improve the framework with offset-aware correlation kernels with attention modules. Jeon et al. <ref type="bibr" target="#b26">[27]</ref> stack multiple affine transformation networks and compute correspondences in coarse-to-fine manner. Wang et al. <ref type="bibr" target="#b61">[62]</ref> adopt the CNN architecture to estimate translation and rotation pa-rameters to learn correspondences from raw video. These methods demonstrate that a series of 2D convolutions acting on correlation tensors is effective in capturing geometric information by exploiting local patterns of similarity.</p><p>4D convolution for visual correspondence. Rocco et al. <ref type="bibr" target="#b53">[54]</ref> introduce the neighbourhood consensus network that uses 4D convolution for visual correspondence. They view 4D convolution as an extension of 2D convolution, which learns multiple similarity patterns of local correspondences, and thus use multiple 4D kernels, requiring a large number of parameters to learn. Following the work, recent methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61]</ref> also adopt 4D convolution in a similar manner. They commonly consume a high computational cost with a large number of parameters in the kernels and only consider translation in space. In contrast, we extends the idea of Hough matching <ref type="bibr" target="#b5">[6]</ref> for high-dimensional convolution and propose an interpretable and light-weight (semi-isotropic) high-dimensional kernel for visual correspondence. In doing so, it naturally generalizes the existing 4D convolution to higher-dimensional ones and achieves superior performance using only a single kernel per layer with a small number of parameters. The results reveal that the role of high-dimensional convolution on a correlation tensor for matching is to learn a reliable voting strategy rather than to capture diverse patterns in the correlation tensor.</p><p>Our contributions can be summarized as follows:</p><p>• We introduce a Hough transform perspective on convolutional matching and propose an effective geometric matching algorithm, CHM, which performs highdimensional Hough voting in a convolutional manner.</p><p>• We develop CHM into a trainable neural layer with a semi-isotropic high-dimensional kernel, which learns non-rigid matching with a small number of interpretable parameters.</p><p>• We propose the convolutional Hough matching network (CHMNet) that performs geometric matching in a translation and scaling space using 6D convolution.</p><p>• The proposed method sets a new state of the art on standard benchmarks for semantic visual correspondence, proving its robustness to challenging intra-class variations across images to match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Convolutional Hough Matching</head><p>In this section, we revisit the Hough matching method for visual correspondence and then propose its convolutional version as a high-dimension convolutional layer, which is readily trainable in neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hough matching &amp; its convolutional extension</head><p>The Hough transform is a powerful detection method for a geometric object, which exploits the duality between parts and parameters of the object <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>. It performs voting in a parameter space of the target object, called the Hough space, where votes from the object parts are accumulated to form local maxima in the space. The objects are then detected simply by identifying the positions of local maxima. The Hough matching method <ref type="bibr" target="#b5">[6]</ref>, inspired by the Hough transform, detects reliable correspondences by geometric voting from candidate matches. Given two images, it constructs the Hough space of parameters of geometric transformation between the two images and then accumulates votes of candidate matches for plausible transformation.</p><p>Let us assume a local region x on an image, that is represented by its geometric attributes, i.e., pose and shape. In principle x can be a form of any parameterization, but in this work we simply describe the region x by its center and scale. Now let us consider two images, I and I , and two sets of local regions, X and X , obtained from the two images, respectively. For any two regions (x, x ) ∈ X × X , a correlation function c computes a non-negative similarity c(x, x ) using appearance features of the regions. The main idea of Hough matching is to create the Hough space H, that is the space of all possible offsets h between two regions, i.e., translation and scaling, and accumulate votes from candidate matches onto the Hough space as</p><formula xml:id="formula_0">v(h) = (x,x )∈X ×X c(x, x )k iso ( (x − x) − h g ), (1)</formula><p>where · g represents a group-wise distance function that computes the distances separately for two groups, center and scale, i.e., x g = [ x xy ; x s ] (subscripts xy for center and s for scale) and k iso is a kernel function that computes similarity between the observed offset, x − x, and the given offset h in the Hough space. <ref type="bibr" target="#b0">1</ref> The kernel k iso is designed to assign a voting weight for each candidate match according to how close the offset induced by the match (x, x ) is to h; we use the group-wise distance to differentiate the effects of center and scale in the kernel. The resultant voting map v(h) over the Hough space H can be used to find reliable matches (x, x ) by suppressing spurious ones corresponding to relatively low voting scores v(x − x), e.g., updating the match score via <ref type="bibr" target="#b19">[20]</ref>. Despite its good empirical performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b57">58]</ref>, the global voting map v(h), which is shared for all candidate matches, is limited in the sense that it cannot capture the reliability of a specific candidate match. This global position-invariant Hough space makes the output less accurate and weak to background clutter, e.g., increasing the score of distant outliers that has a similar offset to that of dominant inliers.</p><formula xml:id="formula_1">c(x, x )v(x − x)</formula><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, in order to address the issue, we create a local and individual voting space for each candidate  </p><formula xml:id="formula_2">v(x, x , h) = (p,p )∈P(x)×P (x ) c(p, p )k iso ( (p − p) − h g ),<label>(2)</label></formula><p>where P(x) denotes the set of neighbor regions within the local window centered on x. Since this local voting space is now dedicated to (x, x ), we can simply assign a match score v for the candidate match by taking the vote value at the bin with offset zero:</p><formula xml:id="formula_3">v(x, x ) = (p,p )∈P(x)×P (x ) c(p, p )k iso ( p − p g ).<label>(3)</label></formula><p>With a slight abuse of notation, let us use k(z, z ) to represent the kernel value corresponding to two positions, z and z , each representing a local region in the parameter space of regions, i.e., 3D space of center and scale in our case. The equation above then can be generalized to a form of 6D convolution with an arbitrary kernel k:</p><formula xml:id="formula_4">c HM (x, x ) = (p,p )∈P(x)×P (x ) c(p, p )k(p − x, p − x ) = (c * k)(x, x ),<label>(4)</label></formula><p>which becomes equivalent to Eq. 3 when the group-wise isotropic kernel k iso is used. Note that this convolutional extension of Hough matching has a generic form; it reduces to a similar form of 4D convolutions in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61]</ref> when the Hough space is restricted to center translation, and generalizes to higher dimensions beyond 6D when additional transformation dimensions is introduced such as rotation, shear, and others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convolutional Hough matching layer</head><p>We design the convolutional Hough matching (CHM) as a learnable convolution layer:</p><formula xml:id="formula_5">c HM (x, x ; k, b) = b + (c * k)(x, x ),<label>(5)</label></formula><p>where b is a bias term for the layer and k represents a kernel with a specific type of weight sharing. The group-wise isotropic kernel k iso , which is directly derived from Hough matching, can be implemented by weight sharing among parameters with the same offset |z − z | in k(z, z ). While it is a reasonable choice, the fully isotropic kernel assigns the same importance to the matches of the same offset regardless of their distances from the kernel position (x, x ). It may be an excessive constraint in the sense that the distance of an object from the center of focus is likely to be relevant to the importance. We thus relax the isotropy and propose the positionsensitive isotropic kernel k psi ( p − p g ; p − x g , p − x g ) that differentiates the distances from the kernel position, p − x g and p − x g . The kernel k psi is implemented by sharing parameters whose triplets,</p><formula xml:id="formula_6">( p − p g , p − x g , p − x g ), are the same.</formula><p>The CHM layer is compatible with any neural network layer that computes correlations between images, and can be stacked multiple times to improve the performance. As a result of substantial parameter sharing, the 6D kernels, k 6D iso and k 6D psi in R H k ×W k ×S k ×H k ×W k ×S k , contain only a small number of parameters, thus making CHM resistant to overfitting in training; e.g., the kernels with H k = W k = 5 and S k = 3 contains only 45 and 220 parameters, respectively, while the full kernel has 5,625. More importantly, the perspective of Hough matching on convolution provides the interpretability of the learned kernel: each element in the kernel is a voting weight of the corresponding neighbor match in the local offset space. Based on this perspective,</p><formula xml:id="formula_7">Fig- ure 3 visualizes the kernel k 6D psi ∈ R H k ×W k ×S k ×H k ×W k ×S k of size H k = W k = 5 and S k = 3 trained in our experi- ment.</formula><p>For the ease of visualizing 6D tensor, we decompose it into multiple (four in case of k 6D psi ) 4D tensors in which each of the map shows parameter values of the kernel with the same offset, where the arrows represent the offset vectors relative to the kernel position (x, x ), and the circles mean zero offset. The maps reveal that weights for matches with smaller offsets and closer distance are learned to be higher (darker), which appears to be a reasonable voting strategy. For more information, refer to our Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Convolutional Hough Matching Networks</head><p>Based on CHM, we develop a family of image matching models, dubbed Convolutional Hough Matching Networks (CHMNet), which consists of three parts: (1) highdimensional correlation computation, (2) convolutional Hough matching, and (3) flow formation (and keypoint transfer). <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the overall architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">High-dimensional correlation computation</head><p>Following other recent methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref>, we also use as a CNN feature extractor pretrained on ImageNet classification <ref type="bibr" target="#b11">[12]</ref>. Given an input image I, the feature extractor outputs a feature map in R C×H×W . We construct feature maps of multiple scales {F s } S s=1 by resizing the output for S − 1 times by the scaling factor of √ 2, followed by 3 × 3 conv layers with parameters {θ s } S s=1 , reducing channel dimensions of input feature map by 1/ρ. The S different conv layers learn to capture effective semantic information of receptive fields with different scales for the subsequent multi-scale (6D) correlation computation. The same is done for {F s } S s=1 given image I . We set S = 3, i.e., {1/ √ 2, 1, √ 2}, and ρ = 4 in our experiments. Given a set of feature pairs from multiple scales {(F s , F s )} S s=1 , we compute all possible 4D correlation tensors placed on the S × S grid:</p><formula xml:id="formula_8">C (0) mn (x m , x n ) = ReLU F m (x m ) · F n (x n ) F m (x n ) F n (x n ) ,<label>(6)</label></formula><p>where x m ∈ X m and x n ∈ X n are spatial positions of feature map at scale m and n, respectively, and ReLU clamps negative correlation scores to zero. To process it in the subsequent 6D CHM layer, we interpolate each 4D correlation C (0) ij to have the same spatial size to build 6D correlation tensor C (1) ∈ R H×W ×S×H×W ×S such that C</p><formula xml:id="formula_9">::i::j = ζ 1 (C (0) ij ) where ζ 1 (·) is a function that interpo- lates input 4D tensor to the size H × W × H × W .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Convolutional Hough Matching</head><p>A CHM layer takes the 6D correlation tensor C <ref type="bibr" target="#b0">(1)</ref> to perform convolutional Hough voting in the space of translation and scaling:</p><formula xml:id="formula_10">C (2) = CHM(C (1) ; k 6D psi ), where k 6D psi ∈ R H k ×W k ×S k ×H k ×W k ×S k is a 6D position-sensitive Predicted keypoints on ′ { መ ′ } =1 6D correlation (1) ∈ ℝ × × × × ×</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-dimensional correlation computation Convolutional Hough Matching Keypoint transfer &amp; Loss</head><p>Conv <ref type="formula" target="#formula_3">( 3 )</ref> Conv <ref type="formula" target="#formula_2">( 2 )</ref> Conv isotropic kernel. In our experiments, we set H k = W k = 5 and S k = 3 with stride 1 for all dimensions and use zero-padding to the input to retain the same size at the output. We then perform max-pooling on C <ref type="bibr" target="#b1">(2)</ref> to select the most dominant vote among candidate match scores in the scale space, reducing the tensor dimension down to 4D: C</p><formula xml:id="formula_11">( 1 ) Copy &amp; interpolate Copy &amp; interpolate ′ Conv ( 3 ) Conv ( 2 ) Conv ( 1 ) 0 ( psi 6D ) CHM 0 ( psi 4D ) CHM Non-linearity + Maxpool + Upsample Predicted correlation ∈ ℝ ഥ× ഥ × ഥ× ഥ Flow formation Keypoint transfer Keypoints on { } =1 Keypoints on ′ { ′ } =1 Training objective ℒ { ′ } =1 { } =1 0 Scale-space maxpool Upsample 4D Sigmoid</formula><formula xml:id="formula_12">(3) ijkl = max m,n C (2) ijmkln . We proceed another CHM with a 4D kernel k 4D psi ∈ R H k ×W k ×H k ×W k : C = CHM(ζ 2 (σ(C (3) )); k 4D psi ), where σ(·)</formula><p>is the sigmoid activation function and ζ 2 (·) is the upsampling function that resizes input 4D tensor to the size ofH ×W ×H ×W for fine-grained localization. We setH = 2H andW = 2W in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Flow formation &amp; keypoint transfer</head><p>Flow formation. The output C can easily be transformed into a dense flow field by applying kernel soft-argmax <ref type="bibr" target="#b37">[38]</ref>. We first normalize the raw correlation scores with softmax:</p><formula xml:id="formula_13">C = exp (G p kl C ijkl ) (k ,l )∈H×W exp (G p k l C ijk l ) ,<label>(7)</label></formula><p>where and G p ∈ RH ×W is 2-dimensional Gaussian kernel centered on p = arg max k,l C ijkl . Using the estimated probability mapĈ, we then transfer all the coordinates on dense regular grid P ∈ RH ×W ×2 of image I to obtain their corresponding coordinatesP ∈ RH ×W ×2 on image I :</p><formula xml:id="formula_14">P ij: = (k,l)∈H×WĈ ijkl P kl:</formula><p>. We now can construct a dense flow field at sub-pixel level using the set of estimated matches (P,P ).</p><p>Keypoint transfer. As in <ref type="bibr" target="#b37">[38]</ref>, one simplest way of assigning a matchk to some keypoint k = (x k , y k ) is to pick a single, discrete sample of a transferred coordinate such that k =P y k x k . However, this may cause mis-localized keypoints as the discrete sampling under sub-pixel level hinders fine-grained localization. To this end, we define a soft sampler W (k) ∈ RH ×W for given keypoint k = (x k , y k ) as follows</p><formula xml:id="formula_15">W (k) ij = max (0, τ − (x k − j) 2 + (y k − i) 2 ) i j max (0, τ − (x k − j ) 2 + (y k − i ) 2 ) ,<label>(8)</label></formula><formula xml:id="formula_16">such that ij W (k) ij = 1 where τ is a distance thresh- old. We assign a match to the keypoint k byk = (i,j)∈H×WP ij: W (k)</formula><p>ij . The soft sampler W (k) effectively samples each transferred keypointP ij by giving weights inversely proportional to the distance to k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training objective</head><p>We assume that keypoint match annotations are given for each training image pair, as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> k m − k m , which minimizes the average Euclidean distance between the predicted keypoints and the ground-truth ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>In this section we evaluate the proposed method, compare it with recent state of the arts, and discuss the results. Implementation detail. For the feature extractor network, we employ ResNet-101 <ref type="bibr" target="#b20">[21]</ref>, truncated after the conv4 23 layer, pre-trained on ImageNet <ref type="bibr" target="#b11">[12]</ref>. Both input and output channel sizes of all the CHM layers are set to 1. We set spatial size of the input image to 240 × 240, thus having H = W = 15 andH =W = 30. Due to parameter sharing structure of k * psi and k * iso , magnitudes of the loss gradient with respect to the shared weights are unevenly distributed during training time. To resolve the numerical instability, the shared weights are normalized before the con-   <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. Numbers in bold indicate the best performance and underlined ones are the second best. Models with an asterisk ( * ) are retrained using keypoint annotations (strong supervision) from <ref type="bibr" target="#b39">[40]</ref>. The first column shows supervisory signals used in training: image-level labels (I), and keypoint matches (K). Superscript † denotes inference time using our implementation of nD conv. volution by dividing by the number of times being shared. The network is implemented in PyTorch <ref type="bibr" target="#b48">[49]</ref> and optimized using Adam <ref type="bibr" target="#b32">[33]</ref> with a learning rate of 1e-3. We finetune the backbone network by setting its learning rate 100 times smaller than CHM layers, e.g., 1e-5.</p><p>Datasets. We evaluate the proposed network on three standard benchmark datasets of semantic correspondence: SPair-71k <ref type="bibr" target="#b45">[46]</ref>, PF-PASCAL <ref type="bibr" target="#b18">[19]</ref>, and PF-WILLOW <ref type="bibr" target="#b17">[18]</ref>. Evaluation metric. We adopt the standard evaluation metric, percentage of correct keypoints (PCK), for the evaluation. Given a set of predicted and ground-truth key-</p><formula xml:id="formula_17">point pairs K = {(k m , k m )} M m=1 , PCK is measured by PCK(K) = 1 M M m=1 1[ k m − k m ≤ α τ · max (w τ , h τ )]</formula><p>where w τ and h τ are the width and height of either an entire image or an object bounding box, e.g., τ ∈ {img, bbox}, and α τ is a tolerance factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results and analysis</head><p>On the SPair-71k dataset, following <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref>, we evaluate two versions for each model: a finetuned model (F), which is trained on SPair-71k, and a transferred model (T), which is trained on PF-PASCAL. On PF-PASCAL and PF-WILLOW, following the common evaluation protocol <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>, our network is trained on the training split of PF-PASCAL <ref type="bibr" target="#b18">[19]</ref> and evaluated on the test splits of PF-PASCAL and PF-WILLOW. We use the same training, validation, and test splits of PF-PASCAL used in <ref type="bibr" target="#b19">[20]</ref>. The quantitative results are summarized in Tab. 1; we note different levels of supervision for each method in the first column to ensure fair comparison. The proposed model finetuned on SPair-71k (F) clearly surpass current state of the art by a significant margin, outperforming <ref type="bibr" target="#b46">[47]</ref> by 9%p of PCK (α bbox = 0.1), i.e., 24.1% relative improvement. On PF-PASCAL, our model achieves 4.4%p and 0.9%p improvement with α img ∈ {0.05, 0.1}. Robust performance on SPair-71k (T) and PF-WILLOW verifies reliable transferability of our model. <ref type="figure" target="#fig_7">Figure 6</ref> visualizes example qualitative results on SPair-71k.</p><p>FLOPs, running time, and memory. We collect publicly available codes of some recent methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref> to measure their FLOPs, inference time 2 , and memory footprint and compare them with ours in Tab. 1. Although the proposed method demands larger memory than some 4D conv based models <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b53">54]</ref>, smaller channel sizes of CHM (6D) layers ({1,1} vs. {16,16,1}) provide noticeable efficiency in terms of GFLOPs (19.6 vs. 44.9). To achieve faster inference time, we further improve the original implementation of 4D conv <ref type="bibr" target="#b53">[54]</ref> and develop an efficient nD conv which enables real-time inference (54ms) without increasing FLOPs and memory. See Appendix B for details on our implementation of nD convolution.</p><p>Robustness to background clutter. Recent methods for semantic correspondence <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref> predict matching scores for all candidate matches but rarely evaluate their robustness to background clutters. Here, we compare some recent methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref> and ours in terms of robustness to background clutter based on the predicted matching scores. Each method, however, exploits its correlation tensor differently from others with its own flow formation (keypoint transfer) scheme. Therefore, given all possible candidate matches in correlation tensor, simply defining matches with top-k scores as positive matches may yield biased estimates. To ensure fair comparison, for each model, we define a set of coordinates on a regular grid on the input pair of images and assign their best matches using its own keypoint transfer method, thus providing the same number of (fairly collected) candidate matches to every model that we compare. For each candidate match, we define its match score as a score nearest to spatial position in the correlation tensor. Given topk matches according to their matching scores, we define true positives (TPs) as matches falling inside object segmentation masks (bounding box) <ref type="bibr" target="#b2">3</ref> and false positives (FPs) as those lying outside object masks (boxes). Precision and recall are measured by NTP NTP+NFP and NTP N mask , respectively, where N TP and N FP are respectively the number of TPs and FPs while N mask is the number of all candidate matches that fall in the object segmentation masks. In defining TPs and FPs, we use masks and boxes only due to the absence of dense flow annotation in SPair-71k and PF-PASCAL, but we find that they are good approximation enough to distinguish inliers from outliers in our experimental setup. <ref type="figure" target="#fig_6">Figure 5</ref> plots precision-recall curves for the recent methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref> and ours. The proposed method clearly outperforms other methods, indicating our model effectively discriminates between semantic parts and background clutters as seen in the last row of <ref type="figure" target="#fig_9">Fig. 8</ref> which visualizes sample pairs with top 300 confident matches. When CHM is either removed (w/o CHM) or replaced with global matching module (CHM − → RHM), predicted matches become unreliable, being mostly scattered on the background and even hardly regularized. For our model evaluated on SPair-71k, precision and recall have inverse relationship in most cases. Although initial growth in our PR curves on <ref type="bibr" target="#b2">3</ref> We use object seg. masks and bounding boxes for SPair-71k and PF-PASCAL respectively due to absence of mask annotation in PF-PASCAL.   SPair-71k indicates that some true matches have in fact low match scores, it still surpasses the other models, revealing the reliability of our approach under large variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study and analysis</head><p>Analyses on CHM kernel. We conduct ablation study on CHM kernel by replacing position-sensitive isotropic kernels with k nD full 4 and full isotropic ones k nD iso . For the ease of notation, we denote by k 6D-4D psi a model with two CHM layers whose kernels are k 6D psi and k 4D psi . <ref type="table" target="#tab_4">Table 2</ref> shows average PCK, its standard deviations, parameter sizes, FLOPs, and average inference time of our model with different kernels over five runs. Despite a huge difference in the number of parameters (110 vs. 1,250), the proposed semi-isotropic kernel k 4D-4D psi outperforms k 4D-4D full on Spair-71k (44.5 vs. 43.9) and extending its voting space to 6D, e.g., k 6D-4D psi , further improves PCK to 46.4 on SPair-71k, which clearly shows efficacy of 6D convolution in scale-space <ref type="bibr" target="#b4">5</ref> . The comparable performance of k 6D-4D iso to k 6D-4D full reveals that full isotropic parameter sharing can also be a reasonable choice for reducing the large capacity of k 6D-4D full . In <ref type="figure" target="#fig_8">Figure 7</ref>, we also plot frequencies over the maxpooled positions in scale-space after 6D CHM layer (k 6D-4D psi ). The maximum votes on both PF-PASCAL and PF-WILLOW are mostly concentrated on the center scale whereas they are distributed over different scales on SPair-71k; this is a <ref type="bibr" target="#b3">4</ref> Note k nD full is a n-dimensional kernel without any parameter sharing. The number of parameters in k nD full is proportional to k n . <ref type="bibr" target="#b4">5</ref> To verify the efficacy of the proposed kernel even with sparse match information, we further limit the set of potential matches in C (0) using K nearest neighbors without using MinkowskiEngine <ref type="bibr" target="#b9">[10]</ref> as it does not provide high-dim. kernel customization. As seen in shaded row in Tab. 2, our model with the sparse correlation is comparably effective to k 6D-4D psi , which is consistent to the results of <ref type="bibr" target="#b52">[53]</ref>. We set K = 10 in our experiment.   <ref type="table">Table 3</ref>: Ablation study of core modules in our model. reasonable voting strategy as objects in PF-PASCAL and PF-WILLOW hardly vary in scale while those in SPair-71k show large variations in both scale and view-point. Ablation study on matching modules. We analyze the effect of CHM, by either removing or replacing them with the matching module of <ref type="bibr" target="#b44">[45]</ref>. <ref type="figure" target="#fig_9">Figure 8</ref> and <ref type="table">Table 3</ref> summarize qualitative and quantitative results, respectively. The output of global offset voting (CHM − → RHM) includes many outliers from the background, showing its weakness to the background clutter. Without the last CHM layer (w/o last CHM), the model fails to effectively refine upsampled correlation scores. The model prediction is severely damaged without any matching modules (w/o CHM) as seen in second row of <ref type="figure" target="#fig_9">Fig. 8</ref>. For keypoint transfer, kernel G and soft sampler A (k) help our model find reliable matches by suppressing noisy match scores in C and effectively aggregating neighborhood transfers, respectively. Effect of channel size. To study the effect of channel size, we train our model 6 using three different kinds of kernels (k 4D-4D psi , k 4D-4D iso , and k 4D-4D full ) with different channel sizes, i.e., different number of kernels. <ref type="table">Table 9</ref> summarizes the results, showing that increasing the channel size rarely brings performance gain and typically harms the quality of prediction for kernels k 4D-4D psi and k 4D-4D full . We train the models on the training split of PF-PASCAL and evaluate on test splits of PF-PASCAL and SPair-71k. For k 4D-4D iso , although increasing channel size improves performance up to certain amount due to its small capacity, it eventually exhibits sim- <ref type="bibr" target="#b5">6</ref> We use the models in the middle section of Tab. 2, e.g., k 4D-4D * ilar patterns to other kernels after all. These experiments imply that the high-dimensional convolution on a correlation tensor may play a different role from 2D convolution on an image feature tensor; the role of convolutional matching is to learn a reliable voting strategy rather than to capture diverse patterns in the correlation tensor. This is consistent with the Hough matching perspective, but previous 4D convolution methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61]</ref> with a different perspective commonly use multiple full kernels (k 4D full ) for layers. To verify our result, we have conducted a similar experiment using the model of <ref type="bibr" target="#b53">[54]</ref> and obtained the consistent result; the original model, which uses channel sizes of {16, 16, 1} for three layers of 4D convolution, achieves 76.2% PCK on our machine while the model with reduced channels of {1, 1, 1} achieves 76.4% PCK. Note that in terms of the number of parameters in a layer, our CHM layers (k 6D-4D psi ) have 247 ∼ 654 times smaller number of parameters than the 4D convolution layers used in previous methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61]</ref>. This light-weight layer design is particularly important in practice, since the use of multiple channels, i.e. kernels, for high-dim convolution quickly increases the cost both in computation and memory.</p><p>For additional results and analyses, we refer the readers to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have introduced the convolutional Hough matching (CHM) and proposed the powerful matching model, CHM-Net, that leverages CHM in a high-dimensional geometric transformation space for establishing reliable visual correspondence. The extensive experiments on several standard benchmarks for semantic visual correspondence demonstrate the benefits of our approach. In particular, our method generalizes existing 4D convolutions and also provides the perspective of Hough transform for geometric matching with interpretable high-dimension kernels. We believe further research on this direction can benefit a wide range of other problems related to correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Additional results and analyses</head><p>Analysis on scale-space maxpool. To further analyze the results in <ref type="figure" target="#fig_8">Fig. 7</ref>, we visualize maxpooled positions of predicted matches on sample pairs of SPair-71k <ref type="bibr" target="#b45">[46]</ref>, PF-PASCAL <ref type="bibr" target="#b18">[19]</ref>, and PF-WILLOW <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure" target="#fig_3">Figure A3</ref> shows the results and describes how we visualize them. Due to large scale-variations in pairs of SPair-71k, our model collects winners of scale-space vote, i.e., CHM(·; k 6D psi ), from diverse positions in scale-space. In contrary, objects in PF-PASCAL and PF-WILLOW exhibit relatively small scalevariations, thus encouraging our model to collect winners of the vote mostly from the original scales. We observe that the maxpooled positions typically depend on scales of object's parts as seen in <ref type="figure" target="#fig_3">Fig. A3</ref>.</p><p>Learned CHM kernels. <ref type="figure" target="#fig_4">Figure A4</ref> describes how we visualized <ref type="figure" target="#fig_3">Fig. 3</ref>. For straightforward visualization of highdimensional geometry on 2D plane, we use tesseracts and their arrangement on a 2D grid to represent 4D and 6D tensors respectively. Learned kernels of k 6D-4D psi (ours), k 6D-4D full , and k 6D-4D iso are respectively visualized in Figs A5, A6, and A7.</p><p>Interestingly, the weight patterns of kernels k 6D-4D psi and k 6D-4D full are remarkably similar; the weights for matches with large offsets and closer distance are learned to be higher (darker) while those with small offsets and far distance are learned to be lower (brighter). Moreover, learned weight patterns of 4D maps in second, fourth, sixth, and eighth rows of k 6D full in <ref type="figure" target="#fig_7">Fig. A6</ref> are noticeably similar to each other. We also observe that patterns in first and last rows, and patterns in third and seventh rows of k 6D</p><p>full are similar to each other as well. In contrast, k nD iso is unable to express diverse weight patterns due to its parameter-sharing constraint that enforces full isotropy. This observation reveals that our kernel k nD psi in CHMNet clearly benefits from its reasonable parameter-sharing strategy, in terms of both efficiency and accuracy as demonstrated in Tab. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. More implementation details</head><p>Coordinate normalization. Following the work of <ref type="bibr" target="#b37">[38]</ref>, we use height and width normalized coordinates to ensure numerical stability of loss gradients such that</p><formula xml:id="formula_18">−1 −1 ≤ P ij: ≤ 1 1 ,<label>(9)</label></formula><p>where P is a set of coordinates on a dense regular grid used for flow formation. This normalization gives spatial bounds [−1, 1] to the intermediate output coordinatesP , k, andk .</p><p>Hyperparameters. During training, the learning rates of CHM layers and backbone feature extractor are set to 1e-3 and 1e-5, respectively with batch size of 16. The distance threshold τ in Eqn.11 is set to 0.1. We set the standard deviation of Gaussian kernel G ∈ R 30×30 to 17.</p><p>Implmentation of high-dimensional convolution. As PyTorch <ref type="bibr" target="#b48">[49]</ref> supports only upto 3D convolution, we must manually implement (dense) high-dimensional convolutions. We first demonstrate the original implementation of 4D convolution <ref type="bibr" target="#b53">[54]</ref>, and how we efficiently reimplemented the same 4D convolution and improved it for high-dimensional convolution. Given B correlation tensors in a minibatch 7 C ∈ R B×H×W ×H ×W and a 4D kernel K ∈ R k×k×k×k , we denote each 4D piece of C by C i := C :i::: ∈ R B×W ×H ×W and each 3D tensor in K by K i := K i::: ∈ R k×k×k . The work of <ref type="bibr" target="#b53">[54]</ref> implements 4D convolution f 4D by performing H times of following operation:</p><formula xml:id="formula_19">f 4D (C) i =f 3D (C i−p , K 1 ) + f 3D (C i−p+1 , K 2 ) (10) + ... + f 3D (C i+p , K k ) + b</formula><p>where f 3D is a function that performs 3D convolution on C * across the batch given 3D kernel K * , p is a padding size, and b is a bias term. We set the padding size p = k/2 in our experiment. As a result, f 4D in Equation 10 performs kH times of 3D convolutions.</p><p>In this work, we implement a fast version of the 4D convolution which performs significantly smaller number of 3D convolutions compared to the original one. We first reshape the correlation tensor of a minibatch as C ∈ R BH×W ×H ×W and make k copies of it. Using the 3D kernels {K i } k i=1 , we apply 3D convolution f 3D on each copy and denote its output byĈ i = f 3D (C, K i ). We again reshape the tensors {Ĉ i } k i=1 to have size B × H × W × H × W and perform the following:</p><formula xml:id="formula_20">f 4D (C) i =Ĉ 1 i−p +Ĉ 2 i−p+1 + ... +Ĉ k i+p + b.<label>(11)</label></formula><p>Note that the number of 3D convolution operations in our implementation is H times smaller compared to that in the original implementation <ref type="bibr" target="#b53">[54]</ref> (k (ours) vs. kH <ref type="bibr" target="#b53">[54]</ref>). Given a 4D correlation tensor C ∈ R 16×30×30×30×30 , our implementation takes about 0.7 ms while the implementation of <ref type="bibr" target="#b53">[54]</ref> takes about 150 ms on a machine with an Intel i7-7820X CPU and an NVIDIA Titan-XP GPU. A highdimensional convolutions (≥ 5D) are implemented in a similar manner; our implementation of 6D convolution with input in R 16×15×15×3×15×15×3 takes about 180 ms on the same machine. We also manually implement parameter-sharing kernels k nD psi and k nD iso : Before applying convolution, we instantiate high-dimensional kernel filled with zeros and assign parameters to their corresponding indices by addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Qualitative results</head><p>The proposed convolutional Hough matching allows a flexible non-rigid matching and even multiple matching surfaces or objects. To demonstrate the ability of the CHM in matching multiple objects, we visualize some qualitative results of our method (CHMNet) on some toy images with multiple instances in <ref type="figure" target="#fig_0">Fig. A1</ref>. Top 300 confident matches predicted by our model (CHMNet) are mostly on common instances in the input pairs of images. Replacing convolutional Hough matching (learnable local voting layer) to regularized Hough matching <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b44">45]</ref> (non-learnable global voting layer) severely damages the model predictions; the confident matches become noisy and unreliable, mostly being scattered on background. Without CHM layers, the model fails to localize common instances in the images. <ref type="figure" target="#fig_9">Figure A8</ref> also visualizes sample pairs of PF-PASCAL with top 300 confident matches predicted by each model. Our model effectively discriminates between semantic parts and background clutters as seen in the second row of <ref type="figure" target="#fig_9">Fig. A8</ref>. The absence of CHM layers severely harms the model predictions as seen in the third and last rows of <ref type="figure" target="#fig_9">Fig. A8</ref>. These results reveal that the proposed CHM layers effectively find reliable matches between common instances across different images while being robust to background clutter even in presence of multiple instances.</p><p>The qualitative comparisons to the recent semantic correspondence approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref> are visualized in Figs. A9, A10, and A11. We warp source images to target images using predicted correspondences: Given source keypoints, each model predicts their corresponding positions in target image by using its own keypoint transfer scheme, e.g., nearest neighbor assignment <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref>, hardassignment by taking mostly likely match <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54]</ref> or soft argmax (ours). Using the keypoint correspondences, we compute thin plate spline (TPS) transformation parameters <ref type="bibr" target="#b12">[13]</ref> and apply the transformation to source image to align target image. <ref type="figure" target="#fig_10">Figure A9</ref> shows the results on PF-PASCAL. <ref type="figure" target="#fig_0">Figures A10 and A11</ref> show the results on SPair-71k. Our model effectively warp the source images to align the source objects to the target ones based on predicted correspondences even in presence of large view-point, illumination, and scale differences. Representative failure cases of our model are shown in <ref type="figure" target="#fig_2">Fig. A2</ref>.  <ref type="figure" target="#fig_0">Figure A1</ref>: Multiple instance matching with top 300 confident matches. <ref type="figure" target="#fig_2">Figure A2</ref>: Failure cases on SPair-71k <ref type="bibr" target="#b45">[46]</ref> dataset in presence of extreme changes in view-point, large intra-class variation, and deformation. We show the keypoints of ground-truth correspondences in circles and the predicted keypoints in crosses with a line that depicts matching error. 2) in scale space. If the size of one circle is medium and that of the other is small, its match score is from position (1, 1/ √ 2) and so on. We show ground-truth target keypoints in crosses with a line that depicts matching error. Best viewed in electronic form.    <ref type="figure" target="#fig_7">Figure A6</ref>: Learned k 6D-4D full . The 6D kernel (k 6D full ) consists of nine 4D kernels each of which has 625 parameters.  <ref type="figure" target="#fig_8">Figure A7</ref>: Learned k 6D-4D iso . The 6D kernel (k 6D iso ) consists of three 4D kernels each of which has 15 parameters.   <ref type="bibr" target="#b46">[47]</ref>, (e) ANC-Net <ref type="bibr" target="#b39">[40]</ref>, (f) HPF <ref type="bibr" target="#b44">[45]</ref>, (g) DCCNet <ref type="bibr" target="#b25">[26]</ref>, and (h) NCNet <ref type="bibr" target="#b53">[54]</ref>.  <ref type="bibr" target="#b46">[47]</ref>, (e) ANC-Net <ref type="bibr" target="#b39">[40]</ref>, (f) HPF <ref type="bibr" target="#b44">[45]</ref>, (g) DCCNet <ref type="bibr" target="#b25">[26]</ref>, and (h) NCNet <ref type="bibr" target="#b53">[54]</ref>.  <ref type="bibr" target="#b46">[47]</ref>, (e) ANC-Net <ref type="bibr" target="#b39">[40]</ref>, (f) HPF <ref type="bibr" target="#b44">[45]</ref>, (g) DCCNet <ref type="bibr" target="#b25">[26]</ref>, and (h) NCNet <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PF-PASCAL &amp; PF-WILLOW SPair-71k</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Convolutional Hough matching (CHM) establishes reliable correspondences across images by performing position-aware Hough voting in a high-dimensional geometric transformation space, e.g., translation and scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Convolutional Hough matching that carries out geometric voting in 6D space, e.g., translation and scale.match (x, x ) by introducing local windows around the regions, x and x :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>k× k× k× k× k× k ( k = k = 5 and k = 3) Visualization of learned CHM kernel (k 6D psi ). Refer Appendix A for the visualization method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Overall architecture of the proposed method that performs (learnable) geometric voting in high-dimensional spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>; each image pair is annotated with a set of coordinate pairs M = {(k m , k m )} M m=1 , where M is the number of annotations. Following the aforementioned keypoint transfer scheme, we obtain a set of predicted and ground-truth keypoint pairs on image I : {(k m , k m )} M m=1 by assigning a matchk m to each k m . Our objective in training is formulated as L = 1 M M m=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>PR curves on SPair-71k (top) and PF-PASCAL (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results on SPair-71k dataset. Our model predicts reliable matches under deformations, and large changes in view-point and scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Frequencies over the maxpooled positions in scale-space on SPair-71k, PF-PASCAL, and PF-WILLOW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Ablation study on matching modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>PCK performance on SPair-71k and PF-PASCAL with different channel sizes of 1, 2, 4, 8, and 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( 2 )Figure A3 :</head><label>2A3</label><figDesc>∈ ℝ × × × × × A match ( , , , ): where ( , ) : position nearest to source keypoint &amp; ( , ): position nearest to predicted keypoint Scale-space maxpool: Visualization of maxpooled position in scale-space. In each image pair, we show source keypoints (given) and their corresponding target keypoints (predicted) in circles in left and right images respectively. The size (large, medium, and small) of each circle indicates maxpooled position in scale-space. If both circles of a match are large, its match score is pooled from position ( √ 2, √</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2 ′Figure A4 :</head><label>2A4</label><figDesc>Description of visualizing learned weights of high-dimensional kernels: (Left) The arrows represent the offset vectors relative to the kernel position (x, x ), and the circles mean zero offset. (Right) For straightforward visualization, we decompose a high-dimensional kernel into multiple 4D kernels (tesseracts) and visualize learned weights of each 4D kernel as a set of maps consisting of offset vectors. Darker offsets mean larger weights while brighter ones mean smaller weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure A8 :</head><label>A8</label><figDesc>Sample pairs with top 300 confident matches. TP and FP matches are colored in blue and red respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(a) Source image (b) Target image (c) CHMNet (ours) (f) HPF (g) DCCNet (h) NCNet (d) DHPF (e) ANC-Net Figure A9: Example results on PF-PASCAL [19]: (a) source image, (b) target image (c) CHMNet (ours), (d) DHPF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure A10 :</head><label>A10</label><figDesc>Example results with large view-point differences from SPair-71k [46]: (a) source image, (b) target image (c) CHMNet (ours), (d) DHPF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure A11 :</head><label>A11</label><figDesc>Example results with large illumination and scale differences, and truncation from SPair-71k [46]: (a) source image, (b) target image (c) CHMNet (ours), (d) DHPF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance on standard benchmarks in accuracy, FLOPs, per-pair inference time, and memory footprint. Subscripts denote backbone networks. Some results are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Ablation study of CHM kernels over multiple runs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Learned k 6D-4D psi used in CHMNet. The 6D kernel (k 6Dpsi ) consists of four 4D kernels each of which has 55 parameters.</figDesc><table><row><cell>psi 6 ∈ ℝ 3×3×5×5×5×5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>psi 4 ∈ ℝ 5×5×5×5</cell><cell>Offset length 0</cell><cell>Offset length 1</cell><cell>Offset length 2</cell><cell>Offset length 2</cell><cell>Offset length 5</cell><cell>Offset length 2 2</cell><cell>Offset length 3</cell><cell>Offset length 10</cell><cell>Offset length 13</cell><cell>Offset length 3 2</cell><cell>Offset length 4</cell><cell>Offset length 17</cell><cell>Offset length 2 5</cell><cell>Offset length 5</cell><cell>Offset length 4 2</cell></row><row><cell cols="2">full 6 ∈ ℝ 3×3×5×5×5×5 Figure A5: Offset length 0 full 4 ∈ ℝ 5×5×5×5</cell><cell>Offset length 1</cell><cell>Offset length 2</cell><cell>Offset length 2</cell><cell>Offset length 5</cell><cell>Offset length 2 2</cell><cell>Offset length 3</cell><cell>Offset length 10</cell><cell>Offset length 13</cell><cell>Offset length 3 2</cell><cell>Offset length 4</cell><cell>Offset length 17</cell><cell>Offset length 2 5</cell><cell>Offset length 5</cell><cell>Offset length 4 2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the kernel function, previous work uses a form of discretized Gaussian<ref type="bibr" target="#b5">[6]</ref> or Dirac delta<ref type="bibr" target="#b19">[20]</ref> without learning the kernel parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Some inference time results are retrieved from<ref type="bibr" target="#b46">[47]</ref>, which is measured on a machine with an Intel i7-7820X and an NVIDIA Titan-XP. For fair comparison, inference time and memory footprint of all the methods are measured on a machine with the same CPU and GPU and includes all the pipelines of a model: from feature extraction to keypoint prediction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We omit channel sizes of the tensor for brevity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>This work was supported by Samsung Advanced Institute of Technology (SAIT), the NRF grants (NRF-2017R1E1A1A01077999, NRF-2021R1A2C3012728), and the IITP grant (No.2019-0-01906, AI Graduate School Program -POSTECH) funded by Ministry of Science and ICT, Korea.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalizing the hough transform to detect arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dense semantic correspondence where every pixel is a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilton</forename><surname>Bristow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust feature matching with alternate hough and inverted hough transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Yu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning graphs to match</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reweighted random walks for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Progressive graph matching: Making a move of graphs via probabilistic voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximate thin plate spline mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep graph matching consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Computer Vision: A Modern Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Class-specific hough forests for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Rafael S Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Method and means for recognizing complex patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<biblScope unit="volume">3069654</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">U.S. Patent</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vehicle logo retrieval based on hough transform and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Yujian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic context correspondence network for semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaiyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parn: Pyramidal affine regression networks for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Sangryul Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Guided semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Sangryul Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deformable spatial pyramid matching for fast dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent transformer networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dctm: Discrete-continuous transformation matching for semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene cut: Class-specific object detection and segmentation in 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukta</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Orientation invariant 3d object classification using hough transform based methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukta</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Workshop on 3D Object Retrieval</title>
		<meeting>the ACM Workshop on 3D Object Retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and tracking in video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sfnet: Learning object-aware semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghyup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interleaved object categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Correspondence networks with adaptive neighbourhood consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><forename type="middle">W</forename><surname>Costain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Howard-Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic correspondence as an optimal transport problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Hough-cnn: Deep learning for segmentation of deep brain regions in mri and ultrasound. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Kroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annika</forename><surname>Plate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rozanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliana</forename><surname>Maiostre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birgit</forename><surname>Ertl-Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Bötzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">SPair-71k: A large-scale benchmark for semantic correspon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10543</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">dence. arXiv prepreint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to compose hypercolumns for visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semi-convolutional operators for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019-03" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-toend weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient neighbourhood consensus networks via submanifold sparse convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep graph matching via blackbox differentiation of combinatorial solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rolínek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Swoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Zietlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vít</forename><surname>Musil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Martius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attentive semantic alignment with offset-aware correlation kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deunsol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">What if we do not have multiple videos of the same action? -video action localization using web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Depth-encoded hough voting for joint object detection and shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint recovery of dense correspondence and cosegmentation in two images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">GLU-Net: Global-local universal network for dense flow and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Object-aware dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leiting</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
