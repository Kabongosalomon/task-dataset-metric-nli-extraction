<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
						</author>
						<title level="a" type="main">Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Visual Learning Lab Heidelberg University (HCI/IWR)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic RANSAC algorithm from robust optimization. NG-RANSAC uses prior information to improve model hypothesis search, increasing the chance of finding outlier-free minimal sets. Previous works use heuristic side information like hand-crafted descriptor distance to guide hypothesis search. In contrast, we learn hypothesis search in a principled fashion that lets us optimize an arbitrary task loss during training, leading to large improvements on classic computer vision tasks. We present two further extensions to NG-RANSAC. Firstly, using the inlier count itself as training signal allows us to train neural guidance in a self-supervised fashion. Secondly, we combine neural guidance with differentiable RANSAC to build neural networks which focus on certain parts of the input data and make the output predictions as good as possible. We evaluate NG-RANSAC on a wide array of computer vision tasks, namely estimation of epipolar geometry, horizon line estimation and camera re-localization. We achieve superior or competitive results compared to state-of-the-art robust estimators, including very recent, learned ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite its simplicity and time of invention, Random Sample Consensus (RANSAC) <ref type="bibr" target="#b11">[12]</ref> remains an important method for robust optimization, and is a vital component of many state-of-the-art vision pipelines <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6]</ref>. RANSAC allows accurate estimation of model parameters from a set of observations of which some are outliers. To this end, RANSAC iteratively chooses random sub-sets of observations, so called minimal sets, to create model hypotheses. Hypotheses are ranked according to their consensus with all observations, and the top-ranked hypothesis is returned as the final estimate.</p><p>The main limitation of RANSAC is its poor performance in domains with many outliers. As the ratio of outliers increases, RANSAC requires exponentially many iterations to find an outlier-free minimal set. Implementations of RANSAC therefore often restrict the maximum number of iterations, and return the best model found so far <ref type="bibr" target="#b6">[7]</ref>.  <ref type="figure">Figure 1</ref>. RANSAC vs. NG-RANSAC. We extract 2000 SIFT correspondences between two images. With an outlier rate of 88%, RANSAC fails to find the correct relative transformation (green correct and red wrong matches). We use a neural network to predict a probability distribution over correspondences. Over 90% of the probability mass falls onto 239 correspondences with an outlier rate of 33%. NG-RANSAC samples minimal sets according to this distribution, and finds the correct transformation up to an angular error of less than 1 • .</p><p>In this work, we combine RANSAC with a neural network that predicts a weight for each observation. The weights ultimately guide the sampling of minimal sets. We call the resulting algorithm Neural-Guided RANSAC (NG-RANSAC). A comparison of our method with vanilla RANSAC can be seen in <ref type="figure">Fig. 1</ref>.</p><p>When developing NG-RANSAC, we took inspiration from recent work on learned robust estimators <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b35">36]</ref>. In particular, Yi et al. <ref type="bibr" target="#b55">[56]</ref> train a neural network to classify observations as outliers or inliers, fitting final model parameters only to the latter. Although designed to replace RANSAC, their method achieves best results when combined with RANSAC during test time, where it would remove any outliers that the neural network might have missed. This motivates us to train the neural network in conjunction with RANSAC in a principled fashion, rather than imposing it afterwards.</p><p>Instead of interpreting the neural network output as soft inlier labels for a robust model fit, we let the output weights guide RANSAC hypothesis sampling. Intuitively, the neural network should learn to decrease weights for outliers, and increase them for inliers. This paradigm yields substantial flexibility for the neural network in allowing a certain misclassification rate without negative effects on the final fitting accuracy due to the robustness of RANSAC. The distinction between inliers and outliers, as well as which misclassifications are tolerable, is solely guided by the minimization of the task loss function during training. Furthermore, our formulation of NG-RANSAC facilitates training with any (non-differentiable) task loss function, and any (nondifferentiable) model parameter solver, making it broadly applicable. For example, when fitting essential matrices, we may use the 5-point algorithm rather than the (differentiable) 8-point algorithm which other learned robust estimators rely on <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b35">36]</ref>. The flexibility in choosing the task loss also allows us to train NG-RANSAC self-supervised by using maximization of the inlier count as training objective.</p><p>The idea of using guided sampling in RANSAC is not new. Tordoff and Murray first proposed to guide the hypothesis search of MLESAC <ref type="bibr" target="#b47">[48]</ref>, using side information <ref type="bibr" target="#b46">[47]</ref>. They formulated a prior probability of sparse feature matches being valid based on matching scores. While this has a positive affect on RANSAC performance in some applications, feature matching scores, or other hand-crafted heuristics, were clearly not designed to guide hypothesis search. In particular, calibration of such ad-hoc measures can be difficult as the reliance on over-confident but wrong prior probabilities can yield situations where the same few observations are sampled repeatedly. This fact was recognized by Chum and Matas who proposed PROSAC <ref type="bibr" target="#b8">[9]</ref>, a variant of RANSAC that uses side information only to change the order in which RANSAC draws minimal sets. In the worst case, if the side information was not useful at all, their method would degenerate to vanilla RANSAC. NG-RANSAC takes a different approach in (i) learning the weights to guide hypothesis search rather than using handcrafted heuristics, and (ii) integrating RANSAC itself in the training process which leads to self-calibration of the predicted weights.</p><p>Recently, Brachmann et al. proposed differentiable RANSAC (DSAC) to learn a camera re-localization pipeline <ref type="bibr" target="#b3">[4]</ref>. Unfortunately, we can not directly use DSAC to learn hypothesis sampling since DSAC is only differentiable w.r.t. to observations, not sampling weights. However, NG-RANSAC applies a similar trick also used to make DSAC differentiable, namely the optimization of the expected task loss during training. While we do not rely on DSAC, neural guidance can be used in conjunction with DSAC (NG-DSAC) to train neural networks that predict observations and observation confidences at the same time. We summarize our main contributions:</p><p>• We present NG-RANSAC, a formulation of RANSAC with learned guidance of hypothesis sampling. We can use any (non-differentiable) task loss, and any (nondifferentiable) minimal solver for training. • Choosing the inlier count itself as training objective facilitates self-supervised learning of NG-RANSAC. • We use NG-RANSAC to estimate epipolar geometry of image pairs from sparse correspondences, where it surpasses competing robust estimators. • We combine neural guidance with differentiable RANSAC (NG-DSAC) to train neural networks that make accurate predictions for parts of the input, while neglecting other parts. These models achieve competitive results for horizontal line estimation, and statefor-the-art for camera re-localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>RANSAC was introduced in 1981 by Fischler and Bolles <ref type="bibr" target="#b11">[12]</ref>. Since then it was extended in various ways, see e.g. the survey by Raguram et al. <ref type="bibr" target="#b34">[35]</ref>. Combining some of the most promising improvements, Raguram et al. created the Universal RANSAC (USAC) framework <ref type="bibr" target="#b33">[34]</ref> which represents the state-of-the-art of classic RANSAC variants. USAC includes guided hypothesis sampling according to PROSAC <ref type="bibr" target="#b8">[9]</ref>, more accurate model fitting according to Locally Optimized RANSAC <ref type="bibr" target="#b10">[11]</ref>, and more efficient hypothesis verification according to Optimal Randomized RANSAC <ref type="bibr" target="#b9">[10]</ref>. Many of the improvements proposed for RANSAC could also be applied to NG-RANSAC since we do not require any differentiability of such add-ons. We only impose restrictions on how to generate hypotheses, namely according to a learned probability distribution.</p><p>RANSAC is not often used in recent machine learningheavy vision pipelines. Notable exceptions include geometric problems like object instance pose estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref>, and camera re-localization <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b45">46]</ref> where RANSAC is coupled with decision forests or neural networks that predict image-to-object correspondences. However, in most of these works, RANSAC is not part of the training process because of its non-differentiability. DSAC <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> overcomes this limitation by making the hypothesis selection a probabilistic action which facilitates optimization of the expected task loss during training. However, DSAC is limited in which derivatives can be calculated. DSAC allows differentiation w.r.t. to observations. For example, we can use it to calculate the gradient of image coordinates for a sparse correspondence. However, DSAC does not model observation selection, and hence we cannot use it to optimize a matching probability. By showing how to learn neural guidance, we close this gap. The combination with DSAC enables the full flexibility of learning both, observations and their selection probability.</p><p>Besides DSAC, a differentiable robust estimator, there has recently been some work on learning robust estimators. We discussed the work of Yi et al. <ref type="bibr" target="#b55">[56]</ref> in the introduction. Ranftl and Koltun <ref type="bibr" target="#b35">[36]</ref> take a similar but iterative approach reminiscent of Iteratively Reweighted Least Squares (IRLS) for fundamental matrix estimation. In each iteration, a neural network predicts observation weights for a weighted model fit, taking into account the residuals of the last iteration. Both, <ref type="bibr" target="#b55">[56]</ref> and <ref type="bibr" target="#b35">[36]</ref>, have shown considerable improvements w.r.t. to vanilla RANSAC but require differentiable minimal solvers, and task loss functions. NG-RANSAC outperforms both approaches, and is more flexible when it comes to defining the training objective. This flexibility also enables us to train NG-RANSAC in a selfsupervised fashion, possible with neither <ref type="bibr" target="#b55">[56]</ref> nor <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Preliminaries. We address the problem of fitting model parameters h to a set of observations y ∈ Y that are contaminated by noise and outliers. For example, h could be a fundamental matrix that describes the epipolar geometry of an image pair <ref type="bibr" target="#b15">[16]</ref>, and Y could be the set of SIFT correspondences <ref type="bibr" target="#b26">[27]</ref> we extract for the image pair. To calculate model parameters from the observations, we utilize a solver f , for example the 8-point algorithm <ref type="bibr" target="#b14">[15]</ref>. However, calculating h from all observations will result in a poor estimate due to outliers. Instead, we can calculate h from a small subset (minimal set) of observations with cardinality N : h = f (y 1 , . . . , y N ). For example, for a fundamental matrix N = 8 when using the 8-point algorithm. RANSAC <ref type="bibr" target="#b11">[12]</ref> is an algorithm to chose an outlier-free minimal set from Y such that the resulting estimate h is accurate. To this end, RANSAC randomly chooses M minimal sets to create a pool of model hypotheses H = (h 1 , . . . , h M ).</p><p>RANSAC includes a strategy to adaptively choose M , based on an online estimate of the outlier ratio <ref type="bibr" target="#b11">[12]</ref>. The strategy guarantees that an outlier-free set will be sampled with a user-defined probability. For tasks with large outlier ratios, M calculated like this can be exponentially large, and is usually clamped to a maximum value <ref type="bibr" target="#b6">[7]</ref>. For notational simplicity, we take the perspective of a fixed M but do not restrict the use of an early-stopping strategy in practice. RANSAC chooses a model hypothesis as the final estimateĥ according to a scoring function s:</p><formula xml:id="formula_0">h = argmax h∈H s(h, Y).</formula><p>(1)</p><p>The scoring function measures the consensus of an hypothesis w.r.t. all observations, and is traditionally implemented as inlier counting <ref type="bibr" target="#b11">[12]</ref>. Neural Guidance. RANSAC chooses observations uniformly random to create the hypothesis pool H. We aim at sampling observations according to a learned distribution instead that is parametrized by a neural network with parameters w. That is, we select observations according to y ∼ p(y; w). Note that p(y; w) is a categorical distribution over the discrete set of observations Y, not a continuous distribution in observation space. We wish to learn parameters w in a way that increases the chance of selecting outlierfree minimal sets, which will result in accurate estimatesĥ. We sample a hypothesis pool H according to p(H; w) by sampling observations and minimal sets independently, i.e.</p><formula xml:id="formula_1">p(H; w) = M j=1 p(h j ; w), with p(h; w) = N i=1 p(y i ; w).<label>(2)</label></formula><p>From a pool H, we estimate model parametersĥ with RANSAC according to Eq. 1. For training, we assume that we can measure the quality of the estimate with a task loss function (ĥ). The task loss can be calculated w.r.t. a ground truth model h * , or self-supervised, e.g. by using the inlier count of the final estimate: (ĥ) = −s(ĥ, Y). We wish to learn the distribution p(H; w) in a way that we receive a small task loss with high probability. Inspired by DSAC <ref type="bibr" target="#b3">[4]</ref>, we define our training objective as the minimization of the expected task loss:</p><formula xml:id="formula_2">L(w) = E H∼p(H;w) (ĥ) .<label>(3)</label></formula><p>We compute the gradients of the expected task loss w.r.t. the network parameters as</p><formula xml:id="formula_3">∂ ∂w L(w) = E H (ĥ) ∂ ∂w log p(H; w) .<label>(4)</label></formula><p>Integrating over all possible hypothesis pools to calculate the expectation is infeasible. Therefore, we approximate the gradients by drawing K samples H k ∼ p(H; w):</p><formula xml:id="formula_4">∂ ∂w L(w) ≈ 1 K K k=1 (ĥ) ∂ ∂w log p(H k ; w) .<label>(5)</label></formula><p>Note that gradients of the task loss function do not appear in the expression above. Therefore, differentiability of the task loss , the robust solverĥ (i.e. RANSAC) or the minimal solver f is not required. These components merely generate a training signal for steering the sampling probability p(H; w) in a good direction. Due to the approximation by sampling, the gradient variance of Eq. 5 can be high. We apply a standard variance reduction technique from reinforcement learning by subtracting a baseline b <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_5">∂ ∂w L(w) ≈ 1 K K k=1 [ (ĥ) − b] ∂ ∂w log p(H k ; w) . (6)</formula><p>We found a simple baseline in the form of the average loss per image sufficient, i.e. b =¯ . Subtracting the baseline will move the probability distribution towards hypothesis pools with lower-than-average loss for each training example. Combination with DSAC. Brachmann et al. <ref type="bibr" target="#b3">[4]</ref> proposed a RANSAC-based pipeline where a neural network with parameters w predicts observations y(w) ∈ Y(w). End-toend training of the pipeline, and therefore learning the observations y(w), is possible by turning the argmax hypothesis selection of RANSAC (cf. Eq. 1) into a probabilistic action:</p><formula xml:id="formula_6">h DSAC = h j ∼ p(j|H) = exp s(h j , Y(w)) M k=1 exp s(h k , Y(w)) .<label>(7)</label></formula><p>This differentiable variant of RANSAC (DSAC) chooses a hypothesis randomly according to a distribution calculated from hypothesis scores. The training objective aims at learning network parameters such that hypotheses with low task loss are chosen with high probability:</p><formula xml:id="formula_7">L DSAC (w) = E j∼p(j) [ (h j )] .<label>(8)</label></formula><p>In the following, we extend the formulation of DSAC with neural guidance (NG-DSAC). We let the neural network predict observations y(w) and, additionally, a probability associated with each observation p(y; w). Intuitively, the neural network can express a confidence in its own predictions through this probability. This can be useful if a certain input for the neural network contains no information about the desired model h. In this case, the observation prediction y(w) is necessarily an outlier, and the best the neural network can do is to label it as such by assigning a low probability. We combine the training objectives of NG-RANSAC (Eq. 3) and DSAC (Eq. 8) which yields:</p><formula xml:id="formula_8">L NG-DSAC (w) = E H∼p(H;w) E j∼p(j|H) [ (h j )] ,<label>(9)</label></formula><p>where we again construct p(H; w) from individual p(y; w)'s according to Eq. 2. The training objective of NG-DSAC consists of two expectations. Firstly, the expectation w.r.t. sampling a hypothesis pool according to the probabilities predicted by the neural network. Secondly, the expectation w.r.t. sampling a final estimate from the pool according to the scoring function. As in NG-RANSAC, we approximate the first expectation via sampling, as integrating over all possible hypothesis pools is infeasible. For the second expectation, we can calculate it analytically, as in DSAC, since it integrates over the discrete set of hypotheses h j in a given pool H. Similar to Eq. 6, we give the approximate gradients ∂ ∂w L(w) of NG-DSAC as:</p><formula xml:id="formula_9">1 K K k=1 [E j [ ] − b] ∂ ∂w log p(H k ; w) + ∂ ∂w E j [ ] ,<label>(10)</label></formula><p>where we use</p><formula xml:id="formula_10">E j [ ] as a stand-in for E j∼p(j|H k ) [ (h j )].</formula><p>The calculation of gradients for NG-DSAC requires the derivative of the task loss (note the last part of Eq. 10) because E j [ ] depends on parameters w via observations y(w). Therefore, training NG-DSAC requires a differentiable task loss function , a differentiable scoring function s, and a differentiable minimal solver f . Note that we inherit these restrictions from DSAC. In return, NG-DSAC allows for learning observations and observation confidences, at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate neural guidance on multiple, classic computer vision tasks. Firstly, we apply NG-RANSAC to estimating epipolar geometry of image pairs in the form of essential matrices and fundamental matrices. Secondly, we apply NG-DSAC to horizon line estimation and camera relocalization. We present the main experimental results here, and refer to the appendix for details about network architectures, hyper-parameters and more qualitative results. Our implementation is based on PyTorch <ref type="bibr" target="#b31">[32]</ref>, and we will make the code publicly available for all tasks discussed below 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Essential Matrix Estimation</head><p>Epipolar geometry describes the geometry of two images that observe the same scene <ref type="bibr" target="#b15">[16]</ref>. In particular, two image points x and x in the left and right image corresponding to the same 3D point satisfy x F x = 0, where the 3 × 3 matrix F denotes the fundamental matrix. We can estimate F uniquely (but only up to scale) from 8 correspondences, or from 7 correspondences with multiple solutions <ref type="bibr" target="#b15">[16]</ref>. The essential matrix E is a special case of the fundamental matrix when the calibration parameters K and K of both cameras are known: E = K F K. The essential matrix can be estimated from 5 correspondences <ref type="bibr" target="#b30">[31]</ref>. Decomposing the essential matrix allows to recover the relative pose between the observing cameras, and is a central step in image-based 3D reconstruction <ref type="bibr" target="#b39">[40]</ref>. As such, estimating the fundamental or essential matrices of image pairs is a classic and wellresearched problem in computer vision. RootSIFT+Ratio+NG-RANSAC (+SI) <ref type="figure">Figure 2</ref>. Essential Matrix Estimation. We calculate the relative pose between outdoor and indoor image pairs via the essential matrix. We measure the AUC of the cumulative angular error up to a threshold of 5 • , 10 • or 20 • . a) We use no side information about the sparse correspondences. b) We use side information in the form of descriptor distance ratios between the best and second best match. We use it to filter correspondences with a threshold of 0.8 (+Ratio), as an additional input for our network (+SI), and as additional input for USAC <ref type="bibr" target="#b33">[34]</ref>. c) We train NG-RANSAC in a self-supervised fashion by using the inlier count as training objective.</p><p>In the following, we firstly evaluate NG-RANSAC for the calibrated case and estimate essential matrices from SIFT correspondences <ref type="bibr" target="#b26">[27]</ref>. For the sake of comparability with the recent, learned robust estimator of Yi et al. <ref type="bibr" target="#b55">[56]</ref> we adhere closely to their evaluation setup, and compare to their results. Datasets. Yi et al. <ref type="bibr" target="#b55">[56]</ref> evaluate their approach in outdoor as well as indoor settings. For the outdoor datasets, they select five scenes from the structure-from-motion (SfM) dataset of <ref type="bibr" target="#b18">[19]</ref>: Buckingham, Notredame, Sacre Coeur, St. Peter's and Reichstag. They pick two additional scenes from <ref type="bibr" target="#b43">[44]</ref>: Fountain and Herzjesu. They reconstruct each scene using a SfM tool <ref type="bibr" target="#b52">[53]</ref> to obtain 'ground truth' camera poses, and co-visibility constraints for selecting image pairs. For indoor scenes Yi et al. choose 16 sequences from the SUN3D dataset <ref type="bibr" target="#b53">[54]</ref> which readily comes with ground truth poses captured by KinectFusion <ref type="bibr" target="#b29">[30]</ref>. See Appendix A for a listing of all scenes. Indoor scenarios are typically very challenging for sparse feature-based approaches because of texture-less surfaces and repetitive elements (see <ref type="figure">Fig. 1</ref> for an example). Yi et al. train their best model using one outdoor scene (St. Peter's) and one indoor scene (Brown 1), and test on all remaining sequences (6 outdoor, 15 indoor). Yi et al. kindly provided us with their exact data splits, and we will use their setup. Note that training and test is performed on completely separate scenes, i.e. the neural network has to generalize to unknown environments. Evaluation Metric. Via the essential matrix, we recover the relative camera pose up to scale, and compare to the ground truth pose as follows. We measure the angular error between the pose rotations, as well as the angular error between the pose translation vectors in degrees. We take the maximum of the two values as the final angular error. We calculate the cumulative error curve for each test sequence, and compute the area under the curve (AUC) up to a threshold of 5 • , 10 • or 20 • . Finally, we report the average AUC over all test sequences (but separately for the indoor and outdoor setting).</p><p>Implementation. Yi et al. train a neural network to classify a set of sparse correspondences in inliers and outliers. They represent each correspondence as a 4D vector combining the 2D coordinate in the left and right image. Their network is inspired by PointNet <ref type="bibr" target="#b32">[33]</ref>, and processes each correspondence independently by a series of multilayer perceptrons (MLPs). Global context is infused by using instance normalization <ref type="bibr" target="#b48">[49]</ref> in-between layers. We re-build this architecture in PyTorch, and train it according to NG-RANSAC (Eq. 3). That is, the network predicts weights to guide RANSAC sampling instead of inlier class labels. We use the angular error between the estimated relative pose, and the ground truth pose as task loss . As minimal solver f , we use the 5-point algorithm <ref type="bibr" target="#b30">[31]</ref>. To speed up training, we initialize the network by learning to predict the distance of each correspondence to the ground truth epipolar line, see Appendix A for details. We initialize for 75k iterations, and train according to Eq. 3 for 25k iterations. We optimize using Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 10 −5 . For each training image, we extract 2000 SIFT correspondences, and sample K = 4 hypothesis pools with M = 16 hypotheses. We use a low number of hypotheses during training to obtain variation when sampling pools. For testing, we increase the number of hypotheses to M = 10 3 . We use an inlier threshold of 10 −3 assuming normalized image coordinates using camera calibration parameters. Results. We compare NG-RANSAC to the inlier classification (InClass) of Yi et al. <ref type="bibr" target="#b55">[56]</ref>. They use their approach with SIFT as well as LIFT <ref type="bibr" target="#b54">[55]</ref> features. We include results for DeMoN <ref type="bibr" target="#b49">[50]</ref>, a learned SfM pipeline, and GMS <ref type="bibr" target="#b1">[2]</ref>, a semi-dense approach using ORB features <ref type="bibr" target="#b37">[38]</ref>. As classical baselines, we compare to vanilla RANSAC <ref type="bibr" target="#b11">[12]</ref> and USAC <ref type="bibr" target="#b33">[34]</ref>. See <ref type="figure">Fig. 2 a)</ref> for results. RANSAC achieves poor results for indoor and outdoor scenes across all thresholds, scoring as the weakest method. In this experiment, we assume no side information is available about the quality of correspondences. Therefore, USAC performs similar to RANSAC, since it cannot use guided sampling. Coupling   <ref type="figure">Figure 3</ref>. Qualitative Results. We compare fitted models for RANSAC and NG-RANSAC. For the indoor and outdoor image pairs, we fit essential matrices, and for the Kitti image pair we fit the fundamental matrix. We draw final model inliers in green if they adhere to the ground truth model, and red otherwise. We also measure the quality of each estimate, see the main text for details on the metrics.</p><p>RANSAC with neural guidance (NG-RANSAC) elevates it to the leading position with a comfortable margin. Different from USAC, NG-RANSAC deduces useful guiding weights solely from the spatial distribution of correspondences. See also <ref type="figure">Fig. 3</ref> for qualitative results.</p><p>NG-RANSAC outperforms InClass of Yi et al. <ref type="bibr" target="#b55">[56]</ref> despite some similarities. Both use the same network architecture, are based on SIFT correspondences, and both use RANSAC at test time. Yi et al. <ref type="bibr" target="#b55">[56]</ref> train using a hybrid classification-regression loss based on the 8-point algorithm, and ultimately compare essential matrices using squared error. Therefore, their training objective is very different from the evaluation procedure. During evaluation, they use RANSAC with the 5-point algorithm on top of their inlier predictions, and measure the angular error. NG-RANSAC incorporates all these components in its training procedure, and therefore optimizes the correct objective.</p><p>Using Side Information. The evaluation procedure of Yi et al. <ref type="bibr" target="#b55">[56]</ref> is designed to test a robust estimator in highoutlier domains. However, it underestimates what classical approaches can achieve on these datasets. The distance ratio of the best and second-best SIFT match is often an indicator of correspondence quality. This side information can be used by USAC <ref type="bibr" target="#b33">[34]</ref> to guide hypothesis sampling according to the PROSAC strategy <ref type="bibr" target="#b8">[9]</ref>. Furthermore, Lowe's ratio criterion <ref type="bibr" target="#b26">[27]</ref> removes ambiguous matches with a distance ratio above a threshold (we use 0.8) before running RANSAC. We denote the ratio filter as +Ratio in <ref type="figure">Fig. 2 b)</ref>, and observe a drastic improvement for all methods. Both classic approaches, RANSAC and USAC, outperform all learned methods of <ref type="figure">Fig. 2 a)</ref>. RootSIFT normalization of SIFT descriptors <ref type="bibr" target="#b0">[1]</ref> improves accuracy further. NG-RANSAC easily incorporates side information. For best accuracy, we train it on ratio-filtered RootSIFT correspondences, using distance ratios as additional network input (denoted by +SI).  The accuracy of USAC <ref type="bibr" target="#b33">[34]</ref> and NG-RANSAC depend on the hypothesis budget M , see <ref type="figure" target="#fig_3">Fig. 4</ref>. NG-RANSAC finds good hypotheses much earlier than USAC, and achieves a reasonable accuracy by drawing as few as 10 hypotheses. <ref type="figure">Fig. 5</ref> shows a visualization of progressive hypotheses search. USAC is designed to draw the same hypotheses as RANSAC but in a different order. Therefore, USAC samples degenerate hypotheses (poor accuracy but high inlier count) eventually, even if it gives them a low priority at first, see <ref type="figure">Fig. 5</ref> bottom. NG-RANSAC learns to suppress such hypotheses more effectively.</p><p>Interestingly, passing our learned weights to USAC achieves significantly lower accuracy than passing matching ratios to USAC. For example, for the outdoor setting, w/o ratio filter and M = 10 3 , USAC achieves -0.27/-0.24/-0.34 AUC for 5 • /10 • /20 • when using our weights. The USAC/PROSAC sampling scheme assumes that the probability of correspondences being inliers increases monotonically with the sampling weight <ref type="bibr" target="#b8">[9]</ref>. In contrast, our training objective optimizes over entire pools of hypotheses where correspondences are sampled independently. Individual outlier correspondences might be ranked high by the neural network, without affecting accuracy negatively, thus violating the assumption of PROSAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Best after 10</head><p>Best after 100</p><p>Best after 1000 RANSAC USAC NG-RANSAC RANSAC USAC NG-RANSAC <ref type="figure">Figure 5</ref>. Hypothesis Search. We visualize the best hypothesis found after M ∈ {10, 100, 1000} iterations for RANSAC <ref type="bibr" target="#b11">[12]</ref>, USAC <ref type="bibr" target="#b33">[34]</ref> and NG-RANSAC. For each result, we give the number of correspondences which are also inliers for the ground truth model (GT Inliers, drawn in green). We perform this experiment in the Indoor scenario, using side information and RootSIFT but without Lowe's ratio filter.</p><p>Self-supervised Learning. We train NG-RANSAC selfsupervised by defining a task loss to assess the quality of an estimate independent of a ground truth model h * . A natural choice is the inlier count of the final estimate. We found the inlier count to be a very stable training signal, even in the beginning of training such that we require no special initialization of the network. We report results of self-supervised NG-RANSAC in <ref type="figure">Fig. 2 c)</ref>. It outperforms all competitors except USAC <ref type="bibr" target="#b33">[34]</ref> which it matches in accuracy. Unsupervised NG-RANSAC achieves slightly worse accuracy than supervised NG-RANSAC. A supervised task loss allows NG-RANSAC to adapt more precisely to the evaluation measure used at test time. For the datasets used so far, the process of image pairing uses co-visibility information, and therefore a form of supervision. In the next section, we learn NG-RANSAC fully self-supervised by using the ordering of sequential data to assemble image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime.</head><p>A forward pass of the network takes 3ms on CPU (similar for GPU). The total runtime (and accuracy) depends on the hypothesis count M . For M = 10 3 , our implementation of NG-RANSAC takes 90ms per image pair. For M = 10, it takes 21ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fundamental Matrix Estimation</head><p>We apply NG-RANSAC to fundamental matrix estimation, comparing it to the learned robust estimator of Ranftl and Koltun <ref type="bibr" target="#b35">[36]</ref>, denoted Deep F-Mat. They propose an iterative procedure where a neural network estimates observation weights for a robust model fit. The residuals of the last iteration are an additional input to the network in the next iteration. The network architecture is similar to the one used in <ref type="bibr" target="#b55">[56]</ref>. Correspondences are represented as 4D vectors, and they use the descriptor matching ratio as an additional input. Each observation is processed by a series  <ref type="figure">Figure 6</ref>. Fundamental Matrix Estimation. We measure the average percentage of inliers of the estimated model, the alignment of estimated inliers and ground truth inliers (F-score), and the mean and median distance of estimated inliers to ground truth epilines. For NG-RANSAC, we compare the performance after training with different objectives. Note that %Inliers is a selfsupervised training objective.</p><p>of MLPs with instance normalization interleaved. Deep F-Mat was published very recently, and the code is not yet available. We therefore follow the evaluation procedure described in <ref type="bibr" target="#b35">[36]</ref> and compare to their results. Datasets. Ranftl and Koltun <ref type="bibr" target="#b35">[36]</ref> evaluate their method on various datasets that involve custom reconstructions not publicly available. Therefore, we compare to their method on the Kitti dataset <ref type="bibr" target="#b13">[14]</ref>, which is online. <ref type="bibr">Ranftl</ref>   <ref type="bibr" target="#b24">[25]</ref> 82.3 <ref type="figure">Figure 7</ref>. Horizon Line Estimation. Left: AUC on the HLW dataset. Right: Qualitative results. We draw the ground truth horizon in green and the estimate in blue. Dots mark the observations predicted by NG-DSAC, and the dot colors mark their confidence (dark = low). Note that the horizon can be outside the image.</p><p>Results. We report results in <ref type="figure">Fig. 6</ref> where we compare NG-RANSAC with RANSAC, USAC <ref type="bibr" target="#b33">[34]</ref> and Deep F-Mat. NG-RANSAC outperforms the classical approaches RANSAC and USAC. NG-RANSAC also performs slightly superior to Deep F-Mat. We observe that the choice of the training objective has small but significant influence on the evaluation. All metrics are highly correlated, and optimizing a metric in training generally also achieves good (but not necessarily best) accuracy using this metric at test time. Interestingly, optimizing the inlier count during training performs competitively, although being a self-supervised objective. <ref type="figure">Fig. 3</ref> shows a qualitative result on Kitti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Horizon Lines</head><p>We fit a parametric model, the horizon line, to a single image. The horizon can serve as a cue in image understanding <ref type="bibr" target="#b51">[52]</ref> or for image editing <ref type="bibr" target="#b24">[25]</ref>. Traditionally, this task is solved via vanishing point detection and geometric reasoning <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b41">42]</ref>, often assuming a Manhattan or Atlanta world. We take a simpler approach and use a general purpose CNN that predicts a set of 64 2D points based on the image to which we fit a line with RANSAC, see <ref type="figure">Fig. 7</ref>. The network has two output branches predicting (i) the 2D points y(w) ∈ Y(w), and (ii) probabilities p(y; w) for guided sampling (see Appendix C for details). Dataset. We evaluate on the HLW dataset <ref type="bibr" target="#b51">[52]</ref> which is a collection of SfM datasets with annotated horizon line. Test and training images partly show the same scenes, and the horizon line can be outside the image area. Evaluation Metric. As is common practice on HLW, we measure the maximum distance between the estimated horizon and ground truth within the image, normalized by image height. We calculate the AUC of the cumulative error curve up to a threshold of 0.25. Implementation. We train using the NG-DSAC objective (Eq. 9) from scratch for 250k iterations. As task loss , we use the normalized maximum distance between estimated and true horizon. For hypothesis scoring s, we use a soft inlier count <ref type="bibr" target="#b5">[6]</ref>. We train using Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 10 −4 . For each training image, we draw K = 2   <ref type="figure">Figure 9</ref>. Camera Re-Localization. We report median position error for Cambridge Landmarks <ref type="bibr" target="#b21">[22]</ref>. DSAC++ (ResNet) is our re-implementation of <ref type="bibr" target="#b5">[6]</ref> with an improved network architecture.</p><p>hypothesis pools with M = 16 hypotheses. We also draw 16 hypotheses at test time. We compare to DSAC which we train similarly but disable the probability branch.</p><p>Results. We report results in <ref type="figure">Fig. 7</ref>. DSAC and NG-DSAC achieve competitive accuracy on this dataset, ranking among the top methods. NG-DSAC has a small but significant advantage over DSAC alone. Our method is only surpassed by SLNet <ref type="bibr" target="#b24">[25]</ref>, an architecture designed to find semantic lines in images. SLNet generates a large number of random candidate lines, selects a candidate via classification, and refines it with a predicted offset. We could couple SLNet with neural guidance for informed candidate sampling. Unfortunately, the code of SLNet is not online and the authors did not respond to inquiries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Camera Re-Localization</head><p>We estimate the absolute 6D camera pose (position and orientation) w.r.t. a known scene from a single RGB image. Dataset. We evaluate on the Cambridge Landmarks <ref type="bibr" target="#b21">[22]</ref> dataset. It is comprised of RGB images depicting five landmark buildings 2 in Cambridge, UK. Ground truth poses were generated by running a SfM pipeline. Evaluation Metric. We measure the median translational error of estimated poses for each scene <ref type="bibr" target="#b2">3</ref> .</p><p>Implementation. We build on the publicly available DSAC++ pipeline <ref type="bibr" target="#b5">[6]</ref> which is a scene coordinate regression method <ref type="bibr" target="#b40">[41]</ref>. A neural network predicts for each image pixel a 3D coordinate in scene space. We recover the pose from the 2D-3D correspondences using a perspectiven-point solver <ref type="bibr" target="#b12">[13]</ref> within a RANSAC loop. The DSAC++ pipeline implements geometric pose optimization in a fully differentiable way which facilitates end-to-end training. We re-implement the neural network integration of DSAC++ with PyTorch (the original uses LUA/Torch). We also update the network architecture of DSAC++ by using a ResNet <ref type="bibr" target="#b17">[18]</ref> instead of a VGGNet <ref type="bibr" target="#b42">[43]</ref>. As with horizon line estimation, we add a second output branch to the network for estimating a probability distribution over scene coordinate predictions for guided RANSAC sampling. We denote this extended architecture NG-DSAC++. We adhere to the training procedure and hyperparamters of DSAC++ (see Appendix D) but optimize the NG-DSAC objective (Eq. 9) during end-to-end training. As task loss , we use the average of the rotational and translational error w.r.t. the ground truth pose. We sample K = 2 hypothesis pools with M = 16 hypotheses per training image, and increase the number of hypotheses to M = 256 for testing. Results. We report our quantitative results in <ref type="figure">Fig. 9</ref>. Firstly, we observe a significant improvement for most scenes when using DSAC++ with a ResNet architecture. Secondly, comparing DSAC++ with NG-DSAC++, we notice a small to moderate, but consistent, improvement in accuracy. The advantage of using neural guidance is largest for the Great Court scene, which features large ambiguous grass areas, and large areas of sky visible in many images. NG-DSAC++ learns to ignore such areas, see the visualization in <ref type="figure" target="#fig_4">Fig. 8 a)</ref>. The network learns to mask these areas solely guided by the task loss during training, as the network fails to predict accurate scene coordinates for them. In <ref type="figure" target="#fig_4">Fig. 8 b)</ref>, we visualize the internal representation learned by DSAC++ and NG-DSAC++ for one scene. The representation of DSAC++ is very noisy, as it tries to optimize geometric constrains for sky and grass pixels. NG-DSAC++ learns a cleaner representation by focusing entirely on buildings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented NG-RANSAC, a robust estimator using guided hypothesis sampling according to learned probabilities. For training we can incorporate non-differentiable task loss functions and non-differentiable minimal solvers. Using the inlier count as training objective allows us to also train NG-RANSAC self-supervised. We applied NG-RANSAC to multiple classic computer vision tasks and observe a consistent improvement w.r.t. RANSAC alone.</p><p>Acknowledgements: This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement No 647769). The computations were performed on an HPC Cluster at the Center for Information Services and High Performance Computing (ZIH) at TU Dresden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Essential Matrix Estimation</head><p>List of Scenes Used for Training and Testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training:</head><p>• Staint Peter's (Outdoor) • brown bm 3 -brown bm 3 (Indoor) Testing (Outdoor):</p><formula xml:id="formula_11">• Buckingham • Notre Dame • Sacre Coeur • Reichstag • Fountain • HerzJesu</formula><p>Testing (Indoor):</p><p>• brown cogsci 2 -brown cogsci 2 • brown cogsci 6 -brown cogsci 6 • brown cogsci 8 -brown cogsci 8 • brown cs 3 -brown cs3 • brown cs 7 -brown cs7 • harvard c4 -hv c4 1 • harvard c10 -hv c10 2 • harvard corridor lounge -hv lounge1 2 • harvard robotics lab -hv s1 2 • hotel florence jx -florence hotel stair room all • mit 32 g725 -g725 1 • mit 46 6conf -bcs floor6 conf 1 • mit 46 6lounge -bcs floor6 long • mit w85g -g 0 • mit w85h -h2 1 Network Architecture. As mentioned in the main paper, we replicated the architecture of Yi et al. <ref type="bibr" target="#b55">[56]</ref> for our experiments on epipolar geometry (estimating essential and fun-damental matrices). For a schematic overview see <ref type="figure">Fig. 10</ref>. The network takes a set of feature correspondences as input, and predicts as output a weight for each correspondence which we use to guide RANSAC hypothesis sampling. The network consists of a series of multilayer perceptrons (MLPs) that process each correspondence independently. We implement the MLPs with 1 × 1 convolutions. The network infuses global context via instance normalization layers <ref type="bibr" target="#b48">[49]</ref>, and it accelerate training via batch normalization <ref type="bibr" target="#b19">[20]</ref>. The main body of the network is comprised of 12 blocks with skip connections <ref type="bibr" target="#b17">[18]</ref>. Each block consists of two linear layers followed by instance normalization, batch normalization and a ReLU activation <ref type="bibr" target="#b16">[17]</ref> each. We apply a Sigmoid activation to the last layer, and normalize by dividing by the sum of outputs. <ref type="bibr" target="#b3">4</ref> Initialization Procedure. We initialize our network in the following way. We define a target sampling distribution g(y; E * ) using the ground truth essential matrix E * given for each training pair. Intuitively, the target distribution should return a high probability when a correspondence y is aligned with the ground truth essential matrix E * , and a low probability otherwise. We assume that correspondence y is a 4D vector containing two 2D image coordinates x and x (3D in homogeneous coordinates). We define the epipolar error of a correspondence w.r.t. essential matrix E:</p><formula xml:id="formula_12">d(y, E) = (x Ex) 2 [Ex] 2 0 + [Ex] 2 1 + [E x ] 2 0 + [E x ] 2 1 ,<label>(11)</label></formula><p>where [·] i returns the ith entry of a vector. Using the epipolar error, we define the target sampling distribution:</p><formula xml:id="formula_13">g(y; E * ) = 1 2πσ 2 exp − d(y, E * ) 2σ 2 .<label>(12)</label></formula><p>Parameter σ controls the softness of the target distribution, and we use σ = 10 −3 which corresponds to the inlier threshold we use for RANSAC. To initialize our network, we minimize the KL divergence between the network prediction p(y; w) and the target distribution g(y; E * ). We initialize for 75k iterations using Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 10 −3 and a batch size of 32. Implementation Details. For the following components we rely on the implementations provided by OpenCV <ref type="bibr" target="#b6">[7]</ref>: the 5-point algorithm <ref type="bibr" target="#b30">[31]</ref>, epipolar error, SIFT features <ref type="bibr" target="#b26">[27]</ref>, feature matching, and essential matrix decomposition. We extract 2000 features per input image which yields 2000 correspondences for image pairs after matching. When applying Lowe's ratio criterion <ref type="bibr" target="#b26">[27]</ref> for filtering and hence reducing the number of correspondences, we randomly duplicate correspondences to restore the number of 2000. We 1x1 -C128 -S1 1x1 -C128 -S1 1x1 -C128 -S1 1x1 -C128 -S1</p><p>Filter Size -Channels -Stride  <ref type="figure">Figure 10</ref>. NG-RANSAC Network Architecture for F/E-matrix Estimation. The network takes a set of feature correspondences as input and predicts as output a weight for each correspondence. The network consists of linear layers interleaved by instance normalization <ref type="bibr" target="#b48">[49]</ref>, batch normalization <ref type="bibr" target="#b19">[20]</ref> and ReLUs <ref type="bibr" target="#b16">[17]</ref>. The arc with a plus marks a skip connection for each of the twelve blocks <ref type="bibr" target="#b17">[18]</ref>. This architecture was proposed by Yi et al. <ref type="bibr" target="#b55">[56]</ref>.</p><p>minimize the expected task loss using Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 10 −5 and a batch size of 32. We choose hyperparameters based on validation error of the Reichstag scene. We observe that the magnitude of the validation error corresponds well to the magnitude of the training error, i.e. a validation set would not be strictly required. When calculating the AUC for evaluation, we adhere to the protocol of Yi et al. <ref type="bibr" target="#b55">[56]</ref> to ensure comparability. Yi et al. approximate the AUC via the area under the cumulative histogram with a bin width of 5 • . Qualitative Results. We present additional qualitative results for indoor and outdoor scenarios in <ref type="figure">Fig. 11</ref>. We compare results of RANSAC and NG-RANSAC, also visualizing neural guidance as predicted by our network. We obtain these results in the high-outlier setup, i.e. without using Lowe's ratio criterion and without using side information as additional network input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fundamental Matrix Estimation</head><p>Implementation Details. We reuse the architecture of <ref type="figure">Fig. 10</ref>. To normalize image coordinates of feature matches, we subtract the mean coordinate and divide by the coordinate standard deviation, where we calculate mean and standard deviation over the training set. Ranftl and Koltun <ref type="bibr" target="#b35">[36]</ref> fit the final fundamental matrix to the top 20 weighted correspondences as predicted by their network. Similarly, we re-fit the final fundamental matrix to the largest inlier set found by NG-RANSAC. This refinement step results in a small but noticeable increase in accuracy. For the following components we rely on the implementations provided by OpenCV <ref type="bibr" target="#b6">[7]</ref>: the 7-point algorithm, epipolar error, SIFT features <ref type="bibr" target="#b26">[27]</ref> and feature matching.</p><p>Qualitative Results. We present additional qualitative results for the Kitti dataset <ref type="bibr" target="#b13">[14]</ref> in <ref type="figure" target="#fig_7">Fig. 12</ref>. We compare results of RANSAC and NG-RANSAC, also visualizing neural guidance as predicted by our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Horizon Lines</head><p>Network Architecture. We provide a schematic of our network architecture for horizon line estimation in <ref type="figure">Fig. 13</ref>. The network takes a 256 × 256px image as input. We re-scale images of arbitrary aspect ratio such that the long side is 256px. We symmetrically zero-pad the short side to 256px. The network has two output branches. The first branch predicts a set of 8 × 8 = 64 2D points, our observations y(w), to which we fit the horizon line. We apply a Sigmoid and re-scale output points to <ref type="bibr">[-1.5,1.5]</ref> in relative image coordinates to support horizon lines outside the image area. We implement the network in a fully convolutional way <ref type="bibr" target="#b25">[26]</ref>, i.e. each output point is predicted for a patch, or restricted receptive field, of the input image. Therefore, we shift the coordinate of each output point to the center of its associated patch.</p><p>The second branch predicts sampling probabilities p(y; w) for each output point. We apply a Sigmoid to the output of the second branch, and normalize by dividing by the sum of outputs. During training, we block the gradients of the second output branch when back propagating to the base network. The sampling gradients have larger variance and magnitude than the observation gradients of the first branch, especially in the beginning of training with a negative effect on convergence of the network as a whole. Intuitively, we want to give priority to the observation prediction because they determine the accuracy of the final model pa-  <ref type="figure">Figure 11</ref>. Qualitative Results for Essential Matrix Estimation. We compare results of RANSAC and NG-RANSAC. Below each result, we give the angular error between estimated and true translation vectors, and estimated and true rotation matrices. We draw correspondences in green if they adhere to the ground truth essential matrix with an inlier threshold of 10 −3 , and red otherwise.</p><p>rameters. The sampling prediction should address deficiencies in the observation predictions without influencing them too much. The gradient blockade ensures these properties. Implementation Details. We use a differentiable soft inlier count <ref type="bibr" target="#b5">[6]</ref> as scoring function, i.e.:</p><formula xml:id="formula_14">s(h, Y) = α y∈Y 1 − sig[βd(y, h) − βτ ],<label>(13)</label></formula><p>where d(y, h) denotes the point-line distance between observation y and line hypothesis h. Hyperparameter α determines the softness of the scoring distribution in DSAC, β determines the softness of the Sigmoid, and τ is the inlier threshold. We use α = 0.1, β = 100 and τ = 0.05. We convert input images to grayscale, and apply the following data augmentation strategy during training: We randomly adjust brightness and contrast in the range of ±10%.  3x3 -C32 -S1 3x3 -C64 -S1 3x3 -C128 -S2 3x3 -C256 -S2 3x3 -C256 -S2 3x3 -C256 -S2 3x3 -C256 -S2 3x3 -C256 -S1 3x3 -C256 -S1 3x3 -C256 -S1 1x1 -C512 -S1 1x1 -C512 -S1 1x1 -C2 -S1 1x1 -C512 -S1 1x1 -C512 -S1 1x1 -C1 -S1 3x3 -C32 -S1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sigmoid Normalization</head><p>Sigmoid Normalization <ref type="figure">Figure 13</ref>. NG-DSAC Network Architecture for Horizon Line Estimation. The network takes a grayscale image as input and predicts as output a set of 2D points and corresponding sampling weights. The network consists of convolution layers interleaved by batch normalization <ref type="bibr" target="#b19">[20]</ref> and ReLUs <ref type="bibr" target="#b16">[17]</ref>. The arc with a plus marks a skip connection <ref type="bibr" target="#b17">[18]</ref>. We use the gradient blockage during training to prevent direct influence of the sampling prediction (second branch) to learning the observations (first branch).</p><p>We randomly rotate/scale/shift images (and ground truth horizon lines) in the range of ±5 • /20%/8px. As discussed in the main paper, we use the normalized maximum distance between a line hypothesis and the ground truth horizon in the image as task loss . This can lead to stability issues when we sample line hypotheses with very steep slope. Therefore, we clamp the task loss to a maximum of 1, i.e. the normalized image height.</p><p>As mentioned before, some images in the HLW dataset <ref type="bibr" target="#b51">[52]</ref> have their horizon outside the image. Some of these images contain virtually no visual cue where the horizon exactly lies. Therefore, we find it beneficial to use a robust variant of the task loss that limits the influence of such outliers. We use:</p><formula xml:id="formula_15">= &lt; 0.25 0.25 √ otherwise ,<label>(14)</label></formula><p>i.e. we use the square root of the task loss after a magnitude of 0.25, which is the magnitude up to which the AUC is calculated when evaluating on HLW <ref type="bibr" target="#b51">[52]</ref>. Qualitative Results. We present additional qualitative results for the HLW dataset <ref type="bibr" target="#b51">[52]</ref> in <ref type="figure" target="#fig_3">Fig. 14.</ref> D. Camera Re-Localization Network Architecture. We provide a schematic of our network architecture for camera re-localization in <ref type="figure">Fig. 15</ref>. The network is a FCN <ref type="bibr" target="#b25">[26]</ref> that takes an RGB image as input, and predicts dense outputs, sub-sampled by a factor of 8.</p><p>The network has two output branches. The first branch predicts 3D scene coordinates <ref type="bibr" target="#b40">[41]</ref>, our observations y(w), to which we fit the 6D camera pose. The second output branch predicts sampling probabilities p(y; w) for the scene coordinates. We apply a Sigmoid to the output of the second branch, and normalize by dividing by the sum of outputs. During training, we block the gradients of the second output branch when back propagating to the base network. The sampling gradients have larger variance and magnitude than the observation gradients of the first branch, especially in the beginning of training. This has a negative effect on convergence of the network as a whole. Intuitively, we want to give priority to the scene coordinate prediction because they determine the accuracy of the pose estimate. The sampling prediction should address deficiencies in the scene coordinate predictions without influencing them too much. The gradient blockade ensures these properties. Implementation details. We follow the three-stage training procedure proposed by Brachmann and Rother for DSAC++ <ref type="bibr" target="#b5">[6]</ref>. Firstly, we optimize the distance between predicted and ground truth scene coordinates. We obtain ground truth scene coordinates by rendering the sparse reconstructions given in the Cambridge Landmarks dataset <ref type="bibr" target="#b21">[22]</ref>. We ignore pixels with no corresponding 3D point in the reconstruction. Since the reconstructions contain outlier 3D points, we use the following robust distance:</p><p>( . y, y * ) = ||y − y * || 2 ||y − y * || 2 &lt; 10 10 ||y − y * || 2 otherwise , <ref type="figure" target="#fig_3">Figure 14</ref>. Qualitative Results for Horizon Line Estimation. Next to each input image, we show the estimated horizon line in blue and the true horizon line in green. We also show the observation points predicted by our network, colored by their sampling weight (dark = low).</p><p>3x3 -C32 -S1 3x3 -C64 -S2 3x3 -C128 -S2 3x3 -C256 -S2 3x3 -C256 -S1 1x1 -C256 -S1 3x3 -C256 -S1 3x3 -C512 -S1 1x1 -C512 -S1 3x3 -C512 -S1 3x3 -C512 -S1 1x1 -C512 -S1 3x3 -C512 -S1 1x1 -C512 -S1 1x1 -C512 -S1 1x1 -C1 -S1 1x1 -C512 -S1 1x1 -C512 -S1 1x1 -C3 -S1</p><p>(480 × 852px) 3x3 -C32 -S1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: RGB Image</head><p>Filter Size -Channels -Stride Gradient Blockade Legend:</p><p>Sigmoid Normalization <ref type="figure">Figure 15</ref>. NG-DSAC++ Network Architecture for Camera Re-Localization. The network takes an RGB image as input and predicts as output dense scene coordinates and corresponding sampling weights. The network consists of convolution layers followed by ReLUs <ref type="bibr" target="#b16">[17]</ref>. Am arc with a plus marks a skip connection <ref type="bibr" target="#b17">[18]</ref>. We use the gradient blockage during training to prevent direct influence of the sampling prediction (second branch) to learning the scene coordinates (first branch).</p><p>i.e. we use the Euclidean distance up to a threshold of 10m after which we use the square root of the Euclidean distance. We train the first stage for 500k iterations using Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 10 −4 and a batch size of 1 image. Secondly, we optimize the reprojection error of the scene coordinate predictions w.r.t. to the ground truth camera pose. Similar to the first stage, we use a robust distance function with a threshold of 10px after which we use the square root of the reprojection error. We train the second stage for 300k iterations using Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 10 −4 and a batch size of 1 image.</p><p>Thirdly, we optimize the expected task loss according to the NG-DSAC objective as explained in the main paper. As task loss we use = ∠(θ, θ * ) + ||t − t * || 2 . We measure the angle between estimated camera rotation θ and ground truth rotation θ * in degree. We measure the distance between the estimated camera position t and ground truth position t * in meters. As with horizon line estimation (see previous section), we use a soft inlier count as hypothesis scoring function with hyperparameters α = 10, β = 0.5 and τ = 10. We train the third stage for 200k iterations using Adam <ref type="bibr" target="#b22">[23]</ref> with a learning rate of 10 −6 and a batch size of 1 image.</p><p>Learned 3D Representations. We visualize the internal 3D scene representations learned by DSAC++ and NG-DSAC++ in <ref type="figure">Fig. 16</ref> for two more scenes.  <ref type="figure">Figure 16</ref>. Learned 3D Representations. We visualize the internal representation of the neural network. We predict scene coordinates for each training image, plotting them with their RGB color. For DSAC++ we choose training pixels randomly, for NG-DSAC++ we choose randomly among the top 1000 pixels per training image according to the predicted distribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, w/o Side Information, w/o Ratio Filter b) RootSIFT, w/ Side Information, w/ Ratio Filter</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Accuracy vs. Hypothesis Budget. We compare the AUC of NG-RANSAC and USAC<ref type="bibr" target="#b33">[34]</ref> for increasing number of hypotheses M . a) with and b) without side information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Neural Guidance for Camera Re-localization. a) Predicted sampling probabilities of NG-DSAC++ throughout training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Δ</head><label></label><figDesc>: 63.1°, Δ : 0.5°Δ : 1.1°, Δ : 0.6°Δ : 80.1°, Δ : 17.8°Δ : 5.2°, Δ : 3.0°Δ : 97.5°, Δ : 5.6°Δ : 1.9°, Δ : 2.0°Δ : 83.1°, Δ : 9.5°Δ : 3.3°, Δ :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>19.9, F-score: 26.4, Mean Error: 0.16 % Inliers: 22.4, F-score: 61.3, Mean Error: 022.9, F-score: 33.8, Mean Error: 0.16 % Inliers: 24.2, F-score: 64.3, Mean Error: 028.7, F-score: 25.3, Mean Error: 0.19 % Inliers: 31.0, F-score: 59.8, Mean Error: 0.10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Qualitative Results for Fundamental Matrix Estimation. We compare results of RANSAC and NG-RANSAC. Below each result, we give the percentage of inliers of the final model, the F-score which measures the alignment of estimated and true fundamental matrix, and the mean epipolar error of estimated inlier correspondences w.r.t. the ground truth fundamental matrix. We draw correspondences in green if they adhere to the ground truth fundamental matrix with an inlier threshold of 0.1px, and red otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We adhere to the training setup described in Sec. 4.1 with the following changes. We observed faster training convergence on Kitti, so we omit the initialization stage, and directly optimize the expected task loss (Eq. 3) for 300k iterations. Since Ranftl and Koltun<ref type="bibr" target="#b35">[36]</ref> evaluate using multiple metrics, the choice of the task loss function is not clear. Hence, we train multiple variants with different objectives (%Inliers, F-score and Mean error) and report the corresponding results. As minimal solver f , we use the 7-point algorithm, a RANSAC threshold of 0.1px, and we draw K = 8 hypothesis pools per training image with M = 16 hypotheses each.</figDesc><table><row><cell></cell><cell>AUC (%)</cell></row><row><cell>Simon et al. [42]</cell><cell>54.4</cell></row><row><cell>Kluger et al. [24]</cell><cell>57.3</cell></row><row><cell>Zhai et al. [57]</cell><cell>58.2</cell></row><row><cell cols="2">Workman et al. [52] 71.2</cell></row><row><cell>DSAC</cell><cell>74.1</cell></row><row><cell>NG-DSAC</cell><cell>75.2</cell></row><row><cell>SLNet</cell><cell></cell></row></table><note>and Koltun [36] train their method on sequences 00-05 of the Kitti odometry benchmark, and test on sequences 06-10. They form image pairs by taking subsequent images within a se- quence. For each pair, they extract SIFT correspondences and apply Lowe's ratio filter [27] with a threshold of 0.8. Evaluation Metric. Ranftl and Koltun [36] evaluate us- ing multiple metrics. They measure the percentage of in- lier correspondences of the final model relative to all cor- respondences. They calculate the F-score over correspon- dences where true positives are inliers of both the ground truth model and the estimated model. The F-score measures the alignment of estimated and true fundamental matrix in image space. Both metrics use an inlier threshold of 0.1px. Finally, they calculate the mean and median epipolar error of inlier correspondences w.r.t. the ground truth model, us- ing an inlier threshold of 1px. Implementation. We cannot use the architecture of Deep F-Mat which is designed for iterative application. There- fore, we re-use the architecture of Yi et al. [56] from the previous section for NG-RANSAC (also see Appendix B for details).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>b)Internal representation of the neural network. We predict scene coordinates for each training image, plotting them with their RGB color. For DSAC++ we choose training pixels randomly, for NG-DSAC++ we choose randomly according to the predicted distribution.</figDesc><table><row><cell></cell><cell>DSAC++ [6]</cell><cell>DSAC++</cell><cell>NG-DSAC++</cell></row><row><cell></cell><cell>(VGGNet)</cell><cell>(ResNet)</cell><cell>(ResNet)</cell></row><row><cell>Great Court</cell><cell>40.3cm</cell><cell>40.3cm</cell><cell>35.0cm</cell></row><row><cell>Kings College</cell><cell>17.7cm</cell><cell>13.0cm</cell><cell>12.6cm</cell></row><row><cell>Old Hospital</cell><cell>19.6cm</cell><cell>22.4cm</cell><cell>21.9cm</cell></row><row><cell>Shop Facade</cell><cell>5.7cm</cell><cell>5.7cm</cell><cell>5.6cm</cell></row><row><cell>St M. Church</cell><cell>12.5cm</cell><cell>9.9cm</cell><cell>9.8cm</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">vislearn.de/research/neural-guided-ransac/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We omitted the Street scene. Like DSAC++<ref type="bibr" target="#b5">[6]</ref> we failed to achieve sensible results, here. By visual inspection, the corresponding SfM reconstruction seems to be of poor quality, which potentially harms training.<ref type="bibr" target="#b2">3</ref> The median rotational accuracies are between 0.2 • to 0.3 • for all scenes, and do hardly vary between methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The original architecture of Yi et al.<ref type="bibr" target="#b55">[56]</ref> uses a slightly different output processing due to using the output as weights for a robust model fit. They use a ReLU activation followed by a tanh activation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GMS: Grid-based motion statistics for fast, ultra-robust feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning 6D object pose estimation using 3D object coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DSAC-Differentiable RANSAC for camera localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6D pose estimation of objects and scenes from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning less is more-6D camera localization via 3D surface regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenCV. Dr. Dobb&apos;s Journal of Software Tools</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On-the-fly adaptation of regression forests for online camera relocalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Matching with PROSAC -Progressive sample consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<title level="m">Optimal randomized RANSAC. TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Locally optimized RANSAC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random Sample Consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Complete solution classification for the perspective-three-point problem. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<title level="m">defense of the eight-point algorithm. TPAMI</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2016. 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heinly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">iPose: Instance-aware 6D pose estimation of partly occluded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PoseNet: A convolutional network for real-time 6-DoF camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning for vanishing point detection using an inverse gnomonic projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kluger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic line detection and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Distinctive image features from scale-invariant keypoints. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Random forests versus neural networks -what&apos;s best for camera localization? In ICRA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">ORB-SLAM2: an opensource SLAM system for monocular, stereo, and RGB-D cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">KinectFusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMAR</title>
		<meeting>ISMAR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An efficient solution to the five-point relative pose problem. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">USAC: A universal framework for random sample consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raguram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A comparative analysis of RANSAC techniques leading to adaptive realtime random sample consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raguram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep fundamental matrix estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A new approach for vanishing point detection in architectural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ORB: An efficient alternative to SIFT or SURF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient &amp; effective prioritized matching for large-scale image-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structure-from-Motion Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A-contrario horizonfirst vanishing point detection using second-order grouping laws</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On benchmarking camera calibration and multi-view stereo for high resolution imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Thoennessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Introduction to Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">InLoc: Indoor visual localization with dense matching and view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Guided sampling and consensus for motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tordoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MLESAC: A new robust estimator with application to estimating image geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DeMoN: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploiting uncertainty in regression forests for accurate camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Horizon lines in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Towards linear-time incremental structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SUN3D: A database of big spaces reconstructed using SfM and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">LIFT: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to find good correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Detecting vanishing points using global image context in a non-manhattan world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
