<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UnICORNN: A recurrent model for learning very long time dependencies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Konstantin</forename><surname>Rusch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
						</author>
						<title level="a" type="main">UnICORNN: A recurrent model for learning very long time dependencies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The design of recurrent neural networks (RNNs) to accurately process sequential inputs with long-time dependencies is very challenging on account of the exploding and vanishing gradient problem. To overcome this, we propose a novel RNN architecture which is based on a structure preserving discretization of a Hamiltonian system of second-order ordinary differential equations that models networks of oscillators. The resulting RNN is fast, invertible (in time), memory efficient and we derive rigorous bounds on the hidden state gradients to prove the mitigation of the exploding and vanishing gradient problem. A suite of experiments are presented to demonstrate that the proposed RNN provides state of the art performance on a variety of learning tasks with (very) long time-dependencies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent Neural Networks (RNNs) have been very successful in solving a diverse set of learning tasks involving sequential inputs <ref type="bibr" target="#b29">[30]</ref>. These include text and speech recognition, time-series analysis and natural language processing. However, the well-known Exploding and Vanishing Gradient Problem (EVGP) <ref type="bibr" target="#b37">[38]</ref> and references therein, impedes the efficiency of RNNs on tasks that require processing (very) long sequential inputs. The EVGP arises from the fact that the backpropagation through time (BPTT) algorithm for training RNNs entails computing products of hidden state gradients over a large number of steps and this product can either be exponentially small or large as the number of recurrent interactions increases.</p><p>Different approaches to solve the EVGP has been suggested in recent years. These include the use of gating mechanisms, such as in LSTMs <ref type="bibr" target="#b22">[23]</ref> and GRUs <ref type="bibr" target="#b11">[12]</ref>, where the additive structure of the gates mitigates the vanishing gradient problem. However, gradients might still explode, impeding the efficiency of LSTMs and GRUs on problems with very long time dependencies (LTDs) <ref type="bibr" target="#b31">[32]</ref>. The EVGP can also be mitigated by constraining the structure of the recurrent weight matrices, for instance requiring them to be orthogonal or unitary <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b24">25]</ref>. Constraining recurrent weight matrices may lead to a loss of expressivity of the resulting RNN, reducing its efficiency in handling realistic learning tasks <ref type="bibr" target="#b24">[25]</ref>. Finally, restricting weights of the RNN to lie within some prespecified bounds might lead to control over the norms of the recurrent weight matrices and alleviate the EVGP. Such an approach has been suggested in the context of independent neurons in each layer in <ref type="bibr" target="#b31">[32]</ref>, and using a coupled system of damped oscillators in <ref type="bibr" target="#b40">[41]</ref>, among others. However, ensuring that weights remain within a pre-defined range during training might be difficult. Furthermore, weight clipping could also reduce expressivity of the resulting RNN.</p><p>In addition to EVGP, the learning of sequential tasks with very long time dependencies can require significant computational resources, for training and evaluating the RNN. Moreover, as the BPTT training algorithms entail storing all hidden states at every time step, the overall memory requirements can be prohibitive. Thus, the design of a fast and memory efficient RNN architecture that can mitigate the EVGP is highly desirable for the effective use of RNNs in realistic learning tasks with very long time dependencies. The main objective of this article is to propose, analyze and test such an architecture.</p><p>The basis of our proposed RNN is the observation that a large class of dynamical systems in physics and engineering, the so-called Hamiltonian systems <ref type="bibr" target="#b2">[3]</ref>, allow for very precise control on the underlying states. Moreover, the fact that the phase space volume is preserved by the trajectories of a Hamiltonian system, makes such systems invertible and allows one to significantly reduce the storage requirements. Furthermore, if the resulting hidden state gradients also evolve according to a Hamiltonian dynamical system, one can obtain precise bounds on the hidden state gradients and alleviate the EVGP. We combine and extend these ideas into an RNN architecture that will allow us to prove rigorous bounds on the hidden states and their gradients, mitigating the EVGP. Moreover, our RNN architecture results in a fast implementation that attains state of the art performance on a variety of learning tasks with very long time dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The proposed RNN</head><p>Our proposed RNN is based on the time-discretization of the following system of second-order ordinary differential equations (ODEs),</p><formula xml:id="formula_0">y = −[ (w y + Vu + b) + y].<label>(1)</label></formula><p>Here, ∈ [0, 1] is the (continuous) time variable, u = u( ) ∈ R is the time-dependent input signal, y = y( ) ∈ R is the hidden state of the RNN with w ∈ R is a weight vector, V ∈ R × a weight matrix, b ∈ R is the bias vector and ≥ 0 is a control parameter. The operation is the Hadamard product and the function : R ↦ → R is the activation function and is applied component wise. For the rest of this paper, we set ( ) = tanh( ). By introducing the auxiliary variable z = y , we can rewrite the second order ODE (1) as a first order ODE system:</p><formula xml:id="formula_1">y = z, z = −[ (w y + Vu + b) + y].<label>(2)</label></formula><p>Assuming that w ≠ 0, for all 1 ≤ ≤ , it is easy to see that the ODE system (2) is a Hamiltonian system,</p><formula xml:id="formula_2">y = z , z = − y ,<label>(3)</label></formula><p>with the time-dependent Hamiltonian, (y, z, ) = 2 y 2 + 1 2 z 2 + ∑︁ =1 1 w log(cosh(w y + (Vu( )) + b )),</p><p>with x 2 = x, x denoting the Euclidean norm of the vector x ∈ R and ·, · the corresponding inner product. The next step is to find a discretization of the ODE system <ref type="bibr" target="#b1">(2)</ref>. Given that it is highly desirable to ensure that the discretization respects the Hamiltonian structure of the underlying continuous ODE, the simplest such structure preserving discretization is the symplectic Euler method <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b18">19]</ref>. Applying the symplectic Euler method to the ODE (2) results in the following discrete dynamical system,</p><formula xml:id="formula_4">y = y −1 + Δ z , z = z −1 − Δ [ (w y −1 + Vu + b) + y −1 ],<label>(5)</label></formula><p>for 1 ≤ ≤ . Here, 0 &lt; Δ &lt; 1 is the time-step and u ≈ u( ), with = Δ , is the input signal. It is common to initialize with y 0 = z 0 = 0. We see from the structure of the discrete dynamical system (5) that there is no interaction between the neurons in the hidden layer of <ref type="bibr" target="#b4">(5)</ref>. Such an RNN will have very limited expressivity. Hence, we stack more hidden layers to propose the following deep or multilayer RNN,</p><formula xml:id="formula_5">y ℓ = y ℓ −1 + Δˆ(c ℓ ) z ℓ , z ℓ = z ℓ −1 − Δˆ(c ℓ ) [ (w ℓ y ℓ −1 + V ℓ y ℓ−1 + b ) + y ℓ −1 ].<label>(6)</label></formula><p>Here y , z ∈ R are hidden states and w ℓ , V ℓ , b ℓ are weights and biases, corresponding to layer ℓ = 1, . . . , . We set y 0 = u in the multilayer RNN <ref type="bibr" target="#b5">(6)</ref>.</p><p>Observe that we use the same step-size Δ for every layer, while multiplying a trainable parameter vector c ∈ R to the time step. The action of c is modulated with the sigmoidal activation function ( ) = 0.5 + 0.5 tanh( /2), which ensures that the time-step Δ is multiplied by a value between 0 and 1. We remark that the presence of this trainable vector c allows us to incorporate multi-scale behavior in the proposed RNN, as the effective time-step is learned during training and can be significantly different from the nominal time-step Δ . It is essential to point out that including this multi-scale time stepping is only possible, as each neuron (within the same hidden layer) is independent of the others and can be integrated with a different effective time step. Finally, we also share the control hyperparameter across the different layers, which results in a memory unit of layers with a total of only 2 hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation and background</head><p>The ODE system (2) is a model for a nonlinear system of uncoupled driven oscillators <ref type="bibr" target="#b17">[18]</ref>. To see this, we denote y ( ) as the displacement and z ( ) as the velocity. Then, the dynamics of the -th oscillator is determined by the frequency and also by the forcing or driving term in the second equation of (2), where the forcing acts through the input signal u and is modulated by the weight V and bias b. Finally, the weight w modulates the frequency and allows each neuron to oscillate with its own frequency, rather than the common frequency of the system. The structure of w implies that each neuron is independent of the others. A key element of the oscillator system (2) is the absence of any damping or friction term. This allows the system to possess a Hamiltonian structure, with desirable long time behavior. Thus, we term the resulting RNN (6), based on the ODE system (2) as Undamped Independent Controlled Oscillatory RNN or UnICORNN. We remark that networks of oscillators are very common in science and engineering <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref> with prominent examples being pendulums in mechanics, electrical circuits in engineering, business cycles in economics and functional brain circuits such as cortical columns in neurobiology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Comparison with related work.</head><p>UnICORNN lies firmly in the class of ODE-based or ODE-inspired RNNs, which have received considerable amount of attention in the machine learning literature in recent years. Neural ODEs, first proposed in <ref type="bibr" target="#b9">[10]</ref>, are a prominent example of using ODEs to construct neural networks. In this architecture, the continuous ODE serves as the learning model and gradients are computed from a sensitivity equation, which allows one to trade accuracy with computing time. Moreover, it is argued that these neural ODEs are invertible and hence, memory efficient. However, it is unclear if a general neural ODE, without any additional structure, can be invertible. Other RNN architectures that are based on discretized ODEs include those proposed in <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b7">[8]</ref>, where the authors proposed an anti-symmetric RNN, based on the discretization of a stable ODE resulting from a skew-symmetric hidden weight matrix, thus constraining the gradient dynamics.</p><p>Our proposed RNN (6) is inspired by two recent RNN architectures. The first one is coRNN, proposed recently in <ref type="bibr" target="#b40">[41]</ref>, where the underlying RNN architecture was also based on the use of a network of oscillators. As long as a constraint on the underlying weights was satisfied, coRNN was shown to mitigate the EVGP. In contrast to coRNN, our proposed RNN does not use a damping term. Moreover, each neuron, for any hidden layer, in UnICORNN (6) is independent. This is very different from coRNN where all the neurons were coupled together. Finally, UnICORNN is a multi-layer architecture whereas coRNN used a single hidden layer. These innovations allow us to admit a Hamiltonian structure for UnICORNN and facilitate a fast and memory efficient implementation.</p><p>Our proposed architecture was also partly inspired by IndRNN, proposed in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, where the neurons in each hidden layers were independent of each other and interactions between neurons were mediated by stacking multiple RNN layers, with output of each hidden layer passed on to the next hidden layer, leading to a deep RNN. We clearly use this construction of independent neurons in each layer and stacking multiple layers in UnICORNN <ref type="bibr" target="#b5">(6)</ref>. However in contrast to IndRNN, our proposed RNN is based on a discretized Hamiltonian system and we will not require any constraints on the weights to mitigate the EVGP.</p><p>Finally, we would like to point out that discrete Hamiltonian systems have already been used to design RNNs, for instance in <ref type="bibr" target="#b16">[17]</ref> and also in <ref type="bibr" target="#b10">[11]</ref>, where a symplectic time-integrator for a Hamiltonian system was proposed as the RNN architecture. However, these approaches are based on underlying time-independent Hamiltonians and are only relevant for mechanical systems as they cannot process time-dependent inputs, which arise in most realistic learning tasks. Moreover, as these methods enforce exact conservation of the Hamiltonian in time, they are not suitable for learning long-time dependencies, see <ref type="bibr" target="#b34">[35]</ref> for a discussion and experiment on that issue. Although we use a Hamiltonian system as the basis of our proposed RNN (6), our underlying Hamiltonian (4) is time-dependent and the resulting RNN can readily process any time-dependent input signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">On the Memory Efficiency of UnICORNN</head><p>As mentioned in the introduction, the standard BPTT training algorithm for RNNs requires one to store all the hidden states at every time step. To see this, we observe that for a standard multi-layer RNN with layers and a mini-batch size of (for any mini-batch stochastic gradient descent algorithm), the storage (in terms of floats) scales as O ( + ), with input and hidden sequences of length . This memory requirement can be very high. Note that we have ignored the storage of trainable weights and biases for the RNN in the above calculation.</p><p>On the other hand, as argued before, our proposed RNN is a symplectic Euler discretization for a Hamiltonian system. Hence, it is invertible. In fact, one can explicitly write the inverse of UnICORNN (6) as,</p><formula xml:id="formula_6">y −1 = y − Δˆ(c ) z , z −1 = z + Δˆ(c ) [ (w y −1 + V ℓ y ℓ−1 + b ) + y −1 ].<label>(7)</label></formula><p>Thus, one can recover all the hidden states in a given hidden layer, only from the stored hidden state at the final time step, for that layer. Moreover, only the input signal needs to be stored as the other hidden states can be reconstructed from the formula <ref type="bibr" target="#b6">(7)</ref>. Hence, a straightforward calculation shows that the storage for UnICORNN scales as O ( + ). As &lt;&lt; , we conclude that UnICORNN allows for a significant saving in terms of storage, when compared to a standard RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Rigorous Analysis of UnICORNN</head><p>Properties of the ODE (2). In order to investigate the EVGP for the proposed RNN (6), we will first explore the dynamics of the gradients of hidden states y, z (solutions of the ODE (2)) with respect to the trainable parameters w, V and b. Denote any scalar parameter as and = , then differentiating the ODE (2) with respect to results in the ODE,</p><formula xml:id="formula_7">y = z , z = − (A) (w y ) − y − (A) C( ),<label>(8)</label></formula><p>where A = w y + Vu + b is the pre-activation and the coefficient C ∈ R is given by</p><formula xml:id="formula_8">C = y if = w , C = u if = V and C = 1 if = b</formula><p>, with all other entries of the vector C being zero. It is easy to check that the ODE system (8) is a Hamiltonian system of form (3), with the following time-dependent Hamiltonian;</p><formula xml:id="formula_9">H (y , z , ) := 2 y 2 + 1 2 z 2 + 1 2 ∑︁ =1 (A )w ((y ) ) 2 + ∑︁ =1 (A )C ( ) (y ) .<label>(9)</label></formula><p>Thus, by the well-known Liouville's theorem <ref type="bibr" target="#b41">[42]</ref>, we know that the phase space volume of (8) is preserved. Hence, this system cannot have any asymptotically stable fixed points. This implies that {0, 0} cannot be a stable fixed point for the hidden state gradients (y , z ). Thus, we can expect that the hidden state gradients with respect to the system of oscillators (2) do not remain near zero.</p><p>On the other hand, as the Hamiltonian (9) for the hidden state gradient system (8) is time-dependent, we cannot directly infer that the gradients satisfy an upper bound. However, we prove (in Appendix C.2) the following bounds, Proposition 3.1. Let y ( ), z ( ) be the solutions of the ODE system (8) for hidden state gradients, at any time ∈ [0, 1]. Then for &gt; 0 and for all ∈ [0, 1], the hidden state gradients are bounded by,</p><formula xml:id="formula_10">y ( ) 2 ≤ y (0) 2 + 1 z (0) 2 w + w , z ( ) 2 ≤ y (0) 2 + z (0) 2 w + w ,<label>(10)</label></formula><p>with w = max w 2 ∞ , 2 , and for some constant that only depends on the dimension and on the constant that bounds the input u.</p><p>A similar upper bound for = 0 is given in Appendix C.</p><p>On the Exploding Gradient Problem for UnICORNN. We train the RNN (6) to minimize the loss function,</p><formula xml:id="formula_11">E := 1 ∑︁ =1 E , E = 1 2 y −ȳ 2 2 ,<label>(11)</label></formula><p>withȳ being the underlying ground truth (training data). Note that the loss function (11) only involves the output at the last hidden layer (we set the affine output layer to identity for the sake of simplicity).</p><p>During training, we compute gradients of the loss function <ref type="bibr" target="#b10">(11)</ref> with respect to the trainable weights and</p><formula xml:id="formula_12">biases Θ = [w ℓ , V ℓ , b ℓ , c ℓ ], for all 1 ≤ ℓ ≤ , i.e., E = 1 ∑︁ =1 E , ∀ ∈ Θ.<label>(12)</label></formula><p>Given the upper bounds (10) on the hidden state gradients for the underlying ODE (2), it is reasonable to expect that the gradient (12) is bounded. This is indeed the case and we have the following proposition, Proposition 3.2. Let &gt; 0, Δ &lt; 1 in the RNN (6) and let y ℓ , z ℓ , for 1 ≤ ℓ ≤ , be the hidden states generated by the RNN <ref type="bibr" target="#b5">(6)</ref>. Then, the gradient of the loss function E (11) with respect to any parameter ∈ Θ is bounded as,</p><formula xml:id="formula_13">E ≤ 1 − (Δ ) 1 − Δ w V( + F) (∆ 1 + ∆ 2 ) ,<label>(13)</label></formula><p>with¯= max 1≤ ≤ ȳ ∞ , be a bound on the underlying training data and other quantities in (13) defined as,</p><formula xml:id="formula_14">w = max 2, w ∞ + , V = =1 max{1, V ∞ }, F = √︂ 2 1 + 2 Δ 3 , = 96 1 4 max{1, √ }, ∆ 1 = 2 + √︃ 2 1 + 2 Δ 3 , ∆ 2 = (2 + ) √︂ 2 1 + 2 Δ 3 .<label>(14)</label></formula><p>This proposition, proved in Appendix D.2, demonstrates that as long as the weights w , V are bounded, there is a uniform bound on the hidden state gradients and the exploding gradient problem is mitigated for UnICORNN.</p><p>On the Vanishing Gradient Problem for UnICORNN. By applying the chain rule repeatedly to each term on the right-hand-side of (12), we obtain</p><formula xml:id="formula_15">E = ∑︁ ℓ=1 ∑︁ =1 E ( , ) ,ℓ , E ( , )</formula><p>,ℓ</p><formula xml:id="formula_16">:= E X X X ℓ + X ℓ , X ℓ = y ℓ,1 , z ℓ,1 , . . . , y ℓ, , z ℓ,</formula><p>, . . . , y ℓ, , z ℓ, .</p><p>Here, the notation + X ℓ refers to taking the partial derivative of X ℓ with respect to the parameter , while keeping the other arguments constant. The quantity</p><formula xml:id="formula_18">E ( , )</formula><p>,ℓ denotes the contribution from the -recurrent step at the -th hidden layer of the deep RNN (6) to the overall hidden state gradient at the step . The vanishing gradient problem <ref type="bibr" target="#b37">[38]</ref> arises if</p><formula xml:id="formula_19">E ( , )</formula><p>,ℓ , defined in <ref type="bibr" target="#b14">(15)</ref>, → 0 exponentially fast in , for &lt;&lt; (long-term dependencies). In that case, the RNN does not have long-term memory, as the contribution of the -th hidden state at the ℓ-th layer to error at time step is infinitesimally small. We have established that the hidden state gradients for the underlying continuous ODE (2) do not vanish. As we use a symplectic Euler discretization, the phase space volume for the discrete dynamical system (5) is also conserved <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b18">19]</ref>. Hence, one can expect that the gradients of the multilayer RNN (6) do not vanish. However, these heuristic considerations need to be formalized. Observe that the vanishing gradient problem for RNNs focuses on the possible smallness of contributions of the gradient over a large number of recurrent steps. As this behavior of the gradient is independent of the number of layers, we focus on the vanishing gradient problem for a single hidden layer here, while presenting the multilayer results in Appendix D.4. Also, for the sake of definiteness, we set the scalar parameter = w 1, for some 1 ≤ ≤ . Similar results also hold for any other ∈ Θ. We have following representation formula (proved in Appendix D.3) for the hidden state gradients, Proposition 3.3. Let y be the hidden states generated by the RNN <ref type="bibr" target="#b5">(6)</ref>. Then the gradient for long-term dependencies, i.e. &lt;&lt; , satisfies the representation formula,</p><formula xml:id="formula_20">E ( ,1) ,1 w 1, = −Δˆ(c 1, ) 2 (A 1, −1 )y 1, −1 y 1, − y + O (Δ 2 ).<label>(16)</label></formula><p>It is clear from the representation formula (16) that there is no -dependence for the gradient. In particular, as long as all the weights are of O (1), the leading-order term in <ref type="formula" target="#formula_0">(16)</ref> is O (Δ ). Hence, the gradient can be small but is independent of the recurrent step . Thus, we claim that the vanishing gradient problem, with respect to recurrent connections, is mitigated for UnICORNN (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The details of the training procedure for each experiment can be found in Appendix A. Code to replicate the experiments can be found at https://github.com/tk-rusch/unicornn. Implementation The structure of UnICORNN (6) enables us to achieve a very fast implementation. First, the transformation of the input (i.e. V ℓ y ℓ−1 for all = 1, . . . , ), which is the most computationally expensive part of UnICORNN, does not have a sequential structure and can thus be computed in parallel over time. Second, as the underlying ODEs of the UnICORNN are uncoupled for each neuron, the remaining recurrent part of UnICORNN is solved independently for each component. Hence, inspired by the implementation of Simple Recurrent Units (SRU) <ref type="bibr" target="#b30">[31]</ref> and IndRNN, we present in Appendix B, the details of an efficient CUDA implementation, where the input transformation is computed in parallel and the dynamical system corresponding to each component of (6) is an independent CUDA thread.</p><p>We benchmark the training speed of UnICORNN with = 2 layers, against the fastest available RNN implementations, namely the cuDNN implementation <ref type="bibr" target="#b0">[1]</ref> of LSTM (with 1 hidden layer), SRU and IndRNN (both with = 2 layers and with batch normalization). <ref type="figure" target="#fig_0">Fig. 1</ref> shows the computational time (measured on a GeForce RTX 2080 Ti GPU) of the combined forward and backward pass for each network, averaged over 100 batches with each of size 128, for two different sequence lengths, i.e. = 1000, 2000. We can see that while the cuDNN LSTM is relatively slow, the SRU, IndRNN and the UnICORNN perform similarly fast. Moreover, we also observe that UnICORNN is about 30 − 40 times faster per combined forward and backward pass, when compared to recently developed RNNs such as expRNN <ref type="bibr" target="#b6">[7]</ref> and coRNN <ref type="bibr" target="#b40">[41]</ref>. We thus conclude that the UnICORNN is among the fastest available RNN architectures.  Permuted sequential MNIST A well-established benchmark for testing RNNs on input sequences with long time-dependencies is the permuted sequential MNIST (psMNIST) task <ref type="bibr" target="#b27">[28]</ref>. Based on the classical MNIST data set <ref type="bibr" target="#b28">[29]</ref>, the flattened grey-scale matrices are randomly permuted (based on a fixed random permutation) and processed sequentially by the RNN. This makes the learning task more challenging than sequential MNIST, where one only flattens the MNIST matrices without permuting them. In order to make different methods comparable, we use the same fixed seed for the random permutation, as in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref>. <ref type="table">Table 1</ref> shows the results for UnICORNN with 3 layers, together with other recently proposed RNNs, which were explicitly designed to learn LTDs as well as two gated baselines. We see that UnICORNN clearly outperforms the other methods. <ref type="table">Table 1</ref>: Test accuracies on permuted sequential MNIST together with number of hidden units as well as total number of parameters for each network. All other results are taken from the corresponding original publication, cited in the main text, except that we are using the results of <ref type="bibr" target="#b8">[9]</ref> for GRU and of <ref type="bibr" target="#b20">[21]</ref>  Noise padded CIFAR-10 A more challenging test for the ability of RNNs to learn LTDs is provided by the recently proposed noise padded CIFAR-10 experiment <ref type="bibr" target="#b7">[8]</ref>. In it, the CIFAR-10 data points <ref type="bibr" target="#b25">[26]</ref> are fed to the RNN row-wise and flattened along the channels resulting in sequences of length 32. To test long term memory, entries of uniform random numbers are added such that the resulting sequences have a length of 1000, i.e. the last 968 entries of each sequences are only noise to distract the RNNs. <ref type="table" target="#tab_1">Table 2</ref> shows the result of the UnICORNN with 3 layers together with the results of other recently proposed RNNs, namely for the LSTM, anti.sym. RNN and gated anti.sym. RNN <ref type="bibr" target="#b7">[8]</ref>, Lipschitz RNN <ref type="bibr" target="#b14">[15]</ref>, Incremental RNN <ref type="bibr" target="#b23">[24]</ref>, FastRNN <ref type="bibr" target="#b26">[27]</ref> and coRNN <ref type="bibr" target="#b40">[41]</ref>. We conclude that the proposed RNN readily outperforms all other methods on this experiment. EigenWorms The EigenWorms data set <ref type="bibr" target="#b3">[4]</ref> is a collecting of 259 very long sequences, i.e. length of 17984, describing the motion of a worm. The task is, based on the 6-dimensional motion sequences, to classify a worm as either wild-type or one of four mutant types. We use the same train/valid/test split as in <ref type="bibr" target="#b35">[36]</ref>, i.e. 70%/15%/15%. As the length of the input sequences is extremely long for this test case, we benchmark UnICORNN against three sub-sampling based baselines. These include the results of <ref type="bibr" target="#b35">[36]</ref>, which is based on signature sub-sampling routine for neural controlled differential equations. Additionally after a hyperparameter fine-tuning procedure, we perform a random sub-sampling as well as truncated back-propagation through time (BPTT) routine using LSTMs, where the random sub-sampling is based on 200 randomly selected time points of the sequences as well as the BPTT is truncated after the last 500 time points of the sequences. Furthermore, we compare UnICORNN with three leading RNN architectures for solving LTD tasks, namely expRNN, IndRNN and coRNN, which are all applied to the full-length sequences. The results, presented in <ref type="table" target="#tab_2">Table 3</ref>, show that while sub-sampling approaches yield moderate test accuracies, expRNN as well as the IndRNN yield very poor accuracies. In contrast, coRNN performs very well. However, the best results are obtained for UnICORNN as it reaches a test accuracy of more than 90%, while at the same time yielding a relatively low standard deviation, further underlining the robustness of the proposed RNN. As this data set has only recently been proposed as a test for RNNs in learning LTDs, it is unclear if the input sequences truly exhibit very long time-dependencies. To investigate this further, we train UnICORNN on a subset of the entries of the sequences. To this end, we consider using only the last entries as well as using a random subset of the entries. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the distributional results (10 re-trainings of the best performing UnICORNN) for the number of entries used in each sub-sampling routine, ranging from only using 1000 entries to using the full sequences for training. We can see that in order to reach a test accuracy of 80% when training on the last entries of the sequences, at least the last 10k entries are needed. Moreover, for both sub-sampling methods the test accuracy increases monotonically as the number of entries for training is increased. On the other hand, using a random subset of the entries increases the test accuracy significantly when compared to using only the last entries of the sequences. This indicates that the important entries of the sequences, i.e. information needed in order to classify them correctly, are uniformly distributed throughout the full sequence. We thus conclude that the EigenWorms data set indeed exhibits very long time-dependencies.</p><p>Healthcare application: Vital signs prediction We apply UnICORNN on two real-world data sets in health care, aiming to predict the vital signs of a patient, based on PPG and ECG signals. The data sets are part of the TSR archive <ref type="bibr" target="#b43">[44]</ref> and are based on clinical data from the Beth Israel Deaconess Medical Center. The PPG and ECG signals are sampled with a frequency of 125Hz for 8 minutes each. The resulting two-dimensional sequences have a length of 4000. The goal is to predict a patient's respiratory rate (RR) and heart rate (HR) based on these signals. We compare UnICORNN to 3 leading RNN architectures for solving LTDs, i.e. expRNN, IndRNN and coRNN. Additionally, we present two baselines using the LSTM as well as the recently proposed sub-sampling method of computing signatures for neural controlled differential equations (NCDE) <ref type="bibr" target="#b35">[36]</ref>. Following <ref type="bibr" target="#b35">[36]</ref>, we split the 7949 sequences in a training set, validation set and testing set, using a 70%/15%/15% split. <ref type="table" target="#tab_3">Table 4</ref> shows the distributional results of all networks using 5 re-trainings of the best performing RNN. We observe that while the LSTM does not reach a low 2 testing error in both experiments, the other RNNs approximate the vital signs reasonably well. However, UnICORNN clearly outperforms all other methods on both benchmarks. We emphasize that UnICORNN significantly outperforms all other state-of-the-art methods on estimating the RR, which is of major importance in modern healthcare applications for monitoring hospital in-patients as well as for mobile health applications, as special invasive equipment (for instance capnometry or measurement of gas flow) is normally needed to do so <ref type="bibr" target="#b39">[40]</ref>.  <ref type="bibr" target="#b33">[34]</ref>, which consists of 50k online movie reviews with 25k reviews used for training and 25k reviews used for testing. This denotes a classical sentiment analysis task, where the model has to decide whether a movie review is positive or negative. We use 30% of the training set (i.e. 7.5k reviews) as the validation set and restrict the dictionary to 25k words. We choose an embedding size of 100 and initialize it with the pretrained 100d GloVe <ref type="bibr" target="#b38">[39]</ref> vectors. <ref type="table" target="#tab_4">Table 5</ref> shows the results for UnICORNN with 2 layers together with other recently proposed RNN architectures and gated baselines (which are known to perform very well on these tasks). The result of ReLU GRU is taken from <ref type="bibr" target="#b12">[13]</ref>, of coRNN from <ref type="bibr" target="#b40">[41]</ref> and all other results are taken from <ref type="bibr" target="#b4">[5]</ref>. We can see that UnICORNN outperforms the other methods while requiring significantly less parameters. We thus conclude, that the UnICORNN can also be successfully applied to problems, which do not necessarily exhibit long term-dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The design of RNNs that can accurately handle sequential inputs with long-time dependencies is very challenging. This is largely on account of the exploding and vanishing gradient problem (EVGP). Moreover, there is a significant increase in both computational time as well as memory requirements when LTD tasks have to be processed. Our main aim in this article was to present a novel RNN architecture which is fast, memory efficient, invertible and mitigates the EVGP. To this end, we proposed UnICORNN <ref type="formula" target="#formula_5">(6)</ref>, an RNN based on the symplectic Euler discretization of a Hamiltonian system of second-order ODEs (2) modeling a network of independent, undamped, controlled and driven oscillators. In order to gain expressivity, we stack layers of RNNs and also endow this construction with a multi-scale feature by training the effective time step in <ref type="bibr" target="#b5">(6)</ref>. Given the Hamiltonian structure of our continuous and discrete dynamical system, invertibility and volume preservation in phase space are guaranteed. Invertibility enables the proposed RNN to be memory efficient. The independence of neurons within each hidden layer allows us to build a highly efficient CUDA implementation of UnICORNN that is as fast as the fastest available RNN architectures. Motivated by the fact that the underlying ODE (1) as well as the ODE <ref type="formula" target="#formula_7">(8)</ref>, governing the evolution of hidden state gradient possess a time-dependent Hamiltonian, we prove rigorous upper bounds (13) on the gradients and show that the exploding gradient problem is mitigated for UnICORNN. Moreover, we derive an explicit representation formula <ref type="bibr" target="#b15">(16)</ref> for the gradients of (6), which shows that the vanishing gradient problem is also mitigated. Finally, we have tested UnICORNN on a suite of benchmarks that includes both synthetic as well as realistic learning tasks, designed to test the ability of an RNN to deal with long-time dependencies. In all the experiments, UnICORNN was able to show state of the art performance.</p><p>It is instructive to compare UnICORNN with two recently proposed RNN architectures, with which it shares some essential features. First, the use of coupled oscillators to design RNNs was already explored in the case of coRNN <ref type="bibr" target="#b40">[41]</ref>. In contrast to coRNN, neurons in UnICORNN are independent (uncoupled) and as there is no damping, UnICORNN possesses a Hamiltonian structure. This paves the way for invertibility as well as for mitigating the EVGP without any assumptions on the weights whereas the mitigation of EVGP with coRNN was conditional on restrictions on weights. Finally, UnICORNN provides even better performance on benchmarks than coRNN, while being significantly faster. While we also use independent neurons in each hidden layer and stack RNN layers together as in IndRNN <ref type="bibr" target="#b31">[32]</ref>, our design principle is completely different as it is based on Hamiltonian ODEs. Consequently, we do not impose weight restrictions, which are necessary for IndRNN to mitigate the EVGP. Moreover, in contrast to IndRNNs, our architecture is invertible and hence, memory efficient.</p><p>This work can be extended in different directions. First, UnICORNN is a very flexible architecture in terms of stacking layers of RNNs together. We have used a fully connected stacking in (6) but other possibilities can be readily explored. See Appendix D.5 for a discussion on the use of residual connections in stacking layers of UnICORNN. Second, the invertibility of UnICORNN can be leveraged in the context of normalizing flows <ref type="bibr" target="#b36">[37]</ref>, where the objective is to parametrize a flow such that the resulting Jacobian is readily computable. Finally, our focus in this article was on testing UnICORNN on learning tasks with long-time dependencies. Given that the underlying ODE (2) models oscillators, one can envisage that UnICORNN will be very competitive with respect to processing different time series data that arise in healthcare AI such as EEG and EMG data, as well as seismic time series from the geosciences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for:</head><p>UnICORNN: A recurrent model for learning very long time dependencies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training details</head><p>All experiments were run on GPU, namely NVIDIA GeForce GTX 1080 Ti and NVIDIA GeForce RTX 2080 Ti. The hidden weights w of the UnICORNN are initialized according to U (0, 1), while all biases are set to zero. The trained vector c is initialized according to U (−0.1, 0.1). The input weights V are initialized according to the Kaiming uniform initialization <ref type="bibr" target="#b19">[20]</ref> based on the input dimension mode and the negative slope of the rectifier set to = 8.</p><p>The hyperparameters of the UnICORNN are selected using a random search algorithm based on a validation set. The hyperparameters of the best performing UnICORNN can be seen in <ref type="table" target="#tab_5">Table 6</ref>. The value for Δ and is shared across all layers, except for the IMDB task and EigenWorms task, where we use a different Δ value for the first layer and the corresponding Δ value in <ref type="table" target="#tab_5">Table 6</ref> for all subsequent layers, i.e. we use Δ = 6.6 × 10 −3 for IMDB and Δ = 2.81 × 10 −5 for EigenWorms in the first layer. Additionally, the dropout column corresponds to variational dropout <ref type="bibr" target="#b15">[16]</ref>, which is applied after each consecutive layer. Note that for the IMDB task also an embedding dropout with = 0.65 is used.</p><p>We train the UnICORNN for a total of 50 epochs on the IMDB task and for a total of 250 epochs on the EigenWorms task. Moreover, we train UnICORNN for 650 epochs on psMNIST, after which we decrease the learning rate by a factor of 10 and proceed training for 3 times the amount of epochs used before reducing the learning rate. On all other tasks, UnICORNN is trained for 250 epochs, after which we decrease the learning rate by a factor of 10 and proceed training for additional 250 epochs. The resulting best performing networks are selected based on a validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation details</head><p>As already described in the implementation details of the main paper, we can speed up the computation of the forward and backward pass, by parallelizing the input transformation and computing the recurrent part for each independent dimension in an independent CUDA thread. While the forward/backward pass for the input transformation is simply that of an affine transformation, we discuss only the recurrent part.</p><p>Since we compute the gradients of each dimension of the UnICORNN independently and add them up afterwards to get the full gradient, we will simplify to the following one-dimensional system:</p><formula xml:id="formula_21">= −1 − Δˆ( ) [ ( −1 + ) + −1 ], = −1 + Δˆ( ) , where</formula><p>= (Vu ) is the transformed input corresponding to the respective dimension = 1, . . . , .</p><p>Since we wish to train the UnICORNN on some given objective</p><formula xml:id="formula_22">E := ∑︁ =1Ẽ ( ),<label>(17)</label></formula><p>whereẼ is some loss function taking the hidden states as inputs, for instance mean-square distance of (possibly transformed) hidden states to some ground truth. During training, we compute gradients of the loss function <ref type="bibr" target="#b16">(17)</ref> with respect to the following quantities Θ = [ , Δ , ], i.e.</p><formula xml:id="formula_23">E = ∑︁ =1Ẽ ( ) , ∀ ∈ Θ.<label>(18)</label></formula><p>We can work out a recursion formula to compute the gradients in <ref type="bibr" target="#b17">(18)</ref>. We will exemplarily provide the formula for the gradient with respect to the hidden weight . The computation of the gradients with respect to the other quantities follow similarly. Thus</p><formula xml:id="formula_24">= −1 + −1 Δˆ( ),<label>(19)</label></formula><formula xml:id="formula_25">= −1 − Δˆ( ) [ ( − + − +1 ) + ] +Ẽ − ,<label>(20)</label></formula><p>with initial values 0 =Ẽ and 0 = 0. The gradient can then be computed as</p><formula xml:id="formula_26">E = ∑︁ =1 , with = − Δˆ( ) ( − + − +1 ) − .<label>(21)</label></formula><p>Note that this recursive formula is a direct formulation of the back-propagation through time algorithm <ref type="bibr" target="#b44">[45]</ref> for the UnICORNN. We can verify formula (19)-(21) by explicitly calculating the gradient in <ref type="formula" target="#formula_0">(18)</ref>:</p><formula xml:id="formula_27">E = ∑︁ =1Ẽ ( ) = −1 ∑︁ =1Ẽ ( ) +Ẽ −1 + Δˆ( ) −1 − Δˆ( ) ( ( −1 + ) ( −1 + −1 ) + −1 = −2 ∑︁ =1Ẽ ( ) + 1 + 1 −1 + 1 −1 = −2 ∑︁ =1Ẽ ( ) + 1 + 1 −2 + ( 1 Δˆ( ) + 1 ) −2 − Δˆ( ) ( ( −2 + −1 ) ( −2 + −2 ) + −2 ) = −3 ∑︁ =1Ẽ ( ) + 2 ∑︁ =1 + 2 −2 + 2 −2 .</formula><p>Iterating the same reformulation yields the desired formula (19)- <ref type="bibr" target="#b20">(21)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Bounds on the ODE (2) of the main text.</head><p>Rewriting the ODE (2) of the main text as,</p><formula xml:id="formula_28">z = −[ (A) + y], y = z, A = w y + Vu + b,<label>(22)</label></formula><p>together with the initial data y 0 = z 0 = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Energy bounds.</head><p>We have the following energy bounds on the solutions of the ODE <ref type="formula" target="#formula_1">(22)</ref>, Proposition C.1. Let y( ), z( ) be the solutions of the ODE system <ref type="bibr" target="#b21">(22)</ref> at any time ∈ [0, 1] and the initial data for (22) is given by,</p><formula xml:id="formula_29">y(0) = z(0) ≡ 0.</formula><p>Then for &gt; 0, the solutions of (22) are bounded by,</p><formula xml:id="formula_30">y( ) 2 ≤ , z( ) 2 ≤ , ∀ ∈ (0, 1],<label>(23)</label></formula><p>and for = 0, the solutions of (22) are bounded by,</p><formula xml:id="formula_31">y( ) 2 ≤ 2 , z( ) 2 ≤ 2 , ∀ ∈ (0, ].<label>(24)</label></formula><p>Here x 2 = x, x , denotes the Euclidean norm of the vector x ∈ R and ·, · , the corresponding inner product.</p><p>Proof. To prove the bound (23), we take an inner product of the first equation in <ref type="bibr" target="#b21">(22)</ref> with y and an inner product of the second equation in <ref type="bibr" target="#b21">(22)</ref> with z and add the result to obtain,</p><formula xml:id="formula_32">y 2 + z 2 2 = − (A), z ≤ (A) 2 2 + z 2 2 (Cauchy s inequality) ≤ 2 + z 2 2 + y 2 2 (| | ≤ 1).</formula><p>Applying the Grönwall's inequality to the above differential inequality directly leads to,</p><formula xml:id="formula_33">y( ) 2 + z( ) 2 ≤ ≤ ,</formula><p>as ≤ 1 and (23) follows as a direct consequence.</p><p>To proof (24), we set = 0 in <ref type="bibr" target="#b21">(22)</ref> and take an inner product of the first equation in <ref type="bibr" target="#b21">(22)</ref> with y and an inner product of the second equation in <ref type="bibr" target="#b21">(22)</ref> with z and add the result to obtain,</p><formula xml:id="formula_34">y 2 + z 2 2 = y, z − (A), z ≤ (A) 2 2 + y 2 2 + z 2 (Cauchy s inequality) ≤ 2 + y 2 + z 2 (| | ≤ 1).</formula><p>The bound (24) follows as a direct consequence of the Grönwall's inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Hidden state gradients and Proof of proposition 3.1 of main text.</head><p>We recall that the ODE (8) in the main text for the hidden state gradients with respect to any trainable parameter ∈ Θ is,</p><formula xml:id="formula_35">y = z , z = − (A) (w y ) − y − (A) C( ),<label>(25)</label></formula><p>with the coefficient C ∈ R given by,</p><formula xml:id="formula_36">C =          [0, 0, . . . , y , . . . 0] , if = w , 0, 0, . . . , u , . . . 0 , if = V , [0, 0, . . . , 1, . . . 0] , if = b .<label>(26)</label></formula><p>Note that only the -th entry of C is non-zero and all the remaining entries are zero. It is clear from the bounds <ref type="formula" target="#formula_1">(23)</ref> and <ref type="formula" target="#formula_1">(24)</ref> that as long as we consider bounded inputs i.e., u( ) 2 &lt; , for some constants &gt; 0 and for all time ∈ [0, 1], the coefficient C( ) is bounded in the following sense,</p><formula xml:id="formula_37">C( ) 2 ≤ , ∀ ∈ [0, 1],<label>(27)</label></formula><p>for some constant that only depends on the dimension and on the constant that bounds the input. We restate Proposition 3.1 of the main text that provides an upper bound on the hidden state gradients, </p><formula xml:id="formula_38">y ( ) 2 ≤ y (0) 2 + 1 z (0) 2 w + w , z ( ) 2 ≤ y (0) 2 + z (0) 2 w + w ,<label>(28)</label></formula><p>with w = max w 2 ∞ , 2 . For = 0, the hidden state gradients are bounded by,</p><formula xml:id="formula_39">max y ( ) 2 , z ( ) 2 ≤ y (0) 2 + z (0) 2 w + w ,<label>(29)</label></formula><formula xml:id="formula_40">with w = max 1 + w 2 ∞ , 3 .</formula><p>Proof. To prove the bound (28), we take an inner product of the first equation in <ref type="bibr" target="#b24">(25)</ref> with y and an inner product of the second equation in <ref type="bibr" target="#b21">(22)</ref> with z and add the result to obtain,</p><formula xml:id="formula_41">y 2 + z 2 2 = − (A) (w y ), z − (A) C, z ≤ w 2 ∞ y 2 2 + z 2 + C 2 2</formula><p>(Cauchy s inequality and | | ≤ 1) ⇒ y 2 + z 2 ≤ w y 2 + z 2 + (by definition of w and <ref type="formula" target="#formula_1">(27)</ref>).</p><p>Applying the Grönwall's inequality to the above differential inequality directly leads to the bound,</p><formula xml:id="formula_42">y ( ) 2 + z ( ) 2 ≤ y (0) 2 + z (0) 2 w + w .<label>(30)</label></formula><p>This bound in turn, readily implies <ref type="bibr" target="#b27">(28)</ref>. The proof of the bound (29) is completely analogous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Rigorous bounds on UnICORNN</head><p>We rewrite UnICORNN (Eqn. <ref type="bibr" target="#b5">(6)</ref> in the main text) in the following form: for all 1 ≤ ℓ ≤ and for all 1 ≤ ≤ y ℓ, = y ℓ,</p><formula xml:id="formula_43">−1 + Δˆ(c ℓ, )z ℓ, , z ℓ, = z ℓ, −1 − Δˆ(c ℓ, ) (A ℓ, −1 ) − Δˆ(c ℓ, )y ℓ, −1 , A ℓ, −1 = w ℓ, y ℓ, −1 + V ℓ y ℓ−1 + b ℓ, .<label>(31)</label></formula><p>Here, we have denoted the -th component of a vector x as x . We follow standard practice and set y ℓ 0 = z ℓ 0 ≡ 0, for all 1 ≤ ℓ ≤ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Energy Bounds.</head><p>We have the following bounds on the discrete hidden states, Proposition D.1. Let y , z be the hidden states at the -th time level for UnICORNN <ref type="bibr" target="#b30">(31)</ref>, then under the assumption that the time step Δ &lt; 1 and &gt; 0, these hidden states are bounded as,</p><formula xml:id="formula_44">y ℓ 2 ≤ 2 1 + 2 Δ 3 , z ℓ 2 ≤ 2 1 + 2 Δ 3 , ∀ , ∀ 1 ≤ ℓ ≤ ,<label>(32)</label></formula><p>with the constant = 96</p><formula xml:id="formula_45">1 4 max{1, √ }.</formula><p>If = 0 and Δ &lt; 1/2, the hidden states satisfy the bound,</p><formula xml:id="formula_46">y ℓ 2 ≤ 2 2 .<label>(33)</label></formula><p>Proof. We fix ℓ, and multiply the first equation in <ref type="bibr" target="#b30">(31)</ref> with y ℓ, −1 and use the elementary identity</p><formula xml:id="formula_47">( − ) = 2 2 − 2 2 − 1 2 ( − ) 2 , to obtain (y ℓ, ) 2 2 = (y ℓ, −1 ) 2 2 + 2 (y ℓ, − y ℓ, −1 ) 2 + Δˆ(c ℓ, )y ℓ, −1 z ℓ, , = (y ℓ, −1 ) 2 2 + Δ 2 2 (ˆ(c ℓ, )) 2 (z ℓ, ) 2 + Δˆ(c ℓ, )y ℓ, −1 z ℓ, .<label>(34)</label></formula><p>Next, we multiply the second equation in <ref type="bibr" target="#b30">(31)</ref> with z ℓ, and use the elementary identity</p><formula xml:id="formula_48">( − ) = 2 2 − 2 2 + 1 2 ( − ) 2 , to obtain (z ℓ, ) 2 2 = (z ℓ, −1 ) 2 2 − 1 2 (z ℓ, − z ℓ, −1 ) 2 − Δˆ(c ℓ, ) (A ℓ, −1 ) z ℓ, − z ℓ, −1 − Δˆ(c ℓ, ) (A ℓ, −1 )z ℓ, −1 − Δˆ(c ℓ, )y ℓ, −1 z ℓ, .<label>(35)</label></formula><p>Adding <ref type="formula" target="#formula_2">(34)</ref> and <ref type="bibr" target="#b34">(35)</ref> and using Cauchy's inequality yields,</p><formula xml:id="formula_49">(y ℓ, ) 2 2 + (z ℓ, ) 2 2 ≤ (y ℓ, −1 ) 2 2 + (1 + Δ ) (z ℓ, −1 ) 2 2 + Δ 2 2 (ˆ(c ℓ, )) 2 (z ℓ, ) 2 + (ˆ(c ℓ, )) 2 ( (A ℓ, −1 )) 2 Δ + Δ − 1 2 (z ℓ, − z ℓ, −1 ) 2 ⇒ (y ℓ, ) 2 + (z ℓ, ) 2 ≤ (y ℓ, −1 ) 2 + (1 + Δ ) (z ℓ, −1 ) 2 + 2Δ + Δ 2 (z ℓ, ) 2 ,</formula><p>where the last inequality follows from the fact that | |, |ˆ| ≤ 1 and Δ &lt; 1. Using the elementary inequality,</p><formula xml:id="formula_50">( + + ) 2 ≤ 4 2 + 4 2 + 2 2 ,</formula><p>and substituting for z ℓ, from the second equation of (31) in the last inequality leads to,</p><formula xml:id="formula_51">(y ℓ, ) 2 + (z ℓ, ) 2 ≤ (1 + 4 2 Δ 4 ) (y ℓ, −1 ) 2 + (1 + Δ + 2 Δ 2 ) (z ℓ, −1 ) 2 + 2Δ + 4 Δ 4 .</formula><p>Denoting = (y ℓ, ) 2 + (z ℓ, ) 2 and</p><formula xml:id="formula_52">:= 1 + Δ + 2 Δ 2 + 4 2 Δ 4 ,</formula><p>yields the following inequality,</p><formula xml:id="formula_53">≤ −1 + 2Δ + 4 Δ 4 .<label>(36)</label></formula><p>Using the Taylor expansion for the exponential and the definition of , it is straightforward to check that,</p><formula xml:id="formula_54">≤ Δ .</formula><p>Thus, we have from (36) that,</p><formula xml:id="formula_55">≤ Δ −1 + 2Δ + 4 Δ 4 .<label>(37)</label></formula><p>Iterating (37) -times and using the fact that the initial data is such that 0 ≡ 0 we obtain,</p><formula xml:id="formula_56">≤ 2Δ + 4 Δ 4 −1 ∑︁ =0 Δ ≤ 2Δ + 4 Δ 4 Δ ≤ 2 + 4 Δ 3 ≤ 2 1 + 2 Δ 3 ,<label>(38)</label></formula><p>as ≤ . The bound <ref type="bibr" target="#b31">(32)</ref> follows directly from the definition of . The proof of (33) follows analogously. D.2 On the exploding gradient problem for UnICORNN and Proof of proposition 3.2 of the main text.</p><p>We train the RNN (31) to minimize the loss function,</p><formula xml:id="formula_57">E := 1 ∑︁ =1 E , E = 1 2 y −ȳ 2 2 ,<label>(39)</label></formula><p>withȳ being the underlying ground truth (training data). Note that the loss function (39) only involves the output at the last hidden layer (we set the affine output layer to identity for the sake of simplicity). During training, we compute gradients of the loss function (39) with respect to the trainable weights and</p><formula xml:id="formula_58">biases Θ = [w ℓ , V ℓ , b ℓ , c ℓ ], for all 1 ≤ ℓ ≤ i.e. E = 1 ∑︁ =1 E , ∀ ∈ Θ.<label>(40)</label></formula><p>We have the following bound on the gradient (40), Proposition D.2. Let &gt; 0, Δ &lt; 1 in the RNN (31) and let y ℓ , z ℓ , for 1 ≤ ℓ ≤ , be the hidden states generated by the RNN <ref type="bibr" target="#b30">(31)</ref>. Then, the gradient of the loss function E (39) with respect to any parameter ∈ Θ is bounded as,</p><formula xml:id="formula_59">E ≤ 1 − (Δ ) 1 − Δ w V( + F)∆,<label>(41)</label></formula><p>with¯= max 1≤ ≤ ȳ ∞ , be a bound on the underlying training data and other quantities in (41) defined as,</p><formula xml:id="formula_60">w = max 2, w ∞ + , V = =1 max{1, V ∞ }, F = √︂ 2 1 + 2 Δ 3 , ∆ = 2 + √︃ 2 1 + 2 Δ 3 + (2 + ) √︂ 2 1 + 2 Δ 3 .<label>(42)</label></formula><p>Proof. For any 1 ≤ ≤ and 1 ≤ ℓ ≤ , let X ℓ ∈ R 2 be the augmented hidden state vector defined by, X ℓ = y ℓ,1 , z ℓ,1 , . . . , y ℓ, , z ℓ, , . . . , y ℓ, , z ℓ, .</p><p>For any ∈ Θ, we can apply the chain rule repeatedly to obtain the following extension of the formula of <ref type="bibr" target="#b37">[38]</ref> to a deep RNN,</p><formula xml:id="formula_62">E = ∑︁ ℓ=1 ∑︁ =1 E X X X ℓ + X ℓ E ( , ) ,ℓ .<label>(44)</label></formula><p>Here, the notation + X ℓ refers to taking the partial derivative of X ℓ with respect to the parameter , while keeping the other arguments constant.</p><p>We remark that the quantity</p><formula xml:id="formula_63">E ( , )</formula><p>,ℓ denotes the contribution from the -recurrent step at the -th hidden layer of the deep RNN (31) to the overall hidden state gradient at the step .</p><p>It is straightforward to calculate that,</p><formula xml:id="formula_64">E X = y ,1 − y 1 , 0, . . . , y , − y , 0, . . . , y , − y , 0 .<label>(45)</label></formula><p>Repeated application of the chain and product rules yields,</p><formula xml:id="formula_65">X X ℓ = = +1 X X −1 =ℓ+1 X X −1 .<label>(46)</label></formula><p>For any , a straightforward calculation using the form of the RNN (31) leads to the following representation formula for the matrix</p><formula xml:id="formula_66">X X −1 ∈ R 2 × R 2 : X X −1 =            B ,1 0 . . . 0 0 B ,2 . . . 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 . . . 0 B ,            ,<label>(47)</label></formula><p>with the block matrices B , ∈ R 2×2 given by,</p><formula xml:id="formula_67">B , =       1 − (ˆ(c , )) 2 Δ 2 w , (A , −1 ) + ˆ(c , )Δ −ˆ(c , )Δ w , (A , −1 ) + 1       .<label>(48)</label></formula><p>Similarly for any , the matrix X X −1 ∈ R 2 ×2 can be readily computed as,</p><formula xml:id="formula_68">X X −1 =              D , 11 0 D , 12 0 . . . . . . D , 1 0 E , 11 0 E , 12 0 . . . . . . E , 1 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D , 1 0 D , 2 0 . . . . . . D , 0 E , 1 0 E , 2 0 . . . . . . E , 0              ,<label>(49)</label></formula><p>with entries given by,</p><formula xml:id="formula_69">D , ,¯= −Δ 2 (ˆ(c , )) 2 A , −1 V¯, E , ,¯= −Δˆ(c , ) A , −1 V¯.<label>(50)</label></formula><p>A direct calculation with (48) leads to,</p><formula xml:id="formula_70">B , ∞ ≤ max 1 + Δ + (|w , | + )Δ 2 , 1 + (|w , | + )Δ ≤ 1 + max 2, |w , | + Δ + max 2, |w , | + 2 Δ 2 2 .<label>(51)</label></formula><p>Using the definition of the ∞ norm of a matrix, we use (51) to the derive the following bound from (47),</p><formula xml:id="formula_71">X X −1 ∞ ≤ 1 + max 2, w ∞ + Δ + max 2, w ∞ + 2 Δ 2 2 ≤ w Δ ,<label>(52)</label></formula><p>with w defined in <ref type="bibr" target="#b41">(42)</ref>. As Δ &lt; 1, it is easy to see that, X</p><formula xml:id="formula_72">X −1 ∞ ≤ V ∞ Δ .<label>(53)</label></formula><p>Combining <ref type="formula" target="#formula_1">(52)</ref> and <ref type="formula" target="#formula_2">(53)</ref> , we obtain from (46)</p><formula xml:id="formula_73">X X ℓ ∞ ≤ = +1 X X −1 ∞ =ℓ+1 X X −1 ∞ ≤ Δ −ℓ =ℓ+1 V ∞ ( − )w Δ ≤ VΔ −ℓ w ,<label>(54)</label></formula><p>where the last inequality follows from the fact that = Δ ≤ and the definition of V in <ref type="bibr" target="#b41">(42)</ref>. Next, we observe that for any ∈ Θ</p><formula xml:id="formula_74">+ X ℓ = + y ℓ,1 , + z ℓ,1 . . . , . . . , + y ℓ, , + z ℓ,</formula><p>, . . . , . . . ,</p><formula xml:id="formula_75">+ y ℓ, , + z ℓ, .<label>(55)</label></formula><p>For any 1 ≤ ≤ , a direct calculation with the RNN (31) yields,</p><formula xml:id="formula_76">+ y ℓ, = Δˆ (c ℓ, ) c ℓ, z ℓ, + Δˆ(c ℓ, ) + z ℓ, , + z ℓ, = −Δˆ (c ℓ, ) c ℓ, (A ℓ, −1 ) − Δˆ(c ℓ, ) (A ℓ,<label>−1 )</label></formula><formula xml:id="formula_77">A ℓ, −1 − Δˆ (c ℓ, ) c ℓ, y ℓ, −1 .<label>(56)</label></formula><p>Next, we have to compute explicitly c ℓ, and A ℓ, −1 in order to complete the expressions (56). To this end, we need to consider explicit forms of the parameter and obtain, If = w , , for some 1 ≤ ≤ and 1 ≤ ≤ , then,</p><formula xml:id="formula_78">A ℓ, −1 = y ℓ, −1 , if = ℓ, = , 0, if otherwise.<label>(57)</label></formula><p>If = b , , for some 1 ≤ ≤ and 1 ≤ ≤ , then,</p><formula xml:id="formula_79">A ℓ, −1 = 1, if = ℓ, = , 0, if otherwise.<label>(58)</label></formula><p>If = V ,¯, for some 1 ≤ ≤ and 1 ≤ ,¯≤ , then,</p><formula xml:id="formula_80">A ℓ, −1 = y ℓ−1,¯, if = ℓ, = , 0, if otherwise.<label>(59)</label></formula><p>If = c , for any 1 ≤ ≤ and 1 ≤ ≤ , then,</p><formula xml:id="formula_81">A ℓ, −1 ≡ 0.<label>(60)</label></formula><p>Similarly, if = w , or = b , , for some 1 ≤ ≤ and 1 ≤ ≤ , or If = V ,¯, for some 1 ≤ ≤ and 1 ≤ ,¯≤ , then c ℓ, ≡ 0.</p><p>On the other hand, if = c , for any 1 ≤ ≤ and 1 ≤ ≤ , then</p><formula xml:id="formula_83">c ℓ, = 1, if = ℓ, = , 0, if otherwise.<label>(62)</label></formula><p>For any ∈ Θ, by substituting (57) to (62) into (56) and doing some simple algebra with norms, leads to the following inequalities,</p><formula xml:id="formula_84">+ z ℓ, ≤ Δ 1 + |y ℓ, −1 | + max |y ℓ, −1 |, |y ℓ−1,¯| , 1 ,<label>(63)</label></formula><p>and,</p><formula xml:id="formula_85">+ y ℓ, ≤ Δ |z ℓ, | + Δ 2 1 + |y ℓ, −1 | + max |y ℓ, −1 |, |y ℓ−1,¯| , 1 ,<label>(64)</label></formula><p>for any 1 ≤¯≤ . By the definition of ∞ of a vector and some straightforward calculations with (64) yields,</p><formula xml:id="formula_86">+ X ℓ ∞ ≤ Δ 2 + z ℓ ∞ + (1 + ) y ℓ −1 ∞ + y ℓ−1 ∞ .<label>(65)</label></formula><p>From the energy bounds (32), we can directly bound the above inequality further as,</p><formula xml:id="formula_87">+ X ℓ ∞ ≤ Δ 2 + √︃ 2 1 + 2 Δ 3 + (2 + ) √︂ 2 1 + 2 Δ 3 .<label>(66)</label></formula><p>By <ref type="bibr" target="#b44">(45)</ref> and the definition of as well as the bound (32) on the hidden states, it is straightforward to obtain that,</p><formula xml:id="formula_88">E X ∞ ≤ + √︂ 2 1 + 2 Δ 3 .<label>(67)</label></formula><p>From the definition in <ref type="bibr" target="#b43">(44)</ref> and the identity (44), we have</p><formula xml:id="formula_89">E ( , ) ,ℓ ≤ E X ∞ X X ℓ ∞ + X ℓ ∞ .<label>(68)</label></formula><p>Substituting (67), (66) and (52) into (68) yields,</p><formula xml:id="formula_90">E ( , ) ,ℓ ≤ Δ −ℓ+1 w V( + F)∆,<label>(69)</label></formula><p>with F and ∆ defined in <ref type="bibr" target="#b41">(42)</ref>. Therefore, from the fact that Δ &lt; 1, = Δ ≤ and (44), we obtain</p><formula xml:id="formula_91">E ≤ 1 − (Δ ) 1 − Δ w V( + F)∆.<label>(70)</label></formula><p>By the definition of the loss function <ref type="bibr" target="#b38">(39)</ref> and the fact that the right-hand-side of (70) is independent of leads to the desired bound (41).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 On the Vanishing gradient problem for UnICORNN and Proof of Proposition 3.3 of the main text.</head><p>By applying the chain rule repeatedly to the each term on the right-hand-side of (40), we obtain</p><formula xml:id="formula_92">E = ∑︁ ℓ=1 ∑︁ =1 E ( , ) ,ℓ , E ( , ) ,ℓ := E X X X ℓ + X ℓ .<label>(71)</label></formula><p>Here, the notation + X ℓ refers to taking the partial derivative of X ℓ with respect to the parameter , while keeping the other arguments constant. The quantity</p><formula xml:id="formula_93">E ( , )</formula><p>,ℓ denotes the contribution from the -recurrent step at the -th hidden layer of the deep RNN (31) to the overall hidden state gradient at the step . The vanishing gradient problem <ref type="bibr" target="#b37">[38]</ref> arises if</p><formula xml:id="formula_94">E ( , )</formula><p>,ℓ , defined in (71), → 0 exponentially fast in , for &lt;&lt; (long-term dependencies). In that case, the RNN does not have long-term memory, as the contribution of the -th hidden state at the ℓ-th layer to error at time step is infinitesimally small. As argued in the main text, the vanishing gradient problem for RNNs focuses on the possible smallness of contributions of the gradient over a large number of recurrent steps. As this behavior of the gradient is independent of the number of layers, we start with a result on the vanishing gradient problem for a single hidden layer here. Also, for the sake of definiteness, we set the scalar parameter = w 1, for some 1 ≤ ≤ . Similar results also hold for any other ∈ Θ. Moreover, we introduce the following order -notation,</p><formula xml:id="formula_95">= O ( ), for , ∈ R + if there exists constants , such that ≤ ≤ . M = O ( ), for M ∈ R 1 × 2 , ∈ R + if there exists constant such that M ≤ .<label>(72)</label></formula><p>We restate Proposition 3.3 of the main text, Proposition D.3. Let y be the hidden states generated by the RNN <ref type="bibr" target="#b30">(31)</ref>. Then the gradient for long-term dependencies, i.e. &lt;&lt; , satisfies the representation formula,</p><formula xml:id="formula_96">E ( ,1) ,1 w 1, = −Δˆ(c 1, ) 2 (A 1, −1 )y 1, −1 y 1, − y + O (Δ 2 ).<label>(73)</label></formula><p>Proof. Following the definition (71) and as = 1 and = w 1, , we have,</p><formula xml:id="formula_97">E ( ,1) ,1 w 1, := E X 1 X 1 X 1 + X 1 w 1, .<label>(74)</label></formula><p>We will explicitly compute all three expressions on the right-hand-side of (74). To start with, using (55), (56) and (57), we obtain,</p><formula xml:id="formula_98">+ X 1 w 1, = 0, 0, . . . , . . . , + y 1, w 1, , + z 1,</formula><p>w 1, , . . . , . . . , 0, 0 ,</p><formula xml:id="formula_99">+ X 1 w 1, 2 −1 = + y 1, w 1, = −Δ 2 (ˆ(c 1, )) 2 (A 1, −1 )y 1, −1 , + X 1 w 1, 2 = + z 1, w 1, = −Δˆ(c 1, ) (A 1, −1 )y 1, −1 .<label>(75)</label></formula><p>Using the product rule <ref type="bibr" target="#b45">(46)</ref> we have,</p><formula xml:id="formula_100">X 1 X 1 = = +1 X 1 X 1 −1 .<label>(76)</label></formula><p>Observing from the expressions (47) and (48) and using the order -notation (72), we obtain that,</p><formula xml:id="formula_101">X 1 X 1 −1 = I 2 ×2 + Δ C 1 + O (Δ 2 ),<label>(77)</label></formula><p>with I × is the × Identity matrix and the matrix C 1 defined by,</p><formula xml:id="formula_102">X X −1 =            C 1,1 0 . . . 0 0 C 1,2 . . . 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 . . . 0 C 1,            ,<label>(78)</label></formula><p>with the block matrices C 1, ∈ R 2×2 given by,</p><formula xml:id="formula_103">C 1, = 0ˆ(c 1, ) −ˆ(c 1, ) w 1, (A 1, −1 ) + 0 .<label>(79)</label></formula><p>By a straightforward calculation and the use of induction, we claim that = +1</p><formula xml:id="formula_104">X 1 X 1 −1 = I 2 ×2 + Δ C 1 + O (Δ 2 ),<label>(80)</label></formula><p>with</p><formula xml:id="formula_105">C 1 =           C 1,1 0 . . . 0 0 C 1,2 . . . 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 . . . 0 C 1,           ,<label>(81)</label></formula><p>with the block matrices C 1, ∈ R 2×2 given by,</p><formula xml:id="formula_106">C 1, =       0 ( − )ˆ(c 1, ) −( − )ˆ(c 1, ) −ˆ(c 1, )w 1, = +1 (A 1, −1 ) 0       .<label>(82)</label></formula><p>By the assumption that &lt;&lt; and using the fact that = Δ , we have that,</p><formula xml:id="formula_107">Δ C 1, =       0ˆ(c 1, ) −ˆ(c 1, ) −ˆ(c 1, )w 1, Δ = +1 (A 1, −1 ) 0       .<label>(83)</label></formula><p>Hence, the non-zero entries in the block matrices can be O (1). Therefore, the product formula (80) is modified to,</p><formula xml:id="formula_108">= +1 X 1 X 1 −1 = C + O (Δ ),<label>(84)</label></formula><p>with the 2 × 2 matrix C defined as,</p><formula xml:id="formula_109">C =           C 1 0 . . . 0 0 C 2 . . . 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 . . . 0 C           ,<label>(85)</label></formula><p>and,</p><formula xml:id="formula_110">C =       1ˆ(c 1, ) −ˆ(c 1, ) −ˆ(c 1, )w 1, Δ = +1 (A 1, −1 ) 1       .<label>(86)</label></formula><p>Thus by taking the product of (84) with (75), we obtain that,</p><formula xml:id="formula_111">= +1 X 1 X 1 −1 + X 1 w 1, = 0, 0, . . . , . . . , + y 1, w 1, + C 12 + z 1, w 1, , C 21 + y 1, w 1, + + z 1, w 1, . . . , . . . , 0, 0 +O (Δ 2 ),<label>(87)</label></formula><p>with C 12 , C 21 are the off-diagonal entries of the corresponding block matrix, defined in (86). Note that the O (Δ 2 ) remainder term arises from the Δ -dependence in (75). From <ref type="bibr" target="#b44">(45)</ref>, we have that,</p><formula xml:id="formula_112">E X 1 = y 1,1 − y 1 , 0, . . . , y 1, − y , 0, . . . , y 1, − y , 0 .<label>(88)</label></formula><p>Therefore, taking the products of (88) and (87) and substituting the explicit expressions in (75), we obtain the desired identity (73).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4</head><p>On the vanishing gradient problem for the multilayer version of UnI-CORNN.</p><p>The explicit representation formula (73) holds for 1 hidden layer in <ref type="bibr" target="#b30">(31)</ref>. What happens when additional hidden layers are stacked together as in UnICORNN <ref type="formula" target="#formula_0">(31)</ref>? To answer this question, we consider the concrete case of = 3 layers as this is the largest number of layers that we have used in the context of UnICORNN with fully connected stacked layers. As before, we set the scalar parameter = w 1, for some 1 ≤ ≤ . Similar results also hold for any other ∈ Θ. We have the following representation formula for the gradient in this case, Proposition D.4. Let y be the hidden states generated by the RNN <ref type="bibr" target="#b30">(31)</ref>. the gradient for long-term dependencies satisfies the representation formula,</p><formula xml:id="formula_113">E ( ,3) ,1 w 1, = Δ 4ˆ( c 1, ) + z 1, w 1, ∑︁ =1Ḡ 2 −1,2 −1 y 3, − y + O (Δ 6 ),<label>(89)</label></formula><p>with the coefficients given by,</p><formula xml:id="formula_114">+ z 1, w 1, = −Δˆ(c 1, ) (A 1, −1 )y 1, −1 , G 2 −1,2 −1 = ∑︁ =1 G 3 G 2 , ∀ 1 ≤ ≤ , G , = −(ˆ(c , )) 2 A , −1 V , = 2, 3.<label>(90)</label></formula><p>Proof. Following the definition (71) and as = 3 and = w 1, , we have,</p><formula xml:id="formula_115">E ( ,3) ,1 w 1, := E X 3 X 3 X 1 + X 1 w 1, .<label>(91)</label></formula><p>We will explicitly compute all three expressions on the right-hand-side of (91). In (75), we have already explicitly computed the right most expression in the RHS of (91). Using the product rule <ref type="bibr" target="#b45">(46)</ref> we have,</p><formula xml:id="formula_116">X 3 X 1 = X 3 X 2 X 2 X 1 = +1 X 1 X 1 −1 .<label>(92)</label></formula><p>Note that we have already obtained an explicit representation formula for = +1</p><formula xml:id="formula_117">X 1 X 1 −1 in (84).</formula><p>Next we consider the matrices X 3 X 2 and X 2 X 1 . By the representation formula (49), we have the following decomposition for any 1 ≤ ≤ , X</p><formula xml:id="formula_118">X −1 = Δ 2 G , + Δ , ,<label>(93)</label></formula><p>with,</p><formula xml:id="formula_119">G , =             G , 11 0 G , 12 0 . . . . . . G , 1 0 0 0 0 0 . . . . . . 0 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G , 1 0 G , 2 0 . . . . . . G , 0 0 0 0 0 . . . . . . 0 0             , G , ,¯= −(ˆ(c , )) 2 A , −1 V¯,<label>(94)</label></formula><p>and</p><formula xml:id="formula_120">H , =             0 0 0 0 . . . . . . 0 0 H , 11 0 H , 12 0 . . . . . . H , 1 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0 0 0 0 . . . . . . 0 0 H , 1 0 H , 2 0 . . . . . . H , 0             , H , ,¯= −ˆ(c , ) A , −1 V¯.<label>(95)</label></formula><p>It is straightforward to see from <ref type="formula" target="#formula_4">(95)</ref> and (94) that,</p><formula xml:id="formula_121">H 3, H 2, ≡ 0 2 ×2 , G 3, H 2, ≡ 0 2 ×2 ,<label>(96)</label></formula><p>and the entries of the 2 × 2 matrixḠ = G 3, G 2, are given by,</p><formula xml:id="formula_122">G 2 −1,2 −1 = ∑︁ =1 G 3, , G 2, , ,Ḡ 2 −1,2 =Ḡ 2 ,2 −1 =Ḡ 2 ,2 = 0, ∀ 1 ≤ , ≤ ,<label>(97)</label></formula><p>while the entries of the 2 × 2 matrixH = H 3, G 2, are given bȳ</p><formula xml:id="formula_123">H 2 ,2 −1 = ∑︁ =1 H 3, , G 2, , ,H 2 −1,2 −1 =H 2 −1,2 =H 2 ,2 = 0, ∀ 1 ≤ , ≤ .<label>(98)</label></formula><p>Hence we have,</p><formula xml:id="formula_124">X 3 X 2 X 2 X 1 = Δ 4 (Ḡ + Δ −1H ).<label>(99)</label></formula><p>Taking the matrix-vector product of (99) with (87), we obtain</p><formula xml:id="formula_125">X 3 X 1 + X 1 w 1, = Δ 4 + y 1, w 1, + C 12 + z 1, w 1, Ḡ 1,2 −1 , Δ −1H 2,2 −1 , . . . ,Ḡ 2 −1,2 −1 , Δ −1H 2 ,2 −1 + O (Δ 6 ) = Δ 4 C 12 + z 1, w 1, Ḡ 1,2 −1 , Δ −1H 2,2 −1 , . . . ,Ḡ 2 −1,2 −1 , Δ −1H 2 ,2 −1 + O (Δ 6 ),<label>(100)</label></formula><p>where the last identify follows from the fact that + y 1,</p><formula xml:id="formula_126">w 1, = O (Δ 2 )</formula><p>. Therefore, taking the products of (88) and (100), we obtain the desired identity (89).</p><p>An inspection of the representation formula (89) shows that as long as the weights are O (1) and from the energy bounds (28), we know that y ∼ O (1), the gradient</p><formula xml:id="formula_127">E ( ,3) ,1 w 1, ∼ O (Δ 5 ),</formula><p>where the additional Δ stems from the Δ -term in (75). Thus the gradient does not depend on the recurrent step . Hence, there is no vanishing gradient problem with respect to the number of recurrent connections, even in the multi-layer case.</p><p>However, it is clear from the representation formulas (73) and (89), as well as the proof of proposition D.4 that for -hidden layers in UnICORNN (31), we have,</p><formula xml:id="formula_128">E ( , ) ,1 w 1, ∼ O Δ 2 −1 .<label>(101)</label></formula><p>Thus, the gradient can become very small if too many layers are stacked together. This is not at all surprising as such a behavior occurs even if there are no recurrent connections in UnICORNN <ref type="bibr" target="#b30">(31)</ref>. In that case, we simply have a fully connected deep neural network and it is well-known that the gradient can vanish as the number of layers increases, making it harder to train deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Residual stacking of layers in UnICORNN.</head><p>Given the above considerations, it makes imminent sense to modify the fully-connected stacking of layers in UnICORNN (31) if a moderately large number of layers ( ≥ 4) are used. It is natural to modify the fully-connected stacking with a residual stacking, see <ref type="bibr" target="#b32">[33]</ref>. We use the following form of residual stacking,</p><formula xml:id="formula_129">y ℓ = y ℓ −1 + Δˆ(c ℓ ) z ℓ ,<label>(102)</label></formula><formula xml:id="formula_130">z ℓ = z ℓ −1 − Δˆ(c ℓ ) [ w ℓ y ℓ −1 + x ℓ + b + y ℓ −1 ],<label>(103)</label></formula><p>where the input x ℓ corresponds to a residual connection skipping layers, i.e.</p><formula xml:id="formula_131">x ℓ = Λ ℓ y ℓ− −1 + V ℓ y ℓ−1 , for &gt; V ℓ y ℓ−1 , for ≤ .</formula><p>The number of skipped layers is 2 ≤ and Λ ℓ ∈ R × is a trainable matrix. The main advantages of using a residual staking such as (102) is to alleviate the vanishing gradient problem that arises from stacking multiple layers together and obtain a better scaling of the gradient than (101). To see this, we can readily follow the proof of proposition D.4, in particular the product,</p><formula xml:id="formula_132">X X 1 = =1 X −( −1) X − − ℓ=2 X ℓ X ℓ−1 + −1 ℓ=1 X ℓ+1 X ℓ ,<label>(104)</label></formula><p>with,</p><formula xml:id="formula_133">= , if mod ≠ 0, − 1, if mod = 0.<label>(105)</label></formula><p>Here [ ] ∈ N is the largest natural number less than or equal to ∈ R. Given the additive structure in the product of gradients and using induction over matrix products as in (96) and (97), we can compute that,</p><formula xml:id="formula_134">X X 1 = O Δ 2( + − −1 ) + O Δ 2( −1) .<label>(106)</label></formula><p>By choosing large enough, we clearly obtain that + − − 1 &lt; − 1. Hence by repeating the arguments of the proof of proposition D.4, we obtain that to leading order, the gradient of the residual stacked version of UnICORNN scales like,</p><formula xml:id="formula_135">E ( , ) ,1 w 1, ∼ O Δ 2 +2 −2 −1 .<label>(107)</label></formula><p>Note that (107) is far more favorable scaling for the gradient than the scaling (101) for a fully connected stacking. As a concrete example, let us consider = 7 i.e., a network of 7 stacked layers of UniCORNN.</p><p>From (101), we see that the gradient scales like O (Δ 13 ) in this case. Even for a very moderate values of Δ &lt; 1, this gradient will be very small and will ensure that the first layer will have very little, if any, influence on the loss function gradients. On the other hand, for the same number of layers = 7, let us consider the residual stacking (102) with = 3 skipped connections. In this case = 2 and one directly concludes from (107) that the gradient scales like O (Δ 5 ), which is significantly larger than the gradient for the fully connected version of UnICORNN. In fact, it is exactly the same as the gradient scaling for fully connected UnICORNN (31) with 3 hidden layers (89). Thus, introducing skipped connections enabled the gradient to behave like a shallower fully-connected network, while possibly showing the expressivity of a deeper network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Further experimental results</head><p>As we compare the results of the UnICORNN to the results of other recent RNN architecture, where only the best results of each RNN were published for the psMNIST, noise padded CIFAR-10 and IMDB task, we as well show the best (based on a validation set) obtained results for the UnICORNN in the main paper. However, distributional results, i.e. statistics of several re-trainings of the best performing UnICORNN based on different random initialization of the trainable parameters, provide additional insights into the performance. <ref type="table" target="#tab_7">Table 7</ref> shows the mean and standard deviation of 10 re-trainings of the best performing UnICORNN for the psMNIST, noise padded CIFAR-10 and IMDB task. We can see that in all experiments the standard deviation of the re-trainings are relatively low, which underlines the robustness of our presented results. As emphasized in the main paper and in the last section, naively stacking of many layers for the UnICORNN might result in a vanishing gradient for the deep multi-layer model, due to the vanishing gradient problem of stacking many (not necessarily recurrent) layers. Following section D.5, one can use skipped residual connections and we see that the estimate on the gradients scale preferably when using residual connections compared to a naively stacking, when using many layers. To test this also numerically, we train a standard UnICORNN (31) as well as a residual UnICORNN (res-UnICORNN) (102), with = 2 skipping layers, on the noise padded CIFAR-10 task. <ref type="figure">Fig. 3</ref> shows the test accuracy (mean and standard deviation) of the best resulting model for different number of network layers = 3, . . . , 6, for the standard UnICORNN and res-UnICORNN. We can see that while both models seem to perform comparably for using only few layers, i.e. = 3, 4, the res-UnICORNN with = 2 skipping connections outperforms the standard UnICORNN when using more layers, i.e. = 5, 6. In particular, we can see that the standard UnICORNN is not able to significantly improve the test accuracy when using more layers, while the res-UnICORNN seems to obtain higher test accuracies when using more layers.</p><p>Moreover, <ref type="figure">Fig. 3</ref> also shows the test accuracy for a UnICORNN with an untrained time-step vector c, resulting in a UnICORNN without the multi-scale property generated by the time-step. We can see that the UnICORNN without the multi-scale feature is inferior in performance, to the standard UnICORNN as well as its residual counterpart.</p><p>Finally, we recall that the estimate (41) on the gradients for UnICORNN <ref type="bibr" target="#b30">(31)</ref> needs the weights to be bounded, see <ref type="bibr" target="#b41">(42)</ref>. One always initializes the training with bounded weights. However, it might happen that the weights explode during training. To check this issue, in <ref type="figure">Fig. 4</ref>, we plot the mean and standard deviation of the norms of the hidden weights w for = 1, 2, 3 during training based on 10 re-trainings  of the best performing UnICORNN on the noise padded CIFAR-10 experiment. We can see that none of the norms of the weights explode during training. In fact the weight norms seem to saturate, mostly on account of reducing the learning rate after 250 epochs. Thus, the upper bound (41) can be explicitly computed and it is finite, even after training has concluded.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Measured computing time for the combined forward and backward pass for the UnI-CORNN as well as for three of the fastest available RNN implementations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Test accuracy (mean and standard deviation) for the UnICORNN on EigenWorms for two types of sub-sampling approaches, i.e. using the last entries of the sequences as well as using a random subset of the entries. Both are shown for increasing number of entries used in each corresponding sub-sampling routine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Test accuracies (mean and standard deviation of 10 re-trainings of the best performing model) of the standard UnICORNN, res-UnICORNN and UnICORNN without multi-scale behavior on the noise padded CIFAR-10 experiment for different number of layers . Norms (mean and standard deviation of 10 re-trainings) of the hidden weights w ∞ , for = 1, 2, 3, of the UnICORNN during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test accuracies on noise padded CIFAR-10 together with number of hidden units as well as total number of parameters for each network. All other results are taken from literature, specified in the main text.</figDesc><table><row><cell>Model</cell><cell cols="2">test accuracy # units</cell><cell># params</cell></row><row><cell>LSTM</cell><cell>11.6%</cell><cell>128</cell><cell>64k</cell></row><row><cell>Incremental RNN</cell><cell>54.5%</cell><cell>128</cell><cell>12k</cell></row><row><cell>Lipschitz RNN</cell><cell>55.8%</cell><cell>256</cell><cell>158k</cell></row><row><cell>FastRNN</cell><cell>45.8%</cell><cell>128</cell><cell>16k</cell></row><row><cell>anti.sym. RNN</cell><cell>48.3%</cell><cell>256</cell><cell>36k</cell></row><row><cell cols="2">gated anti.sym. RNN 54.7%</cell><cell>256</cell><cell>37k</cell></row><row><cell>coRNN</cell><cell>59.0%</cell><cell>128</cell><cell>46k</cell></row><row><cell cols="2">UnICORNN ( =3) 62.4%</cell><cell>128</cell><cell>47k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test accuracies on EigenWorms using 5 re-trainings of each best performing network (based on the validation set) together with number of hidden units as well as total number of parameters for each network.</figDesc><table><row><cell>Model</cell><cell cols="2">test accuracy # units</cell><cell># params</cell></row><row><cell>t-BPTT LSTM</cell><cell>57.9%±7.0%</cell><cell>32</cell><cell>5.3k</cell></row><row><cell>sub-samp. LSTM</cell><cell>69.2%±8.3%</cell><cell>32</cell><cell>5.3k</cell></row><row><cell>sign.-NCDE</cell><cell>77.8%±5.9%</cell><cell>32</cell><cell>35k</cell></row><row><cell>expRNN</cell><cell cols="2">40.0%±10.1% 64</cell><cell>2.8k</cell></row><row><cell>IndRNN ( =2)</cell><cell>49.7%±4.8%</cell><cell>32</cell><cell>1.6k</cell></row><row><cell>coRNN</cell><cell>86.7%±3.0%</cell><cell>32</cell><cell>2.4k</cell></row><row><cell cols="3">UnICORNN ( =2) 90.3%±3.0% 32</cell><cell>1.5k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>2 test error on vital sign prediction using 5 re-trainings of each best performing network (based on the validation set), where the respiratory rate (RR) and heart rate (HR) is estimated based on PPG and ECG data. UnICORNN ( =3) 1.06±0.03 1.39±0.09 Sentiment analysis: IMDB As a final experiment, we test the proposed UnICORNN on the widely used NLP benchmark data set IMDB</figDesc><table><row><cell>Model</cell><cell>RR</cell><cell>HR</cell></row><row><cell>sign.-NCDE</cell><cell>1.51±0.08</cell><cell>2.97±0.45</cell></row><row><cell>LSTM</cell><cell>2.28±0.25</cell><cell>10.7±2.0</cell></row><row><cell>expRNN</cell><cell>1.57±0.16</cell><cell>1.87±0.19</cell></row><row><cell>IndRNN ( =3)</cell><cell>1.47±0.09</cell><cell>2.1±0.2</cell></row><row><cell>coRNN</cell><cell>1.45±0.23</cell><cell>1.71±0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Test accuracies on IMDB together with number of hidden units as well as total number of parameters (without embedding) for each network. All other results are taken from literature, specified in the main text.</figDesc><table><row><cell>Model</cell><cell cols="2">test accuracy # units</cell><cell># params</cell></row><row><cell>LSTM</cell><cell>86.8%</cell><cell>128</cell><cell>220k</cell></row><row><cell>skip LSTM</cell><cell>86.6%</cell><cell>128</cell><cell>220k</cell></row><row><cell>GRU</cell><cell>85.2%</cell><cell>128</cell><cell>99k</cell></row><row><cell>ReLU GRU</cell><cell>84.8%</cell><cell>128</cell><cell>99k</cell></row><row><cell>skip GRU</cell><cell>86.6%</cell><cell>128</cell><cell>165k</cell></row><row><cell>coRNN</cell><cell>87.4 %</cell><cell>128</cell><cell>46k</cell></row><row><cell cols="2">UnICORNN ( =2) 88.4%</cell><cell>128</cell><cell>30k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters of the best performing UnICORNN architecture (based on a validation set) for each experiment.</figDesc><table><row><cell>Experiment</cell><cell cols="2">learning rate dropout</cell><cell cols="2">batch size Δ</cell></row><row><cell>noise padded CIFAR-10</cell><cell>3.14 × 10 −2</cell><cell cols="2">1.0 × 10 −1 30</cell><cell cols="2">1.26 × 10 −1 13.0</cell></row><row><cell cols="2">psMNIST (#units = 128) 1.14 × 10 −3</cell><cell cols="2">1.0 × 10 −1 64</cell><cell cols="2">4.82 × 10 −1 12.53</cell></row><row><cell cols="2">psMNIST (#units = 256) 2.51 × 10 −3</cell><cell cols="2">1.0 × 10 −1 32</cell><cell>1.9 × 10 −1</cell><cell>30.65</cell></row><row><cell>IMDB</cell><cell>1.67 × 10 −4</cell><cell cols="2">6.1 × 10 −1 32</cell><cell cols="2">2.05 × 10 −1 0.0</cell></row><row><cell>EigenWorms</cell><cell>8.59 × 10 −3</cell><cell>0.0</cell><cell>8</cell><cell cols="2">3.43 × 10 −2 0.0</cell></row><row><cell>Healthcare: RR</cell><cell>3.98 × 10 −3</cell><cell cols="2">1.0 × 10 −1 32</cell><cell>1.1 × 10 −2</cell><cell>9.0</cell></row><row><cell>Healthcare: HR</cell><cell>2.88 × 10 −3</cell><cell cols="2">1.0 × 10 −1 32</cell><cell>4.6 × 10 −2</cell><cell>10.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Proposition C.2. Let y ( ), z ( ) be the solutions of the ODE system (25) for hidden state gradients, at any time ∈ [0, 1]. Then for &gt; 0 and for all ∈ [0, 1], the hidden state gradients are bounded by,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Distributional information (mean and standard deviation) on the results for the classification experiment presented in the paper, where only the best results is shown, based on 10 re-trainings of the best performing UnICORNN using different random seeds.</figDesc><table><row><cell>Experiment</cell><cell>Mean Standard deviation</cell></row><row><cell>psMNIST (128 units)</cell><cell>97.7% 0.09%</cell></row><row><cell>psMNIST (256 units)</cell><cell>98.2% 0.22%</cell></row><row><cell cols="2">Noise padded CIFAR-10 61.5% 0.52%</cell></row><row><cell>IMDB</cell><cell>88.1% 0.19%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>The research of TKR and SM was partially supported by European Research Council Consolidator grant ERCCoG 770880: COMANFLO.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Optimizing performance of recurrent neural networks on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01946</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mathematical methods of classical mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Arnold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The uea multivariate time series classification archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Southam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00075</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skip RNN: learning to skip state updates in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giró-I-Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trivializations for gradient-based optimization on manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Casado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9154" to="9164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martínez-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3794" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Antisymmetricrnn: A dynamical system view on recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Symplectic recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gate-variants of gated recurrent unit (gru) neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Salemt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1597" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A proposal on machine learning via dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lipschitz recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hamiltonian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15379" to="15389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Nonlinear oscillations, dynamical systems, and bifurcations of vector fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guckenheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Holmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometric numerical integration illustrated by the störmer-verlet method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hairer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lubich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="399" to="450" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Orthogonal recurrent neural networks with scaled cayley transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Helfrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1969" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent orthogonal networks and long-memory tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>M. F. Balcan and K. Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2034" to="2042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rnns incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Non-normal recurrent neural network (nnrnn): learning long time dependencies while improving expressivity with transient dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kerg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goyette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Touzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lajoie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13591" to="13601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9017" to="9028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06251</idno>
		<title level="m">Deep independently recurrent neural network (indrnn)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reversible recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9029" to="9040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Neural cdes for long time series via the log-ode method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08295</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Normalizing flows for probabilistic modeling and inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02762v1</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">1310</biblScope>
		</imprint>
	</monogr>
	<note>III-1318. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Toward a robust estimation of respiratory rate from pulse oximeters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Birrenkott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Watkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1914" to="1923" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coupled oscillatory recurrent neural network (cornn): An accurate and (gradient) stable architecture for learning long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Numerical Hamiltonian problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanz</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Calvo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Chapman and Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Nonlinear Dynamics and Chaos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strogatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Westview, Boulder CO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Monash university, uea, ucr time series regression archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10996</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
