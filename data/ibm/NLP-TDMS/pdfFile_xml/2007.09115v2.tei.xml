<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scale Equivariance Improves Siamese Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sosnovik</surname></persName>
							<email>i.sosnovik@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">UvA-Bosch Delta Lab University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Moskalev</surname></persName>
							<email>a.moskalev@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">UvA-Bosch Delta Lab University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Smeulders</surname></persName>
							<email>a.w.m.smeulders@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">UvA-Bosch Delta Lab University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scale Equivariance Improves Siamese Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Siamese trackers turn tracking into similarity estimation between a template and the candidate regions in the frame. Mathematically, one of the key ingredients of success of the similarity function is translation equivariance. Non-translation-equivariant architectures induce a positional bias during training, so the location of the target will be hard to recover from the feature space. In real life scenarios, objects undergoe various transformations other than translation, such as rotation or scaling. Unless the model has an internal mechanism to handle them, the similarity may degrade. In this paper, we focus on scaling and we aim to equip the Siamese network with additional built-in scale equivariance to capture the natural variations of the target a priori. We develop the theory for scaleequivariant Siamese trackers, and provide a simple recipe for how to make a wide range of existing trackers scaleequivariant. We present SE-SiamFC, a scale-equivariant variant of SiamFC built according to the recipe. We conduct experiments on OTB and VOT benchmarks and on the synthetically generated T-MNIST and S-MNIST datasets. We demonstrate that a built-in additional scale equivariance is useful for visual object tracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Siamese trackers turn tracking into similarity estimation between a template and the candidate regions in the frame. The Siamese networks are successful because the similarity function is powerful: it can learn the variances of appearance very effectively, to such a degree that even the association of the frontside of an unknown object to its backside is usually successful. And, once the similarity is effective, the location of the candidate region is reduced to simply selecting the most similar candidate.</p><p>Mathematically, one of the key ingredients of the success of the similarity function is translation equivariance, i.e. a * equal contribution Source code: https://github.com/isosnovik/SiamSE translation in the input image is to result in the proportional translation in feature space. Non-translation-equivariant architectures will induce a positional bias during training, so the location of the target will be hard to recover from the feature space <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref>. In real-life scenarios, the target will undergo more transformations than just translation, and, unless the network has an internal mechanism to handle them, the similarity may degrade. We start from the position that equivariance to common transformations should be the guiding principle in designing conceptually simple yet robust trackers. To that end, we focus on scale equivariance for trackers in this paper.</p><p>Measuring scale precisely is crucial when the camera zooms its lens or when the target moves into depth. However, scale is also important in distinguishing among objects in general. In following a marching band or in analyzing a soccer game, or when many objects in the video have a similar appearance (a crowd, team sports), the sim-ilarity power of Siamese trackers has a hard time locating the right target. In such circumstances, spatial-scale equivariance will provide a richer and hence more discriminative descriptor, which is essential to differentiate among several similar candidates in an image. And, even, as we will demonstrate, when the sequence does not show variation over scale, proper scale measurement is important to keep the target bounding box stable in size.</p><p>The common way to implement scale into a tracker is to train the network on a large dataset where scale variations occur naturally. However, as was noted in <ref type="bibr" target="#b19">[20]</ref>, such training procedures may lead to learning groups of re-scaled duplicates of almost the same filters. As a consequence, interscale similarity estimation becomes unreliable, see <ref type="figure" target="#fig_0">Figure  1</ref> top. Scale-equivariant models have an internal notion of scale and built-in weight sharing among different filter scales. Thus, scale equivariance aims to produce the same distinction for all sizes, see <ref type="figure" target="#fig_0">Figure 1</ref> bottom.</p><p>In this paper, we aim to equip the Siamese network with spatial and scale equivariance built-in from the start to capture the natural variations of the target a priori. We aim to improve a broad class of tracking algorithms by enhancing their capacity of candidate distinction. We adopt recent advances <ref type="bibr" target="#b27">[28]</ref> in convolutional neural networks (CNNs) which handle scale variations explicitly and efficiently.</p><p>While scale-equivariant convolutional models have lead to success in image classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>, we focus on their usefulness in object localization. Where scale estimation has been used in the localization for tracking, it typically relies on brute-force multi-scale detection with an obvious computational burden <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2]</ref>, or on a separate network to estimate the scale <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>. Both approaches will require attention to avoid bias and the propagation thereof through the network. Our new method treats scale and scale equivariance as a desirable fundamental property, which makes the algorithm conceptually easier. Hence, scale equivariance should be easy to merge into an existing network for tracking. Then, scale equivariance will enhance the performance of the tracker without further modification of the network or extensive data augmentation during the learning phase.</p><p>We make the following contributions:</p><p>• We propose the theory for scale-equivariant Siamese trackers and provide a simple recipe of how to make a wide range of existing trackers scale-equivariant.</p><p>• We propose building blocks necessary for efficient implementation of scale equivariance into modern Siamese trackers and implement a scale-equivariant extension of the recent SiamFC+ <ref type="bibr" target="#b37">[38]</ref> tracker.</p><p>• We demonstrate the advantage of scale-equivariant Siamese trackers over their conventional counterparts on popular benchmarks for sequences with and without apparent scale changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Siamese tracking The challenge of learning to track arbitrary objects can be addressed by deep similarity learning <ref type="bibr" target="#b1">[2]</ref>. The common approach is to employ Siamese networks to compute the embeddings of the original patches. The embeddings are then fused to obtain a location estimate. Such formulation is general, allowing for a favourable flexibility in the design of the tracker. In <ref type="bibr" target="#b1">[2]</ref> Bertinetto et al. employ off-line trained CNNs as feature extractors. The authors compare dot-product similarities between the feature map of the template with the maps coming from the current frame and measure similarities on multiple scales. Held et al. <ref type="bibr" target="#b13">[14]</ref> suggest a detection-based Siamese tracker, where the similarity function is modeled as a fullyconnected network. Extensive data augmentation is applied to learn a similarity function, which generalizes for various object transformations. Li et al. <ref type="bibr" target="#b21">[22]</ref> consider tracking as a one-shot detection problem to design Siamese region-proposal-networks <ref type="bibr" target="#b24">[25]</ref> by fusing the features from a fully-convolutional backbone. The recent ATOM <ref type="bibr" target="#b5">[6]</ref> and DIMP <ref type="bibr" target="#b2">[3]</ref> trackers employ a multi-stage tracking framework, where an object is coarsely localized by the online classification branch, and subsequently refined in its position by the estimation branch. From a Siamese perspective, in both <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref> the object embeddings are first fused to produce an initial location and subsequently processed by the IoU-Net <ref type="bibr" target="#b16">[17]</ref> to enhance the precision of the bounding box. The aforementioned references have laid the foundation for most of the state-of-the-art trackers. These methods share an implicit or explicit attention to translation equivariance for feature extraction. The decisive role of translation equivariance is noted in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38]</ref>. Bertinetto et al. <ref type="bibr" target="#b1">[2]</ref> utilize fully-convolutional networks where the output directly commutes with a shift in the input image as a function of the total stride. Li et al. <ref type="bibr" target="#b20">[21]</ref> suggest a training strategy to eliminate the spatial bias introduced in nonfully-convolutional backbones. Along the same line, Zhang and Peng <ref type="bibr" target="#b37">[38]</ref> demonstrated that deep state-of-the-art models developed for classification are not directly applicable for localization. And hence these models are not directly applicable to tracking as they induce positional bias, which breaks strict translation equivariance. We argue that transformations, other then translation, such as rotation may be equally important for certain classes of videos like sports and following objects in the sea or in the sky. And we argue that scale transformation is common in the majority of sequences due to the changing distances between objects and the camera. In this paper, we take on the latter class of transformations for tracking.</p><p>Equivariant CNNs Various works on transformationequivariant convolutional networks have been published recently. They extend the built-in property of translation- equivariance of conventional CNNs to a broader set of transformations. Mostly considered was roto-translation, as demonstrated on image classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>, image segmentation <ref type="bibr" target="#b30">[31]</ref> and edge detection <ref type="bibr" target="#b32">[33]</ref>.</p><p>One of the first works on scale-translation-equivariant convolutional networks was by Marcos et al. <ref type="bibr" target="#b23">[24]</ref>. In order to process images on multiple scales, the authors resize and convolve the input of each layer multiple times, forming a stack of features which corresponds to variety of scales. The output of such a convolutional layer is a vector whose length encodes the maximum response in each position among different scales. The direction of the vector is derived from the scale, which gave the maximum. The method has almost no restrictions in the choice of admissible scales. As this approach relies on rescaling the image, the obtained models are significantly slower compared to conventional CNNs. Thus, this approach is not suitable for being applied effectively in visual object tracking.</p><p>Worrall &amp; Welling <ref type="bibr" target="#b31">[32]</ref> propose Deep Scale-Spaces, an equivariant model which generalizes the concept of scalespace to deep networks. The approach uses filter dilation to analyze the images on different scales. It is almost as fast as a conventional CNN with the same width and depth. As the method is restricted to integer scale factors it is unsuited to applications in tracking where the scene dictates arbitrary scale factors.</p><p>Almost simultaneously, three papers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">39]</ref> were proposed to implement scale-translation-equivariant networks with arbitrary scales. What they have in common is that they use a pre-calculated and fixed basis defined on multiple scales. All filters are then calculated as a linear combination of the basis and trainable weights. As a result, no rescaling is used. We prefer to use <ref type="bibr" target="#b27">[28]</ref>, as Sosnovik et al. propose an approach for building general scaletranslation-equivariant networks with an algorithm for the fast implementation of the scale-convolution.</p><p>To date, the application of scale-equivariant networks was mostly demonstrated in image classification. Almost no attention was paid to tasks that involve object localization, such as visual object tracking. As we have noted above, it is a fundamentally different case. To the best of our knowledge, we demonstrate the first application of transformation-equivariant CNNs to visual object tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scale-Equivariant Tracking</head><p>In this work, we consider a wide range of modern trackers which can be described by the following formula:</p><formula xml:id="formula_0">h(z, x) = φ X (x) φ Z (z)<label>(1)</label></formula><p>where z, x are the template and the input frame, and φ X , φ Z are the functions which process them, and is the convolution operator which implements a connection between two signals. The resulting value h(z, x) is a heatmap that can be converted into a prediction by relatively simple calculations. Functions φ X , φ Z here can be parametrized as feed-forward neural networks. For our analysis, it is both suitable if the weights of these networks are fixed or updated during training or inference. This pipeline describes the majority of Siamese trackers such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref> and the trackers based on correlation filters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Convolution is all you need</head><p>Let us consider some mapping g. It is equivariant under a transformation L if and only if there exists L such that g • L = L • g. If L is the identity mapping, then the function g is invariant under this transformation. A function of multiple variables is equivariant when it is equivariant with respect to each of the variables. In our analysis, we consider only transformations that form a transformation group, in other words, L ∈ G. The proof of Theorem 2 is given in the supplementary material. A simple interpretation of this theorem is that a tracker is equivariant to transformations from G if and only if it is fully G-convolutional. The necessity of fullyconvolutional trackers is well-known in tracking community and is related to the ability of the tracker to capture the main variations in the video -the translation. In this paper, we seek to extend this ability to scale variations as well. Which, due to Theorem 2 boils down to using scaleconvolution and building fully scale-translation convolutional trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Scale Modules</head><p>Given a function f : R → R, a scale transformation is defined as follows:</p><formula xml:id="formula_1">L s [f ](t) = f (s −1 t), ∀s ≥ 0 (2)</formula><p>where cases with s &gt; 1 are referred to as upscale and with s &lt; 1 as downscale. Standard convolutional layers and convolutional networks are translation equivariant but not scale-equivariant <ref type="bibr" target="#b27">[28]</ref>.</p><p>Parametric Scale-Convolution In order to build scaleequivariant convolutional networks, we follow the method proposed by Sosnovik et al. <ref type="bibr" target="#b27">[28]</ref>. We begin by choosing a complete basis of functions defined on multiple scales. Choosing the center of the function to be the point (0, 0) in coordinates (u, v), we use functions of the following form:</p><formula xml:id="formula_2">ψ σnm (u, v) = A 1 σ 2 H n u σ H m v σ e − u 2 +v 2 2σ 2<label>(3)</label></formula><p>Here H n is a Hermite polynomial of the n-th order, and A is a constant used for normalization. In order to build a basis of N functions, we iterate over increasing pairs of n and m. As the basis is complete, the number of functions N is equal to the number of pixels in the original filter. We build such a basis for a chosen set of equidistant scales σ and fix it:</p><formula xml:id="formula_3">Ψ σ = ψ σ00 , ψ σ01 , ψ σ10 , ψ σ11 . . .<label>(4)</label></formula><p>Kernels of convolutional layers are parametrized by trainable weights w in the following way:</p><formula xml:id="formula_4">κ σ = i Ψ σi w i<label>(5)</label></formula><p>As a result, each kernel is defined on multiple scales and no image interpolation is used. Given a function of scale and translation f (s, t) and a kernel κ σ (s, t), a scale convolution is defined as:</p><formula xml:id="formula_5">[f H κ σ ](s, t) = s [f (s , ·) κ s·σ (s −1 s , ·)](t) (6)</formula><p>The result of this operation is a stack of features each of which corresponds to a different scale. We end up with a 3dimensional representation of the signal -2-dimensional translation + scale. We follow <ref type="bibr" target="#b27">[28]</ref> and denote scaleconvolution as H in order to distinguish it with the standard one. <ref type="figure" target="#fig_7">Figure 2</ref> demonstrates how a kernel basis is formed and how scale-convolutional layers work.</p><p>Fast 1 × 1 Scale-Convolution An essential building block of many backbone deep networks such as ResNets <ref type="bibr" target="#b12">[13]</ref> and Wide ResNets <ref type="bibr" target="#b35">[36]</ref> is a 1 × 1 convolutional layer. We follow the interpretation of these layers proposed in <ref type="bibr" target="#b22">[23]</ref> -it is a linear combination of channels. Thus, it has no spatial resolution. In order to build a scale-equivariant counterpart of 1 × 1 convolution, we do not utilize a kernel basis. As we pointed out before, the signal is stored as a 3 dimensional tensor for each channel. Therefore, for a kernel defined on N S scales, the convolution of the signal with this kernel is just a 3-dimensional convolution with a kernel of size 1 × 1 in spatial dimension, and with N S values in depth. This approach for 1 × 1 scale-convolution is faster than the special case of the algorithm proposed in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Padding Although zero padding is a standard approach in image classification for saving the spatial resolution of the image, it worsens the localization properties of convolutional trackers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38]</ref>. Nevertheless, a simple replacement of standard convolutional layers with scale-equivariant ones in very deep models is not possible without padding. Scaleequivariant convolutional layers have kernels of a bigger spatial extent because they are defined on multiple scales. For these reasons, we use circular padding during training and zero padding during testing in our models. The introduced padding does not affect the feature maps which are obtained with kernels defined on small scales. It does not violate the translation equivariance of a network. We provide an experimental proof in supplementary material.</p><p>Scale-Pooling In order to capture correlations between different scales and to transform a 3-dimensional signal into a 2-dimensional one, we utilize global max pooling along the scale axis. This operation does not eliminate the scale-equivariant properties of the network. We found that it is useful to additionally incorporate this module in the places where conventional CNNs have spatial max pooling or strides. The mechanism of scale-pooling is illustrated in <ref type="figure" target="#fig_7">Figure 2</ref>.</p><p>Non-parametric Scale-Convolution The convolutional operation which results in the heatmap of a tracker is nonparametric. Both the input and the kernel come from neural networks. Thus, the approach described in Equation 6</p><p>is not suitable for this case. Given two functions f 1 , f 2 of scale and translation the non-parametric scale convolution is defined as follows:</p><formula xml:id="formula_6">[f 1 H f 2 ](s, t) = L s −1 [L s [f 1 ] f 2 ](t)<label>(7)</label></formula><p>Here L s is rescaling implemented as bicubic interpolation.</p><p>Although it is a relatively slow operation, it is used only once in the tracker and does not heavily affect the inference time. The proof of the equivariance of this convolution is provided in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extending a Tracker to Scale Equivariance</head><p>We present a recipe to extend a tracker to scale equivariance.</p><p>1. The first step is to estimate to what degree objects change in size in this domain, and then to select a set of scales σ 1 , σ 2 , . . . σ N . This is a domain-specific hyperparameter. For example, a domain with significant scale variations requires a broader span of scales, while for more smooth sequences, the set may consist of just 3 scales around 1.</p><p>2. For a tracker which can be described by Equation 1, derive φ X and φ Z . The obtained tracker produces a heatmap h(z, x) defined on scale and translation. Therefore, each position is assigned a vector of features that has both the measure of similarity and the scale relation between the candidate and the template. If additional scale-pooling is included, then all scale information is just aggregated in the similarity score.</p><p>Note that the overall structure of the tracker, as well as the training and inference procedures are not changed. Thus, the recipe allows for a simple extension of a tracker with little cost of modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Scale-Equivariant SiamFC</head><p>While the proposed algorithm is applicable to a wide range of trackers, in this work, we focus on Siamese trackers. As a baseline we choose SiamFC <ref type="bibr" target="#b1">[2]</ref>. This model serves as a starting point for modifications for the many modern high-performance Siamese trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture</head><p>Given the recipe, here we discuss the actual implementation of the scale-equivariant SiamFC tracker (SE-SiamFC).</p><p>In the first step of the recipe, we assess the range of scales in the domain (dataset). In sequences presented in most of the tracking benchmarks, like OTB or VOT, objects change their size relatively slowly from one frame to the other. The maximum scale change usually does not exceed a factor of 1.5 − 2. Therefore, we use 3 scales with a step of √ 2 as the basis for the scale-convolutions. The next step in the recipe is to represent the tracker as it is done in Equation 1. SiamFC localizes the object as the coordinate argmax of the heatmap h(z,</p><formula xml:id="formula_7">x) = φ Z (z) φ X (x), where φ Z = φ X are convolutional Siamese backbones.</formula><p>Next, in step number 3, we modify the backbones by replacing standard convolutions by scale-equivariant convolutions. We follow step 4 and utilize scale-pooling in the backbones in order to capture additional scale correlations between features of various scales. According to step 5, the connecting correlation is replaced with non-parametric scale-convolution. SiamFC computes its similarity function as a 2-dimensional map, therefore, we follow step 6 and add extra scale-pooling in order to transform a 3-dimensional heatmap into a 2-dimensional one. Now, we can use exactly the same inference algorithm as in the original paper <ref type="bibr" target="#b1">[2]</ref>. We use the standard approach of scale estimation, based on the greedy selection of the best similarity for 3 different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weight Initialization</head><p>An important ingredient of a successful model training is the initialization of its weights. A common approach is to use weights from an Imagenet <ref type="bibr" target="#b8">[9]</ref> pre-trained model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b20">21]</ref>. In our case, however, this requires additional steps, as there are no available scale-equivariant models pretrained on the Imagenet. We present a method for initializing a scale-equivariant model with weights from a pretrained conventional CNN. The key idea is that a scaleequivariant network built according to Section 3.3 contains a sub-network that is identical to the one of the non-scaleequivariant counterpart. As the kernels of scale-equivariant models are parameterized with a fixed basis and trainable weights, our task is to initialize these weights.</p><p>We begin by initializing the inter-scale correlations by setting to 0 all weights responsible for these connections. At this moment, up to scale-pooling, the scale-equivariant model consists of several networks parallel to, yet disconnected from one another, where the only difference is the size of their filters. For the convolutional layers with a nonunitary spatial extent, we initialize the weights such that the kernels of the smallest scale match those of the source model. Given a source kernel κ (u, v) and a basis Ψ σi (u, v) with σ = 1, weights w i are chosen to satisfy the linear system derived from Equation 5:</p><formula xml:id="formula_8">κ 1 (u, v) = i Ψ 1i (u, v)w i = κ (u, v), ∀u, v<label>(8)</label></formula><p>As the basis is complete by construction, its matrix form is invertible. The system has a unique solution with respect to w i :</p><formula xml:id="formula_9">w i = u,v Ψ −1 1i (u, v)κ (u, v)<label>(9)</label></formula><p>All 1×1 scale-convolutional layers are identical to standard 1 × 1 convolutions after zeroing out inter-scale correlations. We copy these weights from the source model. We provide an additional illustration of the proposed initialization method in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Translation-Scaling MNIST</head><p>To test the ability of a tracker to cope with translation and scaling, we conduct an experiment on a simulated dataset with controlled factors of variation. We construct the datasets of translating (T-MNIST) and translatingscaling (S-MNIST) digits.</p><p>In particular, to form a sequence, we randomly sample up to 8 MNIST digits with backgrounds from the GOT10k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracker T/T T/S S/T S/S # Params</head><p>SiamFC 0.64 0.62 0.64 0.63 999 K SE-SiamFC 0.76 0.69 0.77 0.70 999 K dataset <ref type="bibr" target="#b15">[16]</ref>. Then, on each of the digits in the sequence independently, a smoothed Brownian motion model induces a random translation. Simultaneously, for S-MNIST, a smooth scale change in the range [0.67, 1.5] is induced by the sine rule:</p><formula xml:id="formula_10">s i (t) = h − l 2 sin( t 4 + β i ) + 1) + l<label>(10)</label></formula><p>where s i (t) is the scale factor of the i-th digit in the t-th frame, h, l are upper and lower bounds for scaling, and β i ∈ [0, 100] is a phase, sampled randomly for each of the digits. In total, we simulate 1000 sequences for training and 100 for validation. Each sequence has a length of 100 frames. We compare two configurations of the tracker: (i) SiamFC with a shallow backbone and (ii) its scale-equivariant version SE-SiamFC. We conduct the experiments according to 2 × 2 scenarios: the models are trained on either S-MNIST or T-MNIST and are subsequently tested on either of them.</p><p>The results are listed in   <ref type="figure" target="#fig_3">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Benchmarking</head><p>We compare the scale-equivariant tracker against a non-equivariant baseline on popular tracking benchmarks. We test SE-SiamFC with a backbone from <ref type="bibr" target="#b37">[38]</ref> against other popular Siamese trackers on OTB-2013, OTB-2015, VOT2016, and VOT2017. The benchmarks are chosen to allow direct comparison with the baseline <ref type="bibr" target="#b37">[38]</ref>. We compare against the results published in the original paper. Although additional results are presented online 1 , we couldn't reproduce them in a reasonable amount of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>The parameters of our model are initialized with weights pre-trained on Imagenet by a method described in Section 4.2. We use the same training procedure as in the baseline. See supplementary material for a detailed description of the architecture.</p><p>The pairs for training are collected from the GOT10k <ref type="bibr" target="#b15">[16]</ref> dataset. We adopt the same prepossessing and augmentation techniques as in <ref type="bibr" target="#b37">[38]</ref>. The inference procedure remains unchanged compared to the baseline.</p><p>OTB We test on the OTB-2013 <ref type="bibr" target="#b33">[34]</ref> and OTB-2015 <ref type="bibr" target="#b34">[35]</ref> benchmarks. Each of the sequences in the OTB datasets carries labels from 11 categories of difficulty in tracking the sequence. Examples of these labels include: occlusion, scale variation, in-pane rotation, etc. We employ a standard onepass evaluation (OPE) protocol to compare our method with 1 https://github.com/researchmm/SiamDW other trackers by the area under the success curve (AUC) and precision.</p><p>The results are reported in <ref type="table" target="#tab_2">Table 2</ref>. Our scale-equivariant tracker outperforms its non-equivariant counterpart by more than 3% on OTB-2015 in both AUC and precision, and by 1.4% on OTB-2013. When summarized at each label of difficulty (see <ref type="figure" target="#fig_5">Figure 4</ref>), the proposed scale-equivariant tracker is seen to improve all sequence types, not only those labeled with "scale variation".</p><p>We attribute this to the fact that the "scale variation" tag in the OTB benchmark only indicates the sequences with a relatively big change in scale factors, while up to a certain degree, scaling is present in almost any video sequence. Moreover, scaling may be present implicitly, in the form of  <ref type="table">Table 3</ref>: Ablation study on the OTB-2013 benchmark. The parameter σ stands for the step between scales in scaleequivariant models. Bold numbers represent the best result.</p><p>the same patterns being observed on multiple scales. An ability of our model to exploit this leads to better utilization of trainable parameters and a more discriminative Siamese similarity as a result.</p><p>VOT We next evaluate our tracker on VOT2016 and VOT2017 datasets <ref type="bibr" target="#b18">[19]</ref>. The performance is evaluated in terms of average bounding box overlap ratio (A), and the robustness (R). These two metrics are combined into the Expected Average Overlap (EAO), which is used to rank the overall performance.</p><p>The results are reported in <ref type="table" target="#tab_2">Table 2</ref>. On VOT2016 our scale-equivariant model shows an improvement from 0.30 to 0.36 in terms of EAO, which is a 20% gain compared to the non-equivariant baseline. On VOT2017, the increase in EAO is 17%.</p><p>We qualitatively investigated the sequences with the largest performance gain and observed that the most challenging factor for our baseline is the rapid scaling of the object. Even when the target is not completely lost, the imprecise bounding box heavily influences the overlap with the ground truth and the final EAO. Our scale-equivariant model better adapts to the fast scaling and delivers tighter bounding boxes. We provide qualitative results in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>We conduct an ablation study on the OTB-2013 benchmark to investigate the impact of scale step, weight initialization, and fast 1 × 1 scale-convolution. We also test the baseline SiamFC+ model with various levels of scale data augmentation during the training. We follow the same training and testing procedure as in Section 5.2 for all experiments. In the weight initialization experiment, however, we do not use gradual weights unfreezing, but train the whole model end-to-end from the first epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale step</head><p>We investigate the impact of scale step σ, which defines a set of scales our model operates on. We train and test SE-SiamFC with various scale steps. Results are shown in <ref type="table">Table 3</ref>. It can be seen that the resulting method outperforms the baseline on a range of scale steps. We empirically found that σ = 1.4 achieves the best performance.</p><p>Scale data augmentation Data augmentation is a common way to improve model generalization over different variations. Since our method is focused on scale, we compare SE-SiamFC against a baseline trained with different levels of scale data augmentation. Our results indicate (Table 3) that scale augmentation does not improve the performance of the conventional non-equivariant tracker. Weight Initialization We train and test SE-SiamFC model, where weights initialized randomly <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31]</ref>. As can be seen from <ref type="table">Table 3</ref>, random initialization results in a 19% performance drop compared to the proposed initialization technique. Fast 1 × 1 scale-convolution We compare the speed of 1 × 1 scale-convolution from <ref type="bibr" target="#b27">[28]</ref> and the proposed fast implementation. Implementation from <ref type="bibr" target="#b27">[28]</ref> requires 450 / 1650 µs, while our implementation requires 67 / 750 µs for forward / backward pass respectively, which is more than 6 times faster. In our experiments, the usage of fast 1 × 1 scale-convolution results in 30 − 40% speedup of a tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this work, we argue about the usefulness of additional scale equivariance in visual object tracking for the purpose of enhancing Siamese similarity estimation. We present a general theory that applies to a wide range of modern Siamese trackers, as well as all the components to turn an existing tracker into a scale-equivariant version. Moreover, we prove that the presented components are both necessary and sufficient to achieve built-in scale-translation equivariance. We sum up the theory by developing a simple recipe for extending existing trackers to scale equivariance. We apply it to develop SE-SiamFC -a scale-equivariant modification of the popular SiamFC tracker.</p><p>We experimentally demonstrate that our scaleequivariant tracker outperforms its conventional counterpart on OTB and VOT benchmarks and on the synthetically generated T-MNIST and S-MNIST datasets, where T-MNIST is designed to keep the object at a constant scale, and S-MNIST varies the scale in a known manner.</p><p>The experiments on T-MNIST and S-MNIST show the importance of proper scale measurement for all sequences, regardless of whether they have scale change or not. For the standard OTB and VOT benchmarks, our tracker proves the power of scale equivariance. It is seen to not only improves the tracking in the case of scaling, but also when other factors of variations are present (see <ref type="figure" target="#fig_5">Figure 4</ref>). It affects the performance in two ways: it prevents erroneous jumps to similar objects at a different size and it provides a better consistent estimate of the scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Convolution is all you need</head><p>In the paper we consider trackers of the following form</p><formula xml:id="formula_11">h(z, x) = φ X (x) φ Z (z)<label>(11)</label></formula><p>where φ X and φ Z are parameterized with feed-forward neural networks. Proof. Let us fix z = z 0 and introduce a function</p><formula xml:id="formula_12">h X = h(x, z 0 ) = φ X (x) φ Z (z 0 )</formula><p>. This function is a feed-forward neural network. All its layers but the last one are contained in φ X and the last layer is a convolution with φ Z (z 0 ). According to <ref type="bibr" target="#b17">[18]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Non-parametric scale-convolution</head><p>Given two functions f 1 , f 2 of scale and translation the non-paramteric scale convolution is defined as follows:  For a pair of scale and translation s,t we have the following property of the joint transformation L s Tt = Tt s L s from <ref type="bibr" target="#b27">[28]</ref>, where Tt is the translation operator defined as Tt[f ](t) = f (t −t). Now we can show the following:</p><formula xml:id="formula_13">[f 1 H f 2 ](s, t) = L s −1 [L s [f 1 ] f 2 ](t)<label>(12)</label></formula><formula xml:id="formula_14">[Lŝ[f 1 ] H f 2 ](s, t) = L s −1 [L sŝ [f 1 ] f 2 ](t) = LŝL (sŝ) −1 [L sŝ [f 1 ] f 2 ](t) = Lŝ[f 1 H f 2 ](sŝ, t)<label>(13)</label></formula><formula xml:id="formula_15">[Tt[f 1 ] H f 2 ](s, t) = L s −1 [L s [Tt[f 1 ]] f 2 ](t) = L s −1 [Tt s L s [f 1 ] f 2 ](t) = L s −1 Tt s [L s [f 1 ] f 2 ](t) = TtL s −1 [L s [f 1 ] f 2 ](t) = Tt[f 1 H f 2 ](t)<label>(14)</label></formula><p>Therefore, a function given by Equation 12 is also equivariant under translations of f 1 . The equivariance of the function with respect to a joint transformation follows from the equivariance to each of the transformations separately <ref type="bibr" target="#b27">[28]</ref>.</p><p>We proved the equivariance with respect to f 1 . The proof with respect to f 2 is analogous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weight initialization</head><p>The proposed weight initialization scheme from a pretrained model is depicted in <ref type="figure" target="#fig_10">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Padding</head><p>We conduct an experiment to verify that the proposed padding technique does not violate translation equivariance of convolutional trackers. We choose an image and select a sequence of translated and cropped windows inside of it. We process this sequence with a deep model that consists of the proposed convolutional layers and follows the inference procedure described in <ref type="bibr" target="#b37">[38]</ref>. We derive the predicted location of the object and compare its value to the input shift. <ref type="figure" target="#fig_9">Figure 5</ref> demonstrates that the input and the output translations have nearly identical values.   For both T-MNIST and S-MNIST, we use architectures described in <ref type="table" target="#tab_5">Table 4</ref>. 2D BatchNorm and ReLU are inserted after each of the convolutional layers except the last one. We do not use max pooling to preserve strict translationequivariance.</p><p>We train both models for 50 epochs using SGD with a mini-batch of 8 images and exponentially decay the learning rate from 10 −2 to 10 −5 . We set the momentum to 0.9 and the weight decay to 0.5 −4 . A binary cross-entropy loss as in <ref type="bibr" target="#b1">[2]</ref> is used. The inference algorithm is the same for both SiamFC and SE-SiamFC and follows the original implementation <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. OTB and VOT</head><p>For OTB and VOT experiments we used architectures described in <ref type="table" target="#tab_6">Table 5</ref>. We use the baseline <ref type="bibr" target="#b37">[38]</ref> with Cropping Inside Residual (CIR) units. SE-SiamFC is constructed directly from the baseline as described in the paper.</p><p>In <ref type="table" target="#tab_6">Table 5</ref> the kernel size refers to the smallest scale σ = 1 in the network. The sizes of the kernels, which correspond to bigger scales are 9 × 9 for Conv1 and 5 × 5 for other layers. <ref type="figure" target="#fig_11">Figure 7</ref> gives a qualitative comparison of the proposed method and the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage</head><p>SiamFC+ SE-SiamFC  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The standard version (top) and the scaleequivariant version (bottom) of a basic tracker. The scaleequivariant tracker has an internal notion of scale which allows for the distinction between similar objects which only differ in scale. The operator denotes convolution, and H stands for scale-convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 : 2 .</head><label>22</label><figDesc>Left: convolutional kernels use a fixed kernel basis on multiple scales, each with a set of trainable weights. Middle: a representation of scale-convolution using Equation<ref type="bibr" target="#b5">6</ref> for the first and all subsequent layers. Right: a scheme of scalepooling, which transforms a 3D-signal into a 2D one without losing scale equivariance. As an example, we use a basis of 4 functions and 3 scales with a step of √ Only one channel of each convolutional layer is demonstrated for simplicity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 1 .</head><label>1</label><figDesc>A function given by Equation 1 is equivariant under a transformation L from group G if and only if φ X and φ Z are constructed from G-equivariant convolutional layers and is the G-convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 .</head><label>3</label><figDesc>For the networks represented by φ X and φ Z , all convolutional layers need to be replaced with scaleconvolutional layers. The basis for these layers is based on the chosen scales σ 1 , σ 2 , . . . σ N . 4. (Optional) Scale-pooling can be included to additionally capture inter-scale correlations between all scales. 5. The connection operation needs to be replaced with a non-parametric scale-convolution. 6. (Optional) If the tracker only searches over spatial locations, scale-pooling needs to be included at the very end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Top: examples of simulated T-MNIST and S-MNIST sequences. Bottom: scale estimation for equivariant and non-equivariant models. In the S-MNIST example, SE-SiamFC can estimate the scale more accurately. In the T-MNIST example, our model better preserves the scale of the target unchanged, while the non-scale-equivariant model is prone to oscillations in its scale estimate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of AUC on OTB-2013 with different factors of variations. The red polygon corresponds to the baseline SiamFC+ and the green polygon -to SE-SiamFC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem 2 .</head><label>2</label><figDesc>A function given by Equation 11 is equivariant under a transformation L from group G if and only if φ X and φ Z are constructed from G-equivariant convolutional layers and is the G-convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>a feed-forward neural network is equivariant under transformations from G if and only if it is constructed from G-equivariant convolutional layers. Thus, the function h X is equivariant under transformations from G if and only if • The function φ X is constructed from G-equivariant convolutional layers • The convolution is the G-convolution If we then fix x = x 0 , we can show that a function h Z = h(x 0 , z) = φ X (x 0 ) φ Z (z) is equivariant under transformations from G if and only if • The function φ Z is constructed from G-equivariant convolutional layers • The convolution is the G-convolution The function h is equivariant under G if and only if both the function h X and the function h Z are equivariant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Left: two samples from the simulated sequence. The input image is a translated and cropped version of the source image. The output is the heatmap produced by the proposed model. The red color represents the place where the object is detected. Right: correspondence between the input and the output shifts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>The visualization of the weight initialization scheme from a pretrained model. Dashed connections are initialized with 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison of SE-SiamFC with SiamFC+ on VOT2016/2017 sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>AUC for models trained on T-MNIST and S- MNIST. T/S indicates that the model was trained on T- MNIST and tested on S-MNIST datasets. Bold numbers represent the best result for each of the training/testing sce- narios.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>See supplementary material for a detailed description of the architecture, training, and testing procedures.As can be seen fromTable 1, the equivariant version outperforms its non-equivariant counterpart in all scenarios.</figDesc><table><row><cell>Tracker</cell><cell>Year</cell><cell cols="2">OTB-2013</cell><cell cols="2">OTB-2015</cell><cell cols="2">VOT2016</cell><cell></cell><cell cols="2">VOT2017</cell></row><row><cell></cell><cell></cell><cell cols="2">AUC Prec.</cell><cell cols="2">AUC Prec.</cell><cell>EAO</cell><cell>A</cell><cell>R</cell><cell>EAO</cell><cell>A</cell><cell>R</cell></row><row><cell>SINT [29]</cell><cell>2016</cell><cell cols="2">0.64 0.85</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SiamFC [2]</cell><cell>2016</cell><cell cols="2">0.61 0.81</cell><cell cols="2">0.58 0.77</cell><cell cols="3">0.24 0.53 0.46</cell><cell cols="3">0.19 0.50 0.59</cell></row><row><cell>DSiam [12]</cell><cell>2017</cell><cell cols="2">0.64 0.81</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">StructSiam [37] 2018</cell><cell cols="2">0.64 0.88</cell><cell cols="2">0.62 0.85</cell><cell>0.26</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TriSiam [10]</cell><cell>2018</cell><cell cols="2">0.62 0.82</cell><cell cols="2">0.59 0.78</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.20</cell><cell>-</cell><cell>-</cell></row><row><cell>SiamRPN [22]</cell><cell>2018</cell><cell>-</cell><cell>-</cell><cell cols="2">0.64 0.85</cell><cell cols="3">0.34 0.56 0.26</cell><cell cols="3">0.24 0.49 0.46</cell></row><row><cell>SiamFC+ [38]</cell><cell>2019</cell><cell cols="2">0.67 0.88</cell><cell cols="2">0.64 0.85</cell><cell cols="3">0.30 0.54 0.38</cell><cell cols="3">0.23 0.50 0.49</cell></row><row><cell>SE-SiamFC</cell><cell>Ours</cell><cell cols="2">0.68 0.90</cell><cell cols="2">0.66 0.88</cell><cell cols="3">0.36 0.59 0.24</cell><cell cols="3">0.27 0.54 0.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparisons on OTB-2013, OTB-2015, VOT2016, and VOT2017 benchmarks. Bold numbers represent the best result for each of the benchmarks.</figDesc><table /><note>The experiment on S-MNIST, varying the scale of an arti- ficial object, shows that the scale-equivariant model has a superior ability to precisely follow the change in scale com- pared to the conventional one. The experiment on T-MNIST shows that (proper) measurement of scale is important even in the case when the sequence does not show a change in scale, where the observed scale in SE-SiamFC fluctuates much less than it does in the baseline (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Lemma 1. A function given by Equation 12 is equivariant under scale-translation. Proof. A function given by Equation 12 is equivariant under scale transformations of f 1 , indeed</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>C.2. Translating-Scaling MNIST</figDesc><table><row><cell>Stage</cell><cell>SiamFC</cell><cell>SE-SiamFC</cell></row><row><cell>Conv1</cell><cell cols="2">3 × 3, 96, s = 2</cell></row><row><cell>Conv2</cell><cell cols="2">3 × 3, 128, s = 2</cell></row><row><cell>Conv3</cell><cell cols="2">3 × 3, 256, s = 2</cell></row><row><cell>Conv4</cell><cell cols="2">3 × 3, 256, s = 1</cell></row><row><cell cols="2">Connect. Cross-correlation</cell><cell>Non-parametric scale-convolution</cell></row><row><cell># Params</cell><cell>999 K</cell><cell>999 K</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Architectures used in T/S-MNIST experiment. All convolutions in SE-SiamFC are scale-convolutions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Architectures used in OTB/VOT experiments. All convolutions in SE-SiamFC are scale-convolutions. s refers to stride, sp denotes scale pooling, i -is the size of the kernel in a scale dimension.</figDesc><table><row><cell>Conv1</cell><cell cols="3">7 × 7, 64, s = 2</cell><cell></cell><cell cols="2">7 × 7, 64, s = 2</cell></row><row><cell></cell><cell></cell><cell cols="4">max pool 2 × 2, s = 2</cell></row><row><cell>Conv2</cell><cell></cell><cell>1 × 1, 64</cell><cell></cell><cell cols="3"> 1 × 1, 64, i = 2 </cell></row><row><cell></cell><cell></cell><cell>3 × 3, 64</cell><cell> × 3</cell><cell></cell><cell>3 × 3, 64</cell><cell> × 3</cell></row><row><cell></cell><cell cols="2">1 × 1, 256</cell><cell></cell><cell></cell><cell>1 × 1, 256</cell></row><row><cell></cell><cell cols="2"> 1 × 1, 128</cell><cell></cell><cell cols="2"> 1 × 1, 128, sp</cell><cell></cell></row><row><cell>Conv3</cell><cell cols="2"> 3 × 3, 128</cell><cell> × 3</cell><cell></cell><cell>3 × 3, 128</cell><cell> × 3</cell></row><row><cell></cell><cell cols="2">1 × 1, 512</cell><cell></cell><cell></cell><cell>1 × 1, 512</cell></row><row><cell cols="4">Connect. Cross-correlation</cell><cell></cell><cell cols="2">Non-parametric scale-convolution</cell></row><row><cell># Params</cell><cell></cell><cell cols="2">1.44 M</cell><cell></cell><cell>1.45 M</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Thomas Andy Keller, Konrad Groh, Zenglin Shi and Deepak Gupta for valuable comments, discussions and help with the project. We appreciate the help of Zhipeng Zhang and Houwen Peng in reproducing the experiments from <ref type="bibr" target="#b37">[38]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">B-spline cnns on lie groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bekkers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08498</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Steerable cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6638" to="6646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Triplet loss in siamese network for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1763" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Jorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hexaconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02108</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On the generalization of equivariance and convolution in neural networks to the action of compact groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhendu</forename><surname>Trivedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03690</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2017 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1949" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ti-pooling: transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11703</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Lobry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11783</idno>
		<title level="m">Scale equivariance in cnns with vector fields</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>David W Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03830</idno>
		<title level="m">Attentive group equivariant convolutional networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Co-attentive equivariant neural networks: Focusing equivariance on transformations co-occurring in data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michał</forename><surname>Szmaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11093</idno>
		<title level="m">Scale-equivariant steerable networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">General e (2)-equivariant steerable cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14334" to="14345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="849" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep scale-spaces: Equivariance over scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7364" to="7376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5028" to="5037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="351" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4591" to="4600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Scale-equivariant neural networks with decomposed convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyuan</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11193</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
