<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TAPAS: Weakly Supervised Table Parsing via Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
							<email>jherzig@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><forename type="middle">Krzysztof</forename><surname>Nowak</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
							<email>thomasmueller@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
							<email>piccinno@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TAPAS: Weakly Supervised Table Parsing via Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TAPAS, an approach to question answering over tables without generating logical forms. TAPAS trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. TAPAS extends BERT's architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that TAPAS outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from WIK-ISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering from semi-structured tables is usually seen as a semantic parsing task where the question is translated to a logical form that can be executed against the table to retrieve the correct denotation <ref type="bibr" target="#b35">(Pasupat and Liang, 2015;</ref><ref type="bibr" target="#b46">Zhong et al., 2017;</ref><ref type="bibr" target="#b0">Agarwal et al., 2019)</ref>. Semantic parsers rely on supervised training data that pairs natural language questions with logical forms, but such data is expensive to annotate.</p><p>In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including paraphrasing <ref type="bibr" target="#b43">(Wang et al., 2015)</ref>, human in the loop <ref type="bibr" target="#b19">(Iyer et al., 2017;</ref><ref type="bibr" target="#b24">Lawrence and Riezler, 2018)</ref> and training on examples from other domains <ref type="bibr" target="#b16">(Herzig and Berant, 2017;</ref><ref type="bibr" target="#b38">Su and Yan, 2017)</ref>. One prominent data collection approach focuses on weak supervision where a training example consists of a question and its denotation instead of the full logical form <ref type="bibr" target="#b7">(Clarke et al., 2010;</ref><ref type="bibr" target="#b26">Liang et al., 2011;</ref><ref type="bibr" target="#b3">Artzi and Zettlemoyer, 2013)</ref>. Although appealing, training semantic parsers from this input is often difficult due to the abundance of spurious logical forms <ref type="bibr" target="#b4">(Berant et al., 2013;</ref><ref type="bibr" target="#b14">Guu et al., 2017)</ref> and reward sparsity <ref type="bibr" target="#b0">(Agarwal et al., 2019;</ref><ref type="bibr" target="#b30">Muhlgay et al., 2019)</ref>. In addition, semantic parsing applications only utilize the generated logical form as an intermediate step in retrieving the answer. Generating logical forms, however, introduces difficulties such as maintaining a logical formalism with sufficient expressivity, obeying decoding constraints (e.g. wellformedness), and the label bias problem <ref type="bibr" target="#b1">(Andor et al., 2016;</ref><ref type="bibr" target="#b22">Lafferty et al., 2001)</ref>.</p><p>In this paper we present TAPAS (for <ref type="table">Table  Parser)</ref>, a weakly supervised question answering model that reasons over tables without generating logical forms. TAPAS predicts a minimal program by selecting a subset of the table cells and a possible aggregation operation to be executed on top of them. Consequently, TAPAS can learn operations from natural language, without the need to specify them in some formalism. This is implemented by extending BERT's architecture <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> with additional embeddings that capture tabular structure, and with two classification layers for selecting cells and predicting a corresponding aggregation operator.</p><p>Importantly, we introduce a pre-training method for TAPAS, crucial for its success on the end task. We extend BERT's masked language model objective to structured data, and pre-train the model over millions of tables and related text segments crawled from Wikipedia. During pre-training, the model masks some tokens from the text segment and from the table itself, where the objective is to predict the original masked token based on the textual and tabular context.</p><p>Finally, we present an end-to-end differentiable training recipe that allows TAPAS to train from weak supervision. For examples that only involve selecting a subset of the table cells, we directly train the model to select the gold subset. For examples that involve aggregation, the relevant cells and the aggregation operation are not known from the denotation. In this case, we calculate an expected soft scalar outcome over all aggregation operators given the current model, and train the model with a regression loss against the gold denotation.</p><p>In comparison to prior attempts to reason over tables without generating logical forms <ref type="bibr" target="#b33">(Neelakantan et al., 2015;</ref><ref type="bibr" target="#b44">Yin et al., 2016;</ref><ref type="bibr" target="#b31">Müller et al., 2019)</ref>, TAPAS achieves better accuracy, and holds several advantages: its architecture is simpler as it includes a single encoder with no auto-regressive decoding, it enjoys pre-training, tackles more question types such as those that involve aggregation, and directly handles a conversational setting.</p><p>We find that on three different semantic parsing datasets, TAPAS performs better or on par in comparison to other semantic parsing and question answering models. On the conversational SQA <ref type="bibr" target="#b20">(Iyyer et al., 2017)</ref>, TAPAS improves stateof-the-art accuracy from 55.1 to 67.2, and achieves on par performance on WIKISQL <ref type="bibr" target="#b46">(Zhong et al., 2017)</ref> and WIKITQ <ref type="bibr" target="#b35">(Pasupat and Liang, 2015)</ref>. Transfer learning, which is simple in TAPAS, from WIKISQL to WIKITQ achieves 48.7 accuracy, 4.2 points higher than state-of-the-art. Our code and pre-trained model are publicly available at https: //github.com/google-research/tapas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TAPAS Model</head><p>Our model's architecture ( <ref type="figure">Figure 1</ref>) is based on BERT's encoder with additional positional embeddings used to encode tabular structure (visualized in <ref type="figure">Figure 2</ref>). We flatten the table into a sequence of words, split words into word pieces (tokens) and concatenate the question tokens before the table tokens. We additionally add two classification layers for selecting table cells and aggregation operators that operate on the cells. We now describe these modifications and how inference is performed. Additional embeddings We add a separator token between the question and the table, but unlike <ref type="bibr" target="#b18">Hwang et al. (2019)</ref> not between cells or rows. Instead, the token embeddings are combined with table-aware positional embeddings before feeding them to the model. We use different kinds of positional embeddings:</p><p>• Position ID is the index of the token in the flattened sequence (same as in BERT).</p><p>• Segment ID takes two possible values: 0 for the question, and 1 for the table header and cells.</p><p>• Column / Row ID is the index of the column/row that this token appears in, or 0 if the token is a part of the question.</p><p>• Rank ID if column values can be parsed as floats or dates, we sort them accordingly and assign an embedding based on their numeric rank (0 for not comparable, 1 for the smallest item, i + 1 for an item with rank i). This can assist the model when processing questions that involve superlatives, as word pieces may not represent numbers informatively <ref type="bibr" target="#b41">(Wallace et al., 2019)</ref>.</p><p>• Previous Answer given a conversational setup where the current question might refer to the previous question or its answers (e.g., question 5 in <ref type="figure" target="#fig_0">Figure 3</ref>), we add a special embedding that marks whether a cell token was the answer to the previous question (1 if the token's cell was an answer, or 0 otherwise). aggregation operator, these cells can be the final answer or the input used to compute the final answer. Cells are modelled as independent Bernoulli variables. First, we compute the logit for a token using a linear layer on top of its last hidden vector. Cell logits are then computed as the average over logits of tokens in that cell. The output of the layer is the probability p (c) s to select cell c. We additionally found it useful to add an inductive bias to select cells within a single column. We achieve this by introducing a categorical variable to select the correct column. The model computes the logit for a given column by applying a new linear layer to the average embedding for cells appearing in that column. We add an additional column logit that corresponds to selecting no column or cells. We treat this as an extra column with no cells. The output of the layer is the probability p (co) col to select column co computed using softmax over the column logits. We set cell probabilities p (c) s outside the selected column to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell selection</head><p>Aggregation operator prediction Semantic parsing tasks require discrete reasoning over the table, such as summing numbers or counting cells. To handle these cases without producing logical forms, TAPAS outputs a subset of the table cells together with an optional aggregation operator. The aggregation operator describes an operation to be applied to the selected cells, such as SUM, COUNT, AVERAGE or NONE. The operator is selected by a linear layer followed by a softmax on top of the final hidden vector of the first token (the special [CLS] token). We denote this layer as p a (op), where op is some aggregation operator.</p><p>Inference We predict the most likely aggregation operator together with a subset of the cells (using the cell selection layer). To predict a discrete cell selection we select all table cells for which their probability is larger than 0.5. These predictions are then executed against the table to retrieve the answer, by applying the predicted aggregation over the selected cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pre-training</head><p>Following the recent success of pre-training models on textual data for natural language understanding tasks, we wish to extend this procedure to structured data, as an initialization for our table parsing task. To this end, we pre-train TAPAS on a large number of tables from Wikipedia. This allows the model to learn many interesting correlations between text and the table, and between the cells of a columns and their header.</p><p>We create pre-training inputs by extracting texttable pairs from Wikipedia. We extract 6.2M tables: 3.3M of class Infobox 1 and 2.9M of class WikiTable. We consider tables with at most 500 cells. All of the end task datasets we experiment with only contain horizontal tables with a header row with column names. Therefore, we only extract Wiki tables of this form using the &lt;th&gt; tag to identify headers. We furthermore, transpose Infoboxes into a table with a single header and a single data row. The tables, created from Infoboxes, are arguably not very typical, but we found them to improve performance on the end tasks.</p><p>As a proxy for questions that appear in the end tasks, we extract the table caption, article title, article description, segment title and text of the segment the table occurs in as relevant text snippets. In this way we extract 21.3M snippets.</p><p>We convert the extracted text-table pairs to pretraining examples as follows: Following <ref type="bibr" target="#b10">Devlin et al. (2019)</ref>, we use a masked language model pre-training objective. We also experimented with adding a second objective of predicting whether the table belongs to the text or is a random table but did not find this to improve the performance on the end tasks. This is aligned with <ref type="bibr" target="#b27">Liu et al. (2019)</ref> that similarly did not benefit from a next sentence prediction task.</p><p>For pre-training to be efficient, we restrict our word piece sequence length to a certain budget (e.g., we use 128 in our final experiments). That is, the combined length of tokenized text and table cells has to fit into this budget. To achieve this, we randomly select a snippet of 8 to 16 word pieces from the associated text. To fit the table, we start by only adding the first word of each column name and cell. We then keep adding words turn-wise until we reach the word piece budget. For every table we generate 10 different snippets in this way.</p><p>We follow the masking procedure introduced by BERT. We use whole word masking 2 for the text, and we find it beneficial to apply whole cell masking (masking all the word pieces of the cell if any of its pieces is masked) to the table as well.</p><p>We note that we additionally experimented with data augmentation, which shares a similar goal to pre-training. We generated synthetic pairs of questions and denotations over real tables via a grammar, and augmented these to the end tasks training data. As this did not improve end task performance significantly, we omit these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fine-tuning</head><p>Overview We formally define table parsing in a weakly supervised setup as follows. Given a train-</p><formula xml:id="formula_0">ing set of N examples {(x i , T i , y i )} N i=1</formula><p>, where x i is an utterance, T i is a table and y i is a corresponding set of denotations, our goal is to learn a model that maps a new utterance x to a program z, such that when z is executed against the corresponding table T , it yields the correct denotation y. The program z comprises a subset of the table cells and an optional aggregation operator. The <ref type="table">table T maps a  table cell to its value.</ref> As a pre-processing step described in Section 5.1, we translate the set of denotations y for each example to a tuple (C, s) of cell coordinates C and a scalar s, which is only populated when y is a single scalar. We then guide training according to the content of (C, s). For cell selection examples, for which s is not populated, we train the model to select the cells in C. For scalar answer examples, where s is populated but C is empty, we train the model to predict an aggregation over the table cells that amounts to s. We now describe each of these cases in detail.</p><p>Cell selection In this case y is mapped to a subset of the table cell coordinates C (e.g., question 1 in <ref type="figure" target="#fig_0">Figure 3</ref>). For this type of examples, we use a hierarchical model that first selects a single column and then cells from within that column only.</p><p>We directly train the model to select the column col which has the highest number of cells in C. For our datasets cells C are contained in a single column and so this restriction on the model provides a useful inductive bias. If C is empty we select the additional empty column corresponding to empty cell selection. The model is then trained to select cells C ∩ col and not select (T \ C) ∩ col. The loss is composed of three components: (1) the average binary cross-entropy loss over column selections:</p><formula xml:id="formula_1">J columns = 1 |Columns| co∈Columns CE(p (co) col , 1 co=col )</formula><p>where the set of columns Columns includes the additional empty column, CE(·) is the cross entropy loss, 1 is the indicator function.</p><p>(2) the average binary cross-entropy loss over column cell selections:</p><formula xml:id="formula_2">J cells = 1 |Cells(col)| c∈Cells(col) CE(p (c) s , 1 c∈C ),</formula><p>where Cells(col) is the set of cells in the chosen column. (3) As for cell selection examples no aggregation occurs, we define the aggregation supervision to be NONE (assigned to op 0 ), and the aggregation loss is:</p><formula xml:id="formula_3">J aggr = − log p a (op 0 ).</formula><p>The total loss is then J CS = J columns + J cells + αJ aggr , where α is a scaling hyperparameter.</p><p>Scalar answer In this case y is a single scalar s which does not appear in the table (i.e. C = ∅, e.g., question 2 in <ref type="figure" target="#fig_0">Figure 3</ref>). This usually corresponds to examples that involve an aggregation over one or more   the scalar answer s. To train the model given this form of supervision one could search offline <ref type="bibr" target="#b12">(Dua et al., 2019;</ref><ref type="bibr" target="#b2">Andor et al., 2019)</ref> or online <ref type="bibr" target="#b4">(Berant et al., 2013;</ref><ref type="bibr" target="#b25">Liang et al., 2018)</ref> for programs (table cells and aggregation) that execute to s. In our table parsing setting, the number of spurious programs that execute to the gold scalar answer can grow quickly with the number of table cells (e.g., when s = 5, each COUNT over any five cells is potentially correct). As with this approach learning can easily fail, we avoid it. Instead, we make use of a training recipe where no search for correct programs is needed. Our approach results in an end-to-end differentiable training, similar in spirit to <ref type="bibr" target="#b33">Neelakantan et al. (2015)</ref>. We implement a fully differentiable layer that latently learns the weights for the aggregation prediction layer p a (·), without explicit supervision for the aggregation type.</p><p>Specifically, we recognize that the result of executing each of the supported aggregation operators is a scalar. We then implement a soft differentiable estimation for each operator <ref type="table" target="#tab_4">(Table 1)</ref>, given the token selection probabilities and the table values: compute(op, p s , T ). Given the results for all aggregation operators we then calculate the expected result according to the current model:</p><formula xml:id="formula_4">s pred = i=1p a (op i ) · compute(op i , p s , T ), wherep a (op i ) = pa(op i ) i=1 pa(op i )</formula><p>is a probability distribution normalized over aggregation operators excluding NONE.</p><p>We then calculate the scalar answer loss with Huber loss <ref type="bibr" target="#b17">(Huber, 1964)</ref> given by:  where a = |s pred − s|, and δ is a hyperparameter. Like <ref type="bibr" target="#b33">Neelakantan et al. (2015)</ref>, we find this loss is more stable than the squared loss. In addition, since a scalar answer implies some aggregation operation, we also define an aggregation loss that penalizes the model for assigning probability mass to the NONE class:</p><formula xml:id="formula_5">J scalar = 0.5 · a 2 a ≤ δ δ · a − 0.5 · δ 2 otherwise op compute(op, p s , T ) COUNT c∈T p (c) s SUM c∈T p (c) s · T [c] AVERAGE compute(SUM,p s ,T ) compute(COUNT,p s ,T )</formula><formula xml:id="formula_6">J aggr = − log( i=1 p a (op i ))</formula><p>The total loss is then J SA = J aggr +βJ scalar , where β is a scaling hyperparameter. As for some examples J scalar can be very large, which leads to unstable model updates, we introduce a cutoff hyperparameter. Then, for a training example where J scalar &gt; cutoff, we set J = 0 to ignore the example entirely, as we noticed this behaviour correlates with outliers. In addition, as computation done during training is continuous, while that being done during inference is discrete, we further add a temperature that scales token logits such that p s would output values closer to binary ones.</p><p>Ambiguous answer A scalar answer s that also appears in the table (thus C = ∅) is ambiguous, as in some cases the question implies aggregation (question 3 in <ref type="figure" target="#fig_0">Figure 3</ref>), while in other cases a <ref type="table" target="#tab_3">table   WIKISQL WIKITQ SQA   Logical Form     Conversational    Aggregation     Examples  80654  22033  17553  Tables  24241  2108  982   Table 2</ref>: Dataset statistics.</p><p>cell should be predicted (question 4 in <ref type="figure" target="#fig_0">Figure 3</ref>). Thus, in this case we dynamically let the model choose the supervision (cell selection or scalar answer) according to its current policy. Concretely, we set the supervision to be of cell selection if p a (op 0 ) ≥ S, where 0 &lt; S &lt; 1 is a threshold hyperparameter, and the scalar answer supervision otherwise. This follows hard EM <ref type="bibr" target="#b29">(Min et al., 2019)</ref>, as for spurious programs we pick the most probable one according to the current model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We experiment with the following semantic parsing datasets that reason over single tables (see <ref type="table">Table 2</ref>).</p><p>WIKITQ <ref type="bibr" target="#b35">(Pasupat and Liang, 2015)</ref> This dataset consists of complex questions on Wikipedia tables. Crowd workers were asked, given a table, to compose a series of complex questions that include comparisons, superlatives, aggregation or arithmetic operation. The questions were then verified by other crowd workers.</p><p>SQA <ref type="bibr" target="#b20">(Iyyer et al., 2017)</ref> This dataset was constructed by asking crowd workers to decompose a subset of highly compositional questions from WIKITQ, where each resulting decomposed question can be answered by one or more table cells. The final set consists of 6, 066 question sequences (2.9 question per sequence on average).</p><p>WIKISQL <ref type="bibr" target="#b46">(Zhong et al., 2017)</ref> This dataset focuses on translating text to SQL. It was constructed by asking crowd workers to paraphrase a templatebased question in natural language. Two other crowd workers were asked to verify the quality of the proposed paraphrases.</p><p>As our model predicts cell selection or scalar answers, we convert the denotations for each dataset to question, cell coordinates, scalar answer triples. SQA already provides this information (gold cells for each question). For WIKISQL and WIKITQ, we only use the denotations. Therefore, we derive cell coordinates by matching the denotations against the table contents. We fill scalar answer information if the denotation contains a single element that can be interpreted as a float, otherwise we set its value to NaN. We drop examples if there is no scalar answer and the denotation can not be found in the table, or if some denotation matches multiple cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We apply the standard BERT tokenizer on questions, table cells and headers, using the same vocabulary of 32k word pieces. Numbers and dates are parsed in a similar way as in the Neural Programmer <ref type="bibr" target="#b32">(Neelakantan et al., 2017)</ref>.</p><p>The official evaluation script of WIKITQ and SQA is used to report the denotation accuracy for these datasets. For WIKISQL, we generate the reference answer, aggregation operator and cell coordinates from the reference SQL provided using our own SQL implementation running on the JSON tables. However, we find that the answer produced by the official WIKISQL evaluation script is incorrect for approx. 2% of the examples. Throughout this paper we report accuracies against our reference answers, but we explain the differences and also provide accuracies compared to the official reference answers in Appendix A.</p><p>We start pre-training from BERT-Large (see Appendix B for hyper-parameters). We find it beneficial to start the pre-training from a pre-trained standard text BERT model (while randomly initializing our additional embeddings), as this enhances convergence on the held-out set.</p><p>We run both pre-training and fine-tuning on a setup of 32 Cloud TPU v3 cores with maximum sequence length 512. In this setup pre-training takes around 3 days and fine-tuning around 10 hours for WIKISQL and WIKITQ and 20 hours for SQA (with the batch sizes from table 12). The resource requirements of our model are essentially the same as BERT-large 3 .</p><p>For fine-tuning, we choose hyper-parameters using a black box Bayesian optimizer similar to Google Vizier <ref type="bibr" target="#b13">(Golovin et al., 2017)</ref> for WIKISQL and WIKITQ. For SQA we use grid-search. We discuss the details in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev Test <ref type="bibr" target="#b25">Liang et al. (2018)</ref> 71.8 72.4 <ref type="bibr" target="#b0">Agarwal et al. (2019)</ref> 74.9 74.8  79.4 79.3 <ref type="bibr" target="#b29">Min et al. (2019)</ref> 84.4 83.9</p><p>TAPAS 85.1 83.6 TAPAS (fully-supervised) 88.0 86.4 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Test</head><p>Pasupat and Liang <ref type="formula">(2015)</ref> 37.1 <ref type="bibr" target="#b32">Neelakantan et al. (2017)</ref> 34.2 <ref type="bibr" target="#b15">Haug et al. (2018)</ref> 34.8 <ref type="bibr" target="#b45">Zhang et al. (2017)</ref> 43.7 <ref type="bibr" target="#b25">Liang et al. (2018)</ref> 43.1  43.9 <ref type="bibr" target="#b0">Agarwal et al. (2019)</ref> 44.1  44.5 TAPAS 42.6 TAPAS (pre-trained on WIKISQL) 48.7 TAPAS (pre-trained on SQA) 48.8 <ref type="table">Table 4</ref>: WIKITQ denotation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>All results report the denotation accuracy for models trained from weak supervision. We follow <ref type="bibr" target="#b34">Niven and Kao (2019)</ref> and report the median for 5 independent runs, as BERT-based models can degenerate. We present our results for WIKISQL and WIKITQ in <ref type="table" target="#tab_5">Tables 3 and 4</ref> respectively. <ref type="table" target="#tab_5">Table  3</ref> shows that TAPAS, trained in the weakly supervised setting, achieves close to state-of-the-art performance for WIKISQL (83.6 vs 83.9 <ref type="bibr" target="#b29">(Min et al., 2019)</ref>). If given the gold aggregation operators and selected cell as supervision (extracted from the reference SQL), which accounts as full supervision to TAPAS, the model achieves 86.4. Unlike the full SQL queries, this supervision can be annotated by non-experts. For WIKITQ the model trained only from the original training data reaches 42.6 which surpass similar approaches <ref type="bibr" target="#b33">(Neelakantan et al., 2015)</ref>. When we pre-train the model on WIKISQL or SQA (which is straight-forward in our setup, as we do not rely on a logical formalism), TAPAS achieves 48.7 and 48.8, respectively.   For SQA, <ref type="table" target="#tab_7">Table 5</ref> shows that TAPAS leads to substantial improvements on all metrics: Improving all metrics by at least 11 points, sequence accuracy from 28.1 to 40.4 and average question accuracy from 55.1 to 67.2. <ref type="table" target="#tab_8">Table 6</ref> shows an ablation study on our different embeddings. To this end we pretrain and fine-tune models with different features. As pre-training is expensive we limit it to 200, 000 steps. For all datasets we see that pre-training on tables and column and row embeddings are the most important. Positional and rank embeddings are also improving the quality but to a lesser extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model ablations</head><p>We additionally find that when removing the scalar answer and aggregation losses (i.e., setting J SA=0 ) from TAPAS, accuracy drops for both datasets. For WIKITQ, we observe a substantial drop in performance from 29.0 to 23.1 when removing aggregation. For WIKISQL performance drops from 84.7 to 82.6. The relatively small decrease for WIKISQL can be explained by the fact that most examples do not need aggregation to be answered. In principle, 17% of the examples of the dev set have an aggregation (SUM, AVERAGE or COUNT), however, for all types we find that for more than 98% of the examples the aggregation is only applied to one or no cells. In the case of SUM and AVERAGE, this means that most examples can be answered by selecting one or no cells from the table. For COUNT the model without aggregation operators achieves 28.2 accuracy (by selecting 0 or 1 from the table) vs. 66.5 for the model with aggregation. Note that 0 and 1 are often found in a special index column. These properties of WIK-ISQL make it challenging for the model to decide whether to apply aggregation or not. For WIKITQ on the other hand, we observe a substantial drop in performance from 29.0 to 23.1 when removing aggregation.</p><p>Qualitative Analysis on WIKITQ We manually analyze 200 dev set predictions made by TAPAS on WIKITQ. For correct predictions via an aggregation, we inspect the selected cells to see if they match the ground truth. We find that 96% of the correct aggregation predictions where also correct in terms of the cells selected. We further find that 14% of the correct aggregation predictions had only one cell, and could potentially be achieved by cell selection, with no aggregation.</p><p>We also perform an error analysis and identify the following exclusive salient phenomena: (i) 12% are ambiguous ("Name at least two labels that released the group's albums."), have wrong labels or missing information ; (ii) 10% of the cases require complex temporal comparisons which could also not be parsed with a rich formalism such as SQL ("what country had the most cities founded in the 1830's?") ; (iii) in 16% of the cases the gold denotation has a textual value that does not appear in the table, thus it could not be predicted without performing string operations over cell values ; (iv) on 10%, the table is too big to fit in 512 tokens ; (v) on 13% of the cases TAPAS selected no cells, which suggests introducing penalties for this behaviour ; (vi) on 2% of the cases, the answer is the difference between scalars, so it is outside of the model capabilities ("how long did anne churchill/spencer live?") ; (vii) the other 37% of the cases could not be classified to a particular phenomenon.</p><p>Pre-training Analysis In order to understand what TAPAS learns during pre-training we analyze its performance on 10,000 held-out examples. We split the data such that the tables in the held-out  data do not occur in the training data. <ref type="table" target="#tab_10">Table 7</ref> shows the accuracy of masked word pieces of different types and in different locations. We find that average accuracy across position is relatively high (71.4). Predicting tokens in the header of the table is easiest (96.6), probably because many Wikipedia articles use instances of the same kind of table. Predicting word pieces in cells is a bit harder (63.4) than predicting pieces in the text (68.8). The biggest differences can be observed when comparing predicting words (74.1) and numbers (53.9). This is expected since numbers are very specific and often hard to generalize. The soft-accuracy metric and example (Appendix C) demonstrate, however, that the model is relatively good at predicting numbers that are at least close to the target.</p><p>Limitations TAPAS handles single tables as context, which are able to fit in memory. Thus, our model would fail to capture very large tables, or databases that contain multiple tables. In this case, the table(s) could be compressed or filtered, such that only relevant content would be encoded, which we leave for future work. In addition, although TAPAS can parse compositional structures (e.g., question 2 in <ref type="figure" target="#fig_0">Figure 3</ref>), its expressivity is limited to a form of an aggregation over a subset of table cells. Thus, structures with multiple aggregations such as "number of actors with an average rating higher than 4" could not be handled correctly. Despite this limitation, TAPAS succeeds in parsing three different datasets, and we did not encounter this kind of errors in Section 5.3. This suggests that the majority of examples in semantic parsing datasets are limited in their compositionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Semantic parsing models are mostly trained to produce gold logical forms using an encoder-decoder approach <ref type="bibr" target="#b21">(Jia and Liang, 2016;</ref><ref type="bibr" target="#b11">Dong and Lapata, 2016)</ref>. To reduce the burden in collecting full logical forms, models are typically trained from weak supervision in the form of denotations. These are used to guide the search for correct logical forms <ref type="bibr" target="#b7">(Clarke et al., 2010;</ref><ref type="bibr" target="#b26">Liang et al., 2011)</ref>.</p><p>Other works suggested end-to-end differentiable models that train from weak supervision, but do not explicitly generate logical forms. <ref type="bibr" target="#b33">Neelakantan et al. (2015)</ref> proposed a complex model that sequentially predicts symbolic operations over  <ref type="bibr" target="#b6">Cho et al. (2018)</ref> proposed a supervised model that predicts the relevant rows, column and aggregation operation sequentially. In our work, we propose a model that follow this line of work, with a simpler architecture than past models (as the model is a single encoder that performs computation for many operations implicitly) and more coverage (as we support aggregation operators over selected cells).</p><p>Finally, pre-training methods have been designed with different training objectives, including language modeling <ref type="bibr" target="#b8">(Dai and Le, 2015;</ref><ref type="bibr" target="#b36">Peters et al., 2018;</ref><ref type="bibr" target="#b37">Radford et al., 2018)</ref> and masked language modeling <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Lample and Conneau, 2019)</ref>. These methods dramatically boost the performance of natural language understanding models <ref type="bibr">(Peters et al., 2018, inter alia)</ref>. Recently, several works extended BERT for visual question answering, by pre-training over text-image pairs while masking different regions in the image <ref type="bibr" target="#b40">(Tan and Bansal, 2019;</ref><ref type="bibr" target="#b28">Lu et al., 2019)</ref>. As for tables,  experimented with rendering a table into natural language so that it can be handled with a pre-trained BERT model. In our work we extend masked language modeling for table representations, by masking table cells or text segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we presented TAPAS, a model for question answering over tables that avoids generating logical forms. We showed that TAPAS effectively pre-trains over large scale data of text-table pairs and successfully restores masked words and table cells. We additionally showed that the model can fine-tune on semantic parsing datasets, only using weak supervision, with an end-to-end differentiable recipe. Results show that TAPAS achieves better or competitive results in comparison to stateof-the-art semantic parsers.</p><p>In future work we aim to extend the model to represent a database with multiple tables as context, and to effectively handle large tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>We would like to thank Yasemin Altun, Srini Narayanan, Slav Petrov, William Cohen, Massimo Nicosia, Syrine Krichene, Jordan Boyd-Graber and the anonymous reviewers for their constructive feedback, useful comments and suggestions. This work was completed in partial fulfillment for the PhD degree of the first author, which was also supported by a Google PhD fellowship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A WIKISQL Execution Errors</head><p>In some tables, WIKISQL contains "REAL" numbers stored in "TEXT" format. This leads to incorrect results for some of the comparison and aggregation examples. These errors in the WIK-ISQL execution accuracy penalize systems that do their own execution (rather then producing an SQL query). <ref type="table">Table 8</ref> shows two examples where our result derivation and the one used by WIK-ISQL differ because the numbers in the "Crowd" (col5) column are not represented as numbers in the respective SQL table. <ref type="table" target="#tab_12">Table 9</ref> and 10 contain accuracies compared against the official and our answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model WIKISQL TAPAS</head><p>TAPAS (no answer loss) 81.2 82.5 TAPAS 83.9 85.1 TAPAS (supervised) 86.6 88.0    With this soft metric we get an overall accuracy of 74.5 (instead of 71.4) and an accuracy of 80.5 (instead of 53.9) for numbers. Showing that the model is pretty good at guessing numbers that are at least close to the target. The following example demonstrates this:  In the example, the model correctly restores the Draw (D) and Loss (L) numbers for Spain. It fails to restore the Points For (PF) and Points Against (PA) for Zimbabwe, but gives close estimates. Note that the model also does not produce completely consistent results for each row we should have PA + PD = PF and the column sums of PF and PA should equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D The average of stochastic sets</head><p>Our approach to estimate aggregates of cells in the table operates directly on latent conditionally independent Bernoulli variables G c ∼ Bern(p c ) that indicate whether each cell is included in the aggregation and a latent categorical variable that indicates the chosen aggregation operation op: AVERAGE, SUM or COUNT. Given G c and the table values</p><p>The approximations are then easy to write in any tensor computation language and will be differentiable. In this work we experimented with the zero and second order approximations and found small improvements over the weighted average baseline. It's worth noting that in the dataset the proportion of average examples is very low. We expect this method to be more relevant in the more general setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>A table (left) with corresponding example questions (right). The last example is conversational.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>s</head><label></label><figDesc>outside of the column selected by the model are set to 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>table cells .</head><label>cells</label><figDesc>In this work we handle aggregation operators that correspond to SQL, namely COUNT, AVERAGE and SUM, however our model is not restricted to these.For these examples, the table cells that should be selected and the aggregation operator type are not known, as these cannot be directly inferred from</figDesc><table><row><cell>Rank Name</cell><cell>No. of reigns</cell><cell>Combined days</cell><cell cols="2"># 1 Which wrestler had the most number of reigns? Question</cell><cell>Answer Ric Flair</cell><cell>Example Type Cell selection</cell></row><row><cell>1 Lou Thesz</cell><cell>3</cell><cell>3,749</cell><cell cols="2">2 Average time as champion for top 2 wrestlers?</cell><cell cols="2">AVG(3749,3103)=3426 Scalar answer</cell></row><row><cell>2 Ric Flair</cell><cell>8</cell><cell>3,103</cell><cell cols="2">3 How many world champions are there with only</cell><cell>COUNT(Dory Funk Jr.,</cell><cell>Ambiguous answer</cell></row><row><cell>3 Harley Race</cell><cell>7</cell><cell>1,799</cell><cell></cell><cell>one reign?</cell><cell>Gene Kiniski)=2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">4 What is the number of reigns for Harley Race?</cell><cell>7</cell><cell>Ambiguous answer</cell></row><row><cell>4 Dory Funk Jr.</cell><cell>1</cell><cell>1,563</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Which of the following wrestlers were ranked in</cell><cell>{Dory Funk Jr., Dan</cell><cell>Cell selection</cell></row><row><cell>5 Dan Severn</cell><cell>2</cell><cell>1,559</cell><cell>5</cell><cell>the bottom 3?</cell><cell>Severn, Gene Kiniski}</cell></row><row><cell>6 Gene Kiniski</cell><cell>1</cell><cell>1,131</cell><cell></cell><cell>Out of these, who had more than one reign?</cell><cell>Dan Severn</cell><cell>Cell selection</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table Example questions</head><label>Example</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Aggregation operators soft implementation. AVERAGE approximation is discussed in Appendix D. Note that probabilities p</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>WIKISQL denotation accuracy 4 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>SQA test results. ALL is the average question accuracy, SEQ the sequence accuracy, and QX, the accuracy of the X'th question in a sequence.</figDesc><table><row><cell></cell><cell cols="3">SQA (SEQ) WIKISQL</cell><cell cols="2">WIKITQ</cell></row><row><cell>all</cell><cell>39.0</cell><cell>84.7</cell><cell></cell><cell>29.0</cell></row><row><cell>-pos</cell><cell>36.7</cell><cell>-2.3 82.9</cell><cell cols="2">-1.8 25.3</cell><cell>-3.7</cell></row><row><cell>-ranks</cell><cell>34.4</cell><cell>-4.6 84.1</cell><cell cols="3">-0.6 30.7 +1.8</cell></row><row><cell>-{cols,rows}</cell><cell cols="5">19.6 -19.4 74.1 -10.6 17.3 -11.6</cell></row><row><cell cols="3">-table pre-training 26.5 -12.5 80.8</cell><cell cols="3">-3.9 17.9 -11.1</cell></row><row><cell>-aggregation</cell><cell>-</cell><cell>82.6</cell><cell cols="2">-2.1 23.1</cell><cell>-5.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Dev accuracy with different embeddings re-</cell></row><row><cell>moved from the full model: positional (pos), numeric</cell></row><row><cell>ranks (ranks), column (cols) and row (rows). The</cell></row><row><cell>model without table pre-training was initialized from</cell></row><row><cell>the original BERT model pre-trained on text only. The</cell></row><row><cell>model without aggregation is only trained with the cell</cell></row><row><cell>selection loss.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Mask LM accuracy on held-out data, when the target word piece is located in the text, table header, cell or anywhere (all) and the target is anything, a word or number.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>table segments that are all explicitly predefined by the authors, while Yin et al. (2016) proposed a similar model where the operations themselves are learned during training. Müller et al. (2019) proposed a model that selects table cells, where the table and question are represented as a Graph Neural Network, however their model can not predict aggregations over table cells.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>WIKISQL development denotation accuracy.</figDesc><table><row><cell>Model</cell><cell cols="2">WIKISQL TAPAS</cell></row><row><cell>TAPAS (no answer loss)</cell><cell>80.1</cell><cell>81.2</cell></row><row><cell>TAPAS</cell><cell>82.4</cell><cell>83.6</cell></row><row><cell>TAPAS (supervised)</cell><cell>85.2</cell><cell>86.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>WIKISQL test denotation accuracy.</figDesc><table><row><cell>B Hyperparameters</cell><cell></cell><cell></cell></row><row><cell>Parameter</cell><cell>Values</cell><cell>Scale</cell></row><row><cell>Learning rate</cell><cell>(1e-5, 3e-3)</cell><cell>Log</cell></row><row><cell>Warmup ratio</cell><cell>(0.0, 0.2)</cell><cell>Linear</cell></row><row><cell>Temperature</cell><cell>(0.1, 1)</cell><cell>Linear</cell></row><row><cell>Answer loss cutoff</cell><cell cols="2">(0.1, 10,000) Log</cell></row><row><cell>Huber loss delta</cell><cell cols="2">(0.1, 10,000) Log</cell></row><row><cell>Cell selection preference</cell><cell>(0, 1)</cell><cell>Linear</cell></row><row><cell cols="2">Reset cell selection weights [0, 1]</cell><cell>Discrete</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Hyper-parameters for WIKISQL and WIK-ITQ. Values are constrained to either a range (a, b) or a list [a, b, c, . . .].</figDesc><table><row><cell>Parameter</cell><cell>PRETRAIN</cell><cell cols="2">SQA WIKISQL</cell><cell>WIKITQ</cell></row><row><cell>Training Steps</cell><cell cols="2">1,000,000 200,000</cell><cell>50,000</cell><cell>50,000</cell></row><row><cell>Learning rate</cell><cell cols="4">5e-5 1.25e-5 6.17164e-5 1.93581e-5</cell></row><row><cell>Warmup ratio</cell><cell>0.01</cell><cell>0.2</cell><cell>0.142400</cell><cell>0.128960</cell></row><row><cell>Temperature</cell><cell></cell><cell>1.0</cell><cell cols="2">0.107515 0.0352513</cell></row><row><cell>Answer loss cutoff</cell><cell></cell><cell></cell><cell>0.185567</cell><cell>0.664694</cell></row><row><cell>Huber loss delta</cell><cell></cell><cell></cell><cell>1265.74</cell><cell>0.121194</cell></row><row><cell>Cell selection preference</cell><cell></cell><cell></cell><cell>0.611754</cell><cell>0.207951</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>128</cell><cell>512</cell><cell>512</cell></row><row><cell>Gradient clipping</cell><cell></cell><cell></cell><cell>10</cell><cell>10</cell></row><row><cell>Select one column</cell><cell></cell><cell>1</cell><cell>0</cell><cell>1</cell></row><row><cell>Reset cell selection weights</cell><cell></cell><cell>0</cell><cell>0</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Optimal hyper-parameters found for pretraining (PRETRAIN), SQA, WIKISQL and WIKITQ.</figDesc><table><row><cell cols="3">C Pre-training Example</cell></row><row><cell cols="3">In order to better understand how well the model</cell></row><row><cell cols="3">predicts numbers, we relax our accuracy measure</cell></row><row><cell cols="2">to a soft form of accuracy:</cell><cell></cell></row><row><cell>acc(x, y) =</cell><cell>  1  0</cell><cell>if x = y if x or y is not a number</cell></row><row><cell></cell><cell>  1.0 − |x−y| max(x,y)</cell><cell>else</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Table example from the Wikipedia page describing the 1997 Rugby World Cup Sevens. x marks a correct prediction and x,y an incorrect prediction.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">en.wikipedia.org/wiki/Help:Infobox</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/google-research/ bert/blob/master/README.md</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/google-research/ bert/blob/master/README.md# out-of-memory-issues</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">As explained in Section 5.2, we report TAPAS numbers comparing against our own reference answers. Appendix A contains numbers WRT the official WIKISQL eval script.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 8</ref><p>: <ref type="table">Table "</ref>2-10767641-15" from WIKISQL. "col6" was removed. The "Crowd" column is of type "REAL" but the cell values are actually stored as "TEXT". Below we have two questions from the training set with the answer that is produced by the WIKISQL evaluation script and the answer we derive.</p><p>T we can define a random subset S ⊆ T where p c = P (c ∈ S) for each cell c ∈ T . The expected value of COUNT(S) = c G c can be computed as c p c and SUM(S) = c G c T c as c p c T c as described in <ref type="table">Table 1</ref>. For the average however, this is not straight-forward. We will see in what follows that the quotient of the expected sum and the count, which equals the weighed average of T by p c in general is not the true expected value, which can be written as:</p><p>This quantity differs from the weighted average, a key difference being that the weighted average is not sensitive to constants scaling all the output probabilities, which could in theory find optima where all the p c are below 0.5 for example. By the linearity of the expectation we can write:</p><p>So it comes down to computing that quantity</p><p>The key observation is that this is the expectation of a reciprocal of a Poisson Binomial Distribution 5 (a sum of 5 wikipedia.org/Poisson binomial distribution Bernoulli variables) in the special case where one of the probabilities is 1.</p><p>By using the Jensen inequality we get a lower bound on Q c as 1 E[Xc] = 1 1+ j =c p j . Note that if instead we used 1 j p j then we recover the weighted average, which is strictly bigger than the lower bound and in general not an upper or lower bound. We can get better approximations by computing the Taylor expansion using the moments 6 of X c of order k:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to generalize from sparse and underspecified rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07198</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06042</idno>
		<title level="m">Globally normalized transition-based neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Giving bert a calculator: Finding operations and arguments with reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00109</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1909.02164</idno>
	</analytic>
	<monogr>
		<title level="m">Tabfact: A large-scale dataset for table-based fact verification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial tableqa: Attention supervision for question answering on tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seung won Hwang, and Jonghyuck Park</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ACML</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterative search for weakly supervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2669" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Google Vizier: A Service for Black-Box Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhodeep</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Elliot</forename><surname>Karro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From language to programs: Bridging reinforcement learning and maximum marginal likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural multi-step reasoning for question answering on semistructured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grnarova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural semantic parsing over multiple knowledge-bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonseok</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyeung</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01069</idno>
		<title level="m">A comprehensive exploration on wikisql with table-aware word contextualization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a neural semantic parser from user feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Crosslingual language model pretraining</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving a neural semantic parser by counterfactual learning from human bandit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolin</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1820" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Memory augmented policy optimization for program synthesis with generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A discrete hard EM approach for weakly supervised question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1284</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2851" to="2864" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Valuebased search in execution space for mapping instructions to programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muhlgay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Nicosia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11787</idno>
		<title level="m">Answering conversational questions on structured data without logical forms</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning a natural language interface with neural programmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1511.04834</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probing neural network comprehension of natural language arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Niven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1459</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4658" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-domain semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Knowledgeaware conversational semantic parsing over web tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Xiaocheng Feng, and Bing Qin</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07940</idno>
		<title level="m">Do nlp models know numbers? probing numeracy in embeddings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning semantic parsers from denotations with latent structured alignments and abstract programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1391</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3772" to="3783" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural enquirer: Learning to query tables in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kao</forename><surname>Ben</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-0105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human-Computer Question Answering</title>
		<meeting>the Workshop on Human-Computer Question Answering<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="29" to="35" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Macro grammars and holistic triggering for efficient semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1709.00103</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
