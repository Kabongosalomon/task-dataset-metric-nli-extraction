<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Proposal, Tracking and Segmentation (PTS): A Cascaded Network for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
							<email>lichao.huang@horizon.ai</email>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shen</surname></persName>
							<email>han.shen@horizon.ai</email>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
							<email>yongchao.gong@horizon.ai</email>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
							<email>chang.huang@horizon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
							<email>xgwang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of EIC</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Proposal, Tracking and Segmentation (PTS): A Cascaded Network for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video object segmentation (VOS) aims at pixel-level object tracking given only the annotations in the first frame. Due to the large visual variations of objects in video and the lack of training samples, it remains a difficult task despite the upsurging development of deep learning. Toward solving the VOS problem, we bring in several new insights by the proposed unified framework consisting of object proposal, tracking and segmentation components. The object proposal network transfers objectness information as generic knowledge into VOS; the tracking network identifies the target object from the proposals; and the segmentation network is performed based on the tracking results with a novel dynamic-reference based model adaptation scheme. Extensive experiments have been conducted on the DAVIS'17 dataset and the YouTube-VOS dataset, our method achieves the state-of-the-art performance on several video object segmentation benchmarks. We make the code publicly available at https://github.com/ sydney0zq/PTSNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) aims at segmenting specific objects throughout a video sequence, given only the annotations in the first frame. This task is also known as semi-supervised video object segmentation. It has attracted increasing attention due to the availability of largescale datasets <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46]</ref> and its wide application potential in video editing, autonomous driving etc. There are several traditional research directions, e.g. reduce computa- * Equal contributions. The work was mainly done during an internship at Horizon Robotics. tional effort <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37]</ref>, casting the problem into a bilateral space <ref type="bibr" target="#b31">[32]</ref>, or considering optical flow <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b0">1]</ref>. Deep learning based methods mostly originate from OSVOS <ref type="bibr" target="#b3">[4]</ref> and MaskTrack <ref type="bibr" target="#b34">[35]</ref>.</p><p>Recently some noteworthy methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> attempt to solve the VOS problem by pixel-level metric learning. PML <ref type="bibr" target="#b6">[7]</ref> learns an embedding space via deep metric learning for all labelled pixels of training images; in testing time each pixel of the current frame is assigned to a label by nearest-neighbor matching to the pixels in the first frame. And Hai et al. <ref type="bibr" target="#b8">[9]</ref> propose location-sensitive embedding further for distinguish similar instances. In the field of detection, Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>, one of the state-of-the-art instance segmentation methods in still images, greatly reduces the searching space of detection and segmentation after introducing objectness, which also had been applied for object detection in image and video in many previous works, e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>. The term objectness means a high-level semantic concept showing whether one region contains objects. As for semi-supervised VOS which extremely lacks of training samples, objectness also has the ability to work in a similar manner in Mask R-CNN and transfer generic semantic recognition. However, objectness has not been exploited in the field of VOS.</p><p>Moreover, appearance-based methods like OSVOS <ref type="bibr" target="#b3">[4]</ref>, OnAVOS <ref type="bibr" target="#b40">[41]</ref> and OSVOS-S <ref type="bibr" target="#b30">[31]</ref> rarely depend on temporal consistency, thus less likely to drift when occlusion and abrupt motion occurs. Nevertheless, temporal information is critical for object recognition in video. Propagationbased methods like RGMP <ref type="bibr" target="#b42">[43]</ref> depend on the previous predicted mask and the first annotated frame as temporal and appearance cues respectively. And the two types of VOS methods mix the tasks of localization and segmentation into a single network. How about accomplishing the two tasks in two steps? Our motivation is introducing an object identification mechanism for VOS based on the objectness metioned above. On one hand, it would be relatively straightforward to identify the object of interest in that the searching space of localization is greatly reduced. On the other hand, localization and segmentation desires translation-invariant and translation-equivalent representations respectively <ref type="bibr" target="#b21">[22]</ref>. The object identification idea will be beneficial to avoid the potential mutual exclusion of the two tasks.</p><p>Additionally, model adaptation in VOS is not fully exploited compared to VOT (Visual Object Tracking) <ref type="bibr" target="#b25">[26]</ref>. PML <ref type="bibr" target="#b6">[7]</ref> only adds historical embedding features with high confidence to the reference pool. OnAVOS <ref type="bibr" target="#b40">[41]</ref> utilizes the predicted masks to fine-tune the segmentation network. Both RGMP and OSMN <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47]</ref> extract temporal information only from the previous predicted mask. S2S <ref type="bibr" target="#b45">[46]</ref> applies ConvLSTM to preserve the spatial information of previous frames in the hidden states of the model, but the effectiveness of model update is hard to be quantified. Yet in VOT, ECO <ref type="bibr" target="#b9">[10]</ref> groups historical similar samples to update model and MDNet <ref type="bibr" target="#b32">[33]</ref> applies short-long term model update strategy by memorizing historical samples. Unlike OnAVOS, we use dynamic reference instead of online training to do model update.</p><p>To bring in the objectness information, the object identification mechanism and the dynamic reference based model adaption scheme, we propose PTSNet, a cascaded network for VOS, which consists of an Object Proposal Network (OPN), an Object Tracking Network (OTN) and a Dynamic Reference Segmentation Network (DRSN). OPN generates proposals near the object of interest. Then OTN identifies the tracked object out and gets aware of its scale. Finally DRSN updates the appearance information over time and utilizes multiple dynamic references to guide the segmentation. Unlike the previous state-of-the-art method DyeNet <ref type="bibr" target="#b27">[28]</ref>, PTSNet is a causal system which performs inference online.</p><p>More specifically, given a frame in a sequence, OPN firstly generates class-agnostic object proposals for the current frame and filters out the redundant proposals to obtain candidate proposals. This strategy greatly reduces the searching space for further object identification while maintaining true positive regions of interest. Afterwards, OTN identifies the object of interest by giving confidence scores and updates online for adapting to large and fast changes in object appearance. The highest scoring box is selected to extract the region to be segmented. OTN performs box association in objectness level and identifies reliable proposals in a smaller searching space. DRSN segments the current frame with dynamic frames as reference. It not only uses the first static annotated frame for providing reliable information as reference, which is not up-to-the-minute, but also utilizes historical segmentation results as pseudo ground-truths as dynamic reference to guide the current segmentation.</p><p>In summary, our main contributions are highligted as follows:</p><p>• We propose PTSNet which cascades object proposal, tracking and segmentation sub-networks, which is a novel and effective framework for VOS.</p><p>• PTSNet brings in some new insights for VOS: objectness as generic knowledge to overcome the problem of lacking training examples, and a robust object localization scheme based on visual tracking for avoiding the conflict of between translation-invariant and translation-equivalent desired by the localization and segmentation tasks respectively. Besides, a new segmentation model adaptation mechanism, i.e., DRSN, is also introduced.</p><p>• PTSNet achieves top performance on various competitive VOS datasets regardless with or without online fine-tuning, e.g. DAVIS'17 and YouTube-VOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Video Object Segmentation. Video object segmentation (VOS) is defined as tracking specified objects in pixel-level given the first annotated frame throughout the video sequence. OSVOS <ref type="bibr" target="#b3">[4]</ref> adopts a video-specific segmentation network which learns appearance feature of the target from the first annotated frame to segment the following frames. OnAVOS <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40]</ref>, OSVOS-S <ref type="bibr" target="#b30">[31]</ref> and CINM <ref type="bibr" target="#b1">[2]</ref> extend OSVOS by an mechanism of online adaption, by learning an semantic instance network and by building a graph model on the results of OSVOS, respectively. MaskTrack <ref type="bibr" target="#b34">[35]</ref> uses optical flow to propagate the segmentation mask from previous frame to the current. LucidTracker <ref type="bibr" target="#b23">[24]</ref> extends Mask-Track by a mechanism of data augmentation. RGMP <ref type="bibr" target="#b42">[43]</ref> simultaneously uses both the previously predicted mask to be propagated to the current frame and the first annotated frame as static reference to guide segmentation network to segment the target. MoNet <ref type="bibr" target="#b43">[44]</ref> introduces two motion exploitation components which are feature alignment and a distance transform layer to achieve better performance. Hu et al. <ref type="bibr" target="#b17">[18]</ref> proposed an active contour model to provide a coarse segmentation as a guidance cascaded by a refinement network which outputs the final prediction. OSMN <ref type="bibr" target="#b46">[47]</ref> proposes a modulator network to extract visual information from the first annotated frame and position information from previously predicted mask. S2S <ref type="bibr" target="#b45">[46]</ref> utilizes a Con-vLSTM to learn long-term spatial-temporal information for segmentation. DyeNet <ref type="bibr" target="#b27">[28]</ref> combines a bi-directional mask propagation and a ReID module for retrieving missing objects into a single network. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19</ref>] explore pixel-level embedding matching for VOS.</p><p>- <ref type="figure">Figure 1</ref>. Example proposals of OPN on unseen categories. We randomly pick a portion of all proposals near the objects of interest. The results show excellent generalization ablity of OPN.</p><p>Despite achieving impressive results, the above methods disregards the objectness information and most of them does not take translation representations into account. In PTSNet, we propose a principled pipeline to bring in objectness information as generic knowledge, object identification via visual tracking for localization, and an new dynamic model adaptation for segmentation.</p><p>Video Object Tracking. In recent works that use deep learning for VOS, the power of VOT has not been fully utilized. FAVOS <ref type="bibr" target="#b7">[8]</ref> reformulates VOS as a task to track the parts of the target, while we perform object-level tracking. In the field of VOT, two mainstreams of deep-learning based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16]</ref> and correlation-filter based <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> evolved. We applies the high-performance MDNet <ref type="bibr" target="#b32">[33]</ref> which learns shared features using an offline training set and online learning domain-specific classifiers individually for different testing videos. To further speed up the proposed PTSNet, the adaptive RoIAlign in Real-Time MDNet <ref type="bibr" target="#b20">[21]</ref> can be introduced. In a word, MDNet acts an object identification module based on the provided objectness information, and performs box association in objectness level for the subsequent segmentation module DRSN. It worth to note that the framework of PTSNet is compatible with most of visual tracking methods and can always benefit from the development of visual tracking. This is also an advantage of PTSNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall framework</head><p>The architecture of the proposed PTSNet is given in <ref type="figure">Fig. 2</ref>. It consists of Object Proposal Network (OPN), Object Tracking Network (OTN) and Dynamic Reference Segmentation Network (DRSN). OPN is pre-trained on the COCO dataset <ref type="bibr" target="#b28">[29]</ref> and provides high-quality proposals near the object of interest. OTN is designed to identify the best proposal and to be updated online for adapting to large and fast changes in object appearance, which is inspired by MDNet <ref type="bibr" target="#b32">[33]</ref>. Then, the best proposal is expanded to crop and resize the region of the object of interest for normalizing scale of the object. Finally, DRSN makes use of both cropped region with previously predicted mask and multiple reference frames to segment the target object in the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Object Proposals Network</head><p>Object Proposal Network (OPN) is introduced to generate high-quality proposals near the object of interest in each frame to bring in objectness.</p><p>The proposed OPN works in a progressive way: Given the object location, i.e., bounding box, denoted as bb N-1 , in frame N -1, OPN aims at proposing a small set of bounding boxes, denoted as BB N = {bb i N , i = 1, · · · , k}, in frame N, as the potential locations of the target object. We follow the basic assumption that the boxes in BB N should be close to bb N-1 .</p><p>OPN is based on the state-of-the-art region proposal network (RPN) in Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> pretrained on the largescale COCO dataset. RPN is class-agnostic and provides objectness information as generic knowledge for PTSNet. Since VOS aims at segmenting object from any semantic category but COCO only provides the annotation for the 80 categories, it is important to check whether the pretrained RPN has a good generalization ability on unseen categories. Thus, we test RPN on the COCO validation set and VOS datasets (the DAVIS'17 dataset and the YouTube-VOS dataset) and report the recall rates of RPN. The COCO validation images are not used for training RPN but they are in the same domain with the training images of RPN; the images in the VOS datasets are not in the same domain with the training images of RPN and contain objects from unseen categories. <ref type="table">Table 1</ref> shows that the recall rates of RPN on VOS datasets are higher with the recall rates on the COCO validation set. The results confirm the amazing generalization ability of RPN. Several qualitative results for some unseen categories are shown in <ref type="figure">Fig. 1</ref>.</p><p>Based on the high recall rates of RPN on the VOS datasets, we design a hybrid strategy for OPN. Given frame N, we denote the output boxes of RPN is denoted as BB + N . We calculate the box IoU for every box in BB + N between bb N-1 , and then keep the boxes whose IoU are larger than α, i.e. 0.3. The kept boxes are BB N . However, since RPN has a probability (13% ∼ 15%) of failing to localize the object of interest. Thus, if number of elements in BB t is less . Given a sequence with the annotated first frame (F 1 and M 1 ), PTSNet has already predicted the masks (M 2 toM N-1 ) of F 2 to F N-1 . Firstly, OPN takes F N as input and generates proposals B + N , then preserve these proposals near the object of interest by applying IoU threshold with bb N-1 . These kept proposals are denoted as B N . Secondly, OTN identifies the best top-K proposals from B N to localize the object of interest. Finally, #N cropped current image with previous mask pair, #Q cropped image-mask pair, #P cropped image-mask pair and #1 annotated image-mask pair are fed into the DRSN individually, then we concatenate them and fed it into the subsequent modules to segment the object in the current frame. <ref type="table">Table 1</ref>. Comparison of recall rates between several datasets in different domains. "DET" denotes detection while "VOS" denotes video object segmentation. The recall ratio are obtained by RPN tailored from Mask R-CNN pretrained on the COCO training dataset. It can be observed that RPN achieves higher recall ratio in VOS domain datasets than in DET dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Domain Recall (IoU ≥ 0.5) COCO <ref type="bibr" target="#b28">[29]</ref> DET 83% DAVIS'17 <ref type="bibr" target="#b35">[36]</ref> VOS 85% YouTube-VOS <ref type="bibr" target="#b45">[46]</ref> VOS 87% than 5, we fill up BB t with extra boxes generated by the box sampling method described in <ref type="bibr" target="#b32">[33]</ref>, which is based on the assumption that the location variations of object will not be too large in adjacent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Object Tracking Network</head><p>OPN provides a few proposals near the object of interest, but they are class-agnostic. In this subsection, we aims at building an instance-specific appearance model based on labeled object in the first frame to identify the object of interest throughout the video. This is a reduced visual tracking problem. We call it a "reduced" tracking problem, since the candidate object locations are already given by OPN. Object Tracking Network (OTN) is introduced to solve this problem.</p><p>Our OTN is based on the high-performance deep learning based tracker MDNet <ref type="bibr" target="#b32">[33]</ref>. The differences between OTN and MDNet are (1) OTN is based on OPN while MDNet has its own tracking candidate generation method and (2) they are trained using different datasets. For self-contained of our paper, we introduce the details of OTN/MDNet in details as follows.</p><p>Given BB N , we crop its patches and resize them into the same size, e.g., H×W. Afterwards, these patches are passed into a network pretrained on the DAVIS'17 dataset to predict the confidence scores belonging to the target object for each patch. Then, we draw the top-5 scoring proposals. After that, we compute the mean value of the top-5 scores to check whether the tracking is a success, i.e., the mean value is bigger than 0, a success, otherwise a failure. If succeed, we collect deep features of the patches with spatial confidences into a memory stack. And we average the coordinates of the top-5 proposals as the current prediction. If failed, we do not collect any samples and keep the previous predicted box as the current prediction. Finally, we employ a short-long term model update strategy: If it is a success, we do short term model update by fine-tuning some specified fully connected layers using the collected features in a small window. If it is a failure, we check if it is the time to do regularly long-term model update which draws the whole collected samples to fine-tune the specified layers.</p><p>For the details of the datasets, please refer to Sec. <ref type="bibr">4.1</ref> In our experiment, we notice that the collaboration of OPN and OTN results in a more robust object tracker, which significantly alleviates the the burden of subsequent segmentation module. Consequently, the segmentation module only needs to care about the task of pixel-wise classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dynamic Reference Segmentation Network</head><p>Since coarse localization of target is obtained by the proposed coupled OPN and OTN, the Dynamic Reference Segmentation network (DRSN) is proposed to perform object segmentation with online adaption. Previous referenceguide models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b46">47]</ref> statically refer to the feature from the the first annotated frame. Thus, they are not able to adapt the appearance variations through a video sequence.</p><p>The dynamic inputs of DRSN. The network architecture of DRSN inherits from <ref type="bibr" target="#b42">[43]</ref>. And the main difference is our dynamic reference inputs described as follows: It inputs several (here we use 4 for example) image-mask pairs which consist of the current frame F N and previously predicted maskM N-1 , the first frame F 1 and its annotated mask M 1 , the P-th, Q-th frame F P , F Q and its predicted mask M P ,M Q . In detail, each image-mask pair is cropped from the origin image-mask and resized to the same shape of 4×H×W. Four channels are composed of three channels of RGB image, and one channel of mask. The choice of P and Q is worth discussing, there are various sampling styles like drawing predictions of highest confidence, equally interval spacing or key frames. In practice, we find equally spaced sampling in a recent time window works well, i.e. we could select frame max(1, N -4) and max(1, N -2) when window size and interval is set to 4 and 2, respectively. Note that in <ref type="figure">Fig 2,</ref> we expand the boxes enclosing their corresponding masks by a constant in height and width, e.g., 1.5. Then we crop the patches from frame F 1 , F P , F Q by the expanded boxes. Different from them, the image-mask patch of the current frame F N is cropped by the expanded box provided by OTN.</p><p>The network architecture of DRSN. Firstly the four image-mask patch pairs are fed into an convolutional network, e.g., ResNet-50 <ref type="bibr" target="#b14">[15]</ref>, to extract deep encoded features. Then the four features are concatenated in channel axis and fed into a global convolution network (GCN) <ref type="bibr" target="#b33">[34]</ref> and three refinement modules <ref type="bibr" target="#b37">[38]</ref>. It is expected that the GCN could properly aggregate the information from the labeled first frame and the recent appearance variations, and treat them as the supplementary information to help the prediction of current frame. The refinement modules merge features in three scales (1/4, 1/8, 1/16 shape of the input image-mask patch) and produce the output (1/4) with two channels indicating the probability of foreground and background. Finally bilinear interpolation is applied for the final mask prediction.</p><p>Unlike OnAVOS <ref type="bibr" target="#b40">[41]</ref>, DRSN performs online adaptation without fine-tuning the network with its predictions. The first annotated frame donated as static reference can always provide reliable but not up-to-the-minute appearance feature, which is the opposite of the appearance features generated from frames nearby donated as dynamic reference. Although the predicted masks may be inaccurate, it still offers effective appearance cues of the object of interest. In practice, to balance efficiency and performance, we choose the first annotated frame and another two historical frames close to the current frame and as reference. The proposed DRSN takes the advantage of both the static and dynamic references to obtain better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we will firstly introduce the datasets and our implementation details, then compare PTSNet to stateof-the-art methods. At last, ablation and add-on studies for each component of PTSNet will be revealed to validate their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation metric</head><p>Datasets. We evaluate the proposed PTSNet on the DAVIS'17 <ref type="bibr" target="#b35">[36]</ref> dataset and the recently released YouTube-VOS <ref type="bibr" target="#b45">[46]</ref> dataset. The two datasets contain assorted challenges, such as appearance and pose variations, motion blur and object occlusion.</p><p>DAVIS dataset has two sets, DAVIS'16 and DAVIS'17. The differences of the two sets are the number of sequences and whether multiple instances are annotated in each sequence. Here we choose the latter one yet the harder one for our experiments. DAVIS'16 only separates foreground and background for each frame so that there is no objectness concept when appearing multiple instances. However, the major functionality of PTSNet aims at video object segmentation. This is the reason why we do not conduct experiments on DAVIS'16. DAVIS'17 consists of 60 sequences for training and 30 sequences for evaluation, totally 8294 frames. Each frame is provided with pixel-level annotations, where one single instance or multiple different instances are separated from the background.</p><p>YouTube-VOS dataset is the largest dataset for VOS so far. It contains 4453 sequences and is split into training (3471), online validation (474) and online testing (508) sets. The training set is annotated for every 5 frames, comprising one single instance or multiple different instances just like DAVIS'17. It is noteworthy that the online validation set accepts predictions of every 5 frames while the online testing set requires predictions of each frame.</p><p>Evaluation metric. For DAVIS'17 and YouTube-VOS dataset, we follow <ref type="bibr" target="#b35">[36]</ref> that adopts region similarity (J ), contour accuracy (F) and their average (G) measures for evaluation. Region similarity (J ) is calculated as the average IoU between the proposed masks and the groundtruth masks respectively, while the contour accuracy (F) interprets the proposed masks as a set of closed contours and computes the contour-based F-measure which is a function of precision and recall. Generally speaking, region similarity measures the ratio of correctly labeled pixels predicted by algorithms and contour accuracy measures the precision of the segmentation boundaries. OPN. It is based on RPN tailored from Mask-RCNN <ref type="bibr" target="#b13">[14]</ref>. We use the ResNeXt-152 <ref type="bibr" target="#b44">[45]</ref> backbone edition pretrained on the COCO dataset. The weights of RPN are fixed and NMS (Non-maximum suppression) and threshold restriction are removed to obtain about 2000 proposals for each frame. The α of filtering out irrelevant proposals is set to 0.3. If RPN fails to localize the object of interest, we draw 256 samples and the spatial and scale hyper-parameters of Gaussian box sampling <ref type="bibr" target="#b32">[33]</ref> are set to 0.1 and 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>OTN. It inherits MDNet <ref type="bibr" target="#b32">[33]</ref> and is trained on DAVIS, based on the ImageNet pretrained VGG-M [6] backbone. Considering the success of MDNet with few training videos, it is enough to train a high performance tracker. And the recent Real-Time MDNet <ref type="bibr" target="#b20">[21]</ref> shows that it does not gain much improvement after switching to a larger dataset. The hyper-parameters of OTN are almost same with MD-Net. The main differences are (1) The final bounding box regression is removed as we have high-quality proposals already, because the videos in DAVIS dataset are usually shorter than the ones in VOT datasets and appearance variations over time are usually larger (2) We have cut down the updating window sizes of short term (from 20 to 5) and long term (100 to 20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DRSN.</head><p>The weights of encoder is initialized by ResNet-50 pretrained on ImageNet. For the evaluation of YouTube-VOS, we only train DRSN on YouTube-VOS for 100k iteration and batch size of 64. Adam <ref type="bibr" target="#b24">[25]</ref> optimizer, the initial learning rate of 2e-05 (decay by 0.1 after 60k iterations) on 4 NVIDIA Titan V GPUs. The shape of image-mask pair is 4 × 256 × 256 and the channels of GCN and refinement modules are all 256. For the evaluation of DAVIS'17, we take the model pretrained on YouTube-VOS, then continue training on DAVIS'17 training set for an extra 35k iterations, with a learning rate of 2e-07 (decay to 2e-8 after 12.5k iterations).</p><p>In the training phase, we firstly pick up a target from an arbitrary sequence and respectively its 3 sampling frames, as mentioned in section 3.4. A random shift similar to <ref type="bibr" target="#b15">[16]</ref> is applied to each enclosed box of the ground-truth mask, so as to simulate the characteristic of boxes predicted by OTN. Afterwards, we enlarge the shifted boxes by 1.5 times along the width and height axes, in order to ensure the completeness of the object in patch. Next, the four stacked tensors, including an image-mask pair of the sampled first frame, two image-mask pairs of the sampled P-th and Q-th frames, along with a pair of image in N-th and blurred mask simulating the previous prediction, are fed into DRSN. The loss function is defined as the standard pixel-wise cross entropy to measure the similarity of the final predicted mask and the groundtruth mask. We also use data augmentation strategies, like mirror flipping and illumination enhancement to increase the robustness of our model.</p><p>As for online fine-tuning, we randomly generate image pairs and their masks only from the annotated first frame, keeping the same settings in training stage. The model is fine-tuned on these pairs for 400 iterations with a initial learning rate of 1e-06 with an Adam optimizer. <ref type="table">Table 2</ref>. Quantitative evaluation of our method compared with the state-of-the-art results from previous literature on the DAVIS'17 validation set. For each method, we report whether it employs online fine-tuning (OF) using the first frame as defined in <ref type="bibr" target="#b3">[4]</ref>, is it causal, and the final performance J Mean, F Mean and G. Without OF and under the restriction of causality, our approach surpasses current state-of-the art methods. Further, our approach performs better than DyeNet when applied online fine-tuning but still kept causal.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The DAVIS'17 benchmark</head><p>In <ref type="table">Table 2</ref>, the quantitative results of our approach are compared with other methods in previous literature, and our method achieves top performance. In the circumstance of discarding online fine-tuning (OF), PTSNet outperforms many state-of-the-art methods like OSMN, RGMP and VideoMatch except DyeNet and PReMVOS. However, the inference setting of DyeNet and PReMVOS is iterative inference which makes the method non-casual. It is unfair for the causal methods to compare with the methods with iterative inference. In the circumstance of equipped with online fine-tuning, PTSNet outperforms all listed state-ofthe-art methods by a large margin, and it is even better than DyeNet. The performance of PReMVOS is higher than ours with online fine-tuning, however our method is free of burdened modules, e.g. ReID and FlowNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The YouTube-VOS benchmark</head><p>YouTube-VOS is a recently released largest-scale dataset for VOS. The validation set contains 474 sequences with 65 seen classes in training set and 26 classes which are not included. We compare our results with previous published literature <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b19">20]</ref>. Our results are obtained by submitting to the official evaluation server.</p><p>We also present some state-of-the-art results from literature on the YouTube-VOS official validation set in <ref type="table">Table 3</ref>, the proposed PTSNet significantly outperforms the other <ref type="table">Table 3</ref>. Comparisons with start-of-the-art methods on the YouTube-VOS validation set. J denotes the region similarity and the contour accuracy. "s" and "u" denote the results averaged over the seen categories and unseen categories, respectively. "Mean" denotes the results averaged over J and F. "OF" denotes online fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>OF   <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46]</ref> under the circumstances of with or without online fine-tuning. <ref type="figure" target="#fig_2">Figure 3</ref> shows some successful video object segmentation results. It indicates PTSNet is robust to occlusion (row 1 and 3), large scale and appearance change (row 2), camera motion (row 4) etc. However, PTSNet has the risk of drifting in case of long-term large occlusion. This is mainly because after long-term occlusion, OPN have no elaborate means to keep the proposals near the object of interest when the object appears again. Further speaking, the assumption that the object of interest will not move too much within nearby frames can not be utilized in such circumstance. And when the object reappears after long-term occlusion, OTN has no proper mechanism to find out the precise location of the object, which leads to the failure of PTSNet in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>To demonstrate the effectiveness of the three proposed components, we have designed several groups of ablation studies. All the experiments are conducted on the DAVIS'17 validation set and the performance is measured by J Mean. Besides, we also evaluate DAVIS as a tracking dataset to prove the effectiveness of objectness information.</p><p>Single reference vs. Dynamic reference. As shown in row 1 and 2 in <ref type="table" target="#tab_2">Table 4</ref>, we compare two reference settings of the segmentation network, which are static reference and our dynamic reference, named RGMP and DRSN respectively. Note that we disregard the BPTT <ref type="bibr" target="#b42">[43]</ref> trick proposed  <ref type="table">Table 6</ref>. Study of the ideal number of reference frames. "Gt" means the given first frame; "Ref (i)" denotes the frame index as the dynamic reference. "N" denotes the index of the current frame. The speed is tested on Titan V. in the original paper for fair comparison, and the training dataset and hyper-parameter are kept same as well. It shows that DRSN outperforms RGMP by 5.1, indicting dynamic reference significantly boosts performance. We also analyze the ideal number of reference frames and consuming time (Titan V) used by DRSN, which is shown in <ref type="table">Table 6</ref>.</p><p>To balance efficiency and speed, we select the setting in the third row with 66.1 J Mean and 56.15 ms per frame.</p><p>The effectiveness of objectness clues. We analyze the influence by whether introducing OPN respectively. Several experiments are conducted to test our assumption.</p><p>DRSN w/ OTN (row 3) uses simple Gaussian sampling the same as what MDNet does without objectness information involved. The reliability of tracking is determined by the straight criterion mentioned in the last passage. As shown in <ref type="table" target="#tab_2">Table 4</ref>, it achieves a margin gain of 0.4 (row 3 vs. row 2). DRSN w/ OPN and OTN (row 4) gets a notable 2.2 improvement from J Mean of 63.9 to 66.1 (row 3 vs. row 4) by adding OPN module, which illustrates the fact that the object-level guidance indeed helps mask tracking compared to the experiment in last paragraph. By the way, online finetuning boosts our methods to 71.6 J Mean further.</p><p>Evaluating DAVIS as a tracking dataset. To prove the effectiveness of introduction of OPN by qualitative analysis, we treat the DAVIS'17 validation set as a tracking dataset. Before the evaluation, we firstly train OTN on DAVIS'17 training dataset by extracting tracking boxes from masks. Then we use the same OTN to evaluate: (a) A Gaussian proposal generator proposed in MDNet stacked by OTN (b) OPN stacked by OTN. We use standard AUC <ref type="bibr" target="#b41">[42]</ref> metric as the criterion. In <ref type="table" target="#tab_3">Table 5</ref>, the impressive performance enhancement shows that the introduction of objectness is a key factor in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>In this paper, we propose PTSNet, a cascaded framework for semi-supervised VOS. Our PTSNet reaches the cur-rent state-of-the-art performance in an intuitive way, bringing in objectness and tracking from object level to pixel level. Owning to the decomposing video object segmentation into three sub-modules (Object Proposal Network, Object Tracking Network, Dynamic Reference Segmentation Network), PTSNet can handle large scale and appearance variations, respectively. With the modular design, PT-SNet can easily benefit from other state-of-the-art methods to achieve scalable performance.</p><p>There still remains many future directions in our framework. For example, we can integrate modules in a more elegant way to enable end-to-end training. To make PTSNet more robust on challenging scenes with long time object occlusion or crowded objects, re-identification module could be used for long-term association to alleviate problems such as lost track caused by occlusion or ID switch between similar objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 OPNFigure 2 .</head><label>12</label><figDesc>For the details of the dataset, please refer to Sec. 4.Overview of the proposed PTSNet, which consists of Object Proposal Network (OPN), Object Tracking Network (OTN) and Dynamic Reference Segmentation Network (DRSN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>As mentioned in Sec. 3, the proposed PTSNet is composed of OPN (Sec. 3.2), OTN (Sec. 3.3) and DRSN (Sec. 3.4), whose details are described as follows respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization results uniformly sampled in videos on the DAVIS'17 and YouTube-VOS datasets. The "0%" column presents the annotated first frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies of PTSNet on the DAVIS'17 validation set, measured by J Mean. We use RGMP as our baseline for segmentation. Dy-Ref. denotes dynamic reference. OTN and OPN denote object tracking network and object proposal network respectively. OF denotes online fine-tuning on the first frame.</figDesc><table><row><cell cols="2">RGMP Dy-Ref. OTN OPN OF J Mean ∆</cell></row><row><cell>58.8</cell><cell></cell></row><row><cell>63.9</cell><cell>+5.1</cell></row><row><cell>64.3</cell><cell>+5.5</cell></row><row><cell>66.1</cell><cell>+7.3</cell></row><row><cell cols="2">71.6 +13.6</cell></row><row><cell>methods</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Study the effectiveness of OPN by evaluating DAVIS as a tracking dataset. "G-sampling" denotes the Gaussian sampling proposed in MDNet.<ref type="bibr" target="#b32">[33]</ref> Both entries use a same OTN exactly and the only difference is the module of proposal generator.</figDesc><table><row><cell>G-Sampling OPN OTN AUC ∆</cell></row><row><cell>52.1</cell></row><row><cell>77.8 +25.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cnn in mrf: Video object segmentation via inference in a cnn-based higher-order spatiotemporal mrf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5977" to="5986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5320" to="5329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2051" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7415" to="7424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video object segmentation by learning location-sensitive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="501" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 ieee computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Motionguided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="56" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11611</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time mdnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="89" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mask r-cnn: A perspective on equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="http://kaiminghe.com/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf.Accessed3" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object detection in videos with tubelet proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A novel performance evaluation methodology for single-target trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2137" to="2155" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="93" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Cconference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Premvos: Proposalgeneration, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="743" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large kernel mattersimprove semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1743" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3491" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3227" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-tosequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
