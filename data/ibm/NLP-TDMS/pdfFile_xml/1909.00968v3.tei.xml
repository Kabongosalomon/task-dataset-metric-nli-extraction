<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Inpainting with Learnable Bidirectional Attention Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohao</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
							<email>lichao40@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nankai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<email>wmzuo@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao12@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
							<email>wenshilei@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Inpainting with Learnable Bidirectional Attention Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most convolutional network (CNN)-based inpainting methods adopt standard convolution to indistinguishably treat valid pixels and holes, making them limited in handling irregular holes and more likely to generate inpainting results with color discrepancy and blurriness. Partial convolution has been suggested to address this issue, but it adopts handcrafted feature re-normalization, and only considers forward mask-updating. In this paper, we present a learnable attention map module for learning feature renormalization and mask-updating in an end-to-end manner, which is effective in adapting to irregular holes and propagation of convolution layers. Furthermore, learnable reverse attention maps are introduced to allow the decoder of U-Net to concentrate on filling in irregular holes instead of reconstructing both holes and known regions, resulting in our learnable bidirectional attention maps. Qualitative and quantitative experiments show that our method performs favorably against state-of-the-arts in generating sharper, more coherent and visually plausible inpainting results. The source code and pre-trained models will be available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image inpainting <ref type="bibr" target="#b2">[3]</ref>, aiming at filling in holes of an image, is a representative low level vision task with many realworld applications such as distracting object removal, occluded region completion, etc. However, there may exist multiple potential solutions for the given holes in an image, i.e., the holes can be filled with any plausible hypotheses coherent with the surrounding known regions. And the holes can be of complex and irregular patterns, further increasing the difficulty of image inpainting. Traditional exemplarbased methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>, e.g., PatchMatch <ref type="bibr" target="#b1">[2]</ref>, gradually fill in holes by searching and copying similar patches from known regions. Albeit exemplar-based methods are effective in hallucinating detailed textures, they are still limited † This work was done when Chaohao Xie was a research intern at Baidu * Corresponding author in capturing high-level semantics, and may fail to generate complex and non-repetitive structures (see <ref type="figure">Fig. 1(c)</ref>).</p><p>Recently, considerable progress has been made in applying deep convolutional networks (CNNs) to image inpainting <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>. Benefited from the powerful representation ability and large scale training, CNN-based methods are effective in hallucinating semantically plausible result. And adversarial loss <ref type="bibr" target="#b7">[8]</ref> has also been deployed to improve the perceptual quality and naturalness of the result. Nonetheless, most existing CNN-based methods usually adopt standard convolution which indistinguishably treats valid pixels and holes. Thus, they are limited in handling irregular holes and more likely to generate inpainting results with color discrepancy and blurriness. As a remedy, several postprocessing techniques <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref> have been introduced but are still inadequate in resolving the artifacts (see <ref type="figure">Fig. 1(d)</ref>).</p><p>CNN-based methods have also been combined with exemplar-based one to explicitly incorporate the mask of holes for better structure recovery and detail enhancement <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. In these methods, the mask is utilized to guide the propagation of the encoder features from known regions to the holes. However, the copying and enhancing operation heavily increases the computational cost and is only deployed at one encoding and decoding layers. As a result, they are better at filling in rectangular holes, and perform poorly on handling irregular holes (see <ref type="figure">Fig. 1(e)</ref>).</p><p>For better handling irregular holes and suppressing color discrepancy and blurriness, partial convolution (PConv) <ref type="bibr" target="#b16">[17]</ref> has been suggested. In each PConv layer, mask convolution is used to make the output conditioned only on the unmasked input, and feature re-normalization is introduced for scaling the convolution output. A maskupdating rule is further presented to update a mask for the next layer, making PConv very effective in handling irregular holes. Nonetheless, PConv adopts hard 0-1 mask and handcrafted feature re-normalization by absolutely trusting all filling-in intermediate features. Moreover, PConv considers only forward mask-updating and simply employs allone mask for decoder features. (c) PM <ref type="bibr" target="#b1">[2]</ref> (d) GL <ref type="bibr" target="#b9">[10]</ref> (e) CA <ref type="bibr" target="#b35">[36]</ref> (f) PConv <ref type="bibr" target="#b16">[17]</ref> (g) Ours <ref type="figure">Figure 1</ref>. Qualitative comparison of inpainting results by PatchMatch (PM) <ref type="bibr" target="#b1">[2]</ref>, Global&amp;Local (GL) <ref type="bibr" target="#b9">[10]</ref>, Context Attention (CA) <ref type="bibr" target="#b35">[36]</ref>, and Partial Convolution (PConv) <ref type="bibr" target="#b16">[17]</ref>, and Ours.</p><p>In this paper, we take a step forward and present the modules of learnable bidirectional attention maps for the re-normalization of features on both encoder and decoder of the U-Net <ref type="bibr" target="#b21">[22]</ref> architecture. To begin with, we revisit PConv without bias, and show that the mask convolution can be safely avoided and the feature re-normalization can be interpreted as a re-normalization guided by hard 0-1 mask. To overcome the limitations of hard 0-1 mask and handcrafted mask-updating, we present a learnable attention map module for learning feature re-normalization and mask-updating. Benefited from the end-to-end training, the learnable attention map is effective in adapting to irregular holes and propagation of convolution layers.</p><p>Furthermore, PConv simply uses all-one mask on the decoder features, making the decoder should hallucinate both holes and known regions. Note that the encoder features of known region will be concatenated, it is natural that the decoder is only required to focus on the inpainting of holes. Therefore, we further introduce learnable reverse attention maps to allow the decoder of U-Net concentrate only on filling in holes, resulting in our learnable bidirectional attention maps. In contrast to PConv, the deployment of learnable bidirectional attention maps empirically is beneficial to network training, making it feasible to include adversarial loss for improving visual quality of the result.</p><p>Qualitative and quantitative experiments are conducted on the Paris SteetView <ref type="bibr" target="#b5">[6]</ref> and Places <ref type="bibr" target="#b39">[40]</ref> datasets to evaluate our proposed method. The results show that our proposed method performs favorably against state-of-the-arts in generating sharper, more coherent and visually plausible inpainting results. From <ref type="figure">Fig. 1</ref>(f)(g), our method is more effective in hallucinating clean semantic structure and realistic textures in comparison to PConv. To sum up, the main contribution of this work is three-fold,</p><p>• A learnable attention map module is presented for image inpainting. In contrast to PConv, the learnable attention maps are more effective in adapting to arbitrary irregular holes and propagation of convolution layers. • Forward and reverse attention maps are incorporated to constitute our learnable bidirectional attention maps, further benefiting the visual quality of the result.</p><p>• Experiments on two datasets and real-world object removal show that our method performs favorably against state-of-the-arts in hallucinating shaper, more coherent and visually plausible results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we present a brief survey on the relevant work, especially the propagation process adopted in exemplar-based methods as well as the network architectures of CNN-based inpainting methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Exemplar-based Inpainting</head><p>Most exemplar-based inpainting methods search and paste from the known regions to gradually fill in the holes from the exterior to the interior <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>, and their results highly depend on the propagation process. In general, better inpainting result can be attained by first filling in structures and then other missing regions. To guide the patch processing order, patch priority <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref> measure has been introduced as the product of confidence term and data term. While the confidence term is generally defined as the ratio of known pixels in the input patch, several forms of data terms have been proposed. In particular, <ref type="bibr">Criminisi et al. [4]</ref> suggested a gradient-based data term for filling in linear structure with higher priority. Xu and Sun <ref type="bibr" target="#b31">[32]</ref> assumed that structural patches are sparsely distributed in an image, and presented a sparsity-based data term. Le Meur et al. <ref type="bibr" target="#b17">[18]</ref> adopted the eigenvalue discrepancy of structure tensor <ref type="bibr" target="#b4">[5]</ref> as an indicator of structural patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep CNN-based Inpainting</head><p>Early CNN-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref> are suggested for handling images with small and thin holes. In the past few years, deep CNNs have received upsurging interest and exhibited promising performance for filling in large holes. Phatak et al. <ref type="bibr" target="#b19">[20]</ref> adopted an encoder-decoder network (i.e., context-encoder), and incorporated reconstruction and adversarial losses for better recovering semantic structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward Attention Input</head><p>Reverse Attention Output Partial Conv Iizuka et al. <ref type="bibr" target="#b9">[10]</ref> combined both global and local discriminators for reproducing both semantically plausible structures and locally realistic details. Wang et al. <ref type="bibr" target="#b27">[28]</ref> suggested a generative multi-column CNN incorporating with confidence-driven reconstruction loss and implicit diversified MRF (ID-MRF) term.</p><p>Multi-stage methods have also been investigated to ease the difficulty of training deep inpainting networks. <ref type="bibr">Zhang et al. [37]</ref> presented a progressive generative networks (PGN) for filling in holes with multiple phases, while LSTM is deployed to exploit the dependencies across phases. Nazeri et al. <ref type="bibr" target="#b18">[19]</ref> proposed a two-stage model EdgeConnect first predicting salient edges and then generating inpainting result guided by edges. Instead, Xiong et al. <ref type="bibr" target="#b30">[31]</ref> presented foreground-aware inpainting, which involves three stages, i.e., contour detection, contour completion and image completion, for the disentanglement of structure inference and content hallucination.</p><p>In order to combine exemplar-based and CNN-based methods, Yang et al. <ref type="bibr" target="#b33">[34]</ref> suggested multi-scale neural patch synthesis (MNPS) to refine the result of context-encoder via joint optimization with the holistic content and local texture constraints. Other two-stage feed-forward models, e.g., contextual attention <ref type="bibr" target="#b25">[26]</ref> and patch-swap <ref type="bibr" target="#b35">[36]</ref>, are further developed to overcome the high computational cost of MNPS while explicitly exploiting image features of known regions. Concurrently, Yan et al. <ref type="bibr" target="#b32">[33]</ref> modified the U-Net to form an one-stage network, i.e., Shift-Net, to utilize the shift of encoder feature from known regions for better reproducing plausible semantics and detailed contents. Most recently, Zheng et al. <ref type="bibr" target="#b38">[39]</ref> introduced an enhanced short+long term attention layer, and presented a probabilistic framework with two parallel paths for pluralistic inpainting.</p><p>Most existing CNN-based inpainting methods are usually not well suited for handling irregular holes. To address this issue, Liu et al. <ref type="bibr" target="#b16">[17]</ref> proposed a partial convolution (PConv) layer involving three steps, i.e., mask convolution, feature re-normalization, and mask-updating. Yu et al. <ref type="bibr" target="#b34">[35]</ref> provided gated convolution which learns channel-wise soft mask by considering both corrupted images, masks and user sketches. However, PConv adopts handcrafted feature renormalization and only considers forward mask-updating, making it still limited in handling color discrepancy and blurriness (see <ref type="figure">Fig. 1(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first revisit PConv, and then present our learnable bidirectional attention maps. Subsequently, the network architecture and learning objective of our method are also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting Partial Convolution</head><p>A PConv <ref type="bibr" target="#b16">[17]</ref> layer generally involves three steps, i.e., (i) mask convolution, (ii) feature re-normalization, and (iii) mask-updating. Denote by F in the input feature map and M the corresponding hard 0-1 mask. We further let W be the convolution filter and b be its bias. To begin with, we introduce the convolved mask M c = M ⊗ k <ref type="bibr">1 9</ref> , where ⊗ denotes the convolution operator, k 1 9 denotes a 3 × 3 convolution filter with each element <ref type="bibr">1 9</ref> . The process of PConv can be formulated as,</p><formula xml:id="formula_0">(i) F conv = W T (F in M),<label>(1)</label></formula><formula xml:id="formula_1">(ii) F out = F conv fA(M c ) + b, if M c &gt; 0 0, otherwise<label>(2)</label></formula><formula xml:id="formula_2">(iii) M = fM (M c )<label>(3)</label></formula><p>where A = f A (M c ) denotes the attention map, and M = f M (M c ) denotes the updated mask. We further define the activation functions for attention map and updated mask as,</p><formula xml:id="formula_3">fA(M c ) = 1 M c , if M c &gt; 0 0, otherwise (4) fM (M c ) = 1, if M c &gt; 0 0, otherwise<label>(5)</label></formula><p>From Eqns. (1)∼(5) and <ref type="figure" target="#fig_1">Fig. 2(a)</ref>, PConv can also be explained as a special interplay model between mask and convolution feature map. However, PConv adopts the handcrafted convolution filter k <ref type="bibr">1 9</ref> as well as handcrafted activation functions f A (M c ) and f M (M c ), thereby giving Eqn. <ref type="formula" target="#formula_0">(12)</ref> Skip connection  some leeway for further improvements. Moreover, the nondifferential property of f M (M c ) also increases the difficulty of end-to-end learning. To our best knowledge, it remains a difficult issue to incorporate adversarial loss to train a U-Net with PConv. Furthermore, PConv only considers the mask and its updating for encoder features. As for decoder features, it simply adopts all-one mask, making PConv limited in filling holes.</p><formula xml:id="formula_4">f f f f f f g g g g g g g g g g g g f f g g g g f f g g g g f f f f g g g g f f g g g g f f g g g g f f g g g g f f g g g g f f g g g g f f Element-wise production gA(•) gM(•) M in 1-M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learnable Attention Maps</head><p>The convolution layer without bias has been widely adopted in U-Net for image-to-image translation <ref type="bibr" target="#b10">[11]</ref> and image inpainting <ref type="bibr" target="#b32">[33]</ref>. When the bias is removed, it can be readily seen from Eqn. (2) that the convolution features in updated holes are zeros. Thus, the mask convolution in Eqn. <ref type="formula" target="#formula_0">(1)</ref> is equivalently rewritten as standard convolution,</p><formula xml:id="formula_5">(i) F conv = W T F in .<label>(6)</label></formula><p>Then, the feature re-normalization in Eqn. <ref type="formula" target="#formula_1">(2)</ref> can be interpreted as the element-wise product of convolution feature and attention map,</p><formula xml:id="formula_6">(ii) F out = F conv fA(M c ).<label>(7)</label></formula><p>Even though, the handcrafted convolution filter k 1 9 is fixed and not adapted to the mask. The activation function for updated mask absolutely trusts the inpainting result in the region M c &gt; 0, but it is more sensible to assign higher confidence to the region with higher M c .</p><p>To overcome the above limitations, we suggest learnable attention map which generalizes PConv without bias from three aspects. First, to make the mask adaptive to irregular holes and propagation along with layers, we substitute k 1 9 with layer-wise and learnable convolution filters k M . Second, instead of hard 0-1 mask-updating, we modify the activation function for updated mask as,</p><formula xml:id="formula_7">gM (M c ) = (ReLU (M c )) α ,<label>(8)</label></formula><p>where α ≥ 0 is a hyperparameter and we set α = 0.8.</p><formula xml:id="formula_8">One can see that g M (M c ) degenerates into f M (M c ) when α = 0.</formula><p>Third, we introduce an asymmetric Gaussianshaped form as the activation function for attention map,</p><formula xml:id="formula_9">gA(M c )= a exp −γ l (M c − µ) 2 , if M c &lt; µ 1+(a−1) exp −γr(M c −µ) 2 , else<label>(9)</label></formula><p>where a, µ, γ l , and γ r are the learnable parameters, we initialize them as a = 1.1, µ = 2.0, γ l = 1.0, γ r = 1.0 and learn them in an end-to-end manner.</p><p>To sum up, the learnable attention map adopt Eqn. <ref type="formula" target="#formula_5">(6)</ref> in Step (i), and the next two steps are formulated as, <ref type="figure" target="#fig_1">Fig. 2(b)</ref> illustrates the interplay model of learnable attention map. In contrast to PConv, our learnable attention map is more flexible and can be end-to-end trained, making it effective in adapting to irregular holes and propagation of convolution layers.</p><formula xml:id="formula_10">(ii) F out = F conv gA(M c ),<label>(10)</label></formula><formula xml:id="formula_11">(iii) M = gM (M c ).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learnable Bidirectional Attention Maps</head><p>When incorporating PConv with U-Net for inpainting, the method <ref type="bibr" target="#b16">[17]</ref> only updates the masks along with the convolution layers for encoder features. However, all-one mask is generally adopted for decoder features. As a result, the (L − l)-th layer of decoder feature in both known regions and holes should be hallucinated using both (l + 1)-th layer of encoder feature and (L−l−1)-th layer of decoder feature. Actually, the l-th layer of encoder feature will be concatenated with the (L − l)-th layer of decoder feature, and we can only focus on the generation of the (L − l)-th layer of decoder feature in the holes. We further introduce learnable reverse attention maps to the decoder features. Denote by M c e the convolved mask for encoder feature F in e . Let M c d = M d ⊗ k M d be the convolved mask for decoder feature F in d . The first two steps of Original Input PM <ref type="bibr" target="#b1">[2]</ref> GL <ref type="bibr" target="#b9">[10]</ref> CA <ref type="bibr" target="#b35">[36]</ref> PConv <ref type="bibr" target="#b16">[17]</ref> Ours <ref type="figure">Figure 4</ref>. Qualitative comparison on Paris StreetView dataset. Comparison with PatchMatch (PM) <ref type="bibr" target="#b1">[2]</ref>, Global&amp;Local(GL) <ref type="bibr" target="#b9">[10]</ref>, Context Attention(CA) <ref type="bibr" target="#b35">[36]</ref>, PConv <ref type="bibr" target="#b16">[17]</ref> and Ours. learnable reverse attention map can be formulated as,</p><formula xml:id="formula_12">(i&amp;ii) F out d = (W T e F in e ) gA(M c e )+(W T d F in d ) gA(M c d )</formula><p>. <ref type="formula" target="#formula_0">(12)</ref> where W e and W d are the convolution filters. And we define g A (M c d ) as the reverse attention map. Then, the mask M c d is updated and deployed to the former decoder layer, <ref type="figure" target="#fig_1">Fig. 2(c)</ref> illustrates the interplay model of reverse attention map. In contrast to forward attention maps, both encoder feature (mask) and decoder feature (mask) are considered. Moreover, the updated mask in reverse attention map is applied to the former decoder layer, while that in forward attention map is applied to the next encoder layer. By incorporating forward and reverse attention maps with U-Net, <ref type="figure" target="#fig_2">Fig. 3</ref> shows the full learnable bidirectional attention maps. Given an input image I in with irregular holes, we use M in to denote the binary mask, where ones indicate the valid pixels and zeros indicate the pixels in holes. From <ref type="figure" target="#fig_2">Fig. 3</ref>, the forward attention maps take M in as the input mask for the re-normalization of the first layer of encoder feature, and gradually update and apply the mask to next encoder layer. In contrast, the reverse attention maps take 1 − M in as the input for the re-normalization of the last (i.e., L-th) layer of decoder feature, and gradually update and apply the mask to former decoder layer. Benefited from the end-to-end learning, our learnable bidirectional attention maps (LBAM) are more effective in handling irregular holes. The introduction of reverse attention maps allows the decoder concentrate only on filling in irregular holes, which is also helpful to inpainting performance. Our LBAM is also beneficial to network training, making it feasible to exploit adversarial loss for improving visual quality.</p><formula xml:id="formula_13">(iii) M d = gM (M c d ).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Architecture</head><p>We modify the U-Net architecture <ref type="bibr" target="#b10">[11]</ref> of 14 layers by removing the bottleneck layer and incorporating with bidirectional attention maps (see <ref type="figure" target="#fig_2">Fig. 3</ref>). In particular, forward attention layers are applied to the first six layers of encoder, while reverse attention layers are adopted to the last six layers of decoder. For all the U-Net layers and the forward and reverse attention layers, we use convolution filters with the kernel size of 4 × 4, stride 2 and padding 1, and no bias parameters are used. In the U-Net backbone, batch normalization and leaky ReLU nonlinearity are used to the features after re-normalization, and tanh nonlinearity is deployed right after convolution for the last layer. <ref type="figure" target="#fig_2">Fig. 3</ref> also provides the size of feature map for each layer, and more details of the network architecture are given in the suppl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Functions</head><p>For better recovery of texture details and semantics, we incorporate pixel reconstruction loss, perceptual loss <ref type="bibr" target="#b11">[12]</ref>, style loss <ref type="bibr" target="#b6">[7]</ref> and adversarial loss <ref type="bibr" target="#b7">[8]</ref> to train our LBAM. Pixel Reconstruction Loss. Denote by I in the input image with holes, M in the binary mask region, and I gt the ground-truth image. The output of our LBAM can be defined as I out = Φ(I in , M in ; Θ), where Θ denotes the model parameters to be learned. We adopt the 1 -norm error of the output image as the pixel reconstruction loss,</p><formula xml:id="formula_14">L 1 = I out − I gt 1 .<label>(14)</label></formula><p>Perceptual Loss. The 1 -norm loss is limited in capturing high-level semantics and is not consistent with the human perception of image quality. To alleviate this issue, we introduce the perceptual loss L perc defined on the VGG-16 network [25] pre-trained on ImageNet <ref type="bibr" target="#b22">[23]</ref>, Original Input PM <ref type="bibr" target="#b1">[2]</ref> GL <ref type="bibr" target="#b9">[10]</ref> CA <ref type="bibr" target="#b35">[36]</ref> PConv <ref type="bibr" target="#b16">[17]</ref> Ours <ref type="figure">Figure 5</ref>. Qualitative comparison on Places dataset. Comparison with PatchMatch (PM) <ref type="bibr" target="#b1">[2]</ref>, Global&amp;Local(GL) <ref type="bibr" target="#b9">[10]</ref>, Context Attention(CA) <ref type="bibr" target="#b35">[36]</ref>, PConv <ref type="bibr" target="#b16">[17]</ref> and Ours.</p><formula xml:id="formula_15">Lperc = 1 N N i=1 P i (I gt ) − P i (I out ) 2<label>(15)</label></formula><p>where P i (·) is the feature maps of the i-th pooling layer.</p><p>In our implementation, we use pool-1, pool-2, and pool-3 layers of the pre-trained VGG-16.</p><p>Style Loss. For better recovery of detailed textures, we further adopt the style loss defined on the feature maps from the pooling layers of VGG-16. Analogous to <ref type="bibr" target="#b16">[17]</ref>, we construct a Gram matrix from each layer of feature map. Suppose that the size of feature map P</p><formula xml:id="formula_16">i (I) is H i × W i × C i .</formula><p>The style loss can then be defined as,</p><formula xml:id="formula_17">L style = 1 N N i=1 1 Ci × Ci × P i (I gt )(P i (I gt )) T − P i (I out )(P i (I out )) T 2<label>(16)</label></formula><p>Adversarial Loss. Adversarial loss <ref type="bibr" target="#b7">[8]</ref> has been widely adopted in image generation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref> and low level vision <ref type="bibr" target="#b15">[16]</ref> for improving the visual quality of generated images. In order to improve the training stability of GAN, Arjovsky et al. <ref type="bibr" target="#b0">[1]</ref> exploit the Wasserstein distance for measuring the distribution discrepancy between generated and real images, and Gulrajani et al. <ref type="bibr" target="#b8">[9]</ref> further introduce gradient penalty for enforcing the Lipschitz constraint in discriminator. Following <ref type="bibr" target="#b8">[9]</ref>, we formulate the adversarial loss as,</p><formula xml:id="formula_18">L adv = min Θ max D E I gt ∼p data (I gt ) D(I gt ) − E I out ∼p data (I out ) D(I out ) + λEÎ ∼pÎ (( ∇Î D(Î) ) 2 − 1) 2<label>(17)</label></formula><p>where D(·) represents the discriminator.Î is sampled from I gt and I out by linear interpolation with a randomly selected factor, λ is set to 10 in our experiments. We empirically find that it is difficult to train the PConv model when including adversarial loss. Fortunately, the incorporation of learnable attention maps is helpful to ease the training, making it feasible to learn LBAM with adversarial loss. Please refer to the suppl. for the network architecture of the 7-layer discriminator used in our implementation. Model Objective Taking the above loss functions into account, the model objective of our LBAM can be formed as,</p><formula xml:id="formula_19">L = λ1L 1 + λ2L adv + λ3Lperc + λ4L style<label>(18)</label></formula><p>where λ 1 , λ 2 , λ 3 , and λ 4 are the tradeoff parameters. In our implementation, we empirically set λ 1 = 1, λ 2 = 0.1, λ 3 = 0.05 and λ 4 = 120.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Experiments are conducted for evaluating our LBAM on two datasets, i.e., Paris StreetView <ref type="bibr" target="#b5">[6]</ref> and Places (Places365-standard) <ref type="bibr" target="#b39">[40]</ref>, which have been extensively adopted in image inpainting literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. For Paris StreetView, we use its original splits, 14, 900 images for training, and 100 images for testing. In our experiments, 100 images are randomly selected and removed from the training set to form our validation set. As for Places, we randomly select 10 categories from the 365 categories, and use all the 5, 000 images per category from the original training set to form our training set of 50, 000 images. Moreover, we divide the original validation set from each category of 1, 000 images into two equal non-overlapped sets of 500 images respectively for validation and testing. Our LBAM takes ∼ 70 ms for processing a 256 × 256 image, 5× faster than Context Attention <ref type="bibr" target="#b35">[36]</ref> (∼ 400ms) and ∼ 3× faster than Global&amp;Local(GL) <ref type="bibr">[10] (∼ 200ms)</ref>.</p><p>In our experiments, all the images are resized where the minimal height or width is 350, and then randomly cropped Original Input</p><p>CA <ref type="bibr" target="#b35">[36]</ref> PConv <ref type="bibr" target="#b16">[17]</ref> Ours <ref type="figure">Figure 6</ref>. Results on real-world images. From left to right are: original image, input with objects masked (white area), Context Attention (CA) <ref type="bibr" target="#b35">[36]</ref>, PConv <ref type="bibr" target="#b16">[17]</ref>, and Ours.</p><p>to the size of 256×256. Data augmentation such as flipping is adopted during training. We generate 18, 000 masks with random shape, and 12, 000 masks from <ref type="bibr" target="#b16">[17]</ref> for training and testing. Our model is optimized using the ADAM algorithm <ref type="bibr" target="#b12">[13]</ref> with initial learning rate of 1e − 4 and β = 0.5. The training procedure ends after 500 epochs, and the minibatch size is 48. All the experiments are conducted on a PC equipped with 4 parallel NVIDIA GTX 1080Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with State-of-the-arts</head><p>Our LBAM is compared with four state-of-the-art methods, i.e., Global&amp;Local <ref type="bibr" target="#b9">[10]</ref>, PatchMatch <ref type="bibr" target="#b1">[2]</ref>, Context Attention <ref type="bibr" target="#b35">[36]</ref>, and PConv <ref type="bibr" target="#b16">[17]</ref>. Evaluation on Paris StreetView and Places. <ref type="figure">Fig. 4</ref> and <ref type="figure">Fig. 5</ref> show the results by our LBAM and the competing methods. Global&amp;Local <ref type="bibr" target="#b9">[10]</ref> is limited in handling irregular holes, producing many matchless and meaningless textures. PatchMatch <ref type="bibr" target="#b1">[2]</ref> performs poorly for recovering complex structures, and the results are not consistent with surrounding context. For some complex and irregular holes, context attention <ref type="bibr" target="#b35">[36]</ref> still generates blurry results and may produce unwanted artifacts. PConv <ref type="bibr" target="#b16">[17]</ref> is effective in handling irregular holes, but over-smoothing results are still inevitable in some regions. In contrast, our LBAM performs well generating visually more plausible results with finedetailed, and realistic textures. Quantitative Evaluation. We also compare our LBAM quantitatively with the competing methods on Places <ref type="bibr" target="#b39">[40]</ref> with mask ratio (0. real world object removal task. <ref type="figure">Fig. 6</ref> shows the results by our LBAM, context attention <ref type="bibr" target="#b35">[36]</ref> and PConv <ref type="bibr" target="#b16">[17]</ref>. We mask the object area either with contour shape or with rectangular bounding box. In contrast to the competing methods, our LBAM can produce realistic and coherent contents by both global semantics and local textures. User Study. Besides, user study is conducted on Paris StreetView and Places for subjective visual quality evaluation. We randomly select 30 images from the test set covering with different irregular holes, and the inpainting results are generated by PatchMatch <ref type="bibr" target="#b1">[2]</ref>, Global&amp;Local <ref type="bibr" target="#b9">[10]</ref>, Context Attention <ref type="bibr" target="#b35">[36]</ref>, PConv <ref type="bibr" target="#b16">[17]</ref> and ours. We invited 33 volunteers to vote for the most visually plausible inpainting result, which is assessed by the criteria including coherency with the surrounding context, semantic structure and fine details. For each test image, the 5 inpainting results are randomly arranged and presented to user along with the input image. Our LBAM has 63.2% chance to win out as the most favorable result, largely surpassing PConv <ref type="bibr" target="#b16">[17]</ref> (15.2%), PatchMatch <ref type="bibr" target="#b1">[2]</ref> (11.1%), Context Attention <ref type="bibr" target="#b35">[36]</ref> (a) <ref type="figure">Figure 7</ref>. Visualization of features from the first encoder layer and 13-th decoder layer. (a) Input, (b)(c) Ours(unlearned), (d)(e) Ours(forward), (f)(g) Ours(full). <ref type="figure">Figure 8</ref>. Visualization of updated masks after activation function gA(·) for forward and reverse attention maps. (a) Input, (b)(c)(d) forward masks from the first three <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3)</ref> layers, (e)(f)(g) reverse masks from the last three <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13)</ref> layers. (6.33%) and Global&amp;Local <ref type="bibr" target="#b9">[10]</ref> (4.17%).</p><formula xml:id="formula_20">(b) (c) (d) (e) (f) (g)</formula><formula xml:id="formula_21">(a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>Ablation studies are conducted to compare the performance of several LBAM variants on Paris StreetView, i.e., (i) Ours(full): the full LBAM model, (ii) Ours(unlearned): the LBAM model where all the elements in mask convolution filters are set as <ref type="bibr">1 16</ref> because the filter size is 4 × 4, and we adopt the activation functions defined in Eqn. <ref type="bibr" target="#b3">(4)</ref> and Eqn. <ref type="formula" target="#formula_3">(5)</ref>  <ref type="figure">Fig. 7</ref> shows the visualization of features from the first encoder layer and 13-th decoder layer by Ours(unlearned), Ours(forward), and Ours(full). For Ours(unlearned), blurriness and artifacts can be observed from <ref type="figure" target="#fig_3">Fig. 9(b)</ref>. Ours(forward) is beneficial to reduce the artifacts and noise, but the decoder hallucinates both holes and known regions and produces some blurry effects (see <ref type="figure" target="#fig_3">Fig. 9(c)</ref>). In contrast, Ours(full) is effective in generating semantic structure and detailed textures (see <ref type="figure" target="#fig_3">Fig. 9(d)</ref>), and the decoder focus mainly on hallucinating holes (see <ref type="figure">Fig. 7(g)</ref>). <ref type="table" target="#tab_2">Table 2</ref> gives the quantitative results of the LBAM variants on Paris StreetView, and the performance gain of Ours(full) can be explained by (1) learnable attention maps, (2) reverse attention maps, and <ref type="formula" target="#formula_2">(3)</ref> proper activation functions. Mask Updating. <ref type="figure">Fig. 8</ref> shows the visualization of updated masks from different layers. From the first to third layers, the masks of encoder are gradually updated to reduce the size of holes. Analogously, from the 13-th to 11-th layers, the masks of decoder are gradually updated to reduce the size of known region. Effect of Adversarial Loss. <ref type="table" target="#tab_2">Table 2</ref> also gives the quantitative result w/o L adv . Albeit Ours(w/o L adv ) improves PSNR and SSIM, the use of L adv generally benefits the visual quality of the inpainting results. The qualitative results are given in the suppl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposed a learnable bidirectional attention maps (LBAM) for image inpainting. With the introduction of learnable attention maps, our LBAM is effective in adapting to irregular holes and propagation of convolution layers. Furthermore, reverse attention maps are presented to allow the decoder of U-Net concentrate only on filling in holes. Experiments shows that our LBAM performs favorably against state-of-the-arts in generating sharper, more coherent and fine-detailed results.</p><p>We implement our bidirectional attention maps by employing an asymmetric Gaussian shaped form (Eqn. 9) for activation the attention map and the modified activation function (Eqn. 8) for updating the mask. In this material, we give visual comparison of several variants of our LBAM model, i.e., (i) Ours(full): the full LBAM model, (ii) Ours(unlearned): the LBAM model where all the elements in mask convolution filters are set as <ref type="bibr">1 16</ref> because the filter size is 4 × 4, and we adopt the activation functions defined in Eqn. <ref type="bibr">4 and Eqn. 5,</ref><ref type="bibr">(iii)</ref>   <ref type="figure">Fig. 10</ref> shows qualitatively comparison over variants (i) to (iv). Ours (forward) model benefits from learnable attention map and helps reduce reduce the artifacts and noise of unlearned one, see <ref type="figure">Fig. 10(a)</ref> and (b). But its decoder hallucinates both holes and known regions and produces some blurry effects compared to our full model with learnable reverse attention map <ref type="figure">Fig. 10(d)</ref>.</p><p>The qualitative comparison in ablation studies with the effect of GAN loss is shown in <ref type="figure">Fig. 10(c) and (d)</ref>. The inpainted results of our LBAM model without adversarial loss <ref type="figure">(Fig. 10(c)</ref>), are much better than the unlearned model <ref type="figure">Fig. 10(a)</ref>, and somehow clearer in producing details than ours without reverse attention map which applied GAN loss. Our LBAM full model ( <ref type="figure">Fig. 10(d)</ref>) benefits from GAN loss, is superior in giving fine-detailed structures and capturing global semantics.</p><p>The visual comparison of different activation functions or 3 × 3 filter for mask updating are shown in <ref type="figure">Fig. 11</ref>. Failure cases. <ref type="figure" target="#fig_1">Fig. 12</ref> shows some failure cases of our LBAM model. Our model struggles to recover the highfrequency details while the damaged areas are too large or the background objects are too complex. In some cases, the mask covers a large portion of a specific object, like a car, it is still difficult for our LBAM model to recover the original shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture of Our Learnable Bidirectional Attention Map</head><p>The learnable bidirectional attention model takes the damaged image, the mask M in and the reverse mask 1 − M in as input. We adopt the basic U-Net structure with 14 layers, and both encoder and decoder consists of 7 layers. The fea-tures are normalized by the learnable bidirectional attention maps through element-wise product. We use convolution filters of size 4 × 4, stride = 2, padding = 1 for all layers including the bidirectional attention maps.</p><p>The forward attention map takes the mask M in as input, it contains 7 layers, and the reverse attention map takes the reverse mask 1 − M in as input, which consists of 6 layers. We adopt an asymmetric Gaussian-shaped form as activation function (g A (·) of Eqn. 9) for activating the attention map and a modified ReLU based activating function (g M (·) of Eqn. 8) for updating mask maps. In consideration of the skip connection of the U-Net structure, the symmetric forward and reverse attention maps are concatenated for normalizing the connected features of the corresponding layer in the decoder, under Eqn. 12. Besides, batch normalization and Leaky ReLU non-linearity are used to the features after attention re-normalization. The last layer of our LBAM model are directly de-convoluted with filters of size 4 × 4, stride = 2, padding = 1, followed by a tanh non-linear activation. More details about our model is given in <ref type="table">Table 4</ref>. Note that each activation function g A (·) and mask updating term g M (·) are unique for each layer, and they do not share parameters among layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture of the Discriminator</head><p>The discriminator is trained to produce adversarial loss for minimizing the distance between the generated images and the real data distributions. In our work, we use a twocolumn discriminator with one column takes the remained area of inpainted result or a ground-truth image, and another column takes the missing holes of inpainted result or a ground-truth image as input. The two-column discriminator consists of 7 layers, the two parallel features are emerged after 6 th layer at the resolution of 4 × 4. We specifically use convolution layer with filters size of 4 × 4, stride = 2 and padding = 1, except the last layer with stride = 0. We use sigmoid non-linear activation function at last layer, while the leaky ReLU with slope of 0.2 for other layers. <ref type="table">Table 3</ref> provides a more details of the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Comparisons on Paris StreetView and Places</head><p>More comparisons with PatchMatch (PM) <ref type="bibr" target="#b1">[2]</ref>, Global&amp;Local (GL) <ref type="bibr" target="#b9">[10]</ref>, Context Attention (CA) <ref type="bibr" target="#b35">[36]</ref>, and Partial Convolution (Pconv) <ref type="bibr" target="#b16">[17]</ref> are also conducted. <ref type="figure" target="#fig_2">Fig. 13, 14 and 15</ref> show the qualitative comparison on Paris StreetView dataset and Places dataset. For Paris StreetView <ref type="bibr" target="#b5">[6]</ref> dataset, we use its original splits, 14, 900 images for training, and 100 images for testing.</p><p>For Places <ref type="bibr" target="#b39">[40]</ref> dataset, 10 categories from the total 365 categories are choosed for training our LBAM model, they are: apartment building outdoor, beach, house, ocean, sky, throne room, tower, tundra, valley and wheat field. We gather all 5000 images of each category to form our training set of 50, 000 images. The validation set from each cat- egory of 1, 000 images into two equal non-overlapped sets of 500 images respectively for validation and testing. It can be seen that our model performs better in producing both global consistency and fine-detailed structures.</p><p>Object removal on real world images.</p><p>Finally, we apply our model trained on Places dataset for object removal on real world images. As shown in <ref type="figure">Fig. 16</ref>, although these images contain different objects, background, context and shapes, even some of them have large portion masked regions, our model can handle them well, demonstrating its practicability and generalization ability of our LBAM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Ground Truth Input Ours Ground Truth <ref type="figure" target="#fig_1">Figure 12</ref>. Failure cases of our LBAM model. Each group is ordered as input image, our result and ground truth. All images are scaled to 256 × 256. <ref type="table">Table 3</ref>. The architecture of the discriminator. BN represents BatchNorm, LReLU denotes leaky ReLU with slope of 0.2, and M represents mask with zeros denote the missing pixels and ones denote the remained pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Image</head><p>Input Ours Original Image Input Ours <ref type="figure">Figure 16</ref>. Results of our LBAM on object removal task of real world images. All images are scaled to 256 × 256.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Interplay models between mask and intermediate feature for PConv and our learnable bidirectional attention maps. Here, the white holes in M in denotes missing region with value 0, and the black area denotes the known region with value 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The network architecture of our model. The circle with triangle inside denotes operation form of Eqn.<ref type="bibr" target="#b11">( 12)</ref>, gA and gM represent activation functions of Eqn.<ref type="bibr" target="#b8">( 9)</ref> and mask updating function of Eqn.<ref type="bibr" target="#b7">( 8)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 .</head><label>9</label><figDesc>(a) Input (b) Ours(unlearned) (c) Ours(forward) (d) Ours(full) Visual quality comparison of the effect on the learnable bidirectional attention maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, (iii) Ours(forward): the LBAM model without reverse attention map, (iv) Ours(w/o L adv ): the LBAM model without (w/o) adversarial loss, (v) Ours(Sigmoid/LReLU/ReLU/3 × 3): the LBAM model using Sigmoid/LeakyReLU/ReLU as activation functions or 3 × 3 filter for mask updating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Ours(forward): the LBAM model without reverse attention map, (iv) Ours(w/o L adv ): the LBAM model without (w/o) adversarial loss, (v) Ours(Sigmoid/LReLU/ReLU/3 × 3): the LBAM model using Sigmoid/LeakyReLU/ReLU as activation functions or 3 × 3 filter for mask updating.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Visual comparison of variants (i) to (iii) of our LBAM model. From left to right are: Input, (a) Ours with unlearned model, (b) Ours without reverse attention map, (c) Our without (w/o) adversarial loss, (d) our full LBAM model. All images are scaled to 256 × 256. Visual comparison of different activation functions or 3 × 3 filters on the bidirectional attention maps. From left to right are: Input, (a) Sigmoid as activation function, (b) Leaky ReLU with slope of 0.2 as activation function, (c) ReLU, (e) 3 × 3 filter for mask updating, and (e) our full LBAM model. All images are scaled to 256 × 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>From Table 1, our LBAM performs favorably in terms PSNR, SSIM, and mean 1 loss, especially when the mask ratio is higher than 0.3. Object Removal from Real-world Images. Using the model trained on Places, we further evaluate LBAM on the</figDesc><table><row><cell>1, 0.2], (0.2, 0.3], (0.3, 0.4] and (0.4, 0.5]. Table 1. Quantitative comparison on Places. Results of PConv* are taken from [17]. Mask GL [10] PM [2] CA [36] PConv* [17] Ours PSNR (0.1-0.2] 23.36 26.67 26.27 28.32 28.51 (0.2, 0.3] 20.53 24.21 23.56 25.25 25.59 (0.3, 0.4] 19.37 21.95 21.20 22.89 23.31 (0.4, 0.5] 17.86 20.02 19.95 21.38 21.66 SSIM (0.1-0.2] 0.828 0.876 0.881 0.870 0.872 (0.2, 0.3] 0.744 0.763 0.769 0.779 0.785 (0.3, 0.4] 0.643 0.657 0.667 0.689 0.708 (0.4, 0.5] 0.545 0.572 0.563 0.595 0.602 (0.1-0.2] 2.45 1.43 2.05 1.09 1.12 (0.2, 0.3] 4.01 2.38 3.74 1.88 1.93 (0.3, 0.4] 5.86 3.59 5.65 2.84 2.55 Mean l1(%) (0.4, 0.5] 7.92 5.22 7.43 3.85 3.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies (PSNR/SSIM) on Paris StreetView.</figDesc><table><row><cell>Method</cell><cell>(0.1, 0.2]</cell><cell>(0.2, 0.3]</cell><cell>(0.3, 0.4]</cell><cell>(0.4, 0.5]</cell></row><row><cell>Ours(unlearned)</cell><cell>26.95/0.853</cell><cell>24.39/0.763</cell><cell>22.54/0.677</cell><cell>21.20/0.583</cell></row><row><cell>Ours(forward)</cell><cell>27.80/0.869</cell><cell>25.13/0.775</cell><cell>23.04/0.688</cell><cell>21.76/0.598</cell></row><row><cell>Ours(Sigmoid)</cell><cell>26.93/0.857</cell><cell>24.15/0.768</cell><cell>22.24/0.683</cell><cell>20.32/0.582</cell></row><row><cell>Ours(LReLU)</cell><cell>26.61/0.852</cell><cell>23.59/0.762</cell><cell>20.63/0.667</cell><cell>18.38/0.562</cell></row><row><cell>Ours(ReLU)</cell><cell>27.62/0.864</cell><cell>25.16/0.776</cell><cell>22.96/0.685</cell><cell>21.48/0.596</cell></row><row><cell>Ours(3x3)</cell><cell>28.74/0.886</cell><cell>26.10/0.793</cell><cell>24.03/0.703</cell><cell>22.43/0.617</cell></row><row><cell>Ours(w/o L adv )</cell><cell>29.19/0.903</cell><cell>26.55/0.817</cell><cell>24.46/0.729</cell><cell>22.70/0.626</cell></row><row><cell>Ours(full)</cell><cell>28.73/0.889</cell><cell>26.16/0.795</cell><cell>24.26/0.716</cell><cell>22.62/0.621</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by the NSFC grant under No. 61671182 and 61872116, and National Key Research and Development Project 2018YFC0832105.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Visual comparison of several LBAM variants on Paris StreetView dataset</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Computer Graphics and Interactive Techniques (SIG-GRAPH)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A note on the gradient of a multi-image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><forename type="middle">Di</forename><surname>Zenzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">What makes paris look like paris? Communications of the ACM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>1-107:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask-specific inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image completion using efficient belief propagation via priority scheduling and dynamic pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tziritas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2649" to="2661" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11215</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Examplar-based inpainting based on local geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josselin</forename><surname>Olivier Le Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Gautier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3401" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Ebrahimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00212</idno>
		<title level="m">Generative image inpainting with adversarial edge learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shepard convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Sj Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contextual Based Image Inpainting: Infer, Match and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11206</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image inpainting via generative multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hole filling through photomontage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Wilczkowiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Tordoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Foreground-aware image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image inpainting by patch propagation using patch sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1153" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shift-net: Image inpainting via deep feature rearrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multiscale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4076" to="4084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<title level="m">Free-form image inpainting with gated convolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with progressive generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (ACM MM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1939" to="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5908" to="5916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pluralistic image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1438" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Input: Image (256 × 256 × 3) * M Input: Image (256 × 256 × 3) * (1 − M )</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1452" to="1464" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 64), stride = 2; LReLU; [Layer 1-2</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<publisher>LReLU</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Concatenate</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="6" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<idno>stride = 0</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>Sigmoid</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Output: Real or Fake (1 × 1 × 1)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">BN represents BatchNorm, LReLU denotes leaky ReLU with slope of 0.2, and M in represents mask with zeros indicating the missing pixels and ones indicating the remained pixels. Note that gA(·) and gM (·) are unique among layers and do not share its parameters</title>
	</analytic>
	<monogr>
		<title level="m">Our Modified U-Net Learnable Bidirectional Attention Maps Input: Image (256 × 256 × 3)</title>
		<imprint/>
	</monogr>
	<note>The architecture of our LBAM model. Input: M in (256 × 256 × 3</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 64), stride = 2; [Layer 1-2</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 64), stride = 2</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Layer 1-1, g A (Layer 1-2)); LReLU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Layer 2-1, g A (Layer 2-2))</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 256), stride = 2</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 256), stride = 2</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Layer 3-1, g A (Layer 3-2</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Layer 4-1, g A (Layer 4-2</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Layer 5-1, gA(Layer 5-2)</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Layer 6-1, gA(Layer 6-2)</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Layer 7-1, gA(Layer 7-2)</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Cat(Layer 8-1, Layer 6-1), Cat(gA(Layer 6-3), gA(Layer 6-2)))</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bn; Lrelu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Layer 9-1, Layer 5-1), Cat(gA(Layer 5-3), gA(Layer 5-2)))</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Ewp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Cat(Layer 10-1, Layer 4-1), Cat(gA(Layer 4-3), gA(Layer 4-2</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 256), stride = 2</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 256), stride = 2</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Cat(Layer 11-1, Layer 3-1), Cat(gA(Layer 3-3), gA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Cat(Layer 12-1, Layer 2-1), Cat(gA(Layer 2-3), gA(Layer 2-2)))</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conv</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>4, 4, 64), stride = 2</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Cat(Layer 13-1, Layer 1-1), Cat(gA(Layer 1-3), gA(Layer 1-2)))</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ewp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">stride = 2; tanh; Input: 1 − M in (256 × 256 × 3) Output: Final result (256 × 256 × 3) Reverse Attention Maps Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconv</surname></persName>
		</author>
		<idno>PM [2] GL [10] CA [36] PConv [17</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Global&amp;Local (GL) [10], Context Attention (CA) [36], and Partial Convolution (PConv) [17]. All images are scaled to 256 × 256</title>
	</analytic>
	<monogr>
		<title level="m">Figure 13. Qualitative comparison on Paris StreetView dataset. Comparison with PatchMatch (PM)</title>
		<imprint/>
	</monogr>
	<note>Input PM [2] GL [10] CA [36] PConv [17] Ours</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Comparison with PatchMatch (PM) [2], Global&amp;Local (GL)GL [10], Context Attention (CA) [36], and Partial Convolution (PConv</title>
		<imprint/>
	</monogr>
	<note>Qualitative comparison on Paris StreetView dataset. First three rows are from Paris StreetView dataset and the last four</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
				<title level="m">Figure 15. Qualitative comparison on Places dataset. Comparison with PatchMatch</title>
		<imprint/>
	</monogr>
	<note>PM) [2], Global&amp;Local (GL) [10], Context Attention (CA) [36], and Partial Convolution (PConv) [17]. All images are scaled to 256 × 256</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
