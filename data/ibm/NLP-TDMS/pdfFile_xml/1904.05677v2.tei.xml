<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Back-Projection Networks for Single Image Super-resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
						</author>
						<title level="a" type="main">Deep Back-Projection Networks for Single Image Super-resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image super-resolution</term>
					<term>deep cnn</term>
					<term>back-projection</term>
					<term>deep concatenation</term>
					<term>large scale</term>
					<term>recurrent</term>
					<term>residual !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous feed-forward architectures of recently proposed deep super-resolution networks learn the features of low-resolution inputs and the non-linear mapping from those to a high-resolution output. However, this approach does not fully address the mutual dependencies of low-and high-resolution images. We propose Deep Back-Projection Networks (DBPN), the winner of two image super-resolution challenges (NTIRE2018 and PIRM2018), that exploit iterative up-and down-sampling layers. These layers are formed as a unit providing an error feedback mechanism for projection errors. We construct mutually-connected up-and down-sampling units each of which represents different types of low-and high-resolution components. We also show that extending this idea to demonstrate a new insight towards more efficient network design substantially, such as parameter sharing on the projection module and transition layer on projection step. The experimental results yield superior results and in particular establishing new state-of-the-art results across multiple data sets, especially for large scaling factors such as 8×.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S IGNIFICANT progress in deep neural network (DNN) for vision <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> has recently been propagating to the field of super-resolution (SR) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>.</p><p>Single image SR (SISR) is an ill-posed inverse problem where the aim is to recover a high-resolution (HR) image from a low-resolution (LR) image. A currently typical approach is to construct an HR image by learning non-linear LR-to-HR mapping, implemented as a DNN <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and non DNN <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>. For DNN approach, the networks compute a sequence of feature maps from the LR image, culminating with one or more upsampling layers to increase resolution and finally construct the HR image. In contrast to this purely feed-forward approach, the human visual system is believed to use a feedback connection to simply guide the task for the relevant results <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>. Perhaps hampered by lack of such feedback, the current SR networks with only feed-forward connections have difficulty in representing the LR-to-HR relation, especially for large scaling factors.</p><p>On the other hand, feedback connections were used effectively by one of the early SR algorithms, the iterative backprojection <ref type="bibr" target="#b28">[28]</ref>. It iteratively computes the reconstruction error, then uses it to refine the HR image. Although it has been proven to improve the image quality, results still suffers from ringing and chessboard artifacts <ref type="bibr" target="#b29">[29]</ref>. Moreover, this method is sensitive to choices of parameters such as the number of iterations and the blur operator, leading to variability in results.</p><p>Inspired by <ref type="bibr" target="#b28">[28]</ref>, we construct an end-to-end trainable architecture based on the idea of iterative up-and down-sampling layers: Deep Back-Projection Networks (DBPN). Our networks are not • M. Haris and N. Ukita are with Intelligent Information Media Lab, Toyota Technological Institute (TTI), Nagoya, Japan, 468-8511. E-mail: muhammad.haris@bukalapak.com* and ukita@toyota-ti.ac.jp (*he is currently working at Bukalapak in Indonesia) • G. Shakhnarovich is with TTI at Chicago, US. E-mail: greg@ttic.edu Manuscript received -; revised -. only able to remove the ringing and chessboard effect but also successfully perform large scaling factors, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Furthermore, DBPN has been proven by winning SISR challenges. On NTIRE2018 <ref type="bibr" target="#b32">[32]</ref>, DBPN is the 1 st winner on track 8× Bicubic downscaling. On PIRM2018 <ref type="bibr" target="#b33">[33]</ref>, DBPN got 1 st on Region 2, 3 rd on Region 1, and 5 th on Region 3.</p><p>Our work provides the following contributions: (1) Iterative up-and down-sampling units. Most of the existing methods extract the feature maps with the same size of LR input image, then these feature maps are finally upsampled to produce SR image directly or progressively. However, DBPN performed iterative up-and down-sampling layers so that it can capture features at each resolution that are helpful in recovering features at other resolution in a single framework. Our networks focus not only on generating variants of the HR features using the upsampling unit but also on projecting it back to the LR spaces using the down-sampling unit. It is shown in <ref type="figure" target="#fig_2">Fig. 2 (d</ref>   <ref type="bibr" target="#b16">[17]</ref>, VDSR <ref type="bibr" target="#b11">[12]</ref>, DRRN <ref type="bibr" target="#b13">[14]</ref>) commonly uses the conventional interpolation, such as Bicubic, to upscale LR input images before entering the network. (b) Single upsampling (e.g., FSRCNN <ref type="bibr" target="#b17">[18]</ref>, ESPCN <ref type="bibr" target="#b18">[19]</ref>) propagates the LR features, then construct the SR image at the last step. (c) Progressive upsampling uses a Laplacian pyramid network to gradually predict SR images <ref type="bibr" target="#b12">[13]</ref>. (d) Iterative up-and down-sampling approach is proposed by our DBPN that exploit the mutually connected up-(blue box) and down-sampling (gold box) units to obtain numerous HR feature maps in different depths.</p><p>which represent the mutual relation of LR and HR features. Each set of up-and down-sampling layers achieve data augmentation in the feature space to represent a certain set of features at LR and HR resolution. So, multiple sets of up-and down-sampling layers represent multiple feature maps that are useful for producing the SR image of an input LR image. That is, even if all features extracted by these layers are produced from the single input LR image, these features can be different from each other by training DBPN so that their variety helps to improve its output (i.e., SR image). The detailed explanation can be seen in Section 3.3.</p><p>(2) Error feedback. We propose an iterative error-correcting feedback mechanism for SR, which calculates both up-and downprojection errors to guide the reconstruction for obtaining better results. Here, the projection errors are used to refine the initial features in early layers. The detailed explanation can be seen in Section 3.2.</p><p>(3) Deep concatenation. Our networks represent different types of LR and HR components produced by each up-and down-sampling unit. This ability enables the networks to reconstruct the HR image using concatenation of the HR feature maps from all of the upsampling units. Our reconstruction can directly use different types of HR feature maps from different depths without propagating them through the other layers as shown by the red arrows in <ref type="bibr">Fig. 2 (d)</ref>.</p><p>In this work, we make the following extensions to demonstrate a new insight towards more efficient network design substantially compare to our early results <ref type="bibr" target="#b31">[31]</ref>. This manuscript focuses on optimized DBPN architecture with advanced design methodologies. The detailed explanation can be seen in Section 4.2. Based on this experiment, we found several technical contributions as follows.</p><p>(1) Parameter sharing on projection module. Due to a large amount of parameters, it is hard to train deeper DBPN. Based on our experiments, the deepest setting uses T = 10. However, using parameter sharing, we can avoid the increasing number of parameters, while widening the receptive field. The effectiveness of parameter sharing was shown in <ref type="table" target="#tab_6">Table 7</ref> where DBPN-R64-10 has better performance than D-DBPN while reducing the number of parameters by 10×.</p><p>(2) Transition layer on projection step. Inspired by dense connection network <ref type="bibr" target="#b0">[1]</ref>, we use transition layer to create multiple projection step. The last up-and down-projection units were used as transition layer. The last up-projection was used to produce final HR feature-maps for each iteration, and the last down-projection was used to produce the next input for the next iteration. <ref type="table" target="#tab_6">Table 7</ref> shows that DBPN-RES-MR64-3 outperforms previous setting, D-DBPN, by a large margin. DBPN-RES-MR64-3 successfully improves D-DBPN performance by 0.7 dB on Urban100 without increasing the model parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image super-resolution using deep networks</head><p>Deep Networks SR can be primarily divided into four types as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>(a) Predefined upsampling commonly uses interpolation as the upsampling operator to produce a middle resolution (MR) image. This scheme was proposed by SRCNN <ref type="bibr" target="#b16">[17]</ref> to learn MRto-HR non-linear mapping with simple convolutional layers as in non deep learning based approaches <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[23]</ref>. Later, the improved networks exploited residual learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref> and recursive layers <ref type="bibr" target="#b19">[20]</ref>. However, this approach has higher computation because the input is the MR image which has the same size as the HR image.</p><p>(b) Single upsampling offers a simple way to increase the resolution. This approach was firstly proposed by FSRCNN <ref type="bibr" target="#b17">[18]</ref> and ESPCN <ref type="bibr" target="#b18">[19]</ref>. These methods have been proven effective to increase the resolution and replace predefined operators. Further improvements include residual network <ref type="bibr" target="#b30">[30]</ref>, dense connection <ref type="bibr" target="#b34">[34]</ref>, and channel attention <ref type="bibr" target="#b14">[15]</ref> However, they fail to learn complicated mapping of LR-to-HR image, especially on large scaling factors, due to limited feature maps from the LR image. This problem opens the opportunities to propose the mutual relation from LRto-HR image that can preserve HR components better.</p><p>(c) Progressive upsampling was recently proposed in Lap-SRN <ref type="bibr" target="#b12">[13]</ref>. It progressively reconstructs the multiple SR images with different scales in one feed-forward network. For the sake of simplification, we can say that this network is a stacked of single upsampling networks which only relies on limited LR feature maps. Due to this fact, LapSRN is outperformed even by our shallow networks especially for large scaling factors such as 8× in experimental results.</p><p>(d) Iterative up-and down-sampling is proposed by our networks <ref type="bibr" target="#b31">[31]</ref>. We focus on increasing the sampling rate of HR feature maps in different depths from iterative up-and downsampling layers, then, distribute the tasks to calculate the reconstruction error on each unit. This scheme enables the networks to preserve the HR components by learning various up-and downsampling operators while generating deeper features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feedback networks</head><p>Rather than learning a non-linear mapping of input-to-target space in one step, the feedback networks compose the prediction process into multiple steps which allow the model to have a self-correcting procedure. Feedback procedure has been implemented in various computing tasks <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref>.</p><p>In the context of human pose estimation, Carreira et al. <ref type="bibr" target="#b35">[35]</ref> proposed an iterative error feedback by iteratively estimating and applying a correction to the current estimation. PredNet <ref type="bibr" target="#b41">[41]</ref> is an unsupervised recurrent network to predictively code the future frames by recursively feeding the predictions back into the model. For image segmentation, Li et al. <ref type="bibr" target="#b38">[38]</ref> learn implicit shape priors and use them to improve the prediction. However, to our knowledge, feedback procedures have not been implemented to SR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adversarial training</head><p>Adversarial training, such as with Generative Adversarial Networks (GANs) <ref type="bibr" target="#b42">[42]</ref> has been applied to various image reconstruction problems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>. For the SR task, Johnson et al. <ref type="bibr" target="#b7">[8]</ref> introduced perceptual losses based on high-level features extracted from pre-trained networks. Ledig et al. <ref type="bibr" target="#b43">[43]</ref> proposed SRGAN which is considered as a single upsampling method. It proposed the natural image manifold that is able to create photorealistic images by specifically formulating a loss function based on the euclidian distance between feature maps extracted from VGG19 <ref type="bibr" target="#b45">[45]</ref>. Our networks can be extended with the adversarial training. The detailed explanation is available in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Back-projection</head><p>Back-projection <ref type="bibr" target="#b28">[28]</ref> is an efficient iterative procedure to minimize the reconstruction error. Previous studies have proven the effectiveness of back-projection <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b48">[48]</ref>. Originally, back-projection in SR was designed for the case with multiple LR inputs. However, given only one LR input image, the reconstruction procedure can be obtained by upsampling the LR image using multiple upsampling operators and calculate the reconstruction error iteratively <ref type="bibr" target="#b29">[29]</ref>. Timofte et al. <ref type="bibr" target="#b48">[48]</ref> mentioned that backprojection could improve the quality of the SR images. Zhao et al. <ref type="bibr" target="#b46">[46]</ref> proposed a method to refine high-frequency texture details with an iterative projection process. However, the initialization which leads to an optimal solution remains unknown. Most of the previous studies involve constant and unlearned predefined parameters such as blur operator and number of iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEP BACK-PROJECTION NETWORKS</head><p>Let I h and I l be HR and LR image with (M h × N h ) and (M l × N l ), respectively, where M l &lt; M h and N l &lt; N h . The main building block of our proposed DBPN architecture is the projection unit, which is trained (as part of the end-to-end training of the SR system) to map either an LR feature map to an HR map (up-projection), or an HR map to an LR map (down-projection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Iterative back-projection</head><p>Back-projection is originally designed with multiple LR inputs <ref type="bibr" target="#b28">[28]</ref>. Given only one LR input image, the iterative backprojection <ref type="bibr" target="#b29">[29]</ref> can be summarized as follows.</p><formula xml:id="formula_0">scale up:Î h t = (I l t * p) ↑ s ,<label>(1)</label></formula><p>scale down:</p><formula xml:id="formula_1">Î l t = (Î h t * g) ↓ s ,<label>(2)</label></formula><p>residual:</p><formula xml:id="formula_2">e l t = I l t −Î l t ,<label>(3)</label></formula><p>scale residual up:</p><formula xml:id="formula_3">e h t = (e l t * p) ↑ s ,<label>(4)</label></formula><formula xml:id="formula_4">output image:Î h t+1 =Î h t + e h t<label>(5)</label></formula><p>whereÎ h t+1 is the final SR image at the t-th iteration, p is a constant back-projection kernel and g is a single blur filter, ↑ s and ↓ s are the up-and down-sampling operator, respectively.</p><p>The traditional back-projection relies on constant and unlearned predefined parameters such as single sampling filter and blur operator. To extend this algorithm, our proposal preserves the HR components by learned various up-and down-sampling operators and generates deeper features to construct numerous pair of LR-and-HR feature maps. We develop an end-to-end trainable architecture which focuses to guide the SR task using mutually connected up-and down-sampling layers to learn nonlinear mutual relation of LR-to-HR components. The mutual relation between LR and HR components is constructed by creating iterative up-and down-projection layers where the up-projection unit generates HR feature maps, then the down-projection unit projects it back to the LR spaces as shown in <ref type="figure" target="#fig_2">Fig. 2 (d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Projection units</head><p>The up-projection unit is defined as follows:</p><p>scale up:</p><formula xml:id="formula_5">H t 0 = (L t−1 * p t ) ↑ s ,<label>(6)</label></formula><p>scale down:</p><formula xml:id="formula_6">L t 0 = (H t 0 * g t ) ↓ s ,<label>(7)</label></formula><p>residual:</p><formula xml:id="formula_7">e l t = L t 0 − L t−1 ,<label>(8)</label></formula><p>scale residual up:</p><formula xml:id="formula_8">H t 1 = (e l t * q t ) ↑ s ,<label>(9)</label></formula><p>output feature map:</p><formula xml:id="formula_9">H t = H t 0 + H t 1<label>(10)</label></formula><p>where * is the spatial convolution operator, ↑ s and ↓ s are, respectively, the up-and down-sampling operator with scaling factor s, and p t , g t , q t are (de)convolutional layers at stage t.</p><p>The up-projection unit, illustrated in the upper part of <ref type="figure" target="#fig_3">Fig. 3</ref>, takes the previously computed LR feature map L t−1 as input, and maps it to an (intermediate) HR map H t 0 ; then it attempts to map it back to LR map L t 0 ("back-project"). The residual (difference) e l t between the observed LR map L t−1 and the reconstructed L t 0 is mapped to HR again, producing a new intermediate (residual) map H t 1 ; the final output of the unit, the HR map H t , is obtained by summing the two intermediate HR maps. The down-projection unit, illustrated in the lower part of <ref type="figure" target="#fig_3">Fig. 3</ref>, is defined very similarly, but now its job is to map its input HR map H t to the LR map L t . scale down:</p><formula xml:id="formula_10">L t 0 = (H t * g t ) ↓ s ,<label>(11)</label></formula><p>scale up:</p><formula xml:id="formula_11">H t 0 = (L t 0 * p t ) ↑ s ,<label>(12)</label></formula><p>residual:</p><formula xml:id="formula_12">e h t = H t 0 − H t ,<label>(13)</label></formula><p>scale residual down:</p><formula xml:id="formula_13">L t 1 = (e h t * g t ) ↓ s ,<label>(14)</label></formula><p>output feature map:</p><formula xml:id="formula_14">L t = L t 0 + L t 1<label>(15)</label></formula><p>We organize projection units in a series of stages, alternating between H and L. These projection units can be understood as a self-correcting procedure which feeds a projection error to the sampling layer and iteratively changes the solution by feeding back the projection error.</p><p>The projection unit uses large sized filters such as 8 × 8 and 12 × 12. In the previous approaches, the use of large-sized filters is avoided because it can slow down the convergence speed and might produce sub-optimal results. However, the iterative up-and down-sampling units enable the mutual relation between LR and HR components. These units also take benefit of large receptive fields to perform better performance especially on large scaling factor where the significant amount of pixels is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network architecture</head><p>The proposed DBPN is illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>. It can be divided into three parts: initial feature extraction, projection, and reconstruction, as described below. Here, let conv(f, n) be a convolutional layer, where f is the filter size and n is the number of filters.</p><p>1) Initial feature extraction. We construct initial LR featuremaps L 0 ∈ R M l ×N l ×n0 from the input using conv(3, n 0 ). Then conv(1, n R ) is used to reduce the dimension from n 0 to n R before entering projection step where n 0 is the number of filters used in the initial LR features extraction and n R is the number of filters used in each projection unit. 2) Back-projection stages. Following initial feature extraction is a sequence of projection units, alternating between construction of LR and HR feature maps Due to the definitions of these building blocks, our network architecture is modular. We can easily define and train networks with different numbers of stages, controlling the depth. For a network with T stages, we have the initial extraction stage (2 layers), and then T up-projection units and T − 1 down-projection units, each with 3 layers, followed by the reconstruction (one more layer). However, for the dense projection unit, we add conv(1, n R ) in each projection unit, except the first three units as mentioned in Section 4.1.</p><formula xml:id="formula_15">(L t ∈ R M l ×N l ×n R and H t ∈ R M h ×N h ×n R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE VARIANTS OF DBPN</head><p>In this section, we show how DBPN can be modified to apply the latest deep learning trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dense projection units</head><p>The dense inter-layer connectivity pattern in DenseNets <ref type="bibr" target="#b0">[1]</ref> has been shown to alleviate the vanishing-gradient problem, produce improved features, and encourage feature reuse. Inspired by this we propose to improve DBPN, by introducing dense connections in the projection units called, yielding Dense DBPN.</p><p>Unlike the original DenseNets, we avoid dropout and batch norm, which are not suitable for SR, because they remove the range flexibility of the features <ref type="bibr" target="#b30">[30]</ref>. Instead, we use 1 × 1 convolution layer as the bottleneck layer for feature pooling and dimensional reduction <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b49">[49]</ref> before entering the projection unit.</p><p>In Dense DBPN, the input for each unit is the concatenation of the outputs from all previous units. Let the Lt and Ht be the input for dense up-and down-projection unit, respectively. They are generated using conv(1, n R ) which is used to merge all previous outputs from each unit as shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. This improvement enables us to generate the feature maps effectively, as shown in the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recurrent DBPN</head><p>Here, we propose recurrent DBPN which is able to reduce the number of parameters and widen the receptive field without increasing the model capacity. In SISR, DRCN <ref type="bibr" target="#b19">[20]</ref> proposed recursive layers without introducing new parameters for additional convolutions in the networks. Then, DRRN <ref type="bibr" target="#b13">[14]</ref> improves residual networks by introducing both global and local residual learning using a very deep CNN model (up to 52-layers). DBPN can also be treated as a recurrent network by sharing the projection units across the stages. We divided recurrent DBPN into two variants as mentioned below.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Residual DBPN</head><p>Residual learning helps the network to converge faster and make the network have an easier job to produce only the difference between HR and interpolated LR image. Initially, residual learning has been applied in SR by VDSR <ref type="bibr" target="#b11">[12]</ref>. Residual DBPN takes LR image as an input to reduce the computational time. First, LR image is interpolated using Bicubic interpolation; then, at the last stage, the interpolated image is added to the reconstructed image to produce final SR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation and training details</head><p>In the proposed networks, the filter size in the projection unit is various with respect to the scaling factor. For 2×, we use 6 × 6 kernel with stride = 2 and pad by 2 pixels. Then, 4× use 8 × 8 kernel with stride = 4 and pad by 2 pixels. Finally, the 8× use 12 × 12 kernel with stride = 8 and pad by 2. <ref type="bibr" target="#b0">1</ref> We initialize the weights based on <ref type="bibr" target="#b50">[50]</ref>. Here, standard deviation (std) is computed by ( 2/n l ) where n l = f 2 t n t , f t is the filter size, and n t is the number of filters. For example, with f t = 3 and n t = 8, the std is 0.111. All convolutional and deconvolutional layers are followed by parametric rectified linear units (PReLUs), except the final reconstruction layer.</p><p>We trained all networks using images from DIV2K <ref type="bibr" target="#b51">[51]</ref> with augmentation (scaling, rotating, flipping, and random cropping). To produce LR images, we downscale the HR images on particular scaling factors using Bicubic. We use batch size of 16 with size 40 × 40 for LR image, while HR image size corresponds to the scaling factors. The learning rate is initialized to 1e − 4 for all layers and decrease by a factor of 10 for every 5 × 10 5 iterations for total 10 6 iterations. We used Adam with momentum to 0.9 and trained with L1 Loss. All experiments were conducted using PyTorch 0.3.1 and Python 3.5 on NVIDIA TITAN X GPUs. The code is available in the internet.  <ref type="table">Table 1</ref>. Other methods, VDSR <ref type="bibr" target="#b11">[12]</ref>, DRCN <ref type="bibr" target="#b19">[20]</ref>, DRRN <ref type="bibr" target="#b13">[14]</ref>, LapSRN <ref type="bibr" target="#b12">[13]</ref>, was chosen due to the same nature in number of parameter. Depth analysis. To demonstrate the capability of our projection unit, we construct multiple networks: DBPN-S (T = 2), DBPN-M (T = 4), and DBPN-L (T = 6). In the feature extraction, we use n 0 = 128 and n R = 32. Then, we use conv(1, 1) for the reconstruction. The input and output image are luminance only.</p><p>The results on 4× enlargement are shown in <ref type="figure">Fig. 8</ref>. From the first 50k iteration, our variants are outperformed VDSR. Finally, starting from our shallow network, DBPN-S gives the higher PSNR than VDSR, DRCN, and LapSRN. DBPN-S uses 1. We found these settings to work well based on general intuition and preliminary experiments.</p><p>2. The implementation is available here.</p><p>TABLE 1 Model architecture of DBPN. "Feat0" and "Feat1" refer to first and second convolutional layer in the initial feature extraction stages. Note:</p><p>(f, n, st, pd) where f is filter size, n is number of filters, st is striding, and pd is padding The results of 8× enlargement are shown in <ref type="figure" target="#fig_10">Fig. 9</ref>. Our networks outperform the existing networks for 8× enlargement, from the first 50k iteration, which clearly show the effectiveness of our proposed networks on large scaling factors. However, we found that there is no significant performance gain from each proposed network especially for DBPN-L and DBPN-M networks where the difference only 0.04 dB. Number of parameters. We show the tradeoff between performance and number of network parameters from our networks and existing deep network SR in <ref type="figure" target="#fig_0">Fig. 10 and 11</ref>.</p><formula xml:id="formula_16">Scale DBPN-SS DBPN-S DBPN-M DBPN-L D-DBPN-L D-</formula><p>For the sake of low computation for real-time processing, we construct DBPN-SS which is the lighter version of DBPN-S, (T = 2). We use n 0 = 64 and n R = 18. However, the results outperform SRCNN, FSRCNN, and VDSR on both 4× and 8× enlargement. Moreover, DBPN-SS performs better than VDSR with 72% and 37% fewer parameters on 4× and 8× enlargement, respectively.</p><p>DBPN-S has about 27% fewer parameters and higher PSNR than LapSRN on 4× enlargement. Finally, D-DBPN has about 76% fewer parameters, and approximately the same PSNR, compared to EDSR on 4× enlargement. On the 8× enlargement, D-DBPN has about 47% fewer parameters with better PSNR compare to EDSR. This evidence show that our networks has the best trade-off between performance and number of parameter. Deep concatenation. Each projection unit is used to distribute the reconstruction step by constructing features which represent different details of the HR components. Deep concatenation is also well-related with the number of T (back-projection stage), which shows more detailed features generated from the projection units will also increase the quality of the results. In <ref type="figure" target="#fig_0">Fig. 12</ref>, it is shown that each stage successfully generates diverse features to reconstruct SR image. Error Feedback. As stated before, error feedback (EF) is used to guide the reconstruction in the early layer. Here, we analyze how error feedback can help for better reconstruction. We conduct experiments to see the effectiveness of error feedback procedure. On   the scenario without EF, we replace up-and down-projection unit with single up-(deconvolution) and down-sampling (convolution) layer.</p><p>We show PSNR of DBPN-S with EF and without EF in <ref type="table" target="#tab_2">Table 2</ref>. The result with EF has 0.53 dB and 0.26 dB better than without EF on Set5 and Set14, respectively. In <ref type="figure" target="#fig_0">Fig. 13</ref>, we visually show how error feedback can construct better and sharper HR image especially in the white stripe pattern of the wing.</p><p>Moreover, the performance of DBPN-S without EF is interestingly 0.57 dB and 0.35 dB better than previous approaches such as SRCNN <ref type="bibr" target="#b16">[17]</ref> and FSRCNN <ref type="bibr" target="#b17">[18]</ref>, respectively, on Set5. The results show the effectiveness of iterative up-and downsampling layers to demonstrate the LR-to-HR mutual dependency.</p><p>We further analyze the effectiveness of EF comparing the same model size as shown in <ref type="table" target="#tab_3">Table 3</ref> using D-DBPN model. The better setting is to remove the subtraction (-) and addition (+) operations in the up-/down-projection unit. The results demonstrate the effectiveness of our EF module. Filter Size We analyze the size of filters which is used in the back-projection stage on D-DBPN model. As stated before, the choice of filter size in the back-projection stage is based on the   preliminary results. For the 4× enlargement, we show that filter 8×8 is 0.08 dB and 0.09 dB better than filter 6×6 and 10×10, respectively, as shown in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Luminance vs RGB In D-DBPN, we change the input/output from luminance to RGB color channels. There is no significant improvement in the quality of the result as shown in <ref type="table">Table 5</ref>. However, for running time efficiency, constructing all channels simultaneously is faster than a separated process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison of each DBPN variant</head><p>Dense connection. We implement D-DBPN-L which is a dense connection of the L network to show how dense connection can improve the network's performance in all cases as shown in <ref type="table" target="#tab_5">Table 6</ref>. On 4× enlargement, the dense network, D-DBPN-L, gains 0.13 dB and 0.05 dB higher than DBPN-L on the Set5 and Set14, respectively. On 8×, the gaps are even larger. The D-DBPN-L has 0.23 dB and 0.19 dB higher that DBPN-L on the Set5 and Set14, respectively. Comparison across the variants. We compare six DBPN variants: DBPN-R64-10, DBPN-R128-5, DBPN-MR64-3, DBPN-RES-MR64-3, DBPN-RES, and DBPN. First, DBPN, which was the winner of NTIRE2018 <ref type="bibr" target="#b32">[32]</ref> and PIRM2018 <ref type="bibr" target="#b33">[33]</ref>, uses n 0 = 256, n R = 64, and t = 10 for the back-projection stages, and dense connection between projection units. In the reconstruction, we use conv <ref type="formula">(</ref>  <ref type="table" target="#tab_6">Table 7</ref>. It shows that all variants successfully have better performance than D-DBPN <ref type="bibr" target="#b31">[31]</ref>. DBPN-R64-10 has the least parameter compare to other variants, which is suitable for mobile/real-time application. It can reduce 10× number of parameter compare to DBPN and maintain to get good performance. We can see that increasing n R can improve the performance of DBPN-R which is shown by DBPN-R128-5 compare to DBPN-R64-10. However, better results is obtained by DBPN-MR64-3, especially on Urban100 and Manga109 test set compare to other variants. It is also proven that residual learning can slightly improve the performance of DBPN. Therefore, it is natural that we performed the combination of multiple stages recurrent and residual learning called DBPN-RES-MR64-3 which performs the best results and has lower parameter than DBPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with the-state-of-the-arts on SR</head><p>To confirm the ability of the proposed network, we performed several experiments and analysis. We compare our network with 14 state-of-the-art SR algorithms: A+ <ref type="bibr" target="#b23">[23]</ref>, SRCNN <ref type="bibr" target="#b16">[17]</ref>, FSR-CNN <ref type="bibr" target="#b17">[18]</ref>, VDSR <ref type="bibr" target="#b11">[12]</ref>, DRCN <ref type="bibr" target="#b19">[20]</ref>, DRRN <ref type="bibr" target="#b13">[14]</ref>, LapSRN <ref type="bibr" target="#b12">[13]</ref>, MS-LapSRN <ref type="bibr" target="#b52">[52]</ref>, MSRN <ref type="bibr" target="#b53">[53]</ref>, D-DBPN <ref type="bibr" target="#b31">[31]</ref>, EDSR <ref type="bibr" target="#b30">[30]</ref>, RDN <ref type="bibr" target="#b34">[34]</ref>, RCAN <ref type="bibr" target="#b14">[15]</ref>, and SAN <ref type="bibr" target="#b54">[54]</ref>. We carry out extensive experiments using 5 datasets: Set5 <ref type="bibr" target="#b55">[55]</ref>, Set14 <ref type="bibr" target="#b56">[56]</ref>, BSDS100 <ref type="bibr" target="#b57">[57]</ref>, Urban100 <ref type="bibr" target="#b58">[58]</ref> and Manga109 <ref type="bibr" target="#b59">[59]</ref>. Each dataset has different characteristics. Set5, Set14 and BSDS100 consist of natural scenes; Urban100 contains urban scenes with details in different frequency bands; and Manga109 is a dataset of Japanese manga. <ref type="bibr" target="#b2">3</ref>. It takes around five days to train on PyTorch 1.0 and CUDA10.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Runtime Evaluation</head><p>We present the runtime comparisons between our networks and three existing methods: VDSR <ref type="bibr" target="#b11">[12]</ref>, DRRN <ref type="bibr" target="#b13">[14]</ref>, and EDSR <ref type="bibr" target="#b30">[30]</ref>. The comparison must be done in fair settings. The runtime is calculated using python function timeit which encapsulating only forward function. For EDSR, we use original author code based on Torch and use timer function to obtain the runtime. We evaluate each network using NVIDIA TITAN X GPU (12G Memory). The input image size is 64× 64, then upscaled into 128× 128 (2×), 256× 256 (4×), and 512× 512 (8×). The results are the average of 10 times trials. <ref type="table" target="#tab_8">Table 9</ref> shows the runtime comparisons on 2×, 4×, and 8× enlargement. It shows that DBPN-SS and DBPN-S obtain the best and second best performance on 2×, 4×, and 8× enlargement. Compare to EDSR, D-DBPN shows its effectiveness by having faster runtime with comparable quality on 2× and 4× enlargement. On 8× enlargement, the gap is bigger. It shows that D-DBPN has better results with lower runtime than EDSR.</p><p>Noted that input for VDSR and DRRN is only luminance channel and need preprocessing to create middle-resolution image. So that, the runtime should be added by additional computation of interpolation computation on preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">PERCEPTUALLY OPTIMIZED DBPN</head><p>We also can extend DBPN to produce HR outputs that appear to be better under human perception. Despite many attempts, it remains unclear how to accurately model perceptual quality. Instead, we incorporate the perceptual quality into the generator by using adversarial loss, as introduced elsewhere <ref type="bibr" target="#b43">[43]</ref>, <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b61">[61]</ref>. In the adversarial settings, there are two building blocks: a generator (G) and a discriminator (D). In the context of SR, the generator G produces HR images (from LR inputs). The discriminator D works to differentiate between real HR images and generated HR images (the product of SR network G). In our experiments, the generator is a DBPN network, and the discriminator is a network with five hidden layers with batch norm, followed by the last, fully connected layer.</p><p>The generator loss in this experiment is composed of four loss terms, following <ref type="bibr" target="#b44">[44]</ref>: MSE, VGG, Style, and Adversarial loss.</p><formula xml:id="formula_17">L G = w 1 * L mse + w 2 * L vgg + w 3 * L adv + w 4 * L style (16)</formula><p>• MSE loss is pixel-wise loss which calculated in the image space L mse = ||I h − I sr || 2 2 . • VGG loss is calculated in the feature space using pretrained VGG19 network <ref type="bibr" target="#b45">[45]</ref> on multiple layers. This loss was originally proposed by <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b70">[70]</ref>. Both I h and I sr are first mapped into a feature space by differentiable functions f i from VGG multiple max-pool layers (i = 2, 3, 4, 5) then sum up each</p><formula xml:id="formula_18">layer distances. L vgg = 5 i=2 ||f i (I h ) − f i (I sr )|| 2 2 .</formula><p>• Adversarial loss. L adv = −log(D(G(I l ))), where D(x) is the probability assigned by D to x being a real HR image. • Style loss is used to generate high quality textures. This loss was originally proposed by <ref type="bibr" target="#b71">[71]</ref> which is later modified by <ref type="bibr" target="#b44">[44]</ref>. Style loss uses the same differentiable function f as in VGG loss.   The training objective for D is</p><formula xml:id="formula_19">L style = 5 i=2 ||θ(f i (I h )) − θ(f i (I sr ))|| 2 2 where Gram matrix θ(F ) = F F T ∈ R n×n .</formula><formula xml:id="formula_20">L D = −log(D(I h )) − log(1 − D(G(I l ))).</formula><p>As is common in training adversarial networks, we alternate between stages of training G and training D. We use pre-trained DBPN model which optimized by MSE loss only, then fine-tuned with the perceptual loss. We use batch size of 4 with size 60 × 60 for LR image, while HR image size is 240 × 240. The learning rate is initialized to 1e − 4 for all layers for 2 × 10 5 iteration using Adam with momentum to 0.9. This method was included in the challenge associated with PIRM2018 <ref type="bibr" target="#b33">[33]</ref>, in conjunction with ECCV 2018. In the challenge, evaluation was conducted in three disjoint regimes defined by thresholds on the RMSE; the intuition behind this is the natural tradeoff between RMSE and perceptual quality of the reconstruction. The latter is measured by combining the quality measures of Ma <ref type="bibr" target="#b72">[72]</ref> and NIQE <ref type="bibr" target="#b73">[73]</ref> as below, Perceptual index = 1/2((10 − M a) + N IQE). <ref type="bibr" target="#b16">(17)</ref> The three regimes correspond to Region 1: RMSE ≤ 11.5 , Region 2: 11.5 &lt; RMSE ≤ 12.5, and Region 3: 12.5 &lt; RMSE ≤ 16. We select optimal parameter settings for each regime. This process yields • Region 1 (w 1 : 0.5, w 2 : 0.05, w 3 : 0.001, w 4 : 1) • Region 2 (w 1 : 0.1, w 2 : 0.2, w 3 : 0.001, w 4 : 1) • Region 3 (w 1 : 0.03, w 2 : 0.2, w 3 : 0.001, w 4 : 10)  <ref type="bibr" target="#b33">[33]</ref>. The top 9 submissions in each region. For submissions with a marginal PI difference (up to 0.01), the one with the lower RMSE is ranked higher. Submission with marginal differences in both the PI and RMSE are ranked together (marked by * ). Our method achieved 1 st place on Region 2, 3 rd place on Region 1, and 5 th place on Region 3 <ref type="bibr" target="#b33">[33]</ref> as shown in <ref type="table" target="#tab_9">Table 10</ref>.</p><p>In Region 3, it shows very competitive results where we got 5 th , however, it is noted that our method has the lowest RMSE among other top 5 performers which means the image has less distortion or hallucination w.r.t the original image.</p><p>We show qualitative results from our method which is shown in <ref type="figure" target="#fig_0">Fig. 16</ref>. It can be seen that there are significant improvement on high quality texture on each region compare to MSE-optimized SR image. ESRGAN <ref type="bibr" target="#b63">[63]</ref>, the winner of PIRM2019 on Region 3, gets the best perceptual results among other methods. However, our proposal contains less noise among other methods (the smallest RMSE) while maintaining good perceptual quality on Region 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have proposed Deep Back-Projection Networks for Single Image Super-resolution which is the winner of two single image SR challenge (NTIRE2018 and PIRM2018). Unlike the previous methods which predict the SR image in a feed-forward manner, our proposed networks focus to directly increase the SR features using multiple up-and down-sampling stages and feed the error predictions on each depth in the networks to revise the sampling results, then, accumulates the self-correcting features from each upsampling stage to create SR image. We use error feedbacks from the up-and down-scaling steps to guide the network to achieve a better result. The results show the effectiveness of the proposed network compares to other state-of-the-art methods. Moreover, our proposed network successfully outperforms other state-of-the-art methods on large scaling factors such as 8× enlargement. We also show that DBPN can be modified into several variants to follow the latest deep learning trends to improve its performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Super-resolution result on 8× enlargement. PSNR: LapSRN [13] (15.25 dB), EDSR [30] (15.33 dB), and Ours [31] (16.63 dB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>), alternating between up-(blue box) and down-sampling (gold box) units, arXiv:1904.05677v2 [cs.CV] 13 Jun 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Comparisons of Deep Network SR. (a) Predefined upsampling (e.g., SRCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Proposed up-and down-projection units in the DBPN. These units produce residual e between the initial features and the reconstructed features, then fuse it back by summing it to the initial features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Parameter sharing on projection unit (DBPN-R). This variant uses only one up-projection unit and one down-projection unit which is shared across all stages without dense connection as shown in Fig. 6. (b) Transition layer on projection step (DBPN-MR). This variant uses multiple up-and down-projection units as shown in Fig. 7. The last up-and down-projection units were used as transition layer. Instead of taking the output from each upprojection unit, DBPN-MR takes the HR features only from the last up-projection unit, then concatenates the HR features from each iteration. Here, the output from the last down-projection unit is the input for the first up-projection layer on the next iteration. Then, the last up-projection unit will receive the output of all previous down-projection units in the corresponding iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>An implementation of DBPN for super-resolution which exploits densely connected projection unit to encourage feature reuse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Proposed up-and down-projection unit in the Dense DBPN. The feature maps of all preceding units (i.e., [L 1 , ..., L t−1 ] and [H 1 , ..., H t ] in up-and down-projections units, respectively) are concatenated and used as inputs, and its own feature maps are used as inputs into all subsequent units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Recurrent DBPN with shared parameter (DBPN-R).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Recurrent DBPN with transition layer (DBPN-MR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 5. 2</head><label>22</label><figDesc>Model analysis There are six types of DBPN used for model analysis: DBPN-SS, DBPN-S, DBPN-M, DBPN-L, D-DBPN-L, D-DBPN, and DBPN. The detailed architectures of those networks are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>The depth analysis of DBPN on Set5 dataset for 8× enlargement. DBPN-S (T = 2), DBPN-M (T = 4), and DBPN-L (T = 6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Performance vs number of parameters for 4× enlargement using Set5. The horizontal axis is log-scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Performance vs number of parameters for 8× enlargement using Set5. The horizontal axis is log-scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Sample of feature maps from up-projection units in D-DBPN where t = 7. Each feature has been enhanced using the same grayscale colormap. Zoom in for better visibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>3, 3). DBPN-R64-10 uses n R = 64 with 10 iterations to produce 640 HR features as input of reconstruction layer. DBPN-R128-5, uses n R = 128 with 5 iterations, produces 640 HR features. DBPN-MR64-3 has the same architecture with D-DBPN but the projection units are treated as recurrent network. DBPN-RES-MR64-3 is DBPN-MR64-3 with residual learning. Last, DBPN-RES is DBPN with residual learning. All variants are trained with the same training setup.The results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 .</head><label>14</label><figDesc>Qualitative comparison of our models with other works on 4× super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 15 .</head><label>15</label><figDesc>Qualitative comparison of our models with other works on 8× super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>At the best performance, DBPN-M can achieve 31.74 dB which better 0.39 dB, 0.21 dB, 0.20 dB, 0.06 dB than VDSR, DRCN, LapSRN, and DRRN respectively. In total, DBPN-M uses 24 convolutional layers which has the same depth as LapSRN. Compare to DRRN (up to 52 convolutional layers), DBPN-M undeniable shows the effectiveness of our projection unit. Finally, DBPN-L outperforms all methods with 31.86 dB which better 0.51 dB, 0.33 dB, 0.32 dB, 0.18 dB than VDSR, DRCN, LapSRN, and DRRN, respectively.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DBPN</cell><cell>DBPN</cell></row><row><cell>Input/Output</cell><cell></cell><cell>Luminance</cell><cell>Luminance</cell><cell>Luminance</cell><cell>Luminance</cell><cell>Luminance</cell><cell>RGB</cell><cell>RGB</cell></row><row><cell>Feat0</cell><cell></cell><cell>(3,64,1,1)</cell><cell>(3,128,1,1)</cell><cell>(3,128,1,1)</cell><cell>(3,128,1,1)</cell><cell>(3,128,1,1)</cell><cell>(3,256,1,1)</cell><cell>(3,256,1,1)</cell></row><row><cell>Feat1</cell><cell></cell><cell>(1,18,1,0)</cell><cell>(1,32,1,0)</cell><cell>(1,32,1,0)</cell><cell>(1,32,1,0)</cell><cell>(1,32,1,0)</cell><cell>(1,64,1,0)</cell><cell>(1,64,1,0)</cell></row><row><cell>Reconstruction</cell><cell></cell><cell>(1,1,1,0)</cell><cell>(1,1,1,0)</cell><cell>(1,1,1,0)</cell><cell>(1,1,1,0)</cell><cell>(1,1,1,0)</cell><cell>(3,3,1,1)</cell><cell>(3,3,1,1)</cell></row><row><cell></cell><cell>2×</cell><cell>(6,18,2,2)</cell><cell>(6,32,2,2)</cell><cell>(6,32,2,2)</cell><cell>(6,32,2,2)</cell><cell>(6,32,2,2)</cell><cell>(6,64,2,2)</cell><cell>(6,64,2,2)</cell></row><row><cell>BP stages</cell><cell>4×</cell><cell>(8,18,4,2)</cell><cell>(8,32,4,2)</cell><cell>(8,32,4,2)</cell><cell>(8,32,4,2)</cell><cell>(8,32,4,2)</cell><cell>(8,64,4,2)</cell><cell>(8,64,4,2)</cell></row><row><cell></cell><cell>8×</cell><cell>(12,18,8,2)</cell><cell>(12,32,8,2)</cell><cell>(12,32,8,2)</cell><cell>(12,32,8,2)</cell><cell>(12,32,8,2)</cell><cell>(12,64,8,2)</cell><cell>(12,64,8,2)</cell></row><row><cell></cell><cell>2×</cell><cell>106</cell><cell>337</cell><cell>779</cell><cell>1221</cell><cell>1230</cell><cell>5819</cell><cell>8811</cell></row><row><cell>Parameters (k)</cell><cell>4×</cell><cell>188</cell><cell>595</cell><cell>1381</cell><cell>2168</cell><cell>2176</cell><cell>10426</cell><cell>15348</cell></row><row><cell></cell><cell>8×</cell><cell>421</cell><cell>1332</cell><cell>3101</cell><cell>4871</cell><cell>4879</cell><cell>23205</cell><cell>34026</cell></row><row><cell>Depth</cell><cell></cell><cell>12</cell><cell>12</cell><cell>24</cell><cell>36</cell><cell>40</cell><cell>52</cell><cell>76</cell></row><row><cell>No. of stage (T )</cell><cell></cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>6</cell><cell>7</cell><cell>10</cell></row><row><cell>Dense connection</cell><cell></cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell></row></table><note>Fig. 8. The depth analysis of DBPNs compare to other networks (VDSR [12], DRCN [20], DRRN [14], LapSRN [13]) on Set5 dataset for 4× enlargement.only 12 convolutional layers with smaller number of filters than VDSR, DRCN, and LapSRN. At the best performance, DBPN-S can achieve 31.59 dB which better 0.24 dB, 0.06 dB, 0.05 dB than VDSR, DRCN, and LapSRN, respectively. DBPN-M shows performance improvement which better than all four existing methods (VDSR, DRCN, LapSRN, and DRRN).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Analysis of EF using DBPN-S on 4× enlargement. Red indicates the best performance.</figDesc><table><row><cell></cell><cell>Set5</cell><cell>Set14</cell></row><row><cell>SRCNN [17]</cell><cell>30.49</cell><cell>27.61</cell></row><row><cell>FSRCNN [18]</cell><cell>30.71</cell><cell>27.70</cell></row><row><cell>Without EF</cell><cell>31.06</cell><cell>27.95</cell></row><row><cell>With EF</cell><cell>31.59</cell><cell>28.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Analysis of EF module on same model size (D-DBPN) on 4× enlargement. Red indicates the best performance.Fig. 13. Qualitative comparisons of DBPN-S with EF and without EF on</figDesc><table><row><cell></cell><cell>D-DBPN-w/ EF</cell><cell>D-DBPN-w/o EF</cell></row><row><cell>Set5</cell><cell>32.40/0.897</cell><cell>32.17/0.894</cell></row><row><cell>Set14</cell><cell>28.75/0.785</cell><cell>28.56/0/782</cell></row><row><cell>BSDS100</cell><cell>27.67/0.738</cell><cell>27.56/0.736</cell></row><row><cell>Urban100</cell><cell>26.38/0.793</cell><cell>26.09/0.786</cell></row><row><cell>Manga109</cell><cell>30.89/0.913</cell><cell>30.43/0.908</cell></row></table><note>4× enlargement.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Analysis of filter size in the back-projection stages on 4× enlargement from D-DBPN. Red indicates the best performance.</figDesc><table><row><cell>Filter size</cell><cell>Striding</cell><cell>Padding</cell><cell>Set5</cell><cell>Set14</cell></row><row><cell>6</cell><cell>4</cell><cell>1</cell><cell>32.39</cell><cell>28.78</cell></row><row><cell>8</cell><cell>4</cell><cell>2</cell><cell>32.47</cell><cell>28.82</cell></row><row><cell>10</cell><cell>4</cell><cell>3</cell><cell>32.38</cell><cell>28.79</cell></row><row><cell></cell><cell cols="2">TABLE 5</cell><cell></cell><cell></cell></row><row><cell cols="5">Analysis of input/output color channel using DBPN-L. Red indicates the</cell></row><row><cell></cell><cell cols="3">best performance.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Set5</cell><cell>Set14</cell><cell></cell></row><row><cell></cell><cell>RGB</cell><cell>31.88</cell><cell>28.47</cell><cell></cell></row><row><cell></cell><cell>Luminance</cell><cell>31.86</cell><cell>28.47</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Comparison of the DBPN-L and D-DBPN-L on 4× and 8× enlargement. Red indicates the best performance.</figDesc><table><row><cell></cell><cell></cell><cell>Set5</cell><cell>Set14</cell></row><row><cell>Algorithm</cell><cell cols="3">Scale PSNR SSIM PSNR SSIM</cell></row><row><cell>DBPN-L</cell><cell>4</cell><cell cols="2">31.86 0.891 28.47 0.777</cell></row><row><cell>D-DBPN-L</cell><cell>4</cell><cell cols="2">31.99 0.893 28.52 0.778</cell></row><row><cell>DBPN-L</cell><cell>8</cell><cell cols="2">26.63 0.761 24.73 0.631</cell></row><row><cell>D-DBPN-L</cell><cell>8</cell><cell cols="2">26.86 0.773 24.92 0.638</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Quantitative evaluation of DBPN's variants on 4×. Red indicates the best performance.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Set5</cell><cell cols="2">Set14</cell><cell cols="2">BSDS100</cell><cell cols="2">Urban100</cell><cell cols="2">Manga109</cell></row><row><cell>Method</cell><cell># Parameters (k)</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>D-DBPN [31]</cell><cell>10426</cell><cell>32.40</cell><cell>0.897</cell><cell>28.75</cell><cell>0.785</cell><cell>27.67</cell><cell>0.738</cell><cell>26.38</cell><cell>0.793</cell><cell>30.89</cell><cell>0.913</cell></row><row><cell>DBPN</cell><cell>15348</cell><cell>32.55</cell><cell>0.898</cell><cell>28.91</cell><cell>0.789</cell><cell>27.77</cell><cell>0.742</cell><cell>26.82</cell><cell>0.807</cell><cell>31.46</cell><cell>0.918</cell></row><row><cell>DBPN-R64-10</cell><cell>1614</cell><cell>32.38</cell><cell>0.896</cell><cell>28.83</cell><cell>0.787</cell><cell>27.73</cell><cell>0.740</cell><cell>26.51</cell><cell>0.798</cell><cell>31.12</cell><cell>0.915</cell></row><row><cell>DBPN-R128-5</cell><cell>6349</cell><cell>32.41</cell><cell>0.897</cell><cell>28.83</cell><cell>0.787</cell><cell>27.72</cell><cell>0.740</cell><cell>26.58</cell><cell>0.799</cell><cell>31.15</cell><cell>0.915</cell></row><row><cell>DBPN-MR64-3</cell><cell>10419</cell><cell>32.57</cell><cell>0.898</cell><cell>28.92</cell><cell>0.790</cell><cell>27.79</cell><cell>0.743</cell><cell>26.92</cell><cell>0.810</cell><cell>31.51</cell><cell>0.919</cell></row><row><cell>DBPN-RES</cell><cell>15348</cell><cell>32.54</cell><cell>0.897</cell><cell>28.92</cell><cell>0.789</cell><cell>27.79</cell><cell>0.742</cell><cell>26.89</cell><cell>0.808</cell><cell>31.49</cell><cell>0.918</cell></row><row><cell>DBPN-RES-MR64-3</cell><cell>10419</cell><cell>32.65</cell><cell>0.899</cell><cell>29.03</cell><cell>0.791</cell><cell>27.82</cell><cell>0.744</cell><cell>27.08</cell><cell>0.814</cell><cell>31.74</cell><cell>0.921</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc>Quantitative evaluation of state-of-the-art SR algorithms: average PSNR/SSIM for scale factors 2×, 4×, and 8×. Red indicates the best and blue indicates the second best performance.</figDesc><table><row><cell>Set5</cell><cell>Set14</cell><cell>BSDS100</cell><cell>Urban100</cell><cell>Manga109</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9</head><label>9</label><figDesc>Runtime evaluation with input size 64×64. Red indicates the best and blue indicates the second best performance, * indicates the calculation using function timer in Torch, and N.A. indicates that the algorithm runs out of GPU memory.</figDesc><table><row><cell></cell><cell>2×</cell><cell>4×</cell><cell>8×</cell></row><row><cell></cell><cell>(128×128)</cell><cell>(256×256)</cell><cell>(512×512)</cell></row><row><cell>VDSR [12]</cell><cell>0.022</cell><cell>0.032</cell><cell>0.068</cell></row><row><cell>DRRN [14]</cell><cell>0.254</cell><cell>0.328</cell><cell>N.A.</cell></row><row><cell>*EDSR [30]</cell><cell>0.857</cell><cell>1.245</cell><cell>1.147</cell></row><row><cell>DBPN-SS</cell><cell>0.012</cell><cell>0.016</cell><cell>0.026</cell></row><row><cell>DBPN-S</cell><cell>0.013</cell><cell>0.020</cell><cell>0.038</cell></row><row><cell>DBPN-M</cell><cell>0.023</cell><cell>0.045</cell><cell>0.081</cell></row><row><cell>DBPN-L</cell><cell>0.035</cell><cell>0.069</cell><cell>0.126</cell></row><row><cell>D-DBPN</cell><cell>0.153</cell><cell>0.193</cell><cell>0.318</cell></row><row><cell>DBPN-RES-MR64-3</cell><cell>0.171</cell><cell>0.227</cell><cell>0.339</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10 PIRM2018 Challenge results</head><label>10</label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partly supported by JSPS KAKENHI Grant Number 19K12129 and by AFOSR Center of Excellence in Efficient and Robust Machine Learning, Award FA9550-18-1-0166.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Fractalnet: Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6626" to="6634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inception learning superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Widyanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nobuhara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="6043" to="6048" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conferene on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image superresolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Srfeat: Single image super-resolution with feature discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="439" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Results of DBPN with perceptual loss compare with other methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">First-order derivativebased super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Widyanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nobuhara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Image and Video Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Superresolution for uav images via adaptive multiple sparse representation and its application to 3-d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Widyanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nobuhara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4047" to="4058" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed hierarchical processing in the primate cerebral cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Felleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The ventral visual pathway: an expanded neural framework for the processing of object quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Ungerleider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mishkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="49" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The distinct modes of vision offered by feedforward and recurrent processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Lamme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in neurosciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="571" to="579" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Motion analysis for image enhancement: Resolution, occlusion, and transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="324" to="335" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bilateral back-projection for single image super resolution,&quot; in Multimedia and Expo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1039" to="1042" />
			<date type="published" when="2007" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ntire 2018 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">2018 pirm challenge on perceptual image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07517</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning messagepassing inference machines for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2737" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auto-context and its application to high-level vision tasks and 3d brain image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Iterative instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3659" to="3667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Feedback networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.09508</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contextual priming and feedback for faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="330" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07919</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Iterative projection reconstruction for fast and efficient image upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="page" from="200" to="211" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Nonlocal back-projection for adaptive image enlargement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="349" to="352" />
		</imprint>
	</monogr>
	<note>Image Processing (ICIP)</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Seven ways to improve example-based single image super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1865" to="1873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast and accurate image super-resolution with deep laplacian pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-scale residual network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="517" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="65" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><forename type="middle">A</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Analyzing perceptiondistortion tradeoff using enhanced perceptual super-resolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Thekke</forename><surname>Madam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Generative adversarial network-based image super-resolution using perceptual content losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-scale recursive and perception-distortion controllable image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Navarrete Michelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Deep learning-based image super-resolution considering quantitative and perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04789</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Perception-enhanced image superresolution via relativistic generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bi-gans-st for perceptual image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Scale-recurrent multiresidual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning a no-reference quality metric for single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Making a&quot; completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
