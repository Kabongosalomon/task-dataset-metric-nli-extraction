<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
							<email>zhoudaquan21@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Wang</surname></persName>
							<email>anran.wang@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>2 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Token Labeling: Training an 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper provides a strong baseline for vision transformers on the ImageNet classification task. While recent vision transformers have demonstrated promising results in ImageNet classification, their performance still lags behind powerful convolutional neural networks (CNNs) with approximately the same model size. In this work, instead of describing a novel transformer architecture, we explore the potential of vision transformers in ImageNet classification by developing a bag of training techniques. We show that by slightly tuning the structure of vision transformers and introducing token labeling-a new training objective, our models are able to achieve better results than the CNN counterparts and other transformer-based classification models with similar amount of training parameters and computations. Taking a vision transformer with 26M learnable parameters as an example, we can achieve an 84.4% Top-1 accuracy on ImageNet. When the model size is scaled up to 56M/150M, the result can be further increased to 85.4%/86.2% without extra data. We hope this study could provide researchers with useful techniques to train powerful vision transformers. Our code and all the training details will be made publicly available at https: //github.com/zihangJiang/TokenLabeling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers, originally designed for the machine translation task <ref type="bibr" target="#b34">[35]</ref>, have achieved great performance for almost all natural language processing (NLP) tasks over the past years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22</ref>]. Motivated by the success of transforms on the NLP tasks, very recently, many researchers attempt to build pure or hybrid transformer models for vision tasks and show the potential of transformer based models for image classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5]</ref>. However, as interpreted in <ref type="bibr" target="#b13">[14]</ref>, purely transformer-based models need to * Work done as an intern at ByteDance AI Lab.  <ref type="bibr" target="#b18">[19]</ref> 0.2 (Linear) 0.2 (Fixed) Rand Augmentation <ref type="bibr" target="#b8">[9]</ref> CutMix Augmentation <ref type="bibr" target="#b43">[44]</ref> MixUp Augmentation <ref type="bibr" target="#b46">[47]</ref> LayerScaling <ref type="bibr" target="#b32">[33]</ref> Class Attention <ref type="bibr">[</ref> pretrain on very large datasets (e.g., ImageNet-22k) with tens of millions of training images to achieve a decent performance. Later, DeiT <ref type="bibr" target="#b31">[32]</ref> and T2T-ViT <ref type="bibr" target="#b41">[42]</ref> demonstrate that by exploiting proper data augmentation policies, it is possible to train a vision transformer achieving nearly 80% Top-1 accuracy on ImageNet with 22M learnable parameters using only 1.2M ImageNet training data <ref type="bibr" target="#b11">[12]</ref>. This success delivers a promising way to help vision transformers to surpass the canonical convolutional neural networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref>, which have dominated ImageNet classification for years.</p><p>Inspired by DeiT <ref type="bibr" target="#b31">[32]</ref> and T2T-ViT <ref type="bibr" target="#b41">[42]</ref>, in this paper, we are also interested in investigating the potential of vision transformers in ImageNet classification relying on pure <ref type="bibr">Figure 1</ref>. Comparison between the proposed LV-ViT and other recent works based on transformers. Note that we only show models whose model sizes are under 100M. As can be seen, our LV-ViT achieves the best results using the least amount of learnable parameters. The default test resolution is 224 × 224 unless clearly shown after @.</p><p>ImageNet-1k data. Our goal is to provide the vision community a strong baseline for vision transformers without changing the transformer structures. To achieve this, we first rethink the way of performing patch embedding and propose to explicitly introduce inductive bias as done in <ref type="bibr" target="#b41">[42]</ref>. Furthermore, besides connecting a linear layer to the class token for score prediction, we present a token labeling objective loss by taking a K-dimensional score map as supervision to densely supervise all the tokens except the class one, where K is the number of categories for the target dataset. The K-dimensional score map can be easily generated by exploiting the relabeling strategy as described in <ref type="bibr" target="#b44">[45]</ref>. In addition, we also provide practical advice on adjusting the vision transformer structures.</p><p>Based on the aforementioned training strategies and objectives, we present an improved version of vision transformer, termed LV-ViT. As shown in <ref type="figure">Figure 1</ref>, our LV-ViT with 56M parameters performs better than most of the transformer-based models having no more than 100M parameters. We can also see from <ref type="table" target="#tab_0">Table 1</ref> that using less training techniques, model size, and computations, our LV-ViT achieves better result than the recent state-of-the-art CaiT model <ref type="bibr" target="#b32">[33]</ref>. We hope this paper could be regarded as an instructional work to provide researchers or engineers a strong baseline for training vision transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transformers <ref type="bibr" target="#b34">[35]</ref> refer to the models that entirely rely on the self-attention mechanism to building global dependencies, which are originally designed for natural language processing tasks. Due to the strong capability of capturing spatial information, transformers have been successfully applied into a variety of vision problems, including low-level vision tasks, such as image enhancement <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41]</ref>, as well as more challenging tasks, such as image classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>, object detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b48">49]</ref>, segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref> and image generation <ref type="bibr" target="#b24">[25]</ref>. Some work also extend transformers for video and 3D point cloud processing <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b51">52]</ref>. As the goal of this paper is to introduce a bag of training techniques for ViTs, in what follows, we briefly give a review of transformer models that are closely related to this work.</p><p>Among the recent vision transformers, Vision Transformer (ViT) is one of the earlier attempts that achieve stateof-the-art performance on ImageNet classification using pure transformers as basic building blocks. However, ViTs need pre-training on very large datasets, such as ImageNet-22k and JFT-300M, and huge computation resources to achieve comparable performance to ResNet <ref type="bibr" target="#b15">[16]</ref> with similar model size trained on ImageNet. Later, DeiT <ref type="bibr" target="#b31">[32]</ref> manages to tackle the data-inefficiency problem by simply adjusting the network architecture and adding an additional token along with the class token for Knowledge Distillation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref> to improve model performance.</p><p>Some recent work <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b14">15]</ref> also attempt to introduce the local dependency into vision transformers by modifying the patch embedding block or the transformer block or both, leading to significant performance gains. For example, Tokens-to-Token ViT <ref type="bibr" target="#b41">[42]</ref> presents a T2T module to progressively aggregate the information of the input image to tokens. CvT <ref type="bibr" target="#b38">[39]</ref> proposes a convolutional token embedding layer for patch embedding as well as a convolutional projection for self-attention layer to extract local information. Transformer iN Transformer <ref type="bibr" target="#b14">[15]</ref> designs a TNT block to replace the original transformer block for better modeling both patch-level and pixel-level representations. Con-ViT <ref type="bibr" target="#b10">[11]</ref> uses a gated positional self-attention as a substitution for the self-attention to deal with low-level local information. Moreover, there are also some work <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> adopting the pyramid structure to reduce the overall compu-  tations to maintain the ability to capture low-level features.</p><p>Unlike most of the aforementioned work that target at designing new transformer blocks or transformer architectures, we aim to describe a bag of interesting training techniques to improve the performance of vision transformers without changing the architecture. We demonstrate that by adding the proposed techniques to the training recipe of vision transformers, we can achieve strong baselines for transformer models at multiple different model size levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Bag of Training Techniques for ViT</head><p>In this section, we first briefly review the structure of the vision transformer and then describe how to improve vision transformers by introducing a bag of training techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vision Transformer</head><p>A typical vision transformer <ref type="bibr" target="#b13">[14]</ref> includes five primary components: patch embedding, position encoding, multihead self-attention, feed-forward layers, and a score prediction layer. Both multi-head self-attention and feed-forward comprise a typical transformer block. In what follows, we briefly describe the functionality of these components.</p><p>Patch Embedding: In a vision transformer, the fixed-size input image is first decomposed into a sequence of small patches, each of which is with a predefined size, say 16×16. For example, when the input resolution is set to 224 × 224, there would be 14 × 14 = 196 small patches. Each patch is then projected with a linear layer that maps its overall dimension 3 × 16 × 16 = 768 to a feature vector, or called a token. All these tokens concatenated with a class token for classification score prediction will be sent into a transformer backbone for feature encoding.</p><p>Positional Encoding: Due to the permutation invariant propriety of transformers, an additional learnable positional encoding is often added to the input token embeddings to couple each patch with positional information. This is proven to be necessary and aid the model to better learn the visual structures. Empirically, either fixed sinusoidal positional encoding <ref type="bibr" target="#b34">[35]</ref> or learnable encoding can be used as they result in similar classification performance.</p><p>Multi-Head Self-Attention: Multi-head self-attention, is introduced in <ref type="bibr" target="#b34">[35]</ref>, aiming at building long-range dependencies globally. Given an input tensor X ∈ R n,d , the multihead self-attention applies linear transformations on X and embeds them to the key K, query Q and value V , respectively. Suppose there are H self-attention heads. The key, query and value embeddings are uniformly split into H seg-</p><formula xml:id="formula_0">ments Q i , K i , V i ∈ R n,d h , {i = 1, 2, ..., H} along channel dimension, where d h is the dimension of each head which satisfies d h × H = d.</formula><p>The attention module gives outputs in the form:</p><formula xml:id="formula_1">Attention(X, i) = Softmax Q i K i √ d h V i .<label>(1)</label></formula><p>The outputs of all heads are then again concatenated along the channel dimension and a linear projection is finally ap-plied to produce the final output as follows:</p><formula xml:id="formula_2">SA(X) = Proj(Concat H i=1 (Attention(X, i))).<label>(2)</label></formula><p>Feed-Forward: As described in <ref type="bibr" target="#b34">[35]</ref>, the feed-forward layer consists of two linear layers with a non-linear activation function, which can be denoted as:</p><formula xml:id="formula_3">FF(X) = W 2 (Activation(W 1 X + b 1 )) + b 2 ,<label>(3)</label></formula><p>where W 1 and W 2 are learnable weights and b 1 and b 2 are learnable biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Techniques</head><p>In this subsection, we introduce a bag of simple yet effective techniques to improve vision transformers, which will be specifically described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network depth:</head><p>It is known that increasing the depth of networks can help improve the generalization ability as well as the capacity of models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref>. We investigate how network depth influences the performance of vision transformers by gradually increasing the depth of the baseline vision transformers. As adding more transformer blocks would inevitably introduce more model parameters, we also decrease the hidden dimension size of the feed-forward layer, which we found has nearly no negative effect on the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explicit inductive bias:</head><p>Vision transformers aim at global information aggregation via self-attention and do not introduce inductive bias explicitly as in CNNs. Nevertheless, as demonstrated in DeepVit <ref type="bibr" target="#b50">[51]</ref>, even without explicit inductive bias, the lower transformer blocks can still learn it implicitly. However, a multi-head self-attention layer is much more expensive than a simple convolutional layer in computation. Moreover, it is pointed out in <ref type="bibr" target="#b41">[42]</ref> that the tokenization operation in patch embedding fails to capture the lowlevel and local structures, such as edges and lines, leading to low sample efficiency while training. Taking these into account, we propose to directly add more convolutional layers to enhance the capability of the patch embedding module to explicitly introduce inductive bias. In addition, to gather information from a larger receptive field, we also use convolutions with a smaller stride to provide an overlapped information for each nearby tokens. More settings can be found in our experiment section.</p><p>Rethinking residual connection: Denoting layer normalization as LN, we can rewrite the forward pass of a transformer block as follows:</p><formula xml:id="formula_4">X ←− X + SA(LN(X)) X ←− X + FF(LN(X))<label>(4)</label></formula><p>We can interpret going through multiple transformer blocks as gradually adding information gathered from other tokens and modifying the input. Original transformers use a ratio of 1 to add the modification to the input while we propose to use a smaller ratio α to re-scale the residual feature values:</p><formula xml:id="formula_5">X ←− X + αSA(LN(X)) X ←− X + αFF(LN(X))<label>(5)</label></formula><p>This can help enhance the residual connection since less information will go to the residual branch. It is also found in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref> that adjusting the scaling factor of residual connection can improve the generalization ability of the model and we extend it to the case of vision transformer.</p><p>Re-labeling: At the training phase, random crop is often used as a data augmentation method. However, as pointed out in <ref type="bibr" target="#b44">[45]</ref>, although ImageNet <ref type="bibr" target="#b11">[12]</ref> is a single-label benchmark, the label is not always accurate after cropping as many images in ImageNet include multiple objects and the objects with the ground-truth label may not remain in the cropped image. This problem can be worse when training images with smaller size in that the cropped image may be a much smaller region of the original image. To handle this situation, we apply the re-labeling strategy <ref type="bibr" target="#b44">[45]</ref> on the training set and re-assign each image a K-dimensional score map where K = 1000 for our target ImageNet-1k dataset to get more accurate label while training. Unlike knowledge distillation which needs a teacher model to generate supervision labels online, the re-labeling technique is a cheap operation that can be viewed as a preprocessing step. During training, we only need to calculate the label for the random cropped image from the K-dimensional dense score map. Thus, we can easily improve the quality of the label with negligible additional computations. More details can be found in <ref type="bibr" target="#b44">[45]</ref>.</p><p>Token Labeling: It is quite common to add a special [cls] token used for classification to transformer based models since it can better gather the global information from all tokens. However, previous works, such as <ref type="bibr" target="#b7">[8]</ref>, suggest that defining training objects among all tokens can help to improve the sample efficiency while training. Meanwhile, based on the dense score map provided by the re-labeling technique, we can further use it to assign each patch as well as its corresponding token an individual label. We call this operation token labeling. Specifically, we propose to add a token labeling loss function, which leverages the dense score map for each training image and uses the cross entropy loss between each token and the corresponding label in the dense score map as an auxiliary loss at the training phase. Denote the output of the vision transformer as [X cls , X 1 , ..., X N ], the K-dimensional score map as [y 1 , ..., y N ], and the label for the whole image as y cls . Then, the auxiliary token labeling loss is defined as: where H is the cross entropy loss. Therefore, the total loss function can be written as:</p><formula xml:id="formula_6">L aux = 1 N N i=1 H(X i , y i )<label>(6)</label></formula><formula xml:id="formula_7">L total = H(X cls , y cls ) + βL aux ,<label>(7)</label></formula><p>where β is a hyper-parameter to balance the two terms. In our experiment, we empirically set it to 0.5.</p><p>MixToken: While training vision transformer, previous studies <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b41">42]</ref> have shown that augmentation methods, like MixUp <ref type="bibr" target="#b46">[47]</ref> and CutMix <ref type="bibr" target="#b43">[44]</ref>, can boost the performance and improve the robustness of the models. However, vision transformers rely on patch based tokenization to map each input image to a sequence of tokens and our token labeling strategy also operates on patch based token labels. If we apply CutMix directly on the raw image, some of the resulting patches may contain content from two images, leading to mixed regions within a small patch as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. When performing token labeling, it is difficult to assign each output token a clean and correct label. Taking this situation into account, we rethink the CutMix augmentation method and present MixToken, which can be viewed as an modified version of CutMix operating on the tokens after patch embedding as illustrated in the right part of <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>To be specific, for two images denoted as I 1 , I 2 and their corresponding token labels Y 1 = [y 1 1 , ..., y N 1 ] as well as Y 2 = [y 1 2 , ..., y N 2 ], we first feed the two images into the patch embedding module to tokenize each image as a sequence of tokens, resulting in T 1 = [t 1 1 , ..., t N 1 ] and T 2 = [t 1 2 , ..., t N 2 ]. Then, we produce a new sequence of tokens by applying MixToken using a binary mask M as </p><formula xml:id="formula_8">follows:T = T 1 M + T 2 (1 − M ),<label>(8)</label></formula><p>where is element-wise multiplication. We use the same way to generate the mask M as in <ref type="bibr" target="#b43">[44]</ref>. For the corresponding label, we also mix them using the same mask M :</p><formula xml:id="formula_9">Y = Y 1 M + Y 2 (1 − M ).<label>(9)</label></formula><p>The label for the [cls] token can be written as:</p><formula xml:id="formula_10">y cls =M y cls 1 + (1 −M )y cls 2 ,<label>(10)</label></formula><p>whereM is the average of all element values of M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first describe the experiment setup and the default setting of hyper-parameters. Then, we give an in-depth study of the training techniques proposed in this paper and conduct ablation analysis to evaluate the effectiveness of each training techniques. Finally, we compare our proposed LV-ViT with previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>We evaluate our training techniques to improve vision transformers for image classification on ImageNet <ref type="bibr" target="#b11">[12]</ref>. All experiments are built and conducted upon Py-Torch <ref type="bibr" target="#b25">[26]</ref> and the timm <ref type="bibr" target="#b37">[38]</ref> library. We follow the standard training schedule and train our models on the Ima-geNet dataset for 300 epochs. Besides normal augmentations like CutOut <ref type="bibr" target="#b49">[50]</ref> and RandAug <ref type="bibr" target="#b8">[9]</ref>, we also explore the effect of applying MixUp <ref type="bibr" target="#b46">[47]</ref> and CutMix <ref type="bibr" target="#b43">[44]</ref> together with our training techniques. Empirically, we found that using MixUp together with relabeling or token labeling brings no benefit to the performance and would even degrade the models' generalization ability. Thus, we do not apply it for these two settings in our experiments. For optimization, by default, we use the AdamW optimizer <ref type="bibr" target="#b23">[24]</ref> with a linear learning rate scaling strategy lr = 10 −3 × batch size 1024 and 5 × 10 −2 weight decay rate. While training with token labeling, we found that using a slightly larger learning rate strategy of lr = 10 −3 × batch size 640 gives better performance. For Dropout regularization, we observe that for small models, using dropout hurts the performance. This was also observed in a few other works related to training vision transformers <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>. As a result, we do not apply Dropout <ref type="bibr" target="#b28">[29]</ref> and use Stochastic Depth <ref type="bibr" target="#b18">[19]</ref> instead. More details on hyper-parameters are presented in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Technique Analysis</head><p>We present a summary of our proposed techniques to improve vision transformer models in <ref type="table" target="#tab_3">Table 3</ref>. We take the DeiT-Small [32] model as our baseline and show the performance increment as more training techniqeus are added. In this subsection, we will ablate each proposed training technique and evaluate the effectiveness of them.</p><p>Explicit inductive bias for patch embedding: Ablation analysis of patch embedding is presented in <ref type="table">Table 4</ref>. The baseline is set to the same as the setting as presented in the third row of <ref type="table" target="#tab_3">Table 3</ref>. Clearly, by adding more convolutional layers and narrow the kernel size in the patch embedding, we can see a consistent increase in the performance comparing to the original single-layer patch embedding. However, when further increasing the number of convolutional layer in patch embedding to 6, we do not observe any performance gain. This indicates that using 4-layer convolutions in patch embedding is enough. Meanwhile, if we use a larger stride to reduce the size of the feature map, we can largely reduce the computation cost, but the performance also drops. Thus, we only apply a convolution of stride 2 and kernel size 7 at the beginning of the patch embedding module, followed by two convolutional layers with stride 1 and kernel size 3. The feature map is finally tokenized to <ref type="table">Table 4</ref>. Ablation on patch embedding. Baseline is set as 16 layer ViT with embedding size 384 and MLP expansion ratio of 3. All convolution layers except the last block have 64 filters. #Convs indicatie the total number of convolution for patch embedding, while the kernel size and stride correspond to each layer are shown as a list in the  <ref type="table">Table 5</ref>. Ablation on enhancing residual connection by applying a scaling factor. Baseline is a 16-layer vision transformer with 4layer convolutional patch embedding. Here, function F represents either self-attention (SA) or feed forward (FF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward Function #Parameters</head><p>Top-1 Acc. (%)</p><formula xml:id="formula_11">X ←− X + F (X) 26M 82.2 X ←− X + F (X)/2 26M 82.4 X ←− X + F (X)/3 26M 82.4</formula><p>a sequence of tokens using a convolutional layer of stride 8 and kernel size 8 (see the fifth line in <ref type="table">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enhanced residual connection:</head><p>We found that introducing a residual scaling factor can also bring benefit as shown in <ref type="table">Table 5</ref>. We found that using smaller scaling factor can lead to better performance and faster convergence. Part of the reason is that the weight can get larger gradients with the applied small scaling factor. In the meantime, more information can be preserved in the main branch, leading to less information loss and better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-labeling:</head><p>We use the NFNet-F6 <ref type="bibr" target="#b1">[2]</ref> trained on Ima-geNet with an 86.5% Top-1 accuracy as the machine annotator to re-label the ImageNet dataset and obtain the 1000-dimensional score map for each image for training. The re-labeling procedure is similar to <ref type="bibr" target="#b44">[45]</ref>, but we limit our experiment setting by training all models from scratch on ImageNet without extra data support, such as JFT-300M and ImageNet-22K. This is different from the original Re-labeling paper <ref type="bibr" target="#b44">[45]</ref>, in which the Efficient-L2 model pre-trained on JFT-300M is used. The input resolution for NFNet-F6 is 576 × 576, and the dimension of the corresponding output score map for each image is L ∈ R 18×18×1000 . In practice, we only store the top-5 score maps for each position in half precision to save space as storing the entire score map for all the images will result in 2TB of storage. In our experiment, we only need 10GB storage to store all the score maps. As shown in <ref type="table" target="#tab_3">Table 3</ref>, re-labeling gives a performance gain of 0.4%, bringing the performance from 82.4% to 82.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MixToken:</head><p>In our experiments, we propose to use Mix-Token as a substitution for CutMix while applying token labeling. Our experiments show that MixToken performer better than CutMix. As shown in <ref type="table" target="#tab_6">Table 6</ref>, using the same supervision (Re-labeling), we can see an improvement of 0.2% compared to the CutMix baseline. Token labeling: Since augmentations like rotation, shear, and translation change the spatial information of the image, we need to make corresponding changes in the dense score map to make token labeling work in a right way. Comparing the top two lines in <ref type="table" target="#tab_6">Table 6</ref>, we can see that using our token labeling as supervision results in better performance than re-labeling <ref type="bibr" target="#b44">[45]</ref>. This means that involving all the tokens in the loss calculation does matter for training vision transformers. Other than the above experiments, we also study the effect of different augmentation techniques, such as CutOut and MixUp, while applying token labeling. The ablation results are shown in <ref type="table" target="#tab_5">Table 7</ref>. We can see that when all the four augmentation methods are used, we obtain a top-1 accuracy of 83.1. Interestingly, when the MixUp augmentation is removed, the performance can be slightly improved to 83.3. This may be due to the fact that use MixToken as well as MixUp at the same time would bring too much noise in the label, leading to the confusion of the model. Moreover, the CutOut augmentation, which randomly erases some part of the image, is also effective and removing it brings a performance drop of 0.4% point. Similarly, the RandAug augmentation also contributes to the performance.</p><p>Model Scaling: In <ref type="table" target="#tab_7">Table 8</ref>, we show the performance of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">More Experiments</head><p>Normalization at different positions: Inspired by <ref type="bibr" target="#b39">[40]</ref>, we attempt to utilize both post-LayerNorm and pre-LayerNorm transformer blocks for our models. While the post-LayerNorm is widely used in NLP, it fails to converge for the hyper-parameters we use in the image classification task. Thus, we only use the pre-LayerNorm transformer block in all of our experiments.</p><p>Replacing LayerNorm with GroupNorm: We explore with different normalization layers by replacing the Lay-erNorm in the transformer block with the GroupNorm layer with different group sizes. Results are shown in <ref type="table">Table 9</ref>. We empirically found that using GroupNorm with larger group size would slow down the convergence and does not bring benefit to the final accuracy in the current setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to Other Methods</head><p>We compare our proposed model LV-ViT with previous method as well as concurrent state-of-the-art methods in <ref type="table" target="#tab_0">Table 10</ref>. For small-sized model, when test resolution is set to 224 × 224, we achieves 83.3% accuracy on ImageNet, which is 3.4 point higher than the strong baseline DeiT-S with only 26M parameters. For medium-sized model, when test resolution is set to 384 × 384 we also achieve same per- <ref type="table" target="#tab_0">Table 10</ref>. Top-1 accuracy comparison with other methods on ImageNet <ref type="bibr" target="#b11">[12]</ref> and ImageNet Real <ref type="bibr" target="#b0">[1]</ref>. All models are trained without external data. With the same computation and parameter constrain, our model consistently outperforms other CNN-based and transformer-based counterparts. The results of CNNs and ViT are referenced from <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Params formance of 85.4% as CaiT-S36 with much less computation cost and parameters. Note that they use knowledge distillation based method to improve their model, which will introduce much more computations. However, we do not require any extra computation while training and only have to compute and store the dense score map offline. For largesized model, we also test on input resolution of 448 × 448, which achieves 86.2%, comparable with CaiT-M36 with about half FLOPs and parameters. It's also better than a few other models including CNN based model such as Ef-ficientNet and other transformer based model such as T2T-ViT and Swin transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have introduced a few techniques to improve the performance of a vision transformer model. We have also analyzed each component individually to evaluate its contribution. Combining them together, we were able to obtain a performant model , termed LV-ViT that achieves 84.4% Top-1 accuracy on ImageNet with only 26M parameters, which demonstrate the effectiveness of our proposed training techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Training pipeline comparison between the original vision transformer (a) and the proposed approach (b). After patch embedding, a MixToken method is introduced to mix the tokens from two different images. A token labeling objective is also added along with the original classification loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison between CutMix<ref type="bibr" target="#b43">[44]</ref> (Left) and our proposed MixToken (Right). CutMix is operated on the input images. This results in patches containing regions from the mixed two images (see the patches enclosed by red bounding boxes). Differently, MixToken targets at mixing tokens after patch embedding. This enables each token after patch embedding to have clean content as shown in the right part of this figure. The detailed advantage of MixToken can be found in Sec. 3.2 and Sec. 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with CaiT<ref type="bibr" target="#b32">[33]</ref>. Our model exploits less training techniques, model size, and computations but achieve identical result to CaiT.</figDesc><table><row><cell>Settings</cell><cell>LV-ViT (Ours)</cell><cell>CaiT [33]</cell></row><row><cell>Transformer Blocks</cell><cell>20</cell><cell>36</cell></row><row><cell>#Head in Self-attention</cell><cell>8</cell><cell>12</cell></row><row><cell>MLP Expansion Ratio</cell><cell>3</cell><cell>4</cell></row><row><cell>Embedding Dimension</cell><cell>512</cell><cell>384</cell></row><row><cell>Stochastic Depth</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Default hyper-parameters for our experiments. Note that we do not use the MixUp augmentation method when re-labeling and token labeling are used.</figDesc><table><row><cell>Supervision</cell><cell>Standard</cell><cell cols="2">Re-labeling Token labeling</cell></row><row><cell>Epoch</cell><cell>300</cell><cell>300</cell><cell>300</cell></row><row><cell>Batch size</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell></row><row><cell>LR</cell><cell>1e-3 · batch size 1024</cell><cell>1e-3· batch size 1024</cell><cell>1e-3· batch size 640</cell></row><row><cell>LR decay</cell><cell>cosine</cell><cell>cosine</cell><cell>cosine</cell></row><row><cell>Weight decay</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>Warmup epochs</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>Dropout</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Stoch. Depth</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>MixUp alpha</cell><cell>0.8</cell><cell>-</cell><cell>-</cell></row><row><cell>Erasing prob.</cell><cell>0.25</cell><cell>0.25</cell><cell>0.25</cell></row><row><cell>RandAug</cell><cell>9/0.5</cell><cell>9/0.5</cell><cell>9/0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation path from the DeiT-Small<ref type="bibr" target="#b31">[32]</ref> baseline to our LV-ViT-S. All experiments expect for larger input resolution can be finished within 3 days using a single server node with 8 V100 GPUs. Clearly, with only 26M learnable parameters, the performance can be boosted from 79.9 to 84.4 (+4.5) using the proposed training techniques.</figDesc><table><row><cell>Training techniques</cell><cell cols="2">#Param. Top-1 Acc. (%)</cell></row><row><cell>Baseline (DeiT-Small [32])</cell><cell>22M</cell><cell>79.9</cell></row><row><cell>+ More transformers (12 → 16)</cell><cell>28M</cell><cell>81.2 (+1.2)</cell></row><row><cell cols="2">+ Less MLP expansion ratio (4 → 3) 25M</cell><cell>81.1 (+1.1)</cell></row><row><cell>+ More convs for patch embedding</cell><cell>26M</cell><cell>82.2 (+2.3)</cell></row><row><cell>+ Enhanced residual connection</cell><cell>26M</cell><cell>82.4 (+2.5)</cell></row><row><cell>+ Re-labeling</cell><cell>26M</cell><cell>82.8 (+2.9)</cell></row><row><cell>+ Token labeling</cell><cell>26M</cell><cell>83.3 (+3.4)</cell></row><row><cell>+ Input resolution (224 → 384)</cell><cell>26M</cell><cell>84.4 (+4.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>table .</head><label>.</label><figDesc></figDesc><table><row><cell cols="2">#Convs Kerenl size</cell><cell>Stride</cell><cell cols="2">Params Top-1 Acc. (%)</cell></row><row><cell>1</cell><cell>[16]</cell><cell>[16]</cell><cell>25M</cell><cell>81.1</cell></row><row><cell>2</cell><cell>[7,8]</cell><cell>[2,8]</cell><cell>25M</cell><cell>81.4</cell></row><row><cell>3</cell><cell>[7,3,8]</cell><cell>[2,2,4]</cell><cell>25M</cell><cell>81.4</cell></row><row><cell>3</cell><cell>[7,3,8]</cell><cell>[2,1,8]</cell><cell>26M</cell><cell>81.9</cell></row><row><cell>4</cell><cell>[7,3,3,8]</cell><cell>[2,1,1,8]</cell><cell>26M</cell><cell>82.2</cell></row><row><cell>4</cell><cell>[7,3,3,8]</cell><cell>[2,2,1,4]</cell><cell>26M</cell><cell>81.5</cell></row><row><cell>6</cell><cell cols="3">[7,3,3,3,3,8] [2,1,1,1,1,8] 26M</cell><cell>82.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Ablation on different widely-used data augmentations. We empirically found that our proposed MixToken performs even better than the combination of MixUp and CutMix in vision transformers.</figDesc><table><row><cell>MixToken MixUp CutOut RandAug Top-1 Acc. (%)</cell></row><row><cell>83.3</cell></row><row><cell>81.3</cell></row><row><cell>83.1</cell></row><row><cell>83.0</cell></row><row><cell>82.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Ablation on the proposed MixToken and token labeling augmentations.</figDesc><table><row><cell>Augmentation Method</cell><cell>Supervision</cell><cell>Top-1 Acc. (%)</cell></row><row><cell>MixToken</cell><cell>Token labeling</cell><cell>83.3</cell></row><row><cell>MixToken</cell><cell>Re-labeling</cell><cell>83.0</cell></row><row><cell>CutMix</cell><cell>Re-labeling</cell><cell>82.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Performance of the proposed LV-ViT with different model sizes. Here, 'depth' denotes the number of transformer blocks used in different models. By default, the test resolution is set to 224 × 224 except the last one which is 288 × 288.</figDesc><table><row><cell>Name</cell><cell cols="4">Depth Embed dim. #Parameters Top-1 Acc. (%)</cell></row><row><cell>LV-ViT-T</cell><cell>12</cell><cell>240</cell><cell>8.5M</cell><cell>79.1</cell></row><row><cell>LV-ViT-S</cell><cell>16</cell><cell>384</cell><cell>26M</cell><cell>83.3</cell></row><row><cell cols="2">LV-ViT-M 20</cell><cell>512</cell><cell>56M</cell><cell>84.0</cell></row><row><cell>LV-ViT-L</cell><cell>24</cell><cell>768</cell><cell>150M</cell><cell>85.3</cell></row><row><cell cols="5">Table 9. Ablation on normalization layer. The baseline is set as</cell></row><row><cell cols="5">16 layer vision transformer with 384 embedding dimension and 4</cell></row><row><cell cols="4">layer convolutional patch embedding.</cell><cell></cell></row><row><cell cols="2">Normalization</cell><cell>Groups</cell><cell>#Parameters</cell><cell>Top-1 Acc. (%)</cell></row><row><cell cols="2">LayerNorm</cell><cell>-</cell><cell>26M</cell><cell>82.2</cell></row><row><cell cols="2">GroupNorm</cell><cell>1</cell><cell>26M</cell><cell>82.2</cell></row><row><cell cols="2">GroupNorm</cell><cell>2</cell><cell>26M</cell><cell>82.2</cell></row><row><cell cols="2">GroupNorm</cell><cell>3</cell><cell>26M</cell><cell>81.9</cell></row><row><cell cols="2">GroupNorm</cell><cell>6</cell><cell>26M</cell><cell>81.8</cell></row><row><cell cols="5">our proposed approach under different network settings. By</cell></row><row><cell cols="5">using 16 transformer blocks and setting the embedding di-</cell></row><row><cell cols="5">mension to 192, we can achieve a top-1 accuracy of 79.1.</cell></row><row><cell cols="5">Increasing the embedding dimension and network depth can</cell></row><row><cell cols="5">further boost the performance. More experiments compared</cell></row><row><cell cols="4">to other methods can be found in Sec 4.4.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>FLOPs Train size Test size Top-1(%) Real Top-1 (%)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CNNs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B5 [31]</cell><cell>30M</cell><cell>9.9B</cell><cell>456</cell><cell>456</cell><cell>83.6</cell><cell>88.3</cell></row><row><cell>EfficientNet-B7 [31]</cell><cell>66M</cell><cell>37.0B</cell><cell>600</cell><cell>600</cell><cell>84.3</cell><cell></cell></row><row><cell>Fix-EfficientNet-B8 [31, 34]</cell><cell>87M</cell><cell>89.5B</cell><cell>672</cell><cell>800</cell><cell>85.7</cell><cell>90.0</cell></row><row><cell>NFNet-F0 [2]</cell><cell>72M</cell><cell>12.4B</cell><cell>192</cell><cell>256</cell><cell>83.6</cell><cell>88.1</cell></row><row><cell>NFNet-F1 [2]</cell><cell>133M</cell><cell>35.5B</cell><cell>224</cell><cell>320</cell><cell>84.7</cell><cell>88.9</cell></row><row><cell>NFNet-F2 [2]</cell><cell>194M</cell><cell>62.6B</cell><cell>256</cell><cell>352</cell><cell>85.1</cell><cell>88.9</cell></row><row><cell>NFNet-F3 [2]</cell><cell>255M</cell><cell>114.8B</cell><cell>320</cell><cell>416</cell><cell>85.7</cell><cell>89.4</cell></row><row><cell>NFNet-F4 [2]</cell><cell>316M</cell><cell>215.3B</cell><cell>384</cell><cell>512</cell><cell>85.9</cell><cell>89.4</cell></row><row><cell>NFNet-F5 [2]</cell><cell>377M</cell><cell>289.8B</cell><cell>416</cell><cell>544</cell><cell>86.0</cell><cell>89.2</cell></row><row><cell></cell><cell>56M</cell><cell>42.2B</cell><cell>224</cell><cell>384</cell><cell>85.4</cell><cell>89.5</cell></row><row><cell>LV-ViT-L</cell><cell>150M</cell><cell>59.0B</cell><cell>288</cell><cell>288</cell><cell>85.3</cell><cell>89.3</cell></row><row><cell>LV-ViT-L↑448</cell><cell>150M</cell><cell>157.2B</cell><cell>288</cell><cell>448</cell><cell>85.9</cell><cell>89.7</cell></row><row><cell>LV-ViT-L↑448</cell><cell>150M</cell><cell>157.2B</cell><cell>448</cell><cell>448</cell><cell>86.2</cell><cell>89.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Xiaohua Zhai, and Aäron van den Oord</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
	</analytic>
	<monogr>
		<title level="m">Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<title level="m">High-performance large-scale image recognition without normalization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14899</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Electra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Stéphane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16302</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Selfadaptive scaling for learnable residual structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="862" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking skip connection with layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3586" to="3598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10881</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06423</idno>
		<title level="m">Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution discrepancy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">End-toend video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="528" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Point transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09315</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
