<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised 3D Pose Estimation with Geometric Self-Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
							<email>ambrisht@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
							<email>aaagrawa@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
							<email>droverd@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohith</forename><surname>Mv</surname></persName>
							<email>kurohith@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
							<email>sstojanov@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
							<email>rehg@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Lab126</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised 3D Pose Estimation with Geometric Self-Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an unsupervised learning approach to recover 3D human pose from 2D skeletal joints extracted from a single image. Our method does not require any multiview image data, 3D skeletons, correspondences between 2D-3D points, or use previously learned 3D priors during training. A lifting network accepts 2D landmarks as inputs and generates a corresponding 3D skeleton estimate. During training, the recovered 3D skeleton is reprojected on random camera viewpoints to generate new 'synthetic' 2D poses. By lifting the synthetic 2D poses back to 3D and re-projecting them in the original camera view, we can define self-consistency loss both in 3D and in 2D. The training can thus be self supervised by exploiting the geometric selfconsistency of the lift-reproject-lift process. We show that self-consistency alone is not sufficient to generate realistic skeletons, however adding a 2D pose discriminator enables the lifter to output valid 3D poses. Additionally, to learn from 2D poses 'in the wild', we train an unsupervised 2D domain adapter network to allow for an expansion of 2D data. This improves results and demonstrates the usefulness of 2D pose data for unsupervised 3D lifting. Results on Human3.6M dataset for 3D human pose estimation demonstrate that our approach improves upon the previous unsupervised methods by 30% and outperforms many weakly supervised approaches that explicitly use 3D data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimation of 3D human pose from images and videos is a classical ill-posed inverse problem in computer vision with numerous applications <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref> in human tracking, action understanding, human-robot interaction, augmented reality, video gaming, etc. Current deep learningbased systems attempt to learn a mapping from RGB images or 2D keypoints to 3D skeleton joints via some form of supervision requiring datasets with known 3D pose. However, obtaining 3D motion capture data is time-consuming, difficult, and expensive, and as a result, only a limited amount of 3D data is currently available. On the other hand, 2D image and video data of humans is available in abundance. However, unsupervised learning of 3D joint locations from 2D pose alone remains a holy grail in the field. In this paper, we take a first step towards achieving this goal and present an unsupervised learning algorithm to estimate 3D human pose from 2D pose landmarks/keypoints. Our approach does not use 3D inputs in any form and does not require 2D-3D correspondences or explicit 3D priors.</p><p>Due to perspective projection ambiguity, there exists an infinite number of 3D skeletons corresponding to a given 2D pose. However, all of these solutions are not physically plausible given the anthropomorphic constraints and joint angle limits of a human body articulation. Typically, supervised learning with 2D pose and corresponding 3D skeletons is used to restrict the solution space. In addition, the 3D structure can also be regularized in a weakly-supervised manner by using priors such as symmetry, ratio of length of various skeleton elements, and kinematic constraints, which are learned from 3D data. In contrast, this paper addresses the fundamental problem of lifting 2D image coordinates to 3D space without the use of any additional cues such as video <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b53">54]</ref>, multi-view cameras <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, or depth images <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>We posit that the following properties of the 2D-3D pose mapping render unsupervised lifting possible: 1) Closure: If a 2D skeleton is lifted to 3D accurately, and then randomly rotated and reprojected, the resulting 2D skeleton will lie within the distribution of valid 2D poses. Conversely, a lifted 3D skeleton whose random re-projection falls outside this distribution is likely to be inaccurate. 2) Invariance: 2D projections of the same 3D skeleton from different viewpoints, when lifted, should produce the same 3D output. In other words, lifting should be invariant to change in the viewpoint.</p><p>We employ the above properties in designing a deep neural network, referred to as the lifting network, which is illustrated in <ref type="figure">Figure 1</ref>. We introduce a novel geometrical consistency loss term that allows the network to learn in a self-supervised mode. This self-consistency loss relies on the property of invariance: any 2D projection of the gen-  <ref type="figure">Figure 1</ref>. We train a 2D-3D lifting network (lifter), which estimates the 3D skeleton from 2D pose landmarks. Random projections of generated 3D skeletons are fed to a 2D pose discriminator to provide feedback to the lifter. The random projections also go through a similar lifting and reprojection process, allowing the network to self supervise the training process by exploiting geometric consistency. erated 3D skeleton should produce the same 3D skeleton when processed by the lifting network (Section 3.3). We further demonstrate that self-consistency is a necessary but not a sufficient condition. We add a discriminator to ensure that the projection of lifted skeletons lie within the distribution of 2D poses. However we find that self-supervision does improve performance of the lifting network when used in conjunction with discriminator feedback. Domain Adaptation: Since unsupervised learning methods often need more data for training than supervised methods, it is desirable to exploit multiple data sources. However, domain shifts could occur in multiple data sources due to (a) differences in human pose and viewpoint variations, and (b) semantic differences in the location of the skeletal joints on the body (e.g., hips marked inside/outside legs). We propose a domain adaptation algorithm, where we train a 2D adapter network to convert 2D joints from a source domain to the target domain without the need for any correspondences. Using multiple datasets allows us to enrich the viewpoint, pose, and articulation variations in the target domain using additional domains.</p><p>Temporal consistency during training: Our algorithm only requires 2D joints extracted from a single frame for training and inference. However, if sequences of poses from videos are available during training, we show how they can be used to improve the lifter. To exploit temporal consistency during training, we incorporate an additional temporal discriminator that classifies the differences in 2D joints in subsequent frames as real/fake. Our ablation studies show that adding a temporal discriminator improves performance by an additional 7%, even when inference is performed on a single frame.</p><p>Our paper makes the following contributions:</p><p>• Inspired by <ref type="bibr" target="#b8">[9]</ref>, we present an unsupervised algorithm to lift 2D joints to 3D skeletons by observing samples of real 2D poses, without using 3D data in any form.</p><p>• Our method can learn by exploiting geometric self consistency. We show that self consistency is a necessary but not a sufficient condition for lifting. Self consistency loss improves performance when combined with 2D pose discriminator adversarial loss.</p><p>• We propose a 2D domain adaptation technique which can utilize data from different domains to improve performance on the target domain.</p><p>• We show that adding a temporal discriminator during training can further improve performance, even for single frame 2D-3D lifting during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Pose Estimation: There are numerous deep learning techniques proposed for estimating 3D joint location directly from 2D images <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38]</ref>. Other methods decompose this problem into the estimation of 2D joint locations from images followed by the estimation of 3D joint locations based on the 2D keypoints. 2D pose from images can be obtained using techniques such as CPM <ref type="bibr" target="#b46">[47]</ref>, Stacked-hourglass architecture <ref type="bibr" target="#b29">[30]</ref>, Mask-RCNN <ref type="bibr" target="#b13">[14]</ref> or affinity models <ref type="bibr" target="#b3">[4]</ref>. As discussed, our focus is on estimating 3D pose from 2D landmarks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>, and we are agnostic to the source of landmarks. For the purpose of comparison, prior work on lifting can be organized into four categories:</p><p>Fully Supervised: These include approaches such as <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b48">49]</ref> that use paired 2D-3D data comprised of ground truth 2D locations of joint landmarks and the corresponding 3D ground truth for learning. For example, Martinez et al. <ref type="bibr" target="#b25">[26]</ref> learn a regression network from 2D joints to 3D joints, whereas Moreno-Noguer <ref type="bibr" target="#b28">[29]</ref> learn a regression from 2D distance matrix to 3D distance matrix using 2D-3D correspondences. Exemplar based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b50">51]</ref> use a database/dictionary of 3D skeletons for nearest-neighbor look-up. Tekin et al. <ref type="bibr" target="#b41">[42]</ref> fused 2D and 3D image cues re-lying on 2D-3D correspondences. Wang et al. <ref type="bibr" target="#b45">[46]</ref> use the 3D ground truth to train an intermediate ranking network to extract the depth ordering of pairwise human joints from a single RGB image. Sun et al. <ref type="bibr" target="#b40">[41]</ref> use a 3D regression based on bone segments derived from joint locations as opposed to directly using joint locations. Since these methods model 2D to 3D mappings from a given dataset, they implicitly incorporate dataset-specific parameters such as camera projection matrices, distance of skeleton from the camera, and scale of skeletons. This enables these models to predict metric position of joints in 3D on similar datasets, but requires paired 2D-3D correspondences which are difficult to obtain.</p><p>Weakly Supervised: Approaches such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> do not explicitly use paired 2D-3D correspondences, but use unpaired 3D data to learn priors on shape (3D basis) or pose (articulation priors). For example, Zhou et al. <ref type="bibr" target="#b53">[54]</ref> use a 3D pose dictionary to learn pose priors and Brau et al. <ref type="bibr" target="#b2">[3]</ref> employ an independently trained network that learns a prior distribution over 3D poses (kinematic and self-intersection priors). Tome et al. <ref type="bibr" target="#b43">[44]</ref>, Wu et al. <ref type="bibr" target="#b47">[48]</ref> and Tung et al. <ref type="bibr" target="#b10">[11]</ref> pre-train low-dimensional representations from 3D annotations to obtain priors for plausible 3D poses. Another form of weak supervision is employed by Ronchi et al. <ref type="bibr" target="#b38">[39]</ref>, where they train a network using relative depth ordering of joints to predict 3D pose from images. Dabral et al. <ref type="bibr" target="#b7">[8]</ref> uses supervision of 3D skeletons in conjunction with anatomical losses based on joint angle limits and limb symmetry. Rhodin et al. <ref type="bibr" target="#b36">[37]</ref> train via 2D data, using multiple images of a single pose in addition to supervision in using 3D data when available. An adversarial training paradigm was used by Yang et al. <ref type="bibr" target="#b49">[50]</ref> to improve an existing 3D pose estimation framework, lifting in-thewild images with no 3D ground truth and comparing them to existing 3D skeletons.</p><p>Similar to our work, the weakly supervised approach of Drover et al. <ref type="bibr" target="#b8">[9]</ref> also makes use of 2D projections to learn a 3D prior on human pose. However, Drover et al. utilize the ground-truth 3D points to generate a large amount (12M) of synthetic 2D joints for training, thus augmenting the original 1.5M 2D poses in Human3.6M by almost 10 times. This allows them to synthetically over-sample the space of camera variations/angles to learn the 3D priors from those poses. In contrast, we do not use any ground truth 3D projection or 3D data in any form. The fact that we can utilize multiple 2D datasets without any 3D supervision sets us apart from these previous approaches, and enables our method to exploit the large amount of available 2D pose data.</p><p>Unsupervised: Recently, Rhodin et al. <ref type="bibr" target="#b35">[36]</ref> proposed an unsupervised method to learn a geometry-aware body representation. Their approach maps one view of the human to another view from a set of given multi-view images. It relies on synchronized multi-view images of subjects to learn an encoding of scene geometry and pose. It also uses video sequences to observe the same subject at multiple time instants to learn appearance. In contrast, we do not require multi-view images or the ability to capture the same pose at multiple time instants. We learn 3D pose from 2D projections alone. Kudo et al. <ref type="bibr" target="#b22">[23]</ref> present 3D error results (130.9 mm) that are comparable to the trivial baseline reported in <ref type="bibr" target="#b8">[9]</ref> (127.3 mm).</p><p>Learning Using Adversarial Loss: Generative adversarial learning has emerged as a powerful framework for modeling complex data distributions, some use it to learn generative models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b55">56]</ref>, and <ref type="bibr" target="#b44">[45]</ref> leverages it to synthesize hard examples, etc. Previous approaches have used adversarial loss for human pose estimation by using a discriminator to differentiate real/fake 2D poses <ref type="bibr" target="#b5">[6]</ref> and real/fake 3D poses <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>. To estimate 3D, these techniques still require 3D data or use a prior 3D pose models. In contrast, our approach applies an adversarial loss over randomly projected 2D poses of the generated 3D skeletons. Previous works on image-to-image translation such as CycleGAN <ref type="bibr" target="#b55">[56]</ref> or CyCADA <ref type="bibr" target="#b14">[15]</ref> also rely on a cycle consistency loss in the image domain to enable unsupervised training. However, we use geometric self-consistency and utilize consistency loss in 3D and 2D joint locations, resulting in a novel method for lifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unsupervised 2D-3D Lifting</head><p>In this section, we describe our unsupervised learning approach to lift 2D pose to a 3D skeleton. Let x i = (x i , y i ), i = 1 . . . N, denote N 2D pose landmarks of a skeleton with the root joint (midpoint between hip joints) located at the origin. Let X i denote the corresponding 3D joint for each 2D joint. We assume a camera with unit focal length centered at the origin (0, 0, 0). Note that because of the fundamental perspective ambiguity, absolute metric depths cannot be obtained from a single view. Therefore, we fix the distance of the skeleton to the camera to a constant c units. In addition, we normalize the 2D skeletons such that the mean distance from the head joint to the root joint is 1 c units in 2D. This ensures that 3D skeleton will be generated with a scale of ≈ 1 unit (head to root joint distance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lifting Network</head><p>The lifting network G(x) is a neural network that outputs the 3D joint for each 2D joint.</p><formula xml:id="formula_0">G θ G (x) = X,<label>(1)</label></formula><p>where θ G are the parameters of the lifter learned during training. Internally, the lifter estimates the depth offset d i of each joint relative to the fixed plane at c units. The 3D joint is computed as</p><formula xml:id="formula_1">X i = (x i z i , y i z i , z i ), where z i = max (1, c + d i ) .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Random Projections</head><p>The generated 3D skeletons are projected to 2D using random camera orientations and these 2D poses are sent to the lifter and discriminator. Let R be a random rotation matrix, created by uniformly sampling an azimuth angle between [-π, π] and an elevation angle between [-π/9, π/9], and X r be the location of the root joint of the generated skeleton. The rotated 3D skeleton Y i is obtained as</p><formula xml:id="formula_2">Y i = Q(X i ) = R * (X i − X r ) + T,<label>(3)</label></formula><p>where T = [0, 0, c]. Q represents the rigid transformation between Y and X. The rotated 3D skeleton Y i is then projected to create a 2D skeleton y i = P (Y i ), where P denotes perspective projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self-Supervision via Loop Closure</head><p>We now describe the symmetrical lifting and projection step performed on the synthesized 2D pose, y i . As shown in <ref type="figure">Figure 2</ref>, we lift the randomly projected pose y i to obtaiñ</p><formula xml:id="formula_3">Y iỸ i = G θ G (y i ).<label>(4)</label></formula><p>Y i is transformed toX i by applying the inverse of rigid transformation Q that was used while generating the random projection y i from X i . The 3D skeletonX i is finally projected to the 2D skeletonx i . Note that the lifting network G(·) remains the same in both the forward and backward part of the cycle as illustrated in <ref type="figure">Figure 2</ref>. If the lifting network accurately reconstructs the 3D pose from 2D inputs, then the 3D skeletons Y i andỸ i and the corresponding 2D projections x i andx i should be similar. The cycle described herein provides a strong signal for self-supervision for the lifting network, whose loss term can be updated by adding two additional components, namely, L 3D = Y −Ỹ 2 and</p><formula xml:id="formula_4">L 2D = x −x 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discriminator for 2D Poses</head><p>The 2D pose discriminator D is a neural network (with parameters θ D ) that takes as input a 2D pose and outputs a probability between 0 and 1. It classifies between real 2D pose r (target probability of 1) and fake (projected) 2D pose y (target probability of 0). Note that for any training sample x for lifter, we do not require r to be same as x or any of it's multi-view correspondences. During learning we utilize a standard GAN loss <ref type="bibr" target="#b12">[13]</ref> defined as <ref type="figure">Figure 2</ref>. Self-supervision achieved by closing the loop between the generated skeleton Y, its random projection y. The recovered 3D skeletonỸ is obtained by lifting y. Upon reversing the geometric transformations, training can be self-supervised by comparing x withx, and Y withỸ.</p><formula xml:id="formula_5">min θ G max θ D L adv = E(log(D(r))) + E(log(1 − D(y))). (5) x X Y y real/fakẽ xXỸ G(x) Q(X) P(Y) D(y) G(y) P(X) Q −1 (Ỹ)</formula><p>The discriminator provides feedback to the lifter allowing it to learn priors on 3D skeletons such as the ratio of limb lengths and joint angles using only random 2D projections, thus allowing it to avoid inadequacies as shown in Sect. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Temporal Consistency</head><p>Note that our approach does not require video data for training. However, when available, temporal 2D pose sequences (e.g. video sequence of actions) can improve the accuracy of the single frame lifting network. We exploit the temporal smoothness via an additional loss function to refine the lifting network G(·) as shown in <ref type="figure">Figure 3</ref>. We train an additional discriminator, T (·) that takes as input the difference of 2D poses adjacent in time. The real data for this discriminator comes from a sequence of real 2D poses available during training, r t − r t+1 . The discriminator T (·) is updated to optimize the loss that can distinguish the distribution of real 2D pose differences from those of the fake 2D (sequential) projections y t − y t+1 . Specifically,</p><formula xml:id="formula_6">max θ T L T =E(log(T (r t − r t+1 )))+ E(log(1 − T y t − y t+1 ) ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Learning from 2D Poses in the Wild</head><p>To improve the 3D lifting accuracy in the target domain of interest (e.g. Human 3.6M, x t ), we wish to augment 2D training data from in the wild (e.g. OpenPose joint estimates on Kinetics dataset, x s ). Depending on the choice of 2D pose extraction algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47]</ref>, the position and semantics of 2D keypoints can vary greatly from the representation adopted by the target domain (e.g. center of face vs. top of the head or side of the hips vs. pelvis).</p><p>We train a 2D domain adapter neural network C to map the source domain 2D joints to target domain 2D joints (see <ref type="figure" target="#fig_1">Figure 4</ref>). Let x sc denote the corrected source domain 2D joints, such that x sc = x s + C(x s ). Note that we do not assume any correspondences between the 2D joints in the source and target domains. Thus, we cannot train C using any form of supervised loss. In absence of any supervision,</p><formula xml:id="formula_7">x t x t+1 X t X t+1 y t y t − y t+1 y t+1</formula><p>real/fake real/fake real/fake <ref type="figure">Figure 3</ref>. Discriminator T (·) enforces a distribution on the temporal differences of projected 2D poses. The temporal consistency is an optional element to stabilize the results, and is only added during training, allowing inference on single frame 2D pose inputs. Subscripts t and t + 1 denote two consecutive inputs. Lifting, transformation and projection is done as in <ref type="figure">Figure 2</ref>.  we use a domain discriminator D D to match the distribution between the two domains. Again utilizing the standard GAN loss <ref type="bibr" target="#b12">[13]</ref>, we optimize the following loss</p><formula xml:id="formula_8">G(xt) G(xt+1) P (Q(Xt)) P (Q(Xt+1)) D(y t ) T (·) D(y)</formula><formula xml:id="formula_9">min θ C max θ D D L adv =E(log(D D (x p ))) + λ C(x s ) 2 +E(log(1 − D D (x sc ))),<label>(7)</label></formula><p>where, the λ C(x s ) 2 is a regularizer term to keep the corrections limited to a small magnitude. <ref type="figure" target="#fig_2">Figure 5</ref> shows an example of the difference in semantics between the Human3.6M (target domain) and Open-Pose (source domain). In OpenPose, the top of the head is not marked and the center of the marked eye joints are used. In addition, the shoulder keypoints are marked higher than in Human3.6M. The domain adapted 2D pose (middle) is closer in terms of keypoint locations to the target domain. Our domain correction is an off-line preprocessing step. The domain corrected 2D poses, x sc , are fed both to the lifting network (Sect. 3.1) and the 2D pose discriminator (Sect. 3.4) during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Training</head><p>As discussed, the 2D to 3D lifting network is trained using geometric self-supervision along with 2D pose and temporal discriminators. Network parameters are updated to optimize the total loss given by,</p><formula xml:id="formula_10">L = L adv + w 2D L 2D + w 3D L 3D + w T L T ,<label>(8)</label></formula><p>where, w 2D = 10, w 3D = 0.001, and w T = 1 are the relative weights for each of the 2D, 3D, and temporal loss terms, respectively. Architectures: We do not use any convolutional layers and use fully connected layers (followed by residual blocks) for all the neural networks described above. Both the lifting network and the 2D pose discriminator takes as input 2N dimensional vectors, where N denotes the number of 2D/3D pose points. Similarly, the temporal discriminator takes 2N + 2N M inputs corresponding to the pose joints in the current frame and their temporal differences with M other consecutive frames (before and/or after). We adopt a similar architecture as that of Martinez et al. <ref type="bibr" target="#b25">[26]</ref>, with the lifting network composed of 4 residual blocks and the discriminator with 3 residual blocks. For 2D domain adaptation, we use 4 residual blocks for the adapter and 3 residual blocks for the domain discriminator. Batch normalization was used in the lifter and the adapter but not in either of the discriminators. Our experiments use N = 14 joint locations. For training we used a batch size of 8192, a constant depth c = 10 and the Adam optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>We present quantitative and qualitative results on the widely used Human3.6M dataset <ref type="bibr" target="#b17">[18]</ref> for benchmarking. Additionally, to demonstrate how the unsupervised learning framework can be improved by leveraging 2D pose data from in-the-wild images, we augment our training data by adapting OpenPose estimated 2D human poses from the Kinetics <ref type="bibr" target="#b21">[22]</ref> dataset. We also show qualitative visualizations of reconstructed 3D skeletons from 2D pose landmarks on the MPII <ref type="bibr" target="#b1">[2]</ref> and Leeds Sports Pose (LSP) <ref type="bibr" target="#b19">[20]</ref> datasets, for which the ground truth 3D data is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Metrics</head><p>Human3.6M Dataset: Human3.6M is one of the largest 3D human pose datasets, consisting of 3.6 million 3D human poses. The dataset contains video and motion capture (MoCap) data from 5 female and 6 male subjects. Data is captured from 4 different viewpoints, while subjects perform typical activities such as talking on phone, walking, eating, etc. MPI-INF-3DHP: The MPI-INF-3DHP <ref type="bibr" target="#b26">[27]</ref> is a large human pose dataset containing &gt;1.3M frames taken from diverse viewpoints. The dataset has 4 male and 4 female actors performing an array of actions similar to but more diverse than the Human3.6M dataset. Kinetics dataset: The Kinetics dataset contains 400 video clips each for 400 activities involving one or more persons. The video clips are sourced from Youtube and each clip is approximately 10 seconds in duration. We did not use any of the class annotations from the dataset for our training. Instead, we extracted 2D pose landmarks using Open-Pose <ref type="bibr" target="#b3">[4]</ref> on sampled frames from this dataset. We retained only those frames in which all the landmarks on a person were estimated with sufficient confidence. After this filtering, approximately 9 million 2D skeletons were obtained. Evaluation Metric: We report the Mean Per Joint Position Error (MPJPE) in millimeters after scaling and rigid alignment to the ground truth skeleton. Similar to previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b53">54]</ref>, we report results on subjects S9 and S11. Also, following the convention as in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref>, we only use data from subjects S1, S5, S6, S7, and S8 for training. We do not train class specific models or leverage any motion information during inference to improve the results. The reported metrics are taken from the respective papers for comparisons. We also compare our method to <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b52">53]</ref> which uses the adapted Percentage of Correct Keypoints (PCK) and corresponding Area Under Curve (AUC) metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Results</head><p>We summarize our results for Human3.6M and MPI-INF-3DHP in <ref type="table">Table 1 and Table 2</ref>, respectively. In addition to comparing with the state-of-the-art unsupervised 3D pose estimation method of Rhodin et al. <ref type="bibr" target="#b35">[36]</ref>, we also show results from top fully supervised and weakly supervised methods. Results from <ref type="bibr" target="#b35">[36]</ref> uses images as input and are hence comparable to Ours(SH) results which use 2D joints extracted from the same input images using SH detector <ref type="bibr" target="#b29">[30]</ref>. Our method reduces error by 30% compared to <ref type="bibr" target="#b35">[36]</ref> (68mm vs. 98.2mm). <ref type="table" target="#tab_2">Table 3</ref> shows the results of an ablation study on lifter with various algorithmic components using ground truth 2D points. SS denotes self-consistency (Sect. 3.3), Adv adds the 2D pose discriminator (Sect. 3.4), DA augments the training data by adapting 2D poses from Kinetics  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inadequacy of Geometric Self-Supervision</head><p>At first glance, it may appear that self supervision is sufficient to learn a good lifter, without the need for a discriminator. However, we found that in absence of the 2D pose discriminator, network can produce outputs which are geometrically self-consistent, but not realistic (see <ref type="figure">Figure 7</ref>). We present an analysis of the 3D outputs that the lifting network can generate with only self-supervision. Specifically, we examine the ratios of upper to lower arm and leg, both for the left and right side of human body (4 ratios). <ref type="figure">Figure 6</ref> (Left) shows the distribution of the 4 ratios, for a lifter trained using self-consistency loss alone. Note that the lifter produces different limb length ratios for the left and right side of the body. Thus self-consistency loss alone may not produce symmetric (realistic) skeletons without any 3D priors. <ref type="figure">Figure 6</ref> (Middle) shows that after imposing symmetry constraints, the distributions of the left and right limbs are better aligned. However, the distributions are flatter since enforcing the same ratios for left and right sides does not ensure that these ratios are realistic (conforming to a human body). In other words, the lifter may choose different ratios for different training examples. <ref type="figure">Figure 6 (Right)</ref> shows the distributions when a discriminator that gives feedback to the lifter using real 2D poses is used. Notice that the ratio distributions become sharper and closer to distributions of real ratios in the training set. This is the reason that using self-supervision loss (SS) alone performs worse in our ablation studies as shown in <ref type="table">Table 1</ref>. However, the self-consistency further improves the performance in conjunction with 2D pose discriminator (Adv+SS).</p><p>Note that we do not use symmetry ratios in our framework when the discriminator is present. Our lifting network can learn higher order 3D skeleton statistics (beyond symmetry) based on the feedback from geometric self consistency and the 2D pose discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semi-supervised 3D Pose Estimation</head><p>Other methods have shown improvement in accuracy when a small amount of 3D data is used for supervised fine-tuning. We fine tuned our baseline model (from unsupervised training) using 5% of randomly sampled 3D data available in Human3.6M dataset. With this, our method could achieve performance comparable to fully supervised method (37mm) as shown in <ref type="table" target="#tab_2">Table 3</ref>. <ref type="figure" target="#fig_4">Figure 8</ref> shows some of the 3D pose reconstruction results on Human3.6M dataset using our lifting network. The ground truth 3D skeleton is depicted in gray. Some of the failures are shown in <ref type="figure" target="#fig_5">Figure 9</ref>. Most of these can be attributed to self-occlusions or flip ambiguities in viewing direction (for more details see Suppl. materials).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>To demonstrate generalization, we show some examples of 3D skeletons estimated on MPII <ref type="bibr" target="#b1">[2]</ref> and the Leeds Sports Pose (LSP) <ref type="bibr" target="#b19">[20]</ref> datasets, in <ref type="figure" target="#fig_6">Figures 10 and 11</ref> respectively. MPII has images extracted from short Youtube videos. LSP dataset consists of images of sport activities sampled from Flickr. Our unsupervised method successfully recovers 3D poses on these datasets without being trained on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>Previous unsupervised and weakly supervised methods use additional constraints on training data in lieu of 3D annotations. For example, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b50">51]</ref> leverage synthetic 2D poses obtained from known 3D skeletons to improve results. Similarly, Rhodin et al. <ref type="bibr" target="#b35">[36]</ref> derive an appearance and geometric model by choosing different frames from temporal sequences and multi-view images involving the same person. However, in theory, if multi-view images from synchronized cameras are available, one could triangulate the detected 2D joints to get 3D joints and train a supervised network. In contrast, our method treats each 2D skeleton as an individual training example, without requiring any multi-view correspondence. Hence, there is no restriction on where the 2D input pose originates; it could be obtained from a single image, video, or multi-view sequences. Our work explores the innate geometry of human pose itself, whereas <ref type="bibr" target="#b35">[36]</ref> exploits the consistency in camera geometry and appearance of specific individuals. As shown in Sect. 4.2, our approach is able to augment the training data from other datasets (e.g. Kinetics) with 2D skeletons captured in the wild.</p><p>Our current approach cannot handle occluded/missing joints during training or testing phases. This limits the amount of external domain data that can be used for training. For example, using OpenPose on Kinetics dataset results in 17M skeletons with at least 10 joints, but only 9M complete skeletons (14 joints). Though not the main focus of the paper, we did a small experiment to fill-in missing joints to further augment our training data. We trained a two-layer fully connected neural network which takes incomplete OpenPose 2D pose estimates on Human3.6M images as input and outputs completed 14 joints. The network was trained using the corresponding 2D ground-truth joints from Human3.6M in a supervised manner. Using the completed poses (17M skeletons) from the Kinetic dataset, our method achieved a MPJPE of 48mm on Human3.6M test data. This experiment further underscores the importance   of volume and diversity of training data for unsupervised learning. We believe that by using auto-encoders and other unsupervised methods for data completion will enable utilizing even more diverse datasets, where 2D joints may be extracted from a variety of 2D pose estimation algorithms. Future work includes training the filling network and the domain adaptation network together with the lifting network  in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>For 3D human pose estimation, acquiring 3D MoCap data remains an expensive and challenging endeavor. We presented an unsupervised learning approach to generate 3D skeletons from 2D joints, which does not require 3D data in any form. Our paper introduces geometric self-supervision as a novel constraint to learn the 2D-3D lifter. We showed that while geometric self-supervision is not a sufficient condition and cannot generate realistic skeletons by itself, it improves the reconstruction accuracy when combined with a discriminator. By training a domain adapter, we showed how to utilize data from different domains and datasets in an unsupervised manner. Thus, we believe that our paper has significantly improved the state-of-art in unsupervised learning of 3D skeletons by developing the key idea of geometric self-supervision and utilizing domain adaptation. Future work includes end-to-end training of 3D skeletons from 2D images, using self-supervision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Unsupervised domain adaptation to transform 2D poses from source domain to match the semantics of the target domain. The 2D adapter modifies the input xs to generate corrected pose xsc. The domain discriminator is used to ensure that the distribution of adapted2D poses xsc and the target domain 2D poses xt match. Adapted poses xsc are used for training a domain agnostic lifter as shown in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>An example of unsupervised domain adaptation. (Left) 2D joints estimated using OpenPose for an example DeepMind Kinetics image. (Middle) Resulting 2D pose after adaptation. (Right) Similar pose from Human3.6M dataset. Notice the change in the width of hips and slant of shoulders after adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Distribution of limb lengths ratios on Human3.6M test data. (Left) Training using self-consistency loss alone does not impose symmetry. (Middle) Using self-consistency and symmetry aligns distribution of left/right limbs, but results in flatter (unrealistic) distributions. (Right) Using a discriminator sharpens the distributions and brings them closer to real values (ground truth ratio is ∼ 1.0 and ∼ 1.1 for leg and arms respectively.) Inadequacy of self-consistency loss. Left most Col is input 2D pose. (a) Self-consistency alone is unable to recover the correct 3D skeletons. (b) With symmetry constraints, limb lengths become symmetric but may not have realistic ratios. (c) Adding 2D pose discriminator results in a geometrically consistent and realistic 3D skeletons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results on Human3.6M dataset. (Left to right) Color image with overlaid 2D pose points, estimated and groundtruth 3D skeleton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Examples of failure cases of our algorithm on Hu-man3.6M dataset. Ground truth 3D skeletons are shown in gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Examples of 3D pose reconstruction on images from MPII dataset (no ground-truth 3D skeleton). Each image shows overlaid 2D pose and the estimated 3D skeleton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Examples of 3D pose reconstruction on images from LSP dataset (no ground-truth 3D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) 2019</head><label></label><figDesc>1 arXiv:1904.04812v1 [cs.CV] 9 Apr 2019</figDesc><table><row><cell cols="4">In Real or Fake Estimated 3D Skeleton Transformed Random 2D 3D Skeleton Projection L adv</cell></row><row><cell>Lifting</cell><cell>Random 3D</cell><cell>Projection</cell><cell>2D Pose</cell></row><row><cell>Network</cell><cell>Transformation</cell><cell></cell><cell>Discriminator</cell></row><row><cell>Input 2D Pose</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L 2D</cell><cell></cell><cell>L 3D</cell><cell></cell></row><row><cell>Projection</cell><cell>Inverse 3D</cell><cell>Lifting</cell><cell></cell></row><row><cell></cell><cell>Transformation</cell><cell>Network</cell><cell></cell></row><row><cell>Recovered 2D Pose</cell><cell>3D Skeleton (In original coordinates)</cell><cell>Estimated 3D Skeleton</cell><cell>Real 2D Poses</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation studies.The architecture/loss ablations show the effect of various components on the unsupervised training. Ours: Adv + SS + DA + TD), when available, to obtain an error of 51mm on Human3.6M. It should be noted that the inference for the TD experiment is still done on single frames and the results can be further improved by applying temporal smoothness techniques on video sequences.</figDesc><table><row><cell>Su-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d human pose estimation via deep learning from 2d annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Brau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structureaware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Cong Phuoc Huynh. Can 3D pose be learned from 2D projections alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-toimage translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu Fish</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Computational Studies of Human Motion: Tracking and Motion Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ikemoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-view 3d human pose estimation in complex environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Model-based vision: a program to see a walking person. Image and Vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hogg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d human pose reconstruction using millions of exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2010 20th International Conference on</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Apostol Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised adversarial learning of 3d human pose from 2d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Ogaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Odagiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08244</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<idno>De- cember 2015. 2</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Fifth International Conference on</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A survey of computer vision-based human motion capture. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Granum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modelbased image analysis of human motion using constraint propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;rourke</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><forename type="middle">I</forename><surname>Badler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d human pose estimation using convolutional neural networks with 2d pose information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihye</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="156" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d human pose estimation with relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungheon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A semantic occlusion model for human pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umer</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on CVPR Workshops</title>
		<meeting>the IEEE Conference on CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classificationregression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">It&apos;s all relative: Monocular 3d human pose estimation from weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Matteo Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mat</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to generate synthetic data via compositing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visesh</forename><surname>Chari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Drpose3d: Depth ranking in 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="978" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3D human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashim</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Yong Seok Heo, and Il Dong Yun. Random tree walk toward instantaneous 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><surname>Ho Yub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soochahn</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kostantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
