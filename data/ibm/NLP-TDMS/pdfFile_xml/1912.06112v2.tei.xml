<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Unified Generative Adversarial Networks for Controllable Image-to-Image Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Unified Generative Adversarial Networks for Controllable Image-to-Image Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-GANs</term>
					<term>Controllable Structure</term>
					<term>Image-to-Image Translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a unified Generative Adversarial Network (GAN) for controllable image-to-image translation, i.e., transferring an image from a source to a target domain guided by controllable structures. In addition to conditioning on a reference image, we show how the model can generate images conditioned on controllable structures, e.g., class labels, object keypoints, human skeletons, and scene semantic maps. The proposed model consists of a single generator and a discriminator taking a conditional image and the target controllable structure as input. In this way, the conditional image can provide appearance information and the controllable structure can provide the structure information for generating the target result. Moreover, our model learns the image-to-image mapping through three novel losses, i.e., color loss, controllable structure guided cycle-consistency loss, and controllable structure guided self-content preserving loss. Also, we present the Fréchet ResNet Distance (FRD) to evaluate the quality of the generated images. Experiments on two challenging image translation tasks, i.e., hand gesture-to-gesture translation and cross-view image translation, show that our model generates convincing results, and significantly outperforms other state-of-the-art methods on both tasks. Meanwhile, the proposed framework is a unified solution, thus it can be applied to solving other controllable structure guided image translation tasks such as landmark guided facial expression translation and keypoint guided person image generation. To the best of our knowledge, we are the first to make one GAN framework work on all such controllable structure guided image translation tasks. Code is available at https://github.com/Ha0Tang/GestureGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-We propose a unified Generative Adversarial Network (GAN) for controllable image-to-image translation, i.e., transferring an image from a source to a target domain guided by controllable structures. In addition to conditioning on a reference image, we show how the model can generate images conditioned on controllable structures, e.g., class labels, object keypoints, human skeletons, and scene semantic maps. The proposed model consists of a single generator and a discriminator taking a conditional image and the target controllable structure as input. In this way, the conditional image can provide appearance information and the controllable structure can provide the structure information for generating the target result. Moreover, our model learns the image-to-image mapping through three novel losses, i.e., color loss, controllable structure guided cycle-consistency loss, and controllable structure guided self-content preserving loss. Also, we present the Fréchet ResNet Distance (FRD) to evaluate the quality of the generated images. Experiments on two challenging image translation tasks, i.e., hand gesture-to-gesture translation and cross-view image translation, show that our model generates convincing results, and significantly outperforms other state-of-the-art methods on both tasks. Meanwhile, the proposed framework is a unified solution, thus it can be applied to solving other controllable structure guided image translation tasks such as landmark guided facial expression translation and keypoint guided person image generation. To the best of our knowledge, we are the first to make one GAN framework work on all such controllable structure guided image translation tasks. Code is available at https://github.com/Ha0Tang/GestureGAN. Index Terms-GANs, Controllable Structure, Image-to-Image Translation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>G ENERATIVE Adversarial Networks (GANs) <ref type="bibr" target="#b16">[17]</ref> are generative models based on game theory, which have achieved impressive performance in a wide range of applications such as high-quality image generation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. To generate specific kinds of images, Mirza et al. <ref type="bibr" target="#b19">[20]</ref> propose Conditional GANs (CGANs), which comprise a vanilla GAN model and other external controllable structure information, such as class labels <ref type="bibr" target="#b15">[16]</ref>, reference images <ref type="bibr" target="#b1">[2]</ref>, object keypoints <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b3">[4]</ref>, human skeletons <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and semantic maps <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> In this paper, we mainly focus on the image-to-image translation task using CGANs. At a high level, current image-to-image translation techniques usually fall into one of two types: supervised/paired <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b24">[25]</ref> and unsupervised/unpaired <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, existing image-toimage translation frameworks are inefficient for the multidomain image-to-image translation task. For instance, given n different image domains, Pix2pix <ref type="bibr" target="#b1">[2]</ref> and Bicycle-GAN <ref type="bibr" target="#b2">[3]</ref> need to train A 2 n =n(n−1)=Θ(n 2 ) models. Cy-cleGAN <ref type="bibr" target="#b10">[11]</ref>, DiscoGAN <ref type="bibr" target="#b11">[12]</ref> and DualGAN <ref type="bibr" target="#b12">[13]</ref> need to train C 2 n = n(n−1) 2 =Θ(n 2 ) models, or n(n−1) generator/discriminator pairs since one model has two different generator/discriminator pairs for these methods. ComboGAN <ref type="bibr" target="#b13">[14]</ref> requires n=Θ(n) models. G 2 GAN <ref type="bibr" target="#b14">[15]</ref> needs to train two generators, i.e., the generation generator and the reconstruction generator, while StarGAN <ref type="bibr" target="#b15">[16]</ref> only needs one model. However, for some specific image-to-image translation applications such as hand gesture-to-gesture translation <ref type="bibr" target="#b21">[22]</ref> and person image generation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref>, n could be arbitrary large since hand gestures and human bodies in the wild can have arbitrary poses, sizes, appearances, locations, and self-occlusions.</p><p>To address these limitations, several works have been proposed to generate images based on controllable structures, e.g., object keypoints, human skeleton, and scene semantic maps. These works can be divided into three different categories: 1) Object keypoint guide methods. Reed et al. <ref type="bibr" target="#b20">[21]</ref> proposed GAWWN, which generates bird images conditioned on bird keypoints. Song et al. <ref type="bibr" target="#b4">[5]</ref> propose G2GAN for facial expression synthesis based on facial landmarks. Ma et al. propose PG 2 <ref type="bibr" target="#b3">[4]</ref> and a two-stage reconstruction pipeline <ref type="bibr" target="#b5">[6]</ref> achieving person image translation using a conditional image and a target pose image. 2) Human skeleton guided methods. Siarohin et al. <ref type="bibr" target="#b7">[8]</ref> introduce PoseGAN based on the human skeleton for human image generation. Tang et al. <ref type="bibr" target="#b21">[22]</ref> propose a novel GestureGAN conditioned hand skeleton for hand gesture-togesture image translation tasks. Yan et al. <ref type="bibr" target="#b6">[7]</ref> propose a method to generate human motion sequences with simple backgrounds using CGANs and human skeleton information. 3) Scene semantic guide methods. Wang et al. <ref type="bibr" target="#b24">[25]</ref> propose Pix2pixHD, which can be used for turning semantic label maps into photorealistic images or synthesizing portraits from face label maps. Park et al. <ref type="bibr" target="#b22">[23]</ref> propose the spatially-adaptive normalization, a simple but effective layer for synthesizing images given an input semantic layout. Regmi and Borji <ref type="bibr" target="#b9">[10]</ref> propose X-Fork and X-Seq, which aim to generate images across two drastically different views by using the guidance of semantic maps.</p><p>The aforementioned methods have achieved impressive results on the corresponding tasks. However, each of them is tailored for a specific application limiting their capability to generalize. Our framework does not impose any application-arXiv:1912.06112v2 [cs.CV] 2 Sep 2020  <ref type="bibr" target="#b0">[1]</ref>. (b) Adversarial learning methods, e.g., Pix2pix <ref type="bibr" target="#b1">[2]</ref> and BicycleGAN <ref type="bibr" target="#b2">[3]</ref>. (c) Keypoint-guided image generation methods, e.g., PG 2 <ref type="bibr" target="#b3">[4]</ref>, G2GAN <ref type="bibr" target="#b4">[5]</ref> and DPIG <ref type="bibr" target="#b5">[6]</ref>. (d) Skeleton-guided image generation methods, e.g., SAMG <ref type="bibr" target="#b6">[7]</ref> and PoseGAN <ref type="bibr" target="#b7">[8]</ref>. (e) Semantic-guided image generation methods, e.g., SelectionGAN <ref type="bibr" target="#b8">[9]</ref> and X-Fork <ref type="bibr" target="#b9">[10]</ref>. (f) Adversarial unsupervised learning methods, e.g., CycleGAN <ref type="bibr" target="#b10">[11]</ref>, DiscoGAN <ref type="bibr" target="#b11">[12]</ref> and DualGAN <ref type="bibr" target="#b12">[13]</ref>. (g) Multi-domain image translation methods, e.g., ComboGAN <ref type="bibr" target="#b13">[14]</ref>, G 2 GAN <ref type="bibr" target="#b14">[15]</ref> and StarGAN <ref type="bibr" target="#b15">[16]</ref>. (h) Proposed GAN model in this paper. Note that the proposed GAN model is a unified solution for controllable structure guided image-to-image translation problem, i.e., controllable structure C can be one of class label L, object keypoint K, human skeleton S or semantic map M . Notations: x and y are the real images; x and y are the generated images; x and y are the reconstructed images; K y is the keypoint of y; S y is the skeleton of y; M y is the semantic map of y; L x and L y are the class labels of x and y, respectively; C x and C y are the controllable structures of x and y, respectively; G, G X →Y and G Y →X represent generators; D, D Y and D X denote discriminators. specific constraint. This makes our setup considerably simpler than the other approaches (see <ref type="figure" target="#fig_0">Fig. 1</ref>). To achieve this goal, we propose a unified solution for controllable imageto-image translation. It allows generating high-quality images with arbitrary poses, sizes, structures, and locations in the wild. Our GAN model only consists of one generator and one discriminator, taking a conditional image and the novel controllable structures as inputs. In this way, the conditional image can provide appearance information and the controllable structures can provide structure information for generating the target image. In addition, to better learn the mapping between inputs and outputs, we propose three novel losses, i.e., color loss, controllable structure guided cycle-consistency loss, and self-content preserving loss. The proposed color loss can handle the problem of 'channel pollution' that is frequently occurring in generative models such as PG 2 <ref type="bibr" target="#b3">[4]</ref>, leading the generated images to be sharper and having higher quality. The proposed controllable structure guided cycle-consistency loss is more flexible than the one proposed in CycleGAN <ref type="bibr" target="#b10">[11]</ref>, reducing further the space of possible mappings between different domains. The proposed self-content preserving loss can preserve color composition, object identity, and global layout of generated images. These optimization loss functions and the proposed GAN framework are jointly trained in an end-to-end fashion to improve both fidelity and visual naturalness of the generated images. Furthermore, we propose the Fréchet ResNet Distance (FRD), which is a novel and better evaluation metric to evaluate the generated images of GANs. Extensive experiments on two challenging controllable image-to-image translation tasks with four different datasets, i.e., hand gesture-to-gesture translation and cross-view image translation, demonstrate that the proposed GAN model generates high-quality images with convincing details and achieves state-of-the-art performance on both tasks. Finally, the proposed GAN model is a general-purpose solution that can be applied to solve a wide variety of controllable structure guided image-to-image translation problems.</p><p>In summary, the contributions of this paper are as follows:</p><p>• We propose a unified GAN model for controllable imageto-image translation tasks, which can generate target images with arbitrary poses, sizes, structures, and locations in the wild. • We propose three novel objective functions to better optimize the proposed GAN model, i.e., color loss, controllable structure guided cycle-consistency loss, and self-content preserving loss. These optimization functions and the proposed GAN framework are jointly trained in an end-to-end fashion to improve both the quality and fidelity of the generated images. • We propose an efficient Fréchet ResNet Distance (FRD) metric to evaluate the similarity of the real and generated images, which is more consistent with human judgment. • Qualitative and quantitative results demonstrate the superiority of the proposed GAN model over the state-of-the-art methods on two challenging controllable image translation tasks with four datasets, i.e., hand gesture-to-gesture translation and cross-view image translation. Part of this work has been published in <ref type="bibr" target="#b21">[22]</ref>. We extend it in numerous ways: 1) We extend GestureGAN proposed in <ref type="bibr" target="#b21">[22]</ref> to a unified GAN framework for handling all controllable image-to-image translation tasks. 2) We further tune our whole pipeline and improve its performance and generalizability for hand gesture-to-gesture translation and crossview image translation by employing three additional losses, i.e., controllable structure guided self-content preserving loss, perceptual loss, and Total Variation loss. Moreover, we extend the one-cycle framework in <ref type="bibr" target="#b21">[22]</ref> to a two-cycle framework and validate the effectiveness. 3) We extend the experimental evaluation provided in <ref type="bibr" target="#b21">[22]</ref> in several directions. First, we conduct extensive experiments on two challenging generative tasks with four different datasets, demonstrating the wide application scope of our GAN framework. Second, we conduct exhaustive ablation studies to evaluate each component of the proposed method. Third, we investigate the influence of hyperparameters on generation performance. Forth, we compare the model parameters of different methods. Lastly, we provide arbitrary image translation results on both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Generative Adversarial Networks (GANs) are unsupervised learning methods and have been proposed in <ref type="bibr" target="#b16">[17]</ref>. Recently, GANs have shown promising results in various applications, e.g., image generation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Existing approaches employ the idea of GANs for conditional image generation, such as image-to-image translation <ref type="bibr" target="#b1">[2]</ref>, text-to-image translation <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, audio-to-image <ref type="bibr" target="#b28">[29]</ref>, and sketch generation <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. The key success of GANs is the adversarial loss, which allows the model to generate images that are indistinguishable from real images, and this is exactly the goal that many tasks aim to optimize. In this paper, we mainly focus on image-to-image translation tasks. Image-to-Image Translation is the problem of transferring an image from a source domain to a target domain, which uses input-output data to learn a parametric mapping between inputs and outputs, e.g., Isola et al. <ref type="bibr" target="#b1">[2]</ref> propose Pix2pix, which uses a conditional GAN to learn a translation function from input to output image domain with paired training data. However, collecting large sets of image pairs is often prohibitively expensive or unfeasible. To solve this limitation, Zhu et al. <ref type="bibr" target="#b10">[11]</ref> propose CycleGAN, which can learn to translate between domains without paired input-output examples by using the cycle-consistency loss. Similar ideas have been proposed in several works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>. For example, Choi et al. <ref type="bibr" target="#b15">[16]</ref> introduce StarGAN, which can perform image-toimage translation for multiple domains.</p><p>However, existing image-to-image translation models are inefficient and ineffective. For example, with n image domains, CycleGAN <ref type="bibr" target="#b10">[11]</ref>, DiscoGAN <ref type="bibr" target="#b11">[12]</ref>, and DualGAN <ref type="bibr" target="#b12">[13]</ref> need to train 2C 2 n =n(n−1)=Θ(n 2 ) generators and discriminators, while Pix2pix <ref type="bibr" target="#b1">[2]</ref> and BicycleGAN <ref type="bibr" target="#b2">[3]</ref> have to train A 2 n =n(n−1)=Θ(n 2 ) generator/discriminator pairs. Recently, Anoosheh et al. propose ComboGAN <ref type="bibr" target="#b13">[14]</ref>, which only needs to train n generator/discriminator pairs for n different image domains, having a complexity of Θ(n). Tang et al. <ref type="bibr" target="#b14">[15]</ref> propose G 2 GAN, which can perform image-to-image translations for multiple domains using only two generators, i.e., the generation generator and the reconstruction generator. Additionally, Choi et al. <ref type="bibr" target="#b15">[16]</ref> propose StarGAN, in which a single generator and a discriminator can perform unpaired image-to-image translations for multiple domains. Although the computational complexity of StarGAN is Θ(1), this model has only been validated on the face attributes modification task with clear background and face cropping. More importantly, for some specific image-to-image translation tasks such as hand gesture-to-gesture translation <ref type="bibr" target="#b21">[22]</ref> and person image generation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref> tasks, the image domains could be arbitrarily large, e.g., hand gestures and human bodies in the wild can have arbitrary poses, sizes, appearances, structures, locations, and self-occlusions. The aforementioned approaches are not effective in solving these specific situations.</p><p>Controllable Image-to-Image Translation. To fix these limitations, several recent works have been proposed to generate persons, birds, faces and scene images based on controllable structures, i.e., object keypoints <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, human skeletons <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b33">[34]</ref> and semantic maps <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b34">[35]</ref>. In this way, controllable structures provide four types of information to guide the image generation process, i.e., category, scale, orientation, and location. Although significant efforts have been made to achieve controllable image-to-image translation in the area of computer vision, there has been very limited research on universal controllable image translation. That is, the typical problem with the aforementioned generative models is that each of them is tailored for a specific application, which greatly limits the generalization ability of the proposed models. To handle this problem, we propose a novel and unified GAN model, which can be tailored for handling all kinds of problem settings of controllable structure guided image-toimage translation, including object keypoint guided generative tasks, human skeleton guided generative tasks and semantic map guided generative tasks, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL DESCRIPTION</head><p>In this section, we present the details of the proposed GAN model <ref type="figure">(Fig. 2</ref>). We present a controllable structure guided generator, which utilizes the images from one domain and conditional controllable structures from another domain as inputs and produces images in the target domain. Moreover, we propose a novel discriminator which also takes the controllable structure into consideration. The proposed GAN model is trained in an end-to-end fashion mutually benefiting from the generator and the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Controllable Structure Guided Generator</head><p>Controllable Structure Guided Generation. Image-to-image translation tasks, such as hand gesture-to-gesture transla- <ref type="figure">Fig. 2</ref>: Pipeline of the proposed unified GAN model for controllable image-to-image translation tasks. The proposed GAN framework consists of a single generator G and an associated adversarial discriminator D, which takes a conditional image x and a controllable structure C y as input to produce the target image y . We have two cycles and here we only show one of them. Note that the controllable structure C y can be class labels, object keypoints, human skeletons, semantic maps, etc.</p><p>tion <ref type="bibr" target="#b21">[22]</ref>, person image generation <ref type="bibr" target="#b3">[4]</ref>, facial expressionto-expression translation <ref type="bibr" target="#b32">[33]</ref> and cross-view image translation <ref type="bibr" target="#b9">[10]</ref> are very challenging. In these tasks, the source domain and the target domain may have large deformations. Moreover, these tasks can be treated as an infinite mapping problem leading to ambiguity issues in the translation process. For instance, in the hand gesture-to-gesture translation task, if you input a hand gesture image to the generator, it has no idea which gestures should output.</p><p>To fix this limitation, we employ controllable structures as conditional guidance to guide the image generation process. The controllable structures can be class labels, object keypoints, human skeletons or semantic maps, etc. Following <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> we generate the controllable structures using deep models pretrained from other large-scale datasets, e.g., we apply the pose estimator OpenPose <ref type="bibr" target="#b35">[36]</ref> to obtain approximate human body poses and hand skeletons. Specifically, as shown in <ref type="figure">Fig. 2</ref>, we concatenate the input conditional image x from the source domain and the controllable structure C y from a target domain, and input them into the generator G and synthesize the target image y =G(x, C y ). In this way, the ground-truth controllable structure C y provides stronger supervision and structure information to guide the image-toimage translation in the deep network, while the conditional image x provides the appearance information to produce the final result y . Controllable Structure Guided Cycle. Guided by the controllable structure C y , our generator can produce the corresponding image y . However, state-of-the-art controllable image-to-image translation methods such as <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> only consider the image translation process, i.e., from the source domain to the target domain. Different from them, we consider both the image translation process and image reconstruction process, i.e., from the source domain to the target domain and from the target domain back to the source domain. The intuition behind this is that if we translate from one domain to the other and back again we should arrive at where we started. The proposed controllable structure guided cycle is different from the cycle proposed in CycleGAN <ref type="bibr" target="#b10">[11]</ref>, which uses a cycle-consistency loss to preserve the content of its input images while changing only the domain-related part of the inputs. The main difference is that CycleGAN can only handle two different domains, while an image translation problem such as hand gesture-to-gesture translation task has arbitrary domains, e.g., hand gestures in the wild can have arbitrary poses, sizes, appearances, structures, locations, and self-occlusions. Therefore, we need the controllable structure to guide the learning of the proposed cycle. The proposed controllable structure guided cycle is also different from the cycle proposed in StarGAN <ref type="bibr" target="#b15">[16]</ref>, which translates an original image into an image in the target domain and then reconstructs the original image from the translated image through feeding the target label. However, class labels can only provide the category information, while the controllable structure can provide four types of information for generation at the same time, i.e., category, location, scale, and orientation. Specifically, as shown in <ref type="figure">Fig. 2</ref>, the generated image y and the controllable structure C x are concatenated to input into the generator G. Thus, the proposed controllable structure guided cycle can be formulated as follows,</p><formula xml:id="formula_0">x = G(y , C x ) = G(G(x, C y ), C x ) ≈ x.</formula><p>(1)</p><p>Note that we use a single generator twice, first to translate an original image into an image in the target domain and then to reconstruct the original image from the translated image. Image translation and image reconstruction are simultaneously considered in our framework, constructing a full mapping cycle. Similarly, we have another cycle,</p><formula xml:id="formula_1">y = G(x , C y ) = G(G(y, C x ), C y ) ≈ y.<label>(2)</label></formula><p>Controllable Structure Guided Cycle-Consistency Loss.</p><p>To better optimize the proposed cycle, we propose a novel controllable structure guided cycle-consistency loss. It is worth noting that CycleGAN <ref type="bibr" target="#b10">[11]</ref> is different from the Pix2pix model <ref type="bibr" target="#b1">[2]</ref> as the training data in CycleGAN is unpaired. Cycle-GAN introduces the cycle-consistency loss to enforce forwardbackward consistency. In that case, the cycle-consistency loss can be regarded as 'pseudo' pairs of training data even though we do not have the corresponding data in the target domain which corresponds to the input data from the source domain. However, in this paper, we introduce the controllable structure guided cycle-consistency loss for the paired image-to-image translation task. This loss ensures the consistency between source images and the reconstructed image, and it can be expressed as,</p><formula xml:id="formula_2">L cyc (G, C x , C y ) =E x,Cx,Cy [||x − G(G(x, Cy), C x )|| 1 ] +E y,Cx,Cy ||y − G(G(y, Cx), C y )|| 1 ,<label>(3)</label></formula><p>where G is the generator; x and y are the input images; C x and C y are the controllable structures of image x and y, respectively. As mentioned before, we use the same generator G twice. Equipped with this loss, the proposed generator G further improves the image quality due to its implicit data augmentation effect from a multi-task learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Controllable Structure Guided Discriminator</head><p>Conditional GANs (CGANs) such as Pix2pix <ref type="bibr" target="#b1">[2]</ref> learn the mapping G(x) → y, where x is the input conditional image. Generator G is trained to generate image y that cannot be distinguished from 'real' image y by an adversarially trained discriminator D, while the discriminator D is trained as well as possible to detect the 'fake' images generated by the generator G. The objective function of CGANs is defined as follows,</p><formula xml:id="formula_3">LcGAN (G, D) = Ex,y [log D(x, y)] + Ex [log(1 − D(x, G(x)))] ,<label>(4)</label></formula><p>where generator G tries to minimize this objective function while the discriminator D tries to maximize it. Thus, the solution is</p><formula xml:id="formula_4">G * = arg min G max D L cGAN (G, D).</formula><p>In this paper, we try to learn two mappings through one generator, i.e., G(x, C y ) →y and G(y , C x ) →x. As shown in <ref type="figure">Fig. 2</ref>, in order to learn both mappings, we employ the controllable structures explicitly. Thus, the adversarial losses of the two mappings are defined respectively, as follows:</p><formula xml:id="formula_5">L adv (G, D, Cy) =E [x,Cy ],y [log D([x, Cy], y)] +E [x,Cy ] [log(1 − D([x, Cy], G(x, Cy)))] ,<label>(5)</label></formula><p>where C y is the controllable structure of image y and [·, ·] represents the concatenation operation. This controllable structure guided input encourages D to capture the local-aware information and generate semantic-matched target images. Similarly, we have another adversarial loss,</p><formula xml:id="formula_6">L adv (G, D, Cx) =E [y,Cx],x [log D([y, Cx], x)] +E [y,Cx] [log(1 − D([y, Cx], G(y, Cx)))] .<label>(6)</label></formula><p>Thus, the final adversarial loss is the sum of Eq. <ref type="formula" target="#formula_5">(5)</ref> and <ref type="formula" target="#formula_6">(6)</ref>,</p><formula xml:id="formula_7">L adv (G, D) = L adv (G, D, Cx) + L adv (G, D, Cy).<label>(7)</label></formula><p>C. Optimization Objective Color Loss. Previous work indicates that mixing the adversarial loss with a traditional loss such as L1 loss <ref type="bibr" target="#b1">[2]</ref> or L2 loss <ref type="bibr" target="#b0">[1]</ref> between the generated images and the ground truth images improves the generation performance. The definitions of L1 and L2 losses are:</p><formula xml:id="formula_8">L L{1,2} (G) =E [x,Cy],y y − G([x, C y ]) {1,2} , +E [y,Cx],x x − G([y, C x ]) {1,2} .<label>(8)</label></formula><p>However, we observe that the existing image-to-image translation models such as PG 2 <ref type="bibr" target="#b3">[4]</ref> cannot retain the holistic color of the input images. An example is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, where PG 2 is affected by the pollution issue and produces more unrealistic regions. Therefore, to address this limitation we introduce a novel channel-wise color loss. Traditional generative models convert an entire image into another, which leads to the 'channel pollution' problem. However, the color loss treats r, g, and b channels independently and generates only one channel each time, and then these three channels are combined to produce the final image. Intuitively, since the generation of a three-channel image space is much more complex than the generation of single-channel image space, leading to a higher possibility of artifacts, we independently generate each channel. The objective of r, g and b channel losses can be defined as follows,</p><formula xml:id="formula_9">L colorc{1,2} (G) =E [xc,Cy],yc y c − G([x c , C y ]) {1,2} +E [yc,Cx],xc x c − G([y c , C x ]) {1,2} .<label>(9)</label></formula><p>where c∈{r, g, b}; x r , x g , and x b denote the r, g, and b channel of image x, respectively, similar to y r , y g and y b ; · 1 and · 2 represent L1 and L2 distance losses. Thus, the color L1 and L2 losses can be expressed as,</p><formula xml:id="formula_10">L color{1,2} (G) = L Colorr{1,2} + L Colorg{1,2} + L Color b {1,2} .</formula><p>(10) In Eq. (8), one channel is always influenced by the errors from other channels. On the contrary, if we compute the loss for each channel independently as shown in Eq. (10), we can avoid such influence. In this way, the error in one channel will not influence other channels. We observe that this novel loss can improve the image quality in our experimental section.</p><p>Controllable Structure Guided Self-Content Preserving Loss. To preserve the image content information (e.g., color composition, object identity, global layout) between the input and output, CycleGAN <ref type="bibr" target="#b10">[11]</ref> proposes the identity preserving loss. However, different from <ref type="bibr" target="#b10">[11]</ref>, we propose the controllable structure guided self-content preserving loss, which can be expressed as follows,</p><formula xml:id="formula_11">L con (G) =E x,Cx [||x − G(x, C x )|| 1 ] +E y,Cy ||y − G(y, C y )|| 1 .<label>(11)</label></formula><p>We aim to minimize the L1 difference between the real image x/y and the self-content preserving image G(x, C x )/G(y, C y ) for content information preservation. In this way, we regularize the generator to be near a self-content mapping when real images and self controllable structures are provided as the input to the generator.</p><p>Perceptual Loss measures the perceptual similarity in a highlevel feature space. This loss has been shown to be useful for many tasks such as style transfer <ref type="bibr" target="#b36">[37]</ref> and image transla-tion <ref type="bibr" target="#b24">[25]</ref>. The formulation of this loss is as follows:</p><formula xml:id="formula_12">L vgg (y ) = 1 W i,j H i,j Wi,j w=1 Hi,j h=1 ||F k (y) − F k (G(x, C y ))|| 1 ,<label>(12)</label></formula><p>where F k indicates the feature map obtained by the k-th convolution within the VGG network <ref type="bibr" target="#b37">[38]</ref>, W i,j and H i,j are the dimensions of the respective feature maps within the VGG network. Similarly, we have another loss for the generated image x , which can be formulated as,</p><formula xml:id="formula_13">L vgg (x ) = 1 W i,j H i,j Wi,j w=1 Hi,j h=1 ||F k (x) − F k (G(y, C x ))|| 1 .</formula><p>(13) Thus, the final perceptual loss is the sum of both items, i.e., L vgg =L vgg (y )+L vgg (x ). Total Variation Loss. Usually, the images synthesized by GAN models have many unfavorable artifacts, which deteriorate the visualization and recognition performance. We impose the Total Variation (TV) loss <ref type="bibr" target="#b36">[37]</ref> on the final synthesized image y to alleviate this issue,</p><formula xml:id="formula_14">Ltv(y ) = C c=1 W,H w,h=1 y (w + 1, h, c) − y (w, h, c) + y (w, h + 1, c) − y (w, h, c) ,<label>(14)</label></formula><p>where W and H represent the width and height of the generated image y . Similarly, we have another loss for the generated image x and the final total variation loss is the sum of both. Overall Loss. The total optimization loss is a weighted sum of the above losses. Generator G and discriminator D are trained in an end-to-end fashion to optimize the following min-max function,</p><formula xml:id="formula_15">G * = arg min G max D (L adv + λ color L color + λ cyc L cyc + λ con L con + λ vgg L vgg + λ tv L tv ),<label>(15)</label></formula><p>where λ color , λ cyc , λ con , λ vgg and λ tv are five hyperparameters controlling the relative importance of these six losses. Solving this min-max problem enables our model to generate the target images guided by controllable structures in a photo-realistic manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head><p>Network Architecture. We adopt our generator architecture G from <ref type="bibr" target="#b36">[37]</ref>, which has shown effective in many applications such as unsupervised image translation <ref type="bibr" target="#b10">[11]</ref> and neural style transfer <ref type="bibr" target="#b36">[37]</ref>. We use 9 residual blocks for both 64×64 and 256×256 image resolutions. The last layer of the generator is the Tanh activation function. For the discriminator D, we adopt 70×70 PatchGAN proposed in <ref type="bibr" target="#b1">[2]</ref>. PatchGAN tries to decide if any 70×70 patch in an image is real or fake. The final layer of discriminators employs the Sigmoid activation function to produce a 1-dimensional output. Therefore, we are averaging all responses to provide the ultimate output of the discriminator. Optimization Details. We observe that the proposed controllable structure guided discriminator achieves promising gen-eration results. However, to further improve the image quality, we use the scheme of training a dual-discriminator instead of one discriminator as a more stable way to improve the capacity of discriminators similar to Nguyen et al. <ref type="bibr" target="#b38">[39]</ref>, which have demonstrated that they improve the ability of discriminator to generate more photo-realistic images. To be more specific, dual-discriminator architecture can better approximate optimal discriminator. If one of the discriminators is trained to be far superior over the generators, the generators can still receive instructive gradients from the other one. In addition to the proposed controllable structure guided discriminator, we use a traditional one, which takes the input image and the generated image as input. Both discriminators have the same network architecture structure.</p><p>We follow the standard optimization method in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b1">[2]</ref> to optimize the proposed GAN model, i.e., one gradient descent step on discriminators and generator alternately. We first train generator G with discriminators fixed, and then train discriminators with generator G fixed. In addition, as suggested in <ref type="bibr" target="#b16">[17]</ref>, we train to maximize log D([x, C y ], y ) rather than log(1−D([x, C y ], y )). Moreover, in order to slow down the rate of D relative to G we divide the objective function by 2 while optimizing D. The proposed GAN model is trained in an end-to-end fashion. We employ the Adam <ref type="bibr" target="#b39">[40]</ref> optimizer with momentum terms β 1 =0.5 and β 2 =0.999 as our solver. The initial learning rate for Adam is 0.0002.</p><p>We follow <ref type="bibr" target="#b3">[4]</ref> and exploit OpenPose <ref type="bibr" target="#b40">[41]</ref> to detect the ground-truth hand skeletons as training data for the hand gesture-to-gesture translation task. We then connect the 21 keypoints (hand joints) detected by OpenPose to obtain the hand skeleton. The hand skeleton image visually contains richer hand structure information than the hand keypoint image. In hand skeleton images, the hand joints are connected by the lines with a width of 4 and with white color. In addition, we follow <ref type="bibr" target="#b9">[10]</ref> and use RefineNet <ref type="bibr" target="#b41">[42]</ref> to generate the groundtruth semantic maps as training data for the cross-view image translation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Fréchet ResNet Distance</head><p>We also propose a novel evaluation metric to measure the image quality of the generated images by GAN models, i.e., Fréchet ResNet Distance (FRD). FRD provides an alternative method to quantify the quality of synthesis and is similar to the FID proposed by <ref type="bibr" target="#b42">[43]</ref>. FID is a measure of similarity between two datasets of images. The authors have shown that the FID is more robust to noise than Inception Score (IS) and correlates well with the human judgment of visual quality <ref type="bibr" target="#b42">[43]</ref>. To calculate FID between two image domains y and y , they first embed both into a feature space F given by an Inception model. Then viewing the feature space as a continuous multivariate Gaussian as suggested in <ref type="bibr" target="#b42">[43]</ref>, Fréchet distance between the two Gaussians is used to quantify the quality of the data. The definition of FID is:</p><formula xml:id="formula_16">FID(y, y ) = µ y − µ y 2 2 +Tr( y + y − 2( y y ) 1 2 ),<label>(16)</label></formula><p>where (µ y , y ) and (µ y , y ) are the mean and the covariance of the data distribution and model distribution, re-spectively. Note that we regard the images in y and y as two wholes respectively, and then calculate the Fréchet distance between y and y for calculating FID.</p><p>Unlike FID, which calculates the distance between two distributions, the proposed FRD is inspired by the feature matching method <ref type="bibr" target="#b43">[44]</ref>, and separately calculates the Fréchet distance between each generated image and the corresponding real image from a semantic point of view. In this way, images from the two domains do not affect each other when computing the Fréchet distance. Moreover, for FID the number of samples should be greater than the dimension of the coding layer, while the proposed FRD does not have this limitation. We denote y i and y i as images in the domain y and y , respectively. For calculating FRD, we first embed both images y i and y i into a feature space F with 1,000 dimensions given by a ResNet50 pretrained model. We then calculate the Fréchet distance between two feature maps f (y i ) and f (y i ). The Fréchet distance F (f (y i ), f (y i )) is defined as the infimum over all reparameterizations α and β of [0, 1] of the maximum over all t ∈ [0, 1] of the distance in F between f (y i )(α(t)) and f (y i )(β(t)), where α and β are continuous, non-decreasing surjections of the range [0, 1]. The proposed FRD is a measure of similarity between the feature vector of the real image f (y i ) and the feature vector of the generated image f (y i ) by calculating the Fréchet distance between them. The Fréchet distance is defined as the minimum cord-length sufficient to join a point traveling forward along f (y i ) and one traveling forward along f (y i ), although the rate of travel for each point may not necessarily be uniform. Thus, the definition of FRD between two image domains y and y is:</p><formula xml:id="formula_17">FRD(y, y ) = 1 N N i=1 inf α,β max t∈[0,1] d f (y i )(α(t)), f (y i )(β(t)) ,<label>(17)</label></formula><p>where d is the distance function of F and N is the total number of images in y and y domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To explore the generality of the proposed GAN model, we evaluate the proposed model on a variety of tasks and datasets, including hand gesture-to-gesture translation and cross-view image translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Hand Gesture-to-Gesture Translation</head><p>Datasets. We follow GestureGAN <ref type="bibr" target="#b21">[22]</ref> and evaluate the proposed GAN model on two hand gesture datasets, i.e., NTU Hand Digit <ref type="bibr" target="#b44">[45]</ref> and Creative Senz3D <ref type="bibr" target="#b45">[46]</ref>, which include different hand gestures. We use the hand gesture images and filter out failure cases in hand estimation for both training and testing sets. 1) NTU Hand Digit <ref type="bibr" target="#b44">[45]</ref> contains 10 hand gestures (e.g., decimal digits from 0 to 9) color images and depth maps collected with a Kinect sensor under cluttered backgrounds. We randomly select 84,636 pairs, each of which is comprised of two images of the same person but different gestures. 9,600 pairs are randomly selected for the testing subset and the rest of 75,036 pairs as the training set. 2) Creative Senz3D <ref type="bibr" target="#b45">[46]</ref> includes static hand gestures performed by 4 people, each performing 11 different gestures repeated 30 times each in the front of a Creative Senz3D camera. We randomly select 12,800 pairs and 135,504 pairs as the testing and training set, each pair is composed of two images of the same person but different gestures. Parameter Settings. For both datasets, we do left-right flip and random crops for data augmentation. For optimization, models are trained with a batch size of 4 for 20 epochs on both datasets. At inference time, we follow the same settings of PG 2 <ref type="bibr" target="#b3">[4]</ref> to randomly select the target keypoint or skeleton. Evaluation Metrics. Following GestureGAN <ref type="bibr" target="#b21">[22]</ref>, we use Peak Signal-to-Noise Ratio (PSNR), Inception Score (IS), Fréchet Inception Distance (FID), and the proposed FRD to evaluate the quality of generated images. State-of-the-Art Comparisons. We compare the proposed model with the most related works, i.e., PG 2 <ref type="bibr" target="#b3">[4]</ref>, SAMG <ref type="bibr" target="#b6">[7]</ref>, PoseGAN <ref type="bibr" target="#b7">[8]</ref>, DPIG <ref type="bibr" target="#b5">[6]</ref> and GestureGAN <ref type="bibr" target="#b21">[22]</ref>. PG 2 and DPIG try to generate a person image with different poses based on conditional person images and target keypoints. SAMG and PoseGAN explicitly employ human skeleton information to generate person images. Note that SAMG adopts a CGAN to generate motion sequences based on appearance information and skeleton information by exploiting frame-level smoothness. We re-implemented this model to generate a single frame for a fair comparison. These methods are paired image-toimage models and comparison results are shown in <ref type="figure" target="#fig_2">Fig. 4</ref> and 5. As we can see in both figures, the proposed model consistently produces sharper images with convincing details compared with other baselines on both datasets. We also note that the proposed GAN model is more robust than existing methods as shown in the first row of <ref type="figure" target="#fig_2">Fig. 4</ref>. Existing methods are easy to overfit since they generate the dropping arm as shown in the white dotted box while the proposed model failed to generate it. It is hard to generate the dropping arm since no guidance has been provided to generate it, while exiting methods just simply memorize the blocks from training images to generate new ones rather than to learn the representations between different images.</p><p>Moreover, we also provide quantitative results in <ref type="table">Table I</ref>, and we can see that the proposed GAN model produces more photo-realistic results that other baselines on all metrics expect IS. This phenomenon can also be observed in PG 2 <ref type="bibr" target="#b3">[4]</ref>, GestureGAN <ref type="bibr" target="#b21">[22]</ref>, and other super-resolution work such as <ref type="bibr" target="#b36">[37]</ref>, i.e., sharper results have a lower IS. Finally, we also show some results of the arbitrary hand gesture-to-gesture translation on NTU Hand Digit dataset in <ref type="figure" target="#fig_6">Fig. 8</ref>. Given a single image and several hand skeletons, the proposed model can generate the corresponding hand gestures. User Study. We follow the same settings as in <ref type="bibr" target="#b1">[2]</ref> to perform an Amazon Mechanical Turk (AMT) perceptual study and gather data from 50 participants per algorithm we tested. Specifically, participants were presented a sequence of pairs of images, a 'real' image and a 'fake' image (generated by our algorithm or a baseline), and asked to click on the image they thought was real. The first 10 images of each session were practice and feedback was given. The remaining 40 images were used to assess the rate at which each algorithm fooled participants. Each session only tested a single algorithm, and   <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>NTU Hand Digit Senz3D participants were only allowed to complete a single session. The results on NTU Hand Digit and Senz3D datasets compared with the baseline models are shown in <ref type="table">Table I</ref>. We observe that the proposed model consistently achieves the best performance compared with these baselines.</p><formula xml:id="formula_18">PSNR ↑ IS ↑ AMT ↑ FID ↓ FRD ↓ PSNR ↑ IS ↑ AMT ↑ FID ↓ FRD ↓<label>PG</label></formula><p>FID v.s. FRD. We also compare the performance between FID and the proposed FRD metric. The results are shown in <ref type="table">Table I</ref> and we can observe that FRD is more consistent with the human judgment, i.e., the AMT score, than the FID metric. Moreover, we observe that the difference in FRD between GestureGAN and the other methods is not as obvious as in the results from the user study, i.e., the AMT metric. The reason is that FRD calculates the Fréchet distance between the feature maps extracted from the real image and the generated image using CNNs which are trained with semantic labels. Thus, these feature maps are employed to reflect the semantic distance between the images. The semantic distance between the images is not very large considering they are all hands. On the contrary, the user study measures the generation quality from a perceptual level. The difference on the perceptual level is more obvious than on the semantic level, i.e., the generated images with small artifacts show minor differences on the feature level, while are being judged with a significant difference from the real images by humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-View Image Translation</head><p>Datasets. We follow <ref type="bibr" target="#b9">[10]</ref> and conduct the experiments on two public datasets: 1) For Dayton <ref type="bibr" target="#b46">[47]</ref>, following the same setting of <ref type="bibr" target="#b9">[10]</ref>, we select 76,048 images and create a train/test split of 55,000/21,048 pairs. The images in the original dataset have 354×354 resolution. We resize them to 256×256. 2) CVUSA <ref type="bibr" target="#b47">[48]</ref> consists of 35,532/8,884 image pairs in train/test split. Following <ref type="bibr" target="#b9">[10]</ref>, the aerial images are center-cropped to 224×224 and resized to 256×256. For the ground-level images and corresponding segmentation maps, we take the first quarter of both and resize them to 256×256. Parameter Settings. We follow <ref type="bibr" target="#b9">[10]</ref> and all images are scaled to 256×256, and we enabled random crops for data augmentation. The low-resolution experiments on Dayton are carried out for 100 epochs with a batch size of 16, whereas   the high-resolution experiments for this dataset are trained for 35 epochs with a batch size of 4. For CVUSA, we follow the same setup as in <ref type="bibr" target="#b9">[10]</ref> and train our network for 30 epochs with a batch size of 4. Evaluation Metrics. We follow <ref type="bibr" target="#b9">[10]</ref> and use Inception Score (IS), top-k prediction accuracy, KL score, Structural-Similarity (SSIM), PSNR, and Sharpness Difference (SD) for the quantitative analysis. Moreover, we employ LPIPS <ref type="bibr" target="#b48">[49]</ref> to evaluate the quality of the generated images. LPIPS uses pretrained deep models to evaluate the similarity, which highly agrees well with humans' perception. Specifically, we use the default pretrained AlexNet provided by the authors <ref type="bibr" target="#b48">[49]</ref> to calculate the LPIPS metric.</p><p>State-of-the-Art Comparison. We compare the proposed model with five recently proposed state-of-the-art methods on the cross-view image translation task, i.e., Pix2pix <ref type="bibr" target="#b1">[2]</ref>, Zhai et al. <ref type="bibr" target="#b49">[50]</ref>, X-Fork <ref type="bibr" target="#b9">[10]</ref>, X-Seq <ref type="bibr" target="#b9">[10]</ref> and SelectionGAN <ref type="bibr" target="#b8">[9]</ref>. The comparison results in higher resolution on the Dayton and CVUSA dataset are shown in <ref type="figure" target="#fig_4">Fig. 6 and 7</ref>, respectively. We can see that the proposed model generates better results against other baselines in terms of detail preservation and translation visual effects. In addition, it can be seen that our method generates more clear details on objects/scenes such as road, trees, and clouds than SelectionGAN in the generated groundlevel images (zoom-in for details in <ref type="figure" target="#fig_4">Fig. 6</ref>). For the generated aerial images, we can observe that grass, trees, and house roofs are well-rendered compared to others. Moreover, the results generated by our method are closer to the ground truth in layout and structure. The quantitative comparison results are shown in Tables II, III, IV and V. We can observe the significant improvement of the proposed model in these tables. The proposed model consistently outperforms Pix2pix, Zhai et al., X-Fork, and X-Seq on all the metrics. Moreover, comparing against SelectionGAN, the proposed model still achieves competitive performance on all metrics excepting SSIM, PSNR, and SD. In most cases of the a2g direction in <ref type="table" target="#tab_1">Tables II and III</ref> we achieve a slightly lower performance as compared with SelectionGAN. However, the proposed method consistently achieves better performance than SelectionGAN on the LPIPS metric as shown in <ref type="table" target="#tab_4">Table V</ref>, which agrees more with human judgments as indicated in <ref type="bibr" target="#b48">[49]</ref>. We also report both FID and FRD results compared with the most related SelectionGAN in Tables VI and VII. We can see that the proposed method achieves better results than SelectionGAN in most cases. Finally, we also note that SelectionGAN is carefully designed for the cross-view image translation task while the proposed model is a generic framework. Arbitrary Cross-View Image Translation. Existing methods such as Zhai et al. <ref type="bibr" target="#b49">[50]</ref>, Pix2pix <ref type="bibr" target="#b1">[2]</ref>, X-Fork <ref type="bibr" target="#b9">[10]</ref> and X-Seq [10] focus on the cross-view image translation task.  However, this task is essentially an ill-posed problem and has limited scalability and robustness in handling more than two viewpoints. A recent work SelectionGAN <ref type="bibr" target="#b8">[9]</ref> extends the cross-view image translation task to a more generic task of the problem, i.e., the arbitrary cross-view image translation, in which a single input view can be translated to different target views. For the arbitrary cross-view image translation, conditional labels are usually required since learning a oneto-many mapping is more challenging and extremely hard to optimize. Similarly to the arbitrary hand gesture-to-gesture translation in <ref type="figure" target="#fig_6">Fig. 8</ref>, we show several results of arbitrary crossview image translation on Dayton in <ref type="figure" target="#fig_7">Fig. 9</ref>. We believe this task has many applications such as cross-view image geolocalization. Network Parameter Comparisons. We compare the overall network parameter with Pix2pix <ref type="bibr" target="#b1">[2]</ref>, X-Fork <ref type="bibr" target="#b9">[10]</ref>, X-Seq <ref type="bibr" target="#b9">[10]</ref> and SelectionGAN <ref type="bibr" target="#b8">[9]</ref> on cross-view image translation task. Results are shown in <ref type="table" target="#tab_1">Table VIII</ref>. As we can see, the proposed model achieves superior model capacity and produces better generation performance comparing with existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We perform an ablation study in the a2g (aerial-to-ground) direction on Dayton for cross-view image translation. Following <ref type="bibr" target="#b8">[9]</ref>, to reduce the training time, we randomly select 1/3 samples from the whole 55,000/21,048 samples, i.e., around 18,334 samples for training and 7,017 samples for testing.</p><p>Baseline Models. The proposed GAN model has 9 baselines (A, B, C, D, E1x, E2x, E3, E4x, F) as shown in <ref type="table" target="#tab_8">Table IX</ref>. Baseline A uses a CycleGAN model <ref type="bibr" target="#b10">[11]</ref> and generates y using an unpaired image x. Baseline B uses a Pix2pix structure <ref type="bibr" target="#b1">[2]</ref>, and generates y based on x using a supervised way. Baseline C also uses the Pix2pix structure and inputs the combination of a conditional image x and the controllable structure C y to the proposed controllable structure guided generator G. Baseline D uses the proposed controllable structure guided cycle upon baseline C. Baseline E1x explores the proposed color loss in several different ways to avoid the 'channel pollution' issue. Baseline E2x employs the proposed controllable structure guided discriminator to stabilize the optimization process. Baseline E3 adds the proposed controllable structure guided self-content preserving loss to preserve content information. Baseline E4x adds the perceptual loss and the Total Variation loss on the generated result y . Baseline F is our full model integrating baselines D, E16, E22, E3, and E42. All the baseline models are trained and tested on the same data using the same configuration.</p><p>Note that each baseline in E (i.e., E1x, E2x, E3, and E4x) focuses on improving each aspect of the performance of the generated images. More specifically, the proposed color loss aims to avoid the 'channel pollution' issue and thus improve the pixel-level similarity metrics, i.e., SSIM, PSNR, and SD. The proposed controllable structure guided discriminator tries to improve the structure accuracy since the controllable structure can provide strong supervision to the discriminator. The     proposed controllable structure guided self-content preserving loss can push the generated data distribution close to the real data distribution. Finally, the perceptual loss and the Total Variation loss aim to improve image fidelity. Ablation Analysis. The results of the ablation study are shown in <ref type="table" target="#tab_8">Table IX</ref>. We observe that Baseline B is better than baseline A since the ground truth image y can provide strong supervised information to the generator G. Comparing Baseline B with Baseline C, the controllable structure guided generation improves the performance on all metrics by large margins, which confirms that the controllable structures can provide more structural information to the generator G. By using the proposed controllable structure guided cycle, Baseline D further improves over baseline C, meaning that the cycle structure indeed utilizes the controllable structure information in a more effective way, confirming our design motivation. Baseline E14 outperforms baselines D, E12, and E13 on SSIM, PSNR, and SD metrics showing the importance of using the proposed color loss to avoid the 'channel pollution' issue. Visualization results of L1 loss, L2 loss and the proposed color L1 loss are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. We can see that the proposed color L1 loss generates more clear and visually plausible details than both L1 and L2 losses, which validates the effectiveness of the proposed color loss. By further combining the color L1 loss and the L1 loss on the generated image y , we can further improve the performance as shown in baseline E16. However, replacing the color L1 loss with the color L2 loss will degrade the performance as shown by baseline E15 but the results are still better than using baseline D. We also use the proposed color loss on the reconstructed image x as presented in baseline E11, but it achieves the worst results. Comparing Baseline D with Baseline E21, the proposed controllable structure guide discriminator improves the top-1 accuracy by 0.65 and 1.98, which confirms the importance of our design motivation. By further combining the controllable structure guide discriminator with the traditional discriminator in baseline E22, both top-1 and top-5 accuracies are further boosted. Baseline E3 outperforms D with around 0.13 gains on the KL metric, clearly demonstrating the effectiveness of the proposed controllable structure guided self-content preserving loss. By adding the perceptual loss and the TV loss in baseline E4x, the overall performance is further improved on LPIPS metric <ref type="bibr" target="#b48">[49]</ref>, which uses pretrained deep models to evaluate the similarity and highly agrees with human perception. Finally, we demonstrate the advantage of the proposed full model in baseline F, which integrates baseline D, E14, E22, E3, and E42. It is obvious that baseline F achieves the best results on both accuracy and KL score metrics. However, we observe    that baseline F achieves worse results on SSIM, PSNR, and SD compared with baseline E16, and at the same time, it achieves worse results on the LPIPS metric compared with baseline E42. This is also observed in the LPIPS paper <ref type="bibr" target="#b48">[49]</ref>, i.e., the traditional metrics (i.e., SSIM, PSNR, SD, FSIM) disagree with metrics based on deep architectures such as VGG <ref type="bibr" target="#b37">[38]</ref>. Thus, we try to balance both metrics to reasonable results without dropping significantly the performance, and we still observe that baseline F achieves better performance on all SSIM, PSNR, SD, and LPIPS metrics than baseline D.</p><p>Hyper-parameter Analysis. 1) For cross-view image translation tasks, we follow <ref type="bibr" target="#b1">[2]</ref> and set λ color =100 since L color denotes a pixel-wise reconstruction loss. We then follow <ref type="bibr" target="#b8">[9]</ref> and set λ tv =1e−6. In addition to λ color and λ tv , we also introduce λ cyc , λ con , and λ vgg . Thus, we investigate the influence of λ cyc , λ con , λ vgg to the performance of our model. The results are shown in Tables X, XI and XII. In <ref type="table" target="#tab_9">Table X</ref>, when λ cyc becomes smaller, we achieve better results on most metrics. This means that adjusting the ratio of weighting parameters of the cycle can obtain further performance improvement. This is different from CycleGAN <ref type="bibr" target="#b10">[11]</ref>, which uses the same weights for both forward and backward cycle-consistency losses since CycleGAN tries to learning two mappings, while in our model we only focus on generating photo-realistic result y and do  not care about the quality of the reconstructed image x . Thus, the forward part has a lager weight than the backward part. Moreover, we also investigate the influence of λ con and λ vgg . The results are listed in Tables XI and XII. When both λ con and λ vgg become bigger, the generator with a larger error loss dominates the training, making the whole model generating better results. Therefore, we empirically set λ cyc =0.1, λ con =100, λ vgg =100, λ color =100 and λ tv =1e−6 in Eq. (15) for this task.</p><p>2) For hand gesture-to-gesture translation tasks, we first follow <ref type="bibr" target="#b8">[9]</ref> and set λ tv =1e−6. Next, we investigate the influence of λ cyc , λ con , λ vgg , and λ color to the performance of our model. Results are shown in Tables XIII, XIV, XV, and XVI, respectively. According to these tables, we empirically set λ cyc =0.1, λ con =0.01, λ vgg =1000, λ color =800,  and λ tv =1e−6 in Eq. (15) for this task. Moreover, we also investigate the influence of the number of cycles on this task. Results are shown in <ref type="table" target="#tab_1">Table XVII</ref> and we observe that the two-cycle framework achieves better results than one-cycle framework on most metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we focus on the challenging task of controllable image-to-image translation. To this end, we propose a unified GAN framework, which can generate target images with different poses, sizes, structures, and locations based on a conditional image and controllable structures. In this way, the conditional image can provide appearance information and the controllable structures can provide structure information for generating the final results. Moreover, we also propose three novel losses to learn the mapping from the source domain to the target domain, i.e., color loss, controllable structure guided cycle-consistency loss, and controllable structure guided selfcontent preserving loss. It is worth noting that the proposed color loss handles the 'channel pollution' problem when backpropagating the gradients, which frequently occurs in the existing generative models. The controllable structure guided cycle-consistency loss can reduce the dis-match between the source domain and the target domain. The controllable structure guided self-content preserving loss aims to preserve the image content information of generated images. In addition, we present a novel Fréchet ResNet Distance (FRD) metric to evaluate the quality of generated images. Experimental results show that the proposed unified GAN framework achieves competitive performance compared with existing methods using carefully designed frameworks on two challenging generative tasks, i.e., hand gesture-to-gesture translation and cross-view image translation. Note that the proposed GAN framework is not tuned to any specific controllable image-to-image translation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art image-to-image translation methods. (a) Traditional deep learning methods, e.g., Context Encoder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of the 'channel pollution' issue. From left to right: Conditional Image, Ground Truth, PG 2 [4], and Ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Different methods for hand gesture-to-gesture translation on NTU Hand Digit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Different methods for hand gesture-to-gesture translation on Senz3D. TABLE I: Comparison results with state-of-the-art models for hand gesture-to-gesture translation on NTU Hand Digit and Senz3D. For all metrics except FID and FRD, higher is better. ( * ) These results are reported in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Different methods for cross-view image translation in 256×256 resolution on Dayton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Different methods for cross-view image translation in 256×256 resolution on CVUSA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Arbitrary hand gesture-to-gesture translation of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Arbitrary cross-view image translation of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Comparison results of L1 Loss, L2 Loss, and the proposed Color Loss for cross-view image translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Quantitative evaluation of Dayton in 64×64 resolution. For all metrics except KL score, higher is better. ( * , †) These results are reported in<ref type="bibr" target="#b9">[10]</ref> and<ref type="bibr" target="#b8">[9]</ref>, respectively.<ref type="bibr" target="#b14">15</ref>.33 * 27.61 * 39.07 * 1.8029 * 1.5014 * 1.9300 * 0.4808 * 19.4919 * 16.4489 * 6.29 ± 0.80 * X-Fork [10] 16.63 * 34.73 * 46.35 * 70.01 * 1.9600 * 1.5908 * 2.0348 * 0.4921 * 19.6273 * 16.4928 * 3.42 ± 0.72 * X-Seq [10] 4.83 * 5.56 * 19.55 * 24.96 * 1.8503 * 1.4850 * 1.9623 * 0.5171 * 20.1049 * 16.6836 * 6.22 ± 0.87 * SelectionGAN [9] 45.37 † 79.00 † 83.48 † 97.74 † 2.1606 † 1.7213 † 2.1323 † 0.6865 † 24.6143 † 18.2374 † 1.70 ± 0.45 † 1.7970 * 1.3029 * 1.6101 * 0.3675 * 20.5135 * 14.7813 * 6.39 ± 0.90 15.42 * 35.82 * 1.8557 * 1.3162 * 1.6521 * 0.3682 * 20.6933 * 14.7984 * 4.45 ± 0.84 1.3189 * 1.6219 * 0.3663 * 20.4239 * 14.7657 * 7.20 ± 0.92 * SelectionGAN [9] 14.12 † 51.81 † 39.45 † 74.70 † 2.1571 † 1.4441 † 2.0828 † 0.5118 † 23.2657 † 16.2894 † 2.25 ± 0.56 †</figDesc><table><row><cell>Dir.</cell><cell>Method</cell><cell></cell><cell cols="2">Accuracy (%) ↑</cell><cell></cell><cell cols="3">Inception Score ↑</cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell><cell>SD ↑</cell><cell>KL ↓</cell></row><row><cell></cell><cell></cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a2g</cell><cell cols="2">Pix2pix [2] 7.90  Ours 49.86</cell><cell>84.41</cell><cell>86.14</cell><cell>99.61</cell><cell>2.1059</cell><cell>1.7342</cell><cell>2.0737</cell><cell>0.6754</cell><cell>24.2814</cell><cell>18.1361</cell><cell>1.54 ± 0.39</cell></row><row><cell></cell><cell>Real Data</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">2.3534  † 1.8135  † 2.3250  †</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>g2a</cell><cell cols="6">Pix2pix [2] 7.49  X-Fork [10] 1.65  *  2.24  *  4.00  X-Seq [10] 1.55  *  2.99  *  6.27  *  1.7854  Ours 8.96  *  16.65 44.83 44.03 77.01 2.0802</cell><cell>1.4360</cell><cell>2.0628</cell><cell>0.5064</cell><cell>23.3632</cell><cell>16.4788</cell><cell>2.16 ± 0.59</cell></row><row><cell></cell><cell>Real Data</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">2.3015  † 1.5056  † 2.2095  †</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note>** 12.68*** 16.41***</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Quantitative evaluation of Dayton in 256×256 resolution. For all metrics except KL score, higher is better. ( * , †) SelectionGAN [9] 42.11 † 68.12 † 77.74 † 92.89 † 3.0613 † 2.2707 † 3.1336 † 0.5938 † 23.8874 † 20.0174 † 2.74 ± 0.86 † SelectionGAN [9] 20.66 † 33.70 † 51.01 † 63.03 † 3.2446 † 2.1331 † 3.4091 † 0.3284 † 21.8066 † 17.3817 † 3.55 ± 0.87 †</figDesc><table><row><cell cols="6">These results are reported in [10] and [9], respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dir.</cell><cell>Method</cell><cell></cell><cell cols="2">Accuracy (%) ↑</cell><cell></cell><cell cols="3">Inception Score ↑</cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell><cell>SD ↑</cell><cell>KL ↓</cell></row><row><cell></cell><cell></cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a2g</cell><cell cols="2">Pix2pix [2] 6.80  X-Fork [10] 30.00  Ours 49.12</cell><cell>80.43</cell><cell>81.20</cell><cell>94.87</cell><cell>3.3210</cell><cell>2.3494</cell><cell>3.3522</cell><cell>0.5633</cell><cell>23.3515</cell><cell>19.7692</cell><cell>2.17 ± 0.77</cell></row><row><cell></cell><cell>Real Data</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">3.8319  † 2.5753  † 3.9222  †</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>g2a</cell><cell cols="2">Pix2pix [2] 10.23  Ours 17.31</cell><cell>29.40</cell><cell>43.58</cell><cell>55.27</cell><cell>3.2131</cell><cell>2.0916</cell><cell>3.3637</cell><cell>0.3357</cell><cell>22.0273</cell><cell>17.6542</cell><cell>5.17 ± 1.23</cell></row><row><cell></cell><cell>Real Data</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">3.7196  † 2.3626  † 3.8998  †</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note>* 9.15* 23.55* 27.00* 2.8515* 1.9342* 2.9083* 0.4180* 17.6291* 19.2821* 38.26 ± 1.88** 48.68* 61.57* 78.84* 3.0720* 2.2402* 3.0932* 0.4963* 19.8928* 19.4533* 6.00 ± 1.28* X-Seq [10] 30.16* 49.85* 62.59* 80.70* 2.7384* 2.1304* 2.7674* 0.5031* 20.2803* 19.5258* 5.93 ± 1.32** 16.02* 30.90* 40.49* 3.5676* 2.0325* 2.8141* 0.2693* 20.2177* 16.9477* 7.88 ± 1.24* X-Fork [10] 10.54* 15.29* 30.76* 37.32* 3.1342* 1.8656* 2.5599* 0.2763* 20.5978* 16.9962* 6.92 ± 1.15* X-Seq [10] 12.30* 19.62* 35.95* 45.94* 3.5849* 2.0489* 2.8414* 0.2725* 20.2925* 16.9285* 7.07 ± 1.19*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Quantitative evaluation of CVUSA in the a2g direction. For all metrics except KL score, higher is better. ( * , †) These results are reported in<ref type="bibr" target="#b9">[10]</ref> and<ref type="bibr" target="#b8">[9]</ref>, respectively.Zhai  et al. [50] 13.97 * 14.03 * 42.09 * 52.29 * 1.8434 * 1.5171 * 1.8666 * 0.4147 * 17.4886 * 16.6184 * 27.43 ± 1.63 * Pix2pix [2] 7.33 * 9.25 * 25.81 * 32.67 * 3.2771 * 2.2219 * 3.4312 * 0.3923 * 17.6578 * 18.5239 * 59.81 ± 2.12 * X-Fork [10] 20.58 * 31.24 * 50.51 * 63.66 * 3.4432 * 2.5447 * 3.5567 * 0.4356 * 19.0509 * 18.6706 * 11.71 ± 1.55 * X-Seq [10] 15.98 * 24.14 * 42.91 * 54.41 * 3.8151 * 2.6738 * 4.0077 * 0.4231 * 18.8067 * 18.4378 * 15.52 ± 1.73 * SelectionGAN [9] 41.52 † 65.51 † 74.32 † 89.66 † 3.8074 † 2.7181 † 3.9197 † 0.5323 † 23.1466 † 19.6100 † 2.96 ± 0.97 †</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">Accuracy (%) ↑</cell><cell></cell><cell cols="3">Inception Score ↑</cell><cell>SSIM ↑</cell><cell>PSNR ↑</cell><cell>SD ↑</cell><cell>KL ↓</cell></row><row><cell></cell><cell cols="2">Top-1</cell><cell cols="2">Top-5</cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>45.06</cell><cell>70.04</cell><cell>78.31</cell><cell>93.47</cell><cell>3.9469</cell><cell>2.8779</cell><cell>4.0383</cell><cell>0.5366</cell><cell>22.8223</cell><cell>19.8276</cell><cell>2.60 ± 0.97</cell></row><row><cell>Real Data</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">4.8741  † 3.2959  † 4.9943  †</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>LPIPS of<ref type="bibr" target="#b8">[9]</ref> and the proposed method for crossview image translation. For this metric, lower is better.</figDesc><table><row><cell>Dir.</cell><cell>Method</cell><cell cols="3">Dayton (64×64) Dayton (256×256) CVUSA</cell></row><row><cell>a2g</cell><cell>SelectionGAN [9] Ours</cell><cell>0.1786 0.1712</cell><cell>0.4996 0.3529</cell><cell>0.4652 0.3817</cell></row><row><cell>g2a</cell><cell>SelectionGAN [9] Ours</cell><cell>0.2489 0.2382</cell><cell>0.5264 0.4527</cell><cell>--</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>FID of<ref type="bibr" target="#b8">[9]</ref> and the proposed method for crossview image translation. For this metric, lower is better.</figDesc><table><row><cell>Dir.</cell><cell>Method</cell><cell cols="3">Dayton (64×64) Dayton (256×256) CVUSA</cell></row><row><cell>a2g</cell><cell>SelectionGAN [9] Ours</cell><cell>28.4787 18.7225</cell><cell>38.3498 35.9220</cell><cell>43.1102 47.3500</cell></row><row><cell>g2a</cell><cell>SelectionGAN [9] Ours</cell><cell>60.7903 60.1969</cell><cell>85.4072 88.8195</cell><cell>--</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>FRD of<ref type="bibr" target="#b8">[9]</ref> and the proposed method for crossview image translation. For this metric, lower is better.</figDesc><table><row><cell>Dir.</cell><cell>Method</cell><cell cols="3">Dayton (64×64) Dayton (256×256) CVUSA</cell></row><row><cell>a2g</cell><cell>SelectionGAN [9] Ours</cell><cell>3.3066 3.1658</cell><cell>3.5060 3.3694</cell><cell>3.1641 3.1547</cell></row><row><cell>g2a</cell><cell>SelectionGAN [9] Ours</cell><cell>3.8033 3.7078</cell><cell>3.7646 3.8943</cell><cell>--</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII :</head><label>VIII</label><figDesc>Comparison of the number of network parameters on cross-view image translation.</figDesc><table><row><cell cols="3">Model Pix2pix [2] X-Fork [10]</cell><cell>X-Seq [10]</cell><cell>SelectionGAN [9]</cell><cell>Ours</cell></row><row><cell>G</cell><cell>39.0820 M</cell><cell>39.2163 M</cell><cell>39.0820*2 M</cell><cell>55.4808 M</cell><cell>11.3876 M</cell></row><row><cell>D</cell><cell>2.7696 M</cell><cell>2.7696 M</cell><cell>2.7696*2 M</cell><cell>2.7687 M</cell><cell>2.7678+2.7709 M</cell></row><row><cell>Total</cell><cell>41.8516 M</cell><cell>41.9859 M</cell><cell>83.7032 M</cell><cell>58.2495 M</cell><cell>16.9263 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX :</head><label>IX</label><figDesc>Ablation study of the proposed method on Dayton for cross-view image translation. For all evaluation metrics except KL and LPIPS, higher is better.</figDesc><table><row><cell cols="5">Baseline Experimental Setting</cell><cell cols="2">SSIM ↑ PSNR ↑</cell><cell>SD ↑</cell><cell>Accuracy ↑</cell><cell>KL ↓</cell><cell>LPIPS ↓</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>A</cell><cell>x</cell><cell cols="3">G → y (Unsupervised Learning)</cell><cell>0.4110</cell><cell cols="2">17.9868 18.5195 27.28 47.47 52.47 71.63 8.69±1.36</cell><cell>0.5913</cell></row><row><cell>B</cell><cell>x</cell><cell cols="3">G → y (Supervised Learning)</cell><cell>0.4555</cell><cell cols="2">19.6574 18.8870 27.46 46.84 58.20 77.17 6.25±1.30</cell><cell>0.5520</cell></row><row><cell>C</cell><cell cols="2">[x, Cx]</cell><cell cols="2">G → y (Controllable Structure Guided Generation)</cell><cell>0.5374</cell><cell cols="2">22.8345 19.2075 39.76 68.44 72.22 89.85 3.32±1.10</cell><cell>0.4010</cell></row><row><cell>D</cell><cell cols="2">[x, Cy]</cell><cell>G → [y , Cx]</cell><cell>G → x (Controllable Structure Guided Cycle)</cell><cell>0.5547</cell><cell cols="2">23.1531 19.6032 42.43 70.82 75.40 91.16 2.89±1.05</cell><cell>0.3821</cell></row><row><cell>E11</cell><cell cols="4">D + Color L1 Loss on x</cell><cell>0.5515</cell><cell cols="2">23.1345 19.6257 41.08 68.31 75.26 90.60 3.02±1.09</cell><cell>0.3968</cell></row><row><cell>E12</cell><cell cols="3">D + L1 Loss on y</cell><cell></cell><cell>0.5541</cell><cell cols="2">23.1492 19.6423 41.73 68.99 75.13 89.48 2.89±1.02</cell><cell>0.3835</cell></row><row><cell>E13</cell><cell cols="3">D + L2 Loss on y</cell><cell></cell><cell>0.5481</cell><cell cols="2">23.0939 19.5534 43.51 72.08 75.79 91.23 2.86±0.99</cell><cell>0.3913</cell></row><row><cell>E14</cell><cell cols="4">D + Color L1 Loss on y</cell><cell>0.5600</cell><cell cols="2">23.3692 19.7018 44.38 73.21 75.93 91.69 2.73±0.98</cell><cell>0.3782</cell></row><row><cell>E15</cell><cell cols="4">D + Color L2 Loss on y + L1 loss on y</cell><cell>0.5568</cell><cell cols="2">23.3930 19.6273 43.19 72.58 75.63 91.67 2.77±1.10</cell><cell>0.3793</cell></row><row><cell>E16</cell><cell cols="4">D + Color L1 Loss on y + L1 loss on y</cell><cell>0.5631</cell><cell cols="2">23.4600 19.7650 44.97 73.65 76.28 92.32 2.70±1.08</cell><cell>0.3765</cell></row><row><cell>E21</cell><cell cols="4">D + Controllable Structure Guided Discriminator</cell><cell>0.5340</cell><cell cols="2">22.8176 19.4404 43.08 72.80 74.98 90.89 3.06±1.09</cell><cell>0.4003</cell></row><row><cell>E22</cell><cell cols="4">D + Dual Discriminator</cell><cell>0.5255</cell><cell cols="2">22.5405 19.4104 43.12 74.85 76.14 91.23 2.93±1.02</cell><cell>0.3937</cell></row><row><cell>E3</cell><cell cols="4">D + Controllable Structure Guided Self-Content Preserving Loss</cell><cell>0.5473</cell><cell cols="2">23.0475 19.5561 42.81 70.18 76.71 91.32 2.76±0.99</cell><cell>0.3877</cell></row><row><cell>E41</cell><cell cols="4">D + Perceptual Loss</cell><cell>0.5494</cell><cell cols="2">23.1075 19.5197 45.34 75.40 78.09 93.24 2.87±0.79</cell><cell>0.3545</cell></row><row><cell>E42</cell><cell cols="4">D + Perceptual Loss + Total Variation Loss</cell><cell>0.5577</cell><cell cols="2">23.0242 19.4943 44.76 73.96 77.81 93.69 2.84±0.79</cell><cell>0.3543</cell></row><row><cell>F</cell><cell cols="4">D + E16 + E22 + E3 + E42</cell><cell>0.5603</cell><cell cols="2">23.1626 19.7455 46.43 76.94 79.54 94.33 2.35±0.84</cell><cell>0.3571</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X :</head><label>X</label><figDesc>The influence of λ cyc on Dayton for cross-view image translation.</figDesc><table><row><cell cols="3">λcyc SSIM ↑ PSNR ↑</cell><cell>SD ↑</cell><cell cols="3">Inception Score ↑</cell><cell>Accuracy ↑</cell><cell>KL ↓</cell><cell>LPIPS ↓</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>all</cell><cell>Top-1</cell><cell>Top-5</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>100</cell><cell>0.5383</cell><cell cols="5">23.0283 19.5731 2.9278 1.9960 2.9823 39.22 67.86 69.55 88.03 3.96 ± 1.32</cell><cell>0.4082</cell></row><row><cell>10</cell><cell>0.5475</cell><cell cols="5">23.1264 19.5590 3.2344 2.2321 3.2983 42.30 67.99 74.98 89.54 2.87 ± 1.01</cell><cell>0.3832</cell></row><row><cell>1</cell><cell>0.5478</cell><cell cols="5">23.1153 19.5158 3.1918 2.2025 3.2362 42.11 72.26 75.37 91.33 2.88 ± 1.02</cell><cell>0.3869</cell></row><row><cell>0.1</cell><cell>0.5547</cell><cell cols="5">23.1731 19.6032 3.2823 2.2401 3.3081 42.43 70.82 75.40 91.16 2.89 ± 1.05</cell><cell>0.3821</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XI :</head><label>XI</label><figDesc>The influence of λ con on Dayton for cross-view image translation. 85±1.03 2.93±1.00 2.83±1.01 3.00±1.02 2.76±0.99</figDesc><table><row><cell>λcon</cell><cell>0.1</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>100</cell></row><row><cell>KL ↓ 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XII :</head><label>XII</label><figDesc>The influence of λ vgg on Dayton for cross-view image translation.</figDesc><table><row><cell>λvgg</cell><cell>1</cell><cell>10</cell><cell>20</cell><cell>50</cell><cell>100</cell></row><row><cell cols="6">LPIPS ↓ 0.3812 0.3708 0.3628 0.3571 0.3545</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XIII :</head><label>XIII</label><figDesc>The influence of λ cyc on NTU Hand Digit for hand gesture-to-gesture translation.</figDesc><table><row><cell>λcyc</cell><cell>PSNR ↑</cell><cell>IS ↑</cell><cell>FID ↓</cell><cell>FRD ↓</cell></row><row><cell cols="5">0.001 28.5673 2.4851 23.9935 2.8468</cell></row><row><cell>0.01</cell><cell cols="4">28.5475 2.3719 23.8958 2.8991</cell></row><row><cell>0.1</cell><cell cols="4">28.4967 2.4755 21.6280 2.7571</cell></row><row><cell>1</cell><cell cols="4">28.5370 2.3436 23.5811 2.8467</cell></row><row><cell>10</cell><cell cols="4">28.5627 2.4815 22.5539 2.8401</cell></row><row><cell>100</cell><cell cols="4">28.5854 2.4191 23.5617 2.8080</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIV :</head><label>XIV</label><figDesc>The influence of λ con on NTU Hand Digit for hand gesture-to-gesture translation.</figDesc><table><row><cell>λcon</cell><cell>PSNR ↑</cell><cell>IS ↑</cell><cell>FID ↓</cell><cell>FRD ↓</cell></row><row><cell cols="5">0.001 28.5638 2.4335 20.6123 2.7273</cell></row><row><cell>0.01</cell><cell cols="4">28.4607 2.3665 19.9356 2.6960</cell></row><row><cell>0.1</cell><cell cols="4">28.6696 2.3446 23.2919 2.8326</cell></row><row><cell>1</cell><cell cols="4">28.6478 2.3522 24.4331 2.9171</cell></row><row><cell>10</cell><cell cols="4">28.6642 2.3528 21.7138 2.8778</cell></row><row><cell>100</cell><cell cols="4">28.5207 2.4881 24.3938 2.9104</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XV :</head><label>XV</label><figDesc>The influence of λ vgg on NTU Hand Digit for hand gesture-to-gesture translation. 2.3096 23.7465 2.6976 0.01 28.5580 2.4825 23.4135 2.6966 0.1 28.5741 2.4684 23.1802 2.6872 1 28.5625 2.3182 20.1516 2.6653 10 28.5486 2.2502 19.8930 2.6004 100 28.9545 2.0455 17.1370 2.4461 1000 28.8131 2.0965 14.1617 2.2135 10000 27.4805 2.4538 65.1080 3.2607</figDesc><table><row><cell>λvgg</cell><cell>PSNR ↑</cell><cell>IS ↑</cell><cell>FID ↓</cell><cell>FRD ↓</cell></row><row><cell>0.001</cell><cell>28.4537</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XVI :</head><label>XVI</label><figDesc>The influence of λ color on NTU Hand Digit for hand gesture-to-gesture translation.</figDesc><table><row><cell cols="2">λ color PSNR ↑</cell><cell>IS ↑</cell><cell>FID ↓</cell><cell>FRD ↓</cell></row><row><cell>100</cell><cell cols="4">28.8131 2.0965 14.1617 2.2135</cell></row><row><cell>200</cell><cell cols="4">29.2343 2.1537 13.4811 2.2421</cell></row><row><cell>500</cell><cell cols="4">29.9973 2.1332 13.4823 2.2039</cell></row><row><cell>800</cell><cell cols="4">30.4531 2.1898 13.9475 2.2176</cell></row><row><cell>1000</cell><cell cols="4">30.7087 2.2138 15.3634 2.2134</cell></row><row><cell>2000</cell><cell cols="4">31.4232 2.1991 17.1864 2.2872</cell></row><row><cell>5000</cell><cell cols="4">32.3025 2.1022 28.5587 2.3715</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is partially supported by National Natural Science Foundation of China (No.U1613209, </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward multimodal image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometry guided adversarial facial expression synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Disentangled person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Skeleton-aided articulated motion generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deformable gans for pose-based human image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multichannel attention selection gan with cascaded semantic guidance for cross-view image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-view image synthesis using conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Regmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combogan: Unrestrained scalability for image domain translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anoosheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual generator generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno>arXiv preprint:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gesturegan for hand gesture-to-gesture translation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exocentric to egocentric image generation via parallel generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Latapie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dual in-painting model for unsupervised gaze correction and animation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in ACM MM, 2020. 3</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-toimage generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cascade attention guided residue learning gan for cross-modal translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Latapie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attribute-guided sketch generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sketchygan: Towards diverse and realistic sketch to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bipartite graph reasoning gans for person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cycle in cycle generative adversarial networks for keypoint-guided image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xinggan for person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Local class-specific and global image-level generative adversarial networks for semantic-guided scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dual discriminator generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A novel feature matching strategy for large scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust part-based hand gesture recognition using kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1110" to="1120" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Head-mounted gesture controlled interface for human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Memo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer MTA</publisher>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Localizing and orienting street views using overhead imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Predicting groundlevel scene layout from aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bessinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
