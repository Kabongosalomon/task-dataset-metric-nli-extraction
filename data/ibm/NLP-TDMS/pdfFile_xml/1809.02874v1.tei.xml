<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Person Re-identification by Deep Learning Tracklet Association</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
							<email>minxianli@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Vision Semantics Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<email>s.gong@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Person Re-identification by Deep Learning Tracklet Association</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Person Re-Identification</term>
					<term>Unsupervised Learning</term>
					<term>Tracklet</term>
					<term>Surveillance Video</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing person re-identification (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in practical re-id deployment due to the lack of exhaustive identity labelling of image positive and negative pairs for every camera pair. In this work, we address this problem by proposing an unsupervised re-id deep learning approach capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data from videos in an end-to-end model optimisation. We formulate a Tracklet Association Unsupervised Deep Learning (TAUDL) framework characterised by jointly learning per-camera (within-camera) tracklet association (labelling) and cross-camera tracklet correlation by maximising the discovery of most likely tracklet relationships across camera views. Extensive experiments demonstrate the superiority of the proposed TAUDL model over the state-of-the-art unsupervised and domain adaptation reid methods using six person re-id benchmarking datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (re-id) aims to match the underlying identities of person bounding box images detected from non-overlapping camera views <ref type="bibr" target="#b14">[15]</ref>. In recent years, extensive research attention has been attracted <ref type="bibr">[1, 7, 10, 11, 14, 18, 29-31, 44, 46, 53, 58]</ref> to address the re-id problem. Most existing re-id methods, in particular deep learning models, adopt the supervised learning approach. These supervised deep models assume the availability of a large number of manually labelled cross-view identity (ID) matching image pairs for each camera pair in order to induce a feature representation or a distance metric function optimised just for that camera pair. This assumption is inherently limited for generalising a re-id model to many different camera networks therefore cannot scale in practical deployments <ref type="bibr" target="#b0">1</ref> .</p><p>It is no surprise then that person re-id by unsupervised learning has become a focus in recent research where per-camera pairwise ID labelled training data is not required in model learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59]</ref>. However, all these classical unsupervised learning models are significantly weaker in re-id performance than the supervised models. This is because the lack of cross-view pairwise ID labelled data deprives a model's ability to learn from strong contextaware ID discriminative information in order to cope with significant visual appearance change between every camera pair, as defined by a triplet verification loss function. An alternative approach is to leverage jointly (1) unlabelled data from a target domain which is freely available, e.g. videos of thousands of people travelling through a camera view everyday in a public scene; and (2) pairwise ID labelled datasets from independent source domains <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56]</ref>. The main idea is to first learn a "view-invariant" representation from ID labelled source data, then adapt the model to a target domain by using only unlabelled target data. This approach makes an implicit assumption that the source and target domains share some common cross-view characteristics and a view-invariant representation can be estimated, which is not always true.</p><p>In this work, we consider a pure unsupervised person re-id deep learning problem. That is, no ID labelled training data is assumed, neither cross-view nor within-view ID labelling. Although this learning objective is similar to two domain transfer models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b49">50]</ref>, both those models do require suitable, i.e. visually similar to the target domain, person identity labelled source domain training data. Specifically, we consider unsupervised re-id model learning by jointly optimising unlabelled person tracklet data within-camera view to be more discriminative and cross-camera view to be more associative in an end-to-end manner.</p><p>Our contributions are: We formulate a novel unsupervised person re-id deep learning method using person tracklets without the need for camera pairwise ID labelled training data, i.e. unsupervised tracklet re-id discriminative learning. Specifically, we propose a Tracklet Association Unsupervised Deep Learning (TAUDL) model with two key innovations: (1) Per-Camera Tracklet Discrimination Learning that optimises "local" within-camera tracklet label discrimination for facilitating cross-camera tracklet association given per-camera independently created tracklet label spaces. (2) Cross-Camera Tracklet Association Learning that maximises "global" cross-camera tracklet label association. This is formulated as to maximise jointly cross-camera tracklet similarity and within-camera tracklet dissimilarity in an end-to-end deep learning framework.</p><p>Comparative experiments show the advantages of TAUDL over the stateof-the-art unsupervised and domain adaptation person re-id models using six benchmarks including three multi-shot image based and three video based reid datasets: CUHK03 <ref type="bibr" target="#b28">[29]</ref>, Market-1501 <ref type="bibr" target="#b60">[61]</ref>, DukeMTMC <ref type="bibr" target="#b40">[41]</ref>, iLIDS-VID <ref type="bibr" target="#b50">[51]</ref>, PRID2011 <ref type="bibr" target="#b18">[19]</ref>, and MARS <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most existing re-id models are built by supervised model learning on a separate set of per-camera-pair ID labelled training data <ref type="bibr">[1, 7-11, 18, 20, 29-31, 44, 46, 48, 52,53,58,63]</ref>. Hence, their scalability and usability is poor for real-world re-id deployments where no such large training sets are available for every camera pair. Classical unsupervised learning methods based on hand-crafted features offer poor re-id performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59]</ref> when compared to the supervised learning based re-id models. While a balancing trade-off between model scalability and re-id accuracy can be achieved by semi-supervised learning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b48">49]</ref>, these models still assume sufficiently large sized cross-view pairwise labelled data for model training. More recently, there are some attempts on unsupervised learning of domain adaptation models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56]</ref>. The main idea is to explore knowledge from pairwise labelled data in "related" source domains with model adaptation on unlabelled target domain data. Whilst these domain adaptation models perform better than the classical unsupervised learning methods ( <ref type="table" target="#tab_2">Table 2 and Table 3</ref>), they requires implicitly similar data distributions and viewing conditions between the labelled source domain and the unlabelled target domains. This restricts their scalability to arbitrarily diverse (and unknown) target domains.</p><p>In contrast to all these existing unsupervised learning re-id methods, the proposed tracklet association based method enables unsupervised re-id deep end-to-end learning from scratch without any assumption on either the scene characteristic similarity between source and target domains, or the complexity of handling identity label space (or lack of) knowledge transfer in model optimisation. Instead, our method directly learns to discover the re-id discriminative knowledge from unsupervised tracklet label data automatically generated and annotated from the video data using a common deep learning network architecture. Moreover, this method does not assume any overlap of person ID classes across camera views, therefore scalable to any camera networks without any knowledge about camera space-time topology and/or time-profiling on people cross-view appearing patterns <ref type="bibr" target="#b35">[36]</ref>. Compared to classical unsupervised methods relying on extra hand-crafted features, our method learns tracklet based re-id discriminative features from an end-to-end deep learning process. To our best knowledge, this is the first attempt at unsupervised tracklet association based person re-id deep learning model without relying on any ID labelled training data (either videos or images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Deep Learning Tracklet Association</head><p>To overcome the limitation of supervised re-id model training, we propose a novel Tracklet Association Unsupervised Deep Learning (TAUDL) approach to person re-id in video (or multi-shot images in general) by uniquely exploiting person tracklet labelling obtained by an unsupervised tracklet formation (sampling) mechanism 2 without any ID labelling of the training data (either cross-view or within-view). The TAUDL trains a person re-id model in an end-to-end manner in order to benefit from the inherent overall model optimisation advantages from deep learning. In the following, we first present a data sampling mechanism for unsupervised within-camera tracklet labelling (Sec. 3.1) and then describe our model design for cross-camera tracklet association by joint unsupervised deep learning (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised Within-View Tracklet Labelling</head><p>Given a large quantity of video data from multiple disjoint cameras, we can readily deploy existing pedestrian detection and tracking models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b61">62]</ref>, to extract person tracklets. In general, the space-time trajectory of a person in a single-camera view from a public scene is likely to be fragmented into an arbitrary number of short tracklets due to imperfect tracking and background clutter. Given a large number of person tracklets per camera, we want to annotate them for deep re-id model learning in an unsupervised manner without any manual identity verification on tracklets. To this end, we need an automatic tracklet labelling method to minimise the person ID duplication (i.e. multiple tracklet  labels corresponding the same person ID label) rate among these labelled tracklets. To this end, we propose a Sparse Space-Time Tracklet (SSTT) sampling and label assignment method. Our SSTT method is built on three observations typical in surveillance videos: (1) For most people, re-appearing in a camera view is rare during a short time period. As such, the dominant factor for causing person tracklet duplication (of the same ID) in auto-generated person tracklets is trajectory fragmentation, and if we assign every tracklet with a distinct label. To address this problem, we perform sparse temporal sampling of tracklets ( <ref type="figure" target="#fig_2">Fig. 2</ref>(a)) as follows: (i) At the i-th temporal sampling instance corresponding to a time point S i , we retrieve all tracklets at time S i and annotate each tracklet with a distinct label. This is based on the factor that (2) people co-occurring at the same time in a singleview but at different spatial locations should have distinct ID labels. (ii) Given a time gap P , the next ((i + 1)-th) temporal sampling and label assignment is repeated, where P controls the sparsity of the temporal sampling rate. Based on observation (3) that most people in a public scene travel through a single camera view in a common time period Q &lt; P , it is expected that at most one tracklet per person can be sampled at such a sparse temporal sampling rate (assuming no re-appearing once out of the same camera view). Consequently, we can significantly reduce the ID duplication even in highly crowded scenes with greater degrees of trajectory fragmentation.</p><p>To further mitigate the negative effect of inaccurate person detection and tracking at each temporal sampling instance, we further impose a sparse spatial sampling constraint -only selecting the co-occurring tracklets distantly distributed over the scene space ( <ref type="figure" target="#fig_2">Fig. 2(b)</ref>). In doing so, the tracklet labels are more likely to be of independent person identities with minimum ID duplications in each i-th temporal sampling instance.</p><p>By deploying this SSTT tracklet labelling method in each camera view, we can obtain an independent set of labelled tracklets {S i , y i } per-camera in a camera network, where each tracklet contains a varying number of person bounding boxes as S = {I 1 , I 2 , · · · }. Our objective is to use these SSTT labelled tracklets for optimising a cross-view person re-id deep learning model without any cross-view ID labelled pairwise training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Tracklet Association</head><p>Given per-camera independently-labelled tracklets {S i , y i } generated by SSTT, we perform tracklet label re-id discriminative learning without person ID labels in a conventional classification deep learning framework. To that end, we formulate a Tracklet Association Unsupervised Deep Learning (TAUDL) model. The overall design of our TAUDL architecture is shown in <ref type="figure">Fig. 1</ref>. The TAUDL contains two model components: (I) Per-Camera Tracklet Discrimination Learning with the aim to optimise "local" (within-camera) tracklet label discrimination for facilitating cross-camera tracklet association given independently created tracklet label spaces in different camera views. (II) Cross-Camera Tracklet Association Learning with the aim to maximise "global" (cross-camera) tracklet label association. The two components integrate as a whole in a single deep learning network architecture, learn jointly and mutually benefit each other in an incremental end-to-end manner. (I) Per-Camera Tracklet Discrimination Learning For accurate crosscamera tracklet association, it is important to formulate a robust image feature representation for describing the person appearance of each tracklet that helps cross-view person re-id association. However, it is sub-optimal to achieve "local" per-camera tracklet discriminative learning using only per-camera independent tracklet labels without "global" cross-camera tracklet correlations. We wish to optimise jointly both local tracklet within-view discrimination and global tracklet cross-view association. To that end, we design a Per-Camera Tracklet Discrimination (PCTD) learning algorithm. Our key idea is that, instead of relying on the conventional fine-grained explicit instance-level cross-view ID pairwise supervised learning ( <ref type="figure" target="#fig_3">Fig. 3(a)</ref>), we learn to maximise coarse-grained latent group-level cross-camera tracklet association by set correlation <ref type="figure" target="#fig_3">(Fig. 3(b)</ref>).</p><p>Specifically, we treat each individual camera view separately by optimising per-camera labelled tracklet discrimination as a classification task against the tracklet labels per-camera (not person ID labels). Therefore, we have a total of T different tracklet classification tasks each corresponding to a specific camera view. Importantly, we further formulate these T classification tasks in a multibranch architecture design where every task shares the same feature representation whilst enjoys an individual classification branch ( <ref type="figure">Fig. 1(b)</ref>). Conceptually, this model design is in a spirit of the multi-task learning principle <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Formally, given unsupervised training data {I, y} extracted from a camera view t ∈ {1, · · · , T }, where I specifies a tracklet frame and y ∈ {1, · · · , M t } the tracklet label (obtained as in Sec. 3.1) with a total of M t different labels, we adopt the softmax Cross-Entropy (CE) loss function to optimise the corresponding classification task (the t-th branch). The CE loss on a training image sample (I, y) is computed as:</p><formula xml:id="formula_0">L ce = −log exp(W y x) Mt k=1 exp(W k x) ,<label>(1)</label></formula><p>where x specifies the feature vector of I extracted by the task-shared feature representation component and W y the y-th class prediction function parameters. Given a mini-batch, we compute the CE loss for each such training sample w.r.t. the respective tracklet label space and utilise their average to form the model learning supervision as:</p><formula xml:id="formula_1">L pctd = 1 N bs T t=1 L t ce ,<label>(2)</label></formula><p>where L t ce denotes the CE loss summation of training samples from the t-th camera among a total of T and N bs the batch size.</p><p>Discussion: In PCTD, the deep learning objective loss function (Eqn. (1)) aims to optimise by supervised learning person tracklet discrimination within each camera view without any knowledge on cross-camera tracklet association. However, when jointly learning all the per-camera tracklet discrimination tasks together, the learned representation model is somewhat implicitly and collectively cross-view tracklet discriminative in a latent manner, due to the existence of cross-camera tracklet correlation. In other words, the shared feature representation is optimised concurrently to be discriminative for tracklet discrimination in multiple camera views, therefore propagating model discriminative learning from per-camera to cross-camera. We will evaluate the effect of this model design in our experiments <ref type="table" target="#tab_4">(Table 4</ref>).</p><p>(II) Cross-Camera Tracklet Association Learning While the PCTD algorithm described above achieves somewhat global (all the camera views) tracklet discrimination implicitly, the learned model representation remains sub-optimal due to the lack of explicitly optimising cross-camera tracklet association at the fine-grained instance level. It is significantly harder to impose cross-view person re-id discriminative model learning without camera pairwise ID labels. To address this problem, we introduce a Cross-Camera Tracklet Association (CCTA) loss function. The CCTA loss is formulated based on the idea of batch-wise incrementally aligning cross-view per tracklet feature distribution in the shared multi-task learning feature space. Critically, CCTA integrates seamlessly with PCTD to jointly optimise model learning on discovering cross-camera tracklet association for person re-id in a single end-to-end batch-wise learning process.</p><p>Formally, given a mini-batch including a subset of tracklets {(S t i , y t i )} where S t i specifies the i-th tracklet from t-th camera view with the label y t i where tracklets in a mini-batch come from all the camera views, we want to establish for each in-batch tracklet a discriminative association with other tracklets from different camera views. In absence of person identity pairwise labelling as a learning constraint, we propose to align similar and dissimilar tracklets in each mini-batch given the up-to-date shared multi-task (multi-camera) feature representation from optimising PCTD. More specifically, for each tracklet S t i , we first retrieve K cross-view nearest tracklets N t i in the feature space, with the remainingÑ t i considered as dissimilar ones. We then impose a soft discriminative structure constraint by encouraging the model to pull N t i close to S t i whilst to push awayÑ t i from S t i . Conceptually, this is a per-tracklet cross-view data structure distribution alignment. To achieve this, we formulate a CCTA deep learning objective loss for each tracklet S t i in a training mini-batch as:</p><formula xml:id="formula_2">L ccta = − log z k ∈N t i exp(− 1 2σ 2 s t i − z k 2 ) T t =1 nj j=1 exp(− 1 2σ 2 s t i − s t j 2 ) ,<label>(3)</label></formula><p>where n j denotes the number of in-batch tracklets from j-th camera view, T the camera view number, σ a scaling parameter, s t i the up-to-date feature representation of the tracklet S t i . Given the incremental iterative deep learning nature, we represent a tracklet S by the average of its in-batch frames' feature vectors on-the-fly. Hence, the tracklet representation is kept up-to-date without the need for maintaining external per-tracklet feature representations.</p><p>Discussion: The proposed CCTA loss formulation is conceptually similar to the Histogram Loss <ref type="bibr" target="#b44">[45]</ref> in terms of distribution alignment. However, the Histogram Loss is a supervised loss that requires supervised label training data, whilst the CCTA is purely unsupervised and derived directly from feature similarity measures. CCTA is also related to the surrogate (artificially built) class based unsupervised deep learning loss formulations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, by not requiring groundtruth class-labelled data in model training. Unlike CCTA without the need for creating surrogate classes, the surrogate based models not only require additional global data clustering, but also are sensitive to the clustering quality and initial feature selection. Moreover, they do not consider the label distribution alignment across cameras and label spaces for which the CCTA loss is designed. Joint Loss Function After merging the CCTA and PCTD learning constraints, we obtain the final model objective function as:</p><formula xml:id="formula_3">L taudl = (1 − λ)L pctd + λL ccta ,<label>(4)</label></formula><p>where λ is a weighting parameter estimated by cross-validation. Note that L pctd is an average loss term at the tracklet individual image level whilst L ccta at the tracklet group (set) level, both derived from the same training batch concurrently. As such, the overall TAUDL method naturally enables end-to-end deep model learning using the Stochastic Gradient Descent optimisation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets To evaluate the proposed TAUDL model, we tested both video (MARS <ref type="bibr" target="#b59">[60]</ref>, iLIDS-Video <ref type="bibr" target="#b50">[51]</ref>, PRID2011 <ref type="bibr" target="#b18">[19]</ref>) and image (CUHK03 <ref type="bibr" target="#b28">[29]</ref>, Market-1501 <ref type="bibr" target="#b60">[61]</ref>, DukeMTMC <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b61">62]</ref>) based person re-id benchmarking datasets. In previous studies, these datasets were mostly evaluated separately. We consider since recent large sized image based re-id datasets were typically constructed by sampling person bounding boxes from video, these image datasets share similar characteristics of those video based datasets. We adopted the standard person re-id setting on training/test ID split and the test protocols <ref type="table" target="#tab_0">(Table 1)</ref>.</p><p>Tracklet Label Assignment For all six datasets, we cannot perform real SSTT tracklet sampling and label assignment due to no information available on spatial and temporal location w.r.t. the original video data. In our experiment, we instead conducted simulated SSTT to obtain the per-camera tracklet/image labels. For all datasets, we assume no re-appearing subjects per camera (very rare in these datasets) and sparse spatial sampling. As both iLIDS-VID and PRID2011 provide only one tracklet per ID per camera (i.e. no fragmentation), it is impossible to have per-camera ID duplication. Therefore, each tracklet is assigned a unique label. The MARS gives multiple tracklets per ID per camera. Based on SSTT, at most only one tracklet can be sampled for each ID per camera (see Sec. 3.1). Therefore, a MARS tracklet per ID per camera was randomly selected and assigned a label. For all image based datasets, we assume all images per ID per camera were drawn from a single tracklet, same as in iLIDS-VID and PRID2011. The same tracklet label assignment procedure was adopted as above.</p><p>Performance Metrics We use the common cumulative matching characteristic (CMC) and mean Average Precision (mAP) metrics <ref type="bibr" target="#b60">[61]</ref>. Implementation Details We adopted an ImageNet pre-trained ResNet-50 <ref type="bibr" target="#b16">[17]</ref> as the backbone in evaluating the proposed TAUDL method. We set the feature dimension of the camera-shared representation space derived on top of ResNet-50 to 2,048. Each camera-specific branch contains one FC classification layer. Person images are resized to 256×128 for all datasets. To ensure that each batch has the capacity of containing person images from all cameras, we set the batch size to 384 for all datasets. For balancing the model learning speed over different cameras, we randomly selected the same number of training frame images per camera when sampling each mini-batch. We adopted the Adam optimiser <ref type="bibr" target="#b22">[23]</ref> with the initial learning rate of 3.5×10 −4 . We empirically set λ = 0.7 for Eq. (4), σ = 2 for Eq. (3), and K = T /2 (T is the number of cameras) for cross-view nearest tracklets N t i in Eq. (3) for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparisons to State-Of-The-Arts</head><p>We compared two different sets of state-of-the-art methods on image and video re-id datasets, due to the independent studies on them in the literature.</p><p>Unsupervised Person Re-ID on Image Datasets <ref type="table" target="#tab_2">Table 2</ref> shows the unsupervised re-id performance of the proposed TAUDL and 10 state-of-the-art methods including 3 hand-crafted feature based methods (Dic <ref type="bibr" target="#b24">[25]</ref>, ISR <ref type="bibr" target="#b31">[32]</ref>, RKSL <ref type="bibr" target="#b48">[49]</ref>) and 7 auxiliary knowledge (identity/attribute) transfer based models (AE <ref type="bibr" target="#b26">[27]</ref>, AML <ref type="bibr" target="#b53">[54]</ref>, UsNCA <ref type="bibr" target="#b39">[40]</ref>, CAMEL <ref type="bibr" target="#b55">[56]</ref>, JSTL <ref type="bibr" target="#b52">[53]</ref>, PUL <ref type="bibr" target="#b12">[13]</ref>, TJ-AIDL <ref type="bibr" target="#b49">[50]</ref>   Unsupervised Person Re-ID on Video Datasets We compared the proposed TAUDL with six state-of-the-art unsupervised video person re-id models. Unlike TAUDL, all these existing models are not end-to-end deep learning methods with either hand-crafted or separately trained deep features as model input. <ref type="table" target="#tab_3">Table 3</ref> shows that TAUDL outperforms all existing video-based person re-id models on the large scale video dataset MARS, e.g. by a Rank-1 margin of 7.0% <ref type="bibr">(43.8-36.8</ref>) over the best competitor DGM+IDE (which additionally using the ID label information of one camera view for model initialisation). However, TAUDL is inferior than some of the existing models on the two small benchmarks iLIDS-VID (300 training tracklets) and PRID2011 (178 training tracklets), in comparison to its performance on the MARS benchmark <ref type="bibr">(8,298 training tracklets)</ref>. This shows that TAUDL does need sufficient tracklet data from larger video datasets in order to have its performance advantage. As the tracklet data required are not manually labelled, this requirement is not a hindrance to its scalability to large scale data. Quite the contrary, TAUDL works the best when large scale unlabelled video data is available. A model would benefit particularly from pre-training using TAUDL on large auxiliary unlabelled video data from similar camera viewing conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Component Analysis and Discussions</head><p>Effectiveness of Per-Camera Tracklet Discrimination The PCTD component was evaluated by comparing a baseline that treats all cameras together by concatenating per-camera tracklet label sets and deploying the Cross-Entropy loss to learn a unified classification task. We call this baseline Joint-Camera Classification (JCC). In this analysis, we do not consider the cross-camera tracklet association component for a clear evaluation. <ref type="table" target="#tab_4">Table 4</ref> shows that our PCTD design is significantly superior over the JCC learning algorithm, e.g. achieving Rank-1 gain of 4.0%, 34.6%, 36.3%, and 19.9% on CUHK03, Market-1501, DukeMTMC, and MARS respectively. This verifies the modelling advantages of the proposed per-camera tracklet discrimination learning scheme on the unsupervised tracklet labels in inducing cross-view re-id discriminative feature learning. Effectiveness of Cross-Camera Tracklet Association The CCTA learning component was evaluated by testing the performance drop after eliminating it. <ref type="table" target="#tab_5">Table 5</ref> shows a significant performance benefit from this model component, e.g. a Rank-1 boost of 10.9%, 11.6%, 10.5%, and 5.8% on CUHK03, Market-1501, DukeMTMC, and MARS respectively. This validates the importance of modelling the correlation across cameras in discriminative optimisation and the effectiveness of our CCTA deep learning objective loss formulation in an endto-end manner. Additionally, this also suggests the effectiveness of the PCTD model component in facilitating the cross-view identity discrimination learning by providing re-id sensitive features in a joint incremental learning manner. Model Robustness Analysis Finally, we performed an analysis on model robustness against person ID duplication rates in tracklet labelling. We conducted a controlled evaluation on MARS where multiple tracklets per ID per camera are available for setting simulation. Recall that the ID duplication may mainly come with imperfect temporal sampling due to trajectory fragmentation and when some people stay in the same camera view for a longer time period than the temporal sampling gap. To simulate such a situation, we assume a varying percentage (10%∼50%) of IDs per camera have two random tracklets sampled and annotated with different tracklet labels. More tracklets per ID per camera are likely to be sampled, which can make this analysis more complex due to the interference from the number of duplicated person IDs. <ref type="table" target="#tab_6">Table 6</ref> shows that our TAUDL model is robust against the ID duplication rate, e.g. with only a Rank-1 drop of 3.1% given as high as 50% per-camera ID duplication rate. In reality, it is not too hard to minimise ID duplication rate among tracklets (Sec. 3.1), e.g. conducting very sparse sampling over time and space. Note, we do not care about exhaustive sampling of all the tracklets from video in a given time period. The model learning benefits from very sparse and diverse tracklet sampling from a large pool of unlabelled video data.</p><p>The robustness of our TAUDL comes with two model components:</p><p>(1) The model learning optimisation is not only subject to a single per-camera tracklet label constraint, but also concurrently to the constraints of all cameras. This facilitates optimising cross-camera tracklet association globally across all cameras in a common space, due to the Per-Camera Tracklet Discrimination learning mechanism (Eq. <ref type="formula" target="#formula_1">(2)</ref>). This provides model learning tolerance against per-camera tracklet label duplication errors. (2) The cross-camera tracklet association learning is designed as a feature similarity based "soft" objective learning constraint (Eq. (3)), without a direct dependence on the tracklet ID labels. Therefore, the ID duplication rate has little effect on this objective loss constraint. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we presented a novel Tracklet Association Unsupervised Deep Learning (TAUDL) model for unsupervised person re-identification using unsupervised person tracklet data extracted from videos, therefore eliminating the tedious and exhaustive manual labelling required by all supervised learning based re-id model learning. This enables TAUDL to be much more scalable to real-world re-id deployment at large scale video data. In contrast to most existing re-id methods that either require exhaustively pairwise labelled training data for every camera pair or assume the availability of additional labelled source domain training data for target domain adaptation, the proposed TAUDL model is capable of end-toend deep learning a discriminative person re-id model from scratch on totally unlabelled tracklet data. This is achieved by optimising jointly both the Per-Camera Tracklet Discrimination loss function and the Cross-Camera Tracklet Association loss function in a single end-to-end deep learning framework. To our knowledge, this is the first completely unsupervised learning based re-id model without any identity labels for model learning, neither pairwise cross-view image pair labelling nor single-view image identity class labelling. Extensive comparative evaluations were conducted on six image and video based re-id benchmarks to validate the advantages of the proposed TAUDL model over a wide range of state-of-the-art unsupervised and domain adaptation re-id methods. We also conducted in-depth TAUDL model component evaluation and robustness test to give insights on model performance advantage and model learning stability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Fig. 1 .</head><label>21</label><figDesc>An overview of Tracklet Association Unsupervised Deep Learning (TAUDL) re-id model: (a) Per-camera unsupervised tracklet sampling and label assignment; (b) Joint learning of both within-camera tracklet discrimination and cross-camera tracklet association in an end-to-end global deep learning on tracklets from all the cameras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>An illustration of the Sparse Space-Time Tracklet sampling and annotating method for unsupervised tracklet labelling. Solid box: Sampled tracklets; Dashed box: Non-sampled tracklets; Each colour represents a distinct person ID. (a) Two time instances (Si and Si+1 indicated by vertical lines) of temporal sampling are shown with a time gap P greater than the common transit time Q of a camera view. (b) Three spatially sparse tracklets are formed at a given temporal sampling instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Comparing (a) Fine-grained explicit instance-level cross-view ID labelled image pairs for supervised person re-id model learning and (b) Coarse-grained latent group-level cross-view tracklet (a multi-shot group) label correlation for ID label-free (unsupervised) person re-id learning using TAUDL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Example cross-view matched image/tracklet pairs from (a) CUHK03, (b) Market-1501, (c) DukeMTMC, (d) PRID2011, (e) iLIDS-VID, (f) MARS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset statistics and evaluation setting.</figDesc><table><row><cell>Dataset</cell><cell># ID</cell><cell># Train</cell><cell># Test</cell><cell># Images</cell><cell># Tracklet</cell></row><row><cell>iLIDS-VID [51]</cell><cell>300</cell><cell>150</cell><cell>150</cell><cell>43,800</cell><cell>600</cell></row><row><cell>PRID2011 [19]</cell><cell>178</cell><cell>89</cell><cell>89</cell><cell>38,466</cell><cell>354</cell></row><row><cell>MARS [60]</cell><cell>1,261</cell><cell>625</cell><cell>636</cell><cell>1,191,003</cell><cell>20,478</cell></row><row><cell>CUHK03 [29]</cell><cell>1,467</cell><cell>767</cell><cell>700</cell><cell>14,097</cell><cell>0</cell></row><row><cell>Market-1501 [61]</cell><cell>1,501</cell><cell>751</cell><cell>750</cell><cell>32,668</cell><cell>0</cell></row><row><cell>DukeMTMC [41]</cell><cell>1,812</cell><cell>702</cell><cell>1,110</cell><cell>36,411</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). These results show: (1) Among existing methods, the knowledge transfer based method is superior, e.g. on CUHK03, Rank-1 39.4% by CAMEL vs. 36.5% by Dic; On Market-1501, 58.2% by TJ-AIDL vs. 50.2% by Dic.</figDesc><table><row><cell>To that</cell></row><row><cell>end, CAMEL benefits from learning on 7 different person re-id datasets of di-</cell></row><row><cell>verse domains (CUHK03 [29], CUHK01 [28], PRID [19], VIPeR [16], 3DPeS [3],</cell></row><row><cell>i-LIDS [39], Shinpuhkan [21]) including a total of 44,685 images and 3,791 iden-</cell></row><row><cell>tities; TJ-AIDL utilises labelled Market-1501 (750 IDs and 27 attribute classes)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Unsupervised re-id on image datasets. 1 st /2 nd best results are in red/blue. IDs and 23 attribute classes) as source training data. (2) Our new model TAUDL outperforms all competitors with significant margins. For example, the Rank-1 margin by TAUDL over TJ-AIDL is 5.5% (63.7-58.2) on Market-1501 and 17.4% (61.7-44.3) on DukeMTMC. Moreover, it is worth pointing out that TAUDL dose not benefit from any additional labelled source domain training data as compared to TJ-AIDL. TAUDL is potentially more scalable due to no need to consider source and target domains similarities. (3) Our TAUDL is simpler to train with a simple end-to-end model learning, as compared to the alternated deep CNN training and clustering required by PUL and a two-stage model training of TJ-AIDL. These results show both the performance advantage and model design superiority of the proposed TAUDL model over a wide variety of state-of-the-art re-id models.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CUHK03 [29]</cell><cell cols="2">Market-1501 [61]</cell><cell cols="2">DukeMTMC [62]</cell></row><row><cell>Metric(%)</cell><cell cols="2">Rank-1 mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Dic [25]</cell><cell>36.5</cell><cell>-</cell><cell>50.2</cell><cell>22.7</cell><cell>-</cell><cell>-</cell></row><row><cell>ISR [32]</cell><cell>38.5</cell><cell>-</cell><cell>40.3</cell><cell>14.3</cell><cell>-</cell><cell>-</cell></row><row><cell>RKSL [49]</cell><cell>34.8</cell><cell>-</cell><cell>34.0</cell><cell>11.0</cell><cell>-</cell><cell>-</cell></row><row><cell>SAE [27]</cell><cell>30.5</cell><cell>-</cell><cell>42.4</cell><cell>16.2</cell><cell>-</cell><cell>-</cell></row><row><cell>JSTL [53]</cell><cell>33.2</cell><cell>-</cell><cell>44.7</cell><cell>18.4</cell><cell>-</cell><cell>-</cell></row><row><cell>AML [54]</cell><cell>31.4</cell><cell>-</cell><cell>44.7</cell><cell>18.4</cell><cell>-</cell><cell>-</cell></row><row><cell>UsNCA [40]</cell><cell>29.6</cell><cell>-</cell><cell>45.2</cell><cell>18.9</cell><cell>-</cell><cell>-</cell></row><row><cell>CAMEL [56]</cell><cell>39.4</cell><cell>-</cell><cell>54.5</cell><cell>26.3</cell><cell>-</cell><cell>-</cell></row><row><cell>PUL [13]</cell><cell>-</cell><cell>-</cell><cell>44.7</cell><cell>20.1</cell><cell>30.4</cell><cell>16.4</cell></row><row><cell>TJ-AIDL [50]</cell><cell>-</cell><cell>-</cell><cell>58.2</cell><cell>26.5</cell><cell>44.3</cell><cell>23.0</cell></row><row><cell>TAUDL</cell><cell>44.7</cell><cell>31.2</cell><cell>63.7</cell><cell>41.2</cell><cell>61.7</cell><cell>43.5</cell></row><row><cell>GCS [6](Supervised)</cell><cell>88.8</cell><cell>97.2</cell><cell>93.5</cell><cell>81.6</cell><cell>84.9</cell><cell>69.5</cell></row><row><cell>or DukeMTMC (702</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Unsupervised re-id on video datasets. 1 st /2 nd best results are in red/blue.</figDesc><table><row><cell>Dataset</cell><cell>PRID2011 [19] iLIDS-VID [51]</cell><cell></cell><cell cols="2">MARS [60]</cell><cell></cell></row><row><cell>Metric(%)</cell><cell cols="5">R1 R5 R20 R1 R5 R20 R1 R5 R20 mAP</cell></row><row><cell>DTW [37]</cell><cell>41.7 67.1 90.1 31.5 62.1 82.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GRDL [24]</cell><cell cols="5">41.6 76.4 89.9 25.7 49.9 77.6 19.3 33.2 46.5 9.56</cell></row><row><cell>UnKISS [22]</cell><cell cols="5">58.1 81.9 96.0 35.9 63.3 83.4 22.3 37.4 53.6 10.6</cell></row><row><cell>SMP [35]</cell><cell cols="5">80.9 95.6 99.4 41.7 66.3 80.7 23.9 35.8 44.9 10.5</cell></row><row><cell cols="6">DGM+MLAPG [55] 73.1 92.5 99.0 37.1 61.3 82.0 24.6 42.6 57.2 11.8</cell></row><row><cell>DGM+IDE [55]</cell><cell cols="5">56.4 81.3 96.4 36.2 62.8 82.7 36.8 54.0 68.5 21.3</cell></row><row><cell>TAUDL</cell><cell cols="5">49.4 78.7 98.9 26.7 51.3 82.0 43.8 59.9 72.8 29.1</cell></row><row><cell cols="6">QAN [34](Supervised) 90.3 98.2 100.0 68.0 86.8 97.4 73.7 84.9 91.6 51.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Effect of Per-Camera Tracklet Discrimination (PCTD) learning.</figDesc><table><row><cell>Dataset</cell><cell cols="7">CUHK03 [29] Market-1501 [61] DukeMTMC [41] MARS [60]</cell></row><row><cell cols="2">Metric(%) R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1 mAP</cell></row><row><cell>JCC</cell><cell>29.8</cell><cell>12.5</cell><cell>17.5</cell><cell>7.9</cell><cell>14.9</cell><cell>3.5</cell><cell>18.1 13.1</cell></row><row><cell>PCTD</cell><cell cols="2">33.8 18.9</cell><cell>52.1</cell><cell>26.6</cell><cell>51.2</cell><cell>32.9</cell><cell>38.0 23.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Effect of Cross-Camera Tracklet Association (CCTA)</figDesc><table><row><cell cols="8">Dataset CUHK03 [29] Market-1501 [61] DukeMTMC [62] MARS [60]</cell></row><row><cell>CCTA</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1 mAP</cell></row><row><cell></cell><cell>33.8</cell><cell>18.9</cell><cell>52.1</cell><cell>26.6</cell><cell>51.2</cell><cell>32.9</cell><cell>38.0 23.9</cell></row><row><cell></cell><cell cols="2">44.7 31.2</cell><cell>63.7</cell><cell>41.2</cell><cell>61.7</cell><cell>43.5</cell><cell>43.8 29.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Model robustness analysis on varying ID duplication rates on MARS<ref type="bibr" target="#b59">[60]</ref>.</figDesc><table><row><cell>ID Duplication Rate (%)</cell><cell cols="4">Rank-1 Rank-5 Rank-10 Rank-20</cell><cell>mAP</cell></row><row><cell>0</cell><cell>43.8</cell><cell>59.9</cell><cell>66.0</cell><cell>72.8</cell><cell>29.1</cell></row><row><cell>10</cell><cell>42.8</cell><cell>59.7</cell><cell>65.5</cell><cell>71.6</cell><cell>28.3</cell></row><row><cell>20</cell><cell>42.2</cell><cell>58.8</cell><cell>64.7</cell><cell>70.6</cell><cell>27.4</cell></row><row><cell>30</cell><cell>41.6</cell><cell>57.9</cell><cell>64.5</cell><cell>69.7</cell><cell>26.7</cell></row><row><cell>50</cell><cell>40.7</cell><cell>57.0</cell><cell>63.4</cell><cell>69.6</cell><cell>25.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Exhaustive manual ID labelling of person image pairs for every camera-pair is prohibitively expensive as there are a quadratic number of camera pairs in a network.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although object tracklets can be generated by any independent single-camera-view multi-object tracking (MOT) models widely available today, a conventional MOT model is not end-to-end optimised for cross-camera tracklet association.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Marks: An improved deep learning architecture for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jones</forename><surname>Ahmed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3dpes: 3d people dataset for surveillance and forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J-HGBU</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep unsupervised similarity learning using partially ordered sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cliquecnn: Deep unsupervised exemplar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tikhoncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification by camera correlation aware feature augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="392" to="408" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving person re-identification via pose-aware multi-shot matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Person reidentification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person reidentification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>SCIA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep low-resolution person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Shinpuhkan2014: A multicamera pedestrian dataset for tracking people across multiple cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kawanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mukunoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minoh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>FCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised data association for metric learning in the context of multi-shot person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AVSS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Person re-identification by unsupervised l1 graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dictionary learning with iterative laplacian regularisation for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">Motchallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Sparse deep belief net model for visual area v2</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multiloss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Harmonious attention network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person re-identification by iterative re-weighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1629" to="1642" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-supervised coupled dictionary learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Quality aware network for set to set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stepwise metric promotion for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Time-delayed correlation analysis for multi-camera activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="106" to="129" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Person reidentification by unsupervised video matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Person re-identification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised neighborhood component analysis for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="609" to="617" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mittal: Deep neural networks with inexact matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chatterjee</forename><surname>Subramaniam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Joint learning of single-image and cross-image representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised learning of generative topic saliency for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Person re-identification in identity regression space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Towards unsupervised open-set person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Person re-identification by discriminative selection in video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2501" to="2514" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Adaptive distance metric learning for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Dynamic label graph matching for unsupervised video re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">How far are we from solving pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Person re-identification by saliency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Oyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="356" to="370" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fast openworld person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="page" from="2286" to="2300" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
