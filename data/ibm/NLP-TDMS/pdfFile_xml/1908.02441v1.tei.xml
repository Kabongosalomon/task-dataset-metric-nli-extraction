<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoong</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Div. of EE</orgName>
								<orgName type="institution">Hanyang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung</forename><forename type="middle">Jin</forename><surname>Chang</surname></persName>
							<email>h.j.chang@bham.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyuewang</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
							<email>jychoi@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of ECE</orgName>
								<orgName type="institution" key="instit1">ASRI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a symmetric graph convolutional autoencoder which produces a low-dimensional latent representation from a graph. In contrast to the existing graph autoencoders with asymmetric decoder parts, the proposed autoencoder has a newly designed decoder which builds a completely symmetric autoencoder form. For the reconstruction of node features, the decoder is designed based on Laplacian sharpening as the counterpart of Laplacian smoothing of the encoder, which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture. In order to prevent the numerical instability of the network caused by the Laplacian sharpening introduction, we further propose a new numerically stable form of the Laplacian sharpening by incorporating the signed graphs. In addition, a new cost function which finds a latent representation and a latent affinity matrix simultaneously is devised to boost the performance of image clustering tasks. The experimental results on clustering, link prediction and visualization tasks strongly support that the proposed model is stable and outperforms various state-of-theart algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A graph, which consists of a set of nodes and edges, is a powerful tool to seek the geometric structure of data. There are various applications using graphs in the machine learning and data mining fields such as node clustering <ref type="bibr" target="#b25">[26]</ref>, dimensionality reduction <ref type="bibr" target="#b0">[1]</ref>, social network analysis <ref type="bibr" target="#b14">[15]</ref>, chemical property prediction of a molecular graph <ref type="bibr" target="#b6">[7]</ref>, and image segmentation <ref type="bibr" target="#b29">[30]</ref>. However, conventional methods for analyzing a graph have several problems such as low computational efficiency due to eigendecomposition or singular value decomposition, or only showing a shallow relationship between nodes.</p><p>In recent years, an emerging field called geometric deep learning <ref type="bibr" target="#b1">[2]</ref>, generalizes deep neural network models to <ref type="bibr">1 2</ref> ( )</p><formula xml:id="formula_0">  መ  Encoder</formula><p>Decoder (a) VGAE <ref type="bibr" target="#b12">[13]</ref>   Single-layer Autoencoder = ҧ .</p><p>(b) MGAE <ref type="bibr" target="#b34">[35]</ref> Encoder Decoder   non-Euclidean domains such as meshes, manifolds, and graphs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20]</ref>. Among them, finding deep latent representations of geometrical structures of graphs using an autoencoder framework is getting growing attention. The first attempt is VGAE <ref type="bibr" target="#b12">[13]</ref> which consists of a Graph Convolutional Network (GCN) <ref type="bibr" target="#b13">[14]</ref> encoder and a matrix outerproduct decoder as shown in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. As a variant of VGAE, ARVGA <ref type="bibr" target="#b26">[27]</ref> has been proposed by incorporating an adversarial approach to VGAE. However, VGAE and ARVGA were designed to reconstruct the affinity matrix A instead of node feature matrix X. Hence, the decoder part cannot be learnable, therefore, the graphical feature cannot be used at all in the decoder part. These facts can degrade the capability of graph learning. Following that, MGAE <ref type="bibr" target="#b34">[35]</ref> has been proposed, which uses stacked single layer graph autoencoder with linear activation function and marginalization process as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b). However, since the MGAE reconstructs the feature matrix of nodes without hidden layers, it cannot manipulate the dimension of the latent representation and performs a linear mapping. This is a distinct limitation in finding a latent representation that clearly reveals the structure of the graph.</p><p>To overcome the limitation of the existing graph convolutional autoencoders, in this paper, we propose a novel graph convolutional autoencoder framework which has symmetric autoencoder architecture and uses both graph and node attributes in both the encoding and decoding processes as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (c). Our design of the decoder part is motivated from the analysis in a recent paper <ref type="bibr" target="#b18">[19]</ref>, that the encoder of VGAE <ref type="bibr" target="#b12">[13]</ref> can be interpreted as a special form of Laplacian smoothing <ref type="bibr" target="#b31">[32]</ref> that computes the new representation of each node as a weighted local average of neighbors and itself. This interpretation has inspired us to design a decoder to perform Laplacian sharpening, which is a counterpart of Laplacian smoothing. To realize a decoder to do Laplacian sharpening, we express Laplacian sharpening in the form of Chebyshev polynomial and newly reformulate it in a numerically stable form by utilizing a signed graph <ref type="bibr" target="#b17">[18]</ref>.</p><p>In computer vision fields, there is a popular assumption that, even though image datasets are high-dimensional in their ambient spaces, they usually reside in multiple low-dimensional subspaces <ref type="bibr" target="#b33">[34]</ref>. Thus, especially for image clustering tasks, we apply the concept of subspace clustering, which has such an assumption about the input data in its own definition, to our graph convolutional autoencoder framework. Specifically, to find a latent representation and a latent affinity matrix simultaneously, we merge a subspace clustering cost function into the reconstruction cost of the autoencoder. Contrary to the conventional subspace clustering cost function <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39]</ref>, we could derive a computationally efficient cost function.</p><p>The main contributions of this paper are summarized as follows:</p><p>• We propose the first completely symmetric graph convolutional autoencoder which utilizes both the structure of the graph and node attributes through the whole encoding-decoding process.</p><p>• We derive a new numerically stable form of decoder preventing the numerical instability of the neural network.</p><p>• We design a computationally efficient subspace clustering cost to find both latent representation and a latent affinity matrix simultaneously for image clustering tasks.</p><p>In experiments, the validity of the proposed components is shown by doing ablation experiments on our architecture and cost function. Also, the superior performance of the proposed method is validated by comparing it with the state-of-the-art methods and visualizing the graph clustered by our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Basic notations on graphs</head><p>A graph is represented as G = (V, E, A), where V denotes the node set with v i ∈ V and |V| = n, E denotes the edge set with (v i , v j ) ∈ E, and A ∈ IR n×n denotes an affinity matrix which encodes pairwise affinities between nodes. D ∈ IR n×n denotes a degree matrix which is a diagonal matrix with D ii = j A ij . Unnormalized graph Laplacian ∆ is defined by ∆ = D − A <ref type="bibr" target="#b4">[5]</ref>. Symmetric graph Laplacian L and random walk graph Laplacian L rw are defined by L = I n − D − 1 2 AD − 1 2 and L rw = I n − D −1 A respectively, where I n ∈ IR n×n denotes an identity matrix. Note that the ∆, L and L rw are positive semidefinite matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Spectral convolution on graphs</head><p>A spectral convolution on a graph <ref type="bibr" target="#b30">[31]</ref> is the multiplication of an input signal x ∈ IR n with a spectral filter g θ = diag(θ) parameterized by the vector of Fourier coefficients θ ∈ IR n as follows:</p><formula xml:id="formula_1">g θ * x = U g θ U T x,<label>(1)</label></formula><p>where U is the matrix of eigenvectors of the symmetric graph Laplacian L = U ΛU T . U T x is the graph Fourier transform of the input x, and g θ is a function of the eigenvalues of L, i.e., g θ (Λ), where Λ is the diagonal matrix of eigenvalues of L. However, this operation is inappropriate for large-scale graphs since it requires an eigendecomposition to obtain the eigenvalues and eigenvectors of L. To avoid computationally expensive operations, the spectral filter g θ (Λ) was approximated by K th order Chebyshev polynomials in previous works <ref type="bibr" target="#b9">[10]</ref>. By doing so, the spectral convolution on the graph can be approximated as</p><formula xml:id="formula_2">g θ * x ≈ U K k=0 θ k T k (Λ)U T x = K k=0 θ k T k (L)x,<label>(2)</label></formula><p>where T k (·) and θ denote the Chebyshev polynomials and a vector of the Chebyshev coefficients respectively.Λ is 2 λmax Λ − I n , λ max denotes the largest eigenvalue of L and L is UΛU T = 2 λmax L−I n . The approximated model above is used as a building block of a convolution on graphs in <ref type="bibr" target="#b5">[6]</ref>.</p><p>In the GCN <ref type="bibr" target="#b13">[14]</ref>, the Chebyshev approximation model was simplified by setting K = 1, λ max ≈ 2 and θ = θ 0 = −θ 1 . This makes the spectral convolution simplified as follows:</p><formula xml:id="formula_3">g θ * x ≈ θ(I n + D − 1 2 AD − 1 2 )x.<label>(3)</label></formula><p>However, repeated application of I n + D − 1 2 AD − 1 2 can cause numerical instabilities in neural networks since the spectral radius of I n + D − 1 2 AD − 1 2 is 2, and the Chebyshev polynomials form an orthonormal basis when its spectral radius is 1. To circumvent this issue, the GCN uses renormalization trick:</p><formula xml:id="formula_4">I n + D − 1 2 AD − 1 2 →D − 1 2ÃD − 1 2 ,<label>(4)</label></formula><p>whereÃ = A + I n andD ii = jÃ ij . Since adding selfloop on nodes to an affinity matrix cannot affect the spectral radius of the corresponding graph Laplacian matrix <ref type="bibr" target="#b8">[9]</ref>, this renormalization trick can provide a numerically stable form of I n + D − 1 2 AD − 1 2 while maintaining the meaning of each elements as follows:</p><formula xml:id="formula_5">(I n + D − 1 2 AD − 1 2 ) ij = 1 i = j A ij / D ii D jj i = j (5) (D − 1 2ÃD − 1 2 ) ij = 1/(D ii + 1) i = j A ij / (D ii + 1)(D jj + 1) i = j.</formula><p>(6) Finally, the forward-path of the GCN can be expressed by</p><formula xml:id="formula_6">H (m+1) = ξ(D − 1 2ÃD − 1 2 H (m) Θ (m) ),<label>(7)</label></formula><p>where H (m) is the activation matrix in the m th layer and H (0) is the input nodes' feature matrix X. ξ(·) is a nonlinear activation function like ReLU(·) = max(0, ·), and Θ (m) is a trainable weight matrix. The GCN presents a computationally efficient convolutional process (given the assumption thatÃ is sparse) and achieves an improved accuracy over the state-of-the-art methods in semi-supervised node classification task by using features of nodes and geometric structure of graph simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Laplacian smoothing</head><p>Li et al. <ref type="bibr" target="#b18">[19]</ref> demystify GCN <ref type="bibr" target="#b13">[14]</ref> and show that GCN is a special form of Laplacian smoothing <ref type="bibr" target="#b31">[32]</ref>. Laplacian smoothing is a process that calculates a new representation of the input as a weighted local average of its neighbors and itself. When we add a self-loop on the nodes, the affinity matrix becomesÃ = A+I n and the degree matrix becomes D = D + I n . Then, the Laplacian smoothing equation is given as follows:</p><formula xml:id="formula_7">x (m+1) i = (1 − γ)x (m) i + γ jÃ ij D ii x (m) j ,<label>(8)</label></formula><formula xml:id="formula_8">where x (m+1) i is the new representation of x (m) i</formula><p>, and γ (0 &lt; γ ≤ 1) is a regularization parameter which controls the importance between itself and its neighbors. We can rewrite the above equation in a matrix form as follows:</p><formula xml:id="formula_9">X (m+1) = (1 − γ)X (m) + γD −1Ã X (m) = X (m) − γ(I n −D −1Ã )X (m)<label>(9)</label></formula><p>= X (m) − γL rw X (m) .</p><p>If we set γ = 1 and replaceL rw withL, then Eq. (9) is changed into X (m+1) =D − 1 2ÃD − 1 2 X (m) and this equation is the same as the renormalized version of spectral convolution in Eq. <ref type="bibr" target="#b6">(7)</ref>. From the above interpretation, Li et al. explain that the superior performance of GCN in semi-supervised node classification task is due to Laplacian smoothing which makes the features of nodes in the same clusters become similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed method</head><p>In this section, we propose a novel graph convolutional autoencoder framework, named as GALA (Graph convolutional Autoencoder using LAplacian smoothing and sharpening). In GALA, there are M layers in total, from the first to M 2 th layers for the encoder and from the M 2 + 1 th to M th layers for the decoder where M is an even number. The encoder part of GALA is designed to perform the computationally efficient spectral convolution on the graph with a numerically stable form of Laplacian smoothing in the Eq. (7) <ref type="bibr" target="#b13">[14]</ref>. Along with this, its decoder part is designed to be a special form of Laplacian sharpening <ref type="bibr" target="#b31">[32]</ref>, unlike the existing VGAE-related algorithms. By this decoder part, GALA reconstructs the feature matrix of nodes directly, instead of yielding an affinity matrix as in the existing VGAE-related algorithms whose decoder parts are incomplete. Furthermore, to enhance the performance of image clustering, we devise a computationally efficient subspace clustering cost term which is added to the reconstruction cost of GALA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Laplacian sharpening</head><p>Because the encoder performs Laplacian smoothing that makes the latent representation of each node similar to those of its neighboring nodes, we design the decoder part to perform Laplacian sharpening as the counterpart of Laplacian smoothing. Laplacian sharpening is a process that makes the reconstructed feature of each node farther away from the centroid of its neighbors, which accelerates the reconstruction along with the reconstruction cost and is governed by</p><formula xml:id="formula_10">x (m+1) i = (1 + γ)x (m) i − γ j A ij D ii x (m) j ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_11">x (m+1) i is the new representation of x (m) i</formula><p>, and γ is the regularization parameter which controls the importance between itself and its neighbors. The matrix form of Eq. (10) is given by</p><formula xml:id="formula_12">X (m+1) = (1 + γ)X (m) − γD −1 AX (m) = X (m) + γ(I n − D −1 A)X (m)<label>(11)</label></formula><p>= X (m) + γL rw X (m) .</p><p>Analogous to the encoder, we set γ = 1 and replace L rw with L. Similar to Eq. (3), we can express Laplacian sharp-ening in the form of Chebyshev polynomial and simplify it with K = 1, λ max ≈ 2, and θ = 1 2 θ 0 = θ 1 . Then, a decoder layer can be expressed by <ref type="bibr" target="#b11">(12)</ref> where H (m) is the matrix of the activation in the m th layer,</p><formula xml:id="formula_13">H (m+1) = ξ((2I n − D − 1 2 AD − 1 2 )H (m) Θ (m) ),</formula><formula xml:id="formula_14">2I n − D − 1 2 AD − 1 2</formula><p>is a special form of Laplacian sharpening, ξ(·) is the nonlinear activation function like ReLU(·) = max(0, ·), and Θ (m) is a trainable weight matrix. However, since the spectral radius of 2I n − D − 1 2 AD − 1 2 is 3, repeated application of this operator can be numerically instable. Hence, as GCN finds a numerically stable form of Chebyshev polynomials, we have to find a numerically stable form of Laplacian sharpening while maintaining its meaning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Numerically stable Laplacian sharpening</head><p>To find a new representation of Laplacian sharpening whose spectral radius is 1, we use a signed graph <ref type="bibr" target="#b17">[18]</ref>. A signed graph is denoted by Γ = (V, E,Â) which is induced from the unsigned graph G = (V, E, A), where each element inÂ has the same absolute value with A, but its sign is changed into minus or keeps plus. The degree matrix of the signed graph Γ is denoted byD which is obtained fromÂ. In the signed graph, a problem occurs when calculating the degree matrixD by the conventional way that may cancel the mixed signed weights in summation and so fails to yield the degree value representing the connectivity of a node to its neighbors. Thus, by following the practice for signed graphs, we calculate the degree of each node bŷ D ii = j |Â ij | that has the same value (degree of connectivity) as in the unsigned graph. By usingÂ andD, we can construct an unnormalized graph Laplacian∆ =D −Â and symmetric graph LaplacianL = I n −D − 1 2ÂD − 1 2 of the signed graph. From Theorem 1 of <ref type="bibr" target="#b17">[18]</ref>, the range of the eigenvalue ofL is [0, 2], thus the spectral radius of D − 1 2ÂD − 1 2 is 1 for any choice ofÂ. Using this result, instead of Eq. (12), we use a numerically stable form of Laplacian sharpening with spectral radius of 1, given by</p><formula xml:id="formula_15">H (m+1) = ξ(D − 1 2ÂD − 1 2 H (m) Θ (m) ).<label>(13)</label></formula><p>The remaining issue is to chooseÂ induced from A so thatD − 1 2ÂD − 1 2 maintains the meaning of each element of 2I n − D − 1 2 AD − 1 2 in Eq. <ref type="bibr" target="#b11">(12)</ref>. To achieve this, we map all weights of the unsigned A to negative weights and adding a self-loop with a weight value 2 to each node, that is, which has the same meaning with the original one given by</p><formula xml:id="formula_16">A = 2I n − A andD = 2I n + D. Then, each element ofD − 1 2ÂD − 1 2 is obtained by (D − 1 2ÂD − 1 2 ) ij = 2/(D ii + 2) i = j −A ij / (D ii + 2)(D jj + 2) i = j,<label>(14)</label></formula><formula xml:id="formula_17">(2I n − D − 1 2 AD − 1 2 ) ij = 2 i = j −A ij / D ii D jj i = j.</formula><p>(15) From Eqs. <ref type="bibr" target="#b12">(13)</ref>, <ref type="bibr" target="#b13">(14)</ref> and <ref type="formula" target="#formula_1">(15)</ref>, the numerically stable decoder layer of GALA is given as</p><formula xml:id="formula_18">H (m+1) = ξ(D − 1 2ÂD − 1 2 H (m) Θ (m) ), (m = M 2 , ..., M − 1),<label>(16)</label></formula><p>whereÂ = 2I n − A andD = 2I n + D. The encoder part of GALA is constructed by using Eq. <ref type="formula" target="#formula_6">(7)</ref> as in GCN <ref type="bibr" target="#b13">[14]</ref> as</p><formula xml:id="formula_19">H (m+1) = ξ(D − 1 2ÃD − 1 2 H (m) Θ (m) ), (m = 0, ..., M 2 − 1),<label>(17)</label></formula><p>where H (0) = X is the feature matrix of the input nodes, A = I n + A andD = I n + D. The complexity of propagation functions, Eqs. <ref type="formula" target="#formula_1">(16)</ref> and <ref type="formula" target="#formula_1">(17)</ref>, are both O(mpc), where m is the cardinality of edges in the graph, p is the feature dimension of the previous layer, and c is the feature dimension of the current layer. Since the complexity is linear in the number of edges in the graph, the proposed algorithm is computationally efficient (given the assumption that A is sparse). Also, from Eq. <ref type="formula" target="#formula_1">(17)</ref>, since GALA decodes the latent representation using both the graph structure and node features, the enhanced decoder of GALA can help to find more distinct latent representation.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we show the reason why the Laplacian smoothing is not appropriate to the decoder and the necessity of numerically stable Laplacian sharpening by node clustering experiments (the higher values imply the more correct results). Laplacian smoothing decoder (Eq. 7) shows the lowest performances, since Laplacian smoothing which makes the representation of each node similar to those of its neighboring nodes conflicts with the purpose of reconstruction cost. A numerically instable form of Laplacian sharpening decoder (Eq. 12) shows higher performance compared to smoothing decoder because the role of Laplacian sharpening coincide with reconstructing the node feature. The performance of proposed numerically stable Laplacian sharpening decoder (Eq. 16) significantly higher than others, since it solves instability issue of neural network while maintaining the meaning of original Laplacian sharpening.</p><p>The basic cost function of GALA is given by whereX is the reconstructed feature matrix of nodes, the column ofX corresponds to the output of the decoder for an input feature of a node, and · F denotes the Frobenius norm.</p><formula xml:id="formula_20">min X 1 2 X −X 2 F ,<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Subspace clustering cost for image clustering</head><p>It is a well-known assumption that image datasets are often drawn from multiple low-dimensional subspaces, although their data dimensions are high. Accordingly, subspace clustering, which has such an assumption about the input data in its own definition, has shown prominent clustering performance on various image datasets. Hence, we add an element of subspace clustering to the proposed method in the case of image clustering tasks. Among the various subspace clustering models, we add Least Squares Regression (LSR) <ref type="bibr" target="#b21">[22]</ref> model for computational efficiency. Then the cost function for training of GALA becomes</p><formula xml:id="formula_21">min X,H,A H 1 2 X −X 2 F + λ 2 H − HAH 2 F + µ 2 AH 2 F ,<label>(19)</label></formula><p>where H ∈ IR k×n denotes the latent representations (i.e., the output of the encoder), A H ∈ IR n×n denotes the affinity matrix which is a new latent variable for subspace clustering, and λ, µ are the regularization parameters. The second term of Eq. (19) aims at the self-expressive model of subspace clustering and the third term of Eq. (19) is for regularizing A H . If we only consider minimizing A H , the problem becomes:</p><formula xml:id="formula_22">min A H λ 2 H − HA H 2 F + µ 2 A H 2 F .<label>(20)</label></formula><p>We can easily obtain the analytic solution A * H = (H T H + µ λ I n ) −1 H T H by the fact that LSR model is quadratic on the variable A H . By using this analytic solution and singular value decomposition, we derive a computationally efficient subspace clustering cost function as follows (The details are reported in the supplementary material):</p><formula xml:id="formula_23">min X,H 1 2 X −X 2 F + µλ 2 tr((µI k + λHH T ) −1 HH T ),<label>(21)</label></formula><p>where tr(·) denotes the trace of the matrix. The above problem can be solved by k ×k matrix inversion instead of n×n matrix inversion. Since the dimension of latent representation (k) is much smaller than the number of nodes (n), this simplification can reduce the computational burden significantly from O(n 3 ) to O(k 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>We train GALA to minimize Eq. (18) by using the ADAM algorithm <ref type="bibr" target="#b11">[12]</ref>. We train GALA deterministically by using the full batch in each training epoch and stop when the cost is converged, so the number of epochs of each dataset varies. Note here that using the full batch during training is a common approach in neural networks based on spectral convolution on graph. Specifically, we set the learning rate to 1.0 × 10 −4 for training and train GALA in an unsupervised way without any cluster labels. When the subspace clustering cost is added to reconstruction cost for image clustering tasks, we use pre-training and finetuning strategies similar to the ones in <ref type="bibr" target="#b10">[11]</ref> to train GALA. First, in the pre-training stage, the training method is the same as that of minimizing Eq. <ref type="bibr" target="#b17">(18)</ref>. After pre-training, we fine-tune GALA to minimize Eq. (21) using ADAM. As in the pre-training, we train GALA deterministically by using full batch in each training epoch, and we set the number of epochs of the fine-tuning stage as 50 for all dataset. We set the learning rate to 1.0 × 10 −6 for fine-tuning.</p><p>After the training process are over, we construct knearest neighborhood graph using attained latent representations H * . Then we perform spectral clustering <ref type="bibr" target="#b25">[26]</ref> and get the clustering performance. In the case of image clustering, after all training processes are over, we construct the optimal affinity matrix A * H noted in the previous subsection by using the attained latent representation matrix H * from GALA. Then we perform spectral clustering <ref type="bibr" target="#b25">[26]</ref> on the affinity matrix and get the optimal clustering with respect to our cost function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use four network datasets (Cora, Citeseer, Wiki, and Pubmed) and three image datasets (COIL20, YALE, and MNIST) for node clustering and link prediction tasks. Every network dataset has the feature matrix X and the affinity matrix A and every image dataset has the feature matrix X only. The summary of each dataset are presented in <ref type="table" target="#tab_2">Table 3</ref> and details are reported in the supplementary material. Also, the sample images of each image dataset are described in <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental settings</head><p>To measure the performance of node clustering task, we use three metrics: accuracy (ACC), normalized mutual in-   <ref type="bibr" target="#b36">[37]</ref> 2405 4973 17 17981 Pubmed <ref type="bibr" target="#b28">[29]</ref> 19717 500 3 44338 COIL20 <ref type="bibr" target="#b24">[25]</ref> 1440 1024 20 − YALE <ref type="bibr" target="#b7">[8]</ref> 5850 1200 10 − MNIST <ref type="bibr" target="#b15">[16]</ref> 10000 784 10 − formation (NMI), and adjusted rand index (ARI) as in <ref type="bibr" target="#b34">[35]</ref>. We report the mean values of the three metrics for each algorithm after executing 50 times, and the higher values imply the more correct results. For link prediction task, we partitioned the dataset following the work of GAE <ref type="bibr" target="#b12">[13]</ref>, and reported mean scores and standard errors of Area Under Curve (AUC) and Average Precision (AP) with 10 random initializations. The implementation details such as hyperparameters are reported in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparing methods</head><p>We compare the performance of 15 algorithms. Compared algorithms can be categorized into four groups as described below:</p><p>• i) Using features only. 'Kmeans' <ref type="bibr" target="#b20">[21]</ref> is the K-means clustering based on only the features of the data, which is the baseline clustering algorithm in our experiment.</p><p>• ii) Using network structures only. 'Spectral' <ref type="bibr" target="#b25">[26]</ref> is a spectral clustering algorithm using eigendecomposition on graph Laplacian. 'Big-Clam' <ref type="bibr" target="#b37">[38]</ref> is a large-scale community detection algorithm utilizing a variant of nonnegative matrix factorization. 'DeepWalk' <ref type="bibr" target="#b27">[28]</ref> learns the latent social representation of nodes using local information through a neural network. 'GraEnc' <ref type="bibr" target="#b32">[33]</ref> is a graphencoding neural network derived from the relation be-tween autoencoder and spectral clustering. 'DNGR' <ref type="bibr" target="#b2">[3]</ref> generates a low-dimensional representation of each node by using a graph structure and a stacked denoised autoencoder.</p><p>• iii) Using both. 'Circles' <ref type="bibr" target="#b16">[17]</ref> is an algorithm which discovers social circles through a node clustering algorithm. 'RTM' <ref type="bibr" target="#b3">[4]</ref> presents a relational topic model of documents and links between the documents. 'RMSC' <ref type="bibr" target="#b35">[36]</ref> is a robust multi-view spectral clustering algorithm which can handle noises in the data and recover transition matrix through low-rank and sparse decomposition. 'TADW' <ref type="bibr" target="#b36">[37]</ref> interprets DeepWalk from the view of matrix factorization and incorporates text features of nodes.</p><p>• iv) Using both with spectral convolution on graphs. 'GAE' <ref type="bibr" target="#b12">[13]</ref> is the first attempt to graft the spectral convolution on graphs onto autoencoder framework. 'VGAE' <ref type="bibr" target="#b12">[13]</ref> is the variational variant of GAE. 'MGAE' <ref type="bibr" target="#b34">[35]</ref> is an autoencoder which combines the marginalization process with spectral convolution on graphs. 'ARGA' <ref type="bibr" target="#b26">[27]</ref> learns the latent representation by adding an adversarial model to a non-probabilistic variant of VGAE. 'ARVGA' <ref type="bibr" target="#b26">[27]</ref> is an algorithm which adds an adversarial model to VGAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Node clustering results</head><p>The experimental results of node clustering are presented in <ref type="table" target="#tab_1">Table 2</ref>. It can be observed that for every dataset, the methods which use features and network structures simultaneously show better performance than the methods which use only one of them. Furthermore, among the methods which use both features and network structures, algorithms with neural network models which exploit spectral convolution on graphs present outstanding   <ref type="bibr" target="#b25">[26]</ref> 0.5282 0.0971 0.0620 GAE <ref type="bibr" target="#b12">[13]</ref> 0.6861 0.2957 0.3046 VGAE <ref type="bibr" target="#b12">[13]</ref> 0.6887 0.3108 0.3018 MGAE <ref type="bibr" target="#b34">[35]</ref> 0.5932 0.2822 0.2483 ARGA <ref type="bibr" target="#b26">[27]</ref> 0.6807 0.2757 0.2910 ARVGA <ref type="bibr" target="#b26">[27]</ref> 0.5130 0.1169 0.0777 GALA 0.6939 0.3273 0.3214 performance since they can learn deeper relationships between nodes than the methods which do not use spectral convolution on graphs.</p><p>In every experiments, GALA shows superior performance to other methods. Especially, for the Cora dataset, GALA outperforms VGAE, which is the first graph convolution autoencoder framework, by about 24.39%, 24.75% and 27.68%, and MGAE, which is the state-of-the-art graph convolutional autoencoder algorithm, by about 6.15%, 6.56% and 8.68% on ACC, NMI and ARI, respectively. The better performance of GALA comes from the better decoder design based on the numerically stable form of Laplacian sharpening both and full utilizing of graph structure and node attributes in the whole autoencoder framework. Furthermore, we conduct another node clustering experiment on a large network dataset (Pubmed), and the results are reported in <ref type="table" target="#tab_4">Table 5</ref>. We can observe that GALA outperforms every baselines and state-of-the-art graph convolution algorithms. Although Kmeans clustering, a baseline algorithm, shows higher performance over several graph convolution algorithms on NMI and ARI, the proposed method presents better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Image clustering results</head><p>The experimental results of image clustering are presented in <ref type="table" target="#tab_3">Table 4</ref>. We report both GALA's performance of reconstruction cost only case and the subspace clustering cost added case (GALA+SCC). It can be seen that GALA outperforms several baselines and the state-of-the-art graph convolution algorithms for most of the cases. Also, for ev-ery case, the proposed subspace clustering cost term contributes to improve the performance of the image clustering. On the YALE dataset, notably, we can observe that the proposed subspace clustering cost term significantly enhances the image clustering performance and achieves nearly perfect accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation studies</head><p>We validate the effectiveness of the proposed stable decoder and the subspace clustering cost by image clustering experiments on the three image datasets (COIL20, YALE and MNIST). There are four configurations as shown in Table 6. We would like to note that the reconstruction cost only (Eq. 18) is a subset of subspace clustering cost (Eq. 21), thus the last configuration is the full proposed method. Reported numbers are mean values after executing 50 times. It can be clearly noticed that the numerically stable form of Laplacian sharpening and subspace clustering cost are helpful to find the latent representations which reflect the graph structures certainly and using both components can boost the performance of clustering. In addition, it can be seen that the stable decoder with the reconstruction cost only outperforms the state-of-the-art algorithms in most cases because GALA can utilize the graph structure in the whole processes of the autoencoder architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Link prediction results</head><p>We provide some results on link prediction task on Citeseer dataset. For link prediction task, we minimized the below cost function that added link prediction cost of GAE <ref type="bibr" target="#b12">[13]</ref> to the reconstruction cost, where H is the latent representation,Â = sigmoid(HH T ) is the reconstructed affinity matrix and γ is the regularization parameter.</p><formula xml:id="formula_24">min X,H 1 2 X −X 2 F + γE H [log p(Â|H)].<label>(22)</label></formula><p>The results are shown in <ref type="table" target="#tab_6">Table 7</ref>, and our model outperforms the compared methods in terms of the link prediction task as well as the node clustering task.    <ref type="bibr" target="#b12">[13]</ref> 89.5 ± 0.04 89.9 ± 0.05 VGAE <ref type="bibr" target="#b12">[13]</ref> 90.8 ± 0.02 92.0 ± 0.02 ARGA <ref type="bibr" target="#b26">[27]</ref> 91.9 ± 0.003 93.0 ± 0.003 ARVGA <ref type="bibr" target="#b26">[27]</ref> 92.4 ± 0.003 93.0 ± 0.003 GALA 94.4 ± 0.009 94.8 ± 0.010</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Visualization</head><p>One of the key ideas of the proposed autoencoder is that the encoder makes the feature of each node becomes similar with its neighbors, and the decoder makes the features of each node distinguishable with its neighbors using the geometrical structure of the graphs. To validate the proposed model, we visualize the distribution of learned latent representations and the input features of each node in two-dimensional space using t-SNE <ref type="bibr" target="#b22">[23]</ref> as shown in <ref type="figure" target="#fig_3">Figure  3</ref>. From the visualization, we can see that GALA is wellclustering the data according to their corresponding labels even though GALA performs in an unsupervised manner. Also, we can see through the red dotted line in embedding results of the latent representation on YALE that GALA embeds the representation of nodes better than the compared methods by minimizing inter-cluster affinity and maximiz-ing intra-cluster affinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a novel autoencoder framework which can extract low-dimensional latent representations from a graph in irregular domains. We designed a symmetric graph convolutional autoencoder architecture where the encoder performs Laplacian smoothing while the decoder performs Laplacian sharpening. Also, to prevent numerical instabilities, we designed a new representation of Laplacian sharpening with spectral radius one by incorporating the concept of the signed graph. To enhance the performance of image clustering tasks, we added a subspace clustering cost term to the reconstruction cost of the autoencoder. Experimental results on the network and image datasets demonstrated the validity of the proposed framework and had shown superior performance over various graph-based clustering algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architectures of existing graph convolutional autoencoders and proposed one. A, X, H and W denote the affinity matrix (structure of graph), node attributes, latent representations and the learnable weight of network respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sample images of three image datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The two-dimensional visualizations of raw features of each node and the latent representations of compared methods and GALA for Cora, Citeseer and YALE are presented. The same color indicates the same cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness of various decoders</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>Eq. (7)</cell><cell cols="6">0.5628 0.4074 0.3289 0.5296 0.2588 0.2437</cell></row><row><cell>Eq. (12)</cell><cell cols="6">0.5999 0.4274 0.3775 0.5915 0.3177 0.3126</cell></row><row><cell>Eq. (16)</cell><cell cols="6">0.7459 0.5767 0.5315 0.6932 0.4411 0.4460</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of node clustering</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Wiki</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>Kmeans[21]</cell><cell>0.4922</cell><cell>0.3210</cell><cell>0.2296</cell><cell>0.5401</cell><cell>0.3054</cell><cell>0.2786</cell><cell>0.4172</cell><cell>0.4402</cell><cell>0.1507</cell></row><row><cell>Spectral[26]</cell><cell>0.3672</cell><cell>0.1267</cell><cell>0.0311</cell><cell>0.2389</cell><cell>0.0557</cell><cell>0.0100</cell><cell>0.2204</cell><cell>0.1817</cell><cell>0.0146</cell></row><row><cell>Big-Clam[38]</cell><cell>0.2718</cell><cell>0.0073</cell><cell>0.0011</cell><cell>0.2500</cell><cell>0.0357</cell><cell>0.0071</cell><cell>0.1563</cell><cell>0.0900</cell><cell>0.0070</cell></row><row><cell>DeepWalk[28]</cell><cell>0.4840</cell><cell>0.3270</cell><cell>0.2427</cell><cell>0.3365</cell><cell>0.0878</cell><cell>0.0922</cell><cell>0.3846</cell><cell>0.3238</cell><cell>0.1703</cell></row><row><cell>GraEnc[33]</cell><cell>0.3249</cell><cell>0.1093</cell><cell>0.0055</cell><cell>0.2252</cell><cell>0.0330</cell><cell>0.0100</cell><cell>0.2067</cell><cell>0.1207</cell><cell>0.0049</cell></row><row><cell>DNGR[3]</cell><cell>0.4191</cell><cell>0.3184</cell><cell>0.1422</cell><cell>0.3259</cell><cell>0.1802</cell><cell>0.0429</cell><cell>0.3758</cell><cell>0.3585</cell><cell>0.1797</cell></row><row><cell>Circles[17]</cell><cell>0.6067</cell><cell>0.4042</cell><cell>0.3620</cell><cell>0.5716</cell><cell>0.3007</cell><cell>0.2930</cell><cell>0.4241</cell><cell>0.4180</cell><cell>0.2420</cell></row><row><cell>RTM[4]</cell><cell>0.4396</cell><cell>0.2301</cell><cell>0.1691</cell><cell>0.4509</cell><cell>0.2393</cell><cell>0.2026</cell><cell>0.4364</cell><cell>0.4495</cell><cell>0.1384</cell></row><row><cell>RMSC[36]</cell><cell>0.4066</cell><cell>0.2551</cell><cell>0.0895</cell><cell>0.2950</cell><cell>0.1387</cell><cell>0.0488</cell><cell>0.3976</cell><cell>0.4150</cell><cell>0.1116</cell></row><row><cell>TADW[37]</cell><cell>0.5603</cell><cell>0.4411</cell><cell>0.3320</cell><cell>0.4548</cell><cell>0.2914</cell><cell>0.2281</cell><cell>0.3096</cell><cell>0.2713</cell><cell>0.0454</cell></row><row><cell>VGAE[13]</cell><cell>0.5020</cell><cell>0.3292</cell><cell>0.2547</cell><cell>0.4670</cell><cell>0.2605</cell><cell>0.2056</cell><cell>0.4509</cell><cell>0.4676</cell><cell>0.2634</cell></row><row><cell>MGAE[35]</cell><cell>0.6844</cell><cell>0.5111</cell><cell>0.4447</cell><cell>0.6607</cell><cell>0.4122</cell><cell>0.4137</cell><cell>0.5146</cell><cell>0.4852</cell><cell>0.3490</cell></row><row><cell>ARGA[27]</cell><cell>0.6400</cell><cell>0.4490</cell><cell>0.3520</cell><cell>0.5730</cell><cell>0.3500</cell><cell>0.3410</cell><cell>0.3805</cell><cell>0.3445</cell><cell>0.1122</cell></row><row><cell>ARVGA[27]</cell><cell>0.6380</cell><cell>0.4500</cell><cell>0.3740</cell><cell>0.5440</cell><cell>0.2610</cell><cell>0.2450</cell><cell>0.3867</cell><cell>0.3388</cell><cell>0.1069</cell></row><row><cell>GALA</cell><cell>0.7459</cell><cell>0.5767</cell><cell>0.5315</cell><cell>0.6932</cell><cell>0.4411</cell><cell>0.4460</cell><cell>0.5447</cell><cell>0.5036</cell><cell>0.3888</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary of datasets</figDesc><table><row><cell></cell><cell># Nodes</cell><cell>Dimension</cell><cell>Classes</cell><cell># Edges</cell></row><row><cell>Cora[29]</cell><cell>2708</cell><cell>1433</cell><cell>7</cell><cell>5429</cell></row><row><cell>Citeseer[29]</cell><cell>3312</cell><cell>3703</cell><cell>6</cell><cell>4732</cell></row><row><cell>Wiki</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Experimental results of image clustering</figDesc><table><row><cell></cell><cell></cell><cell>COIL20</cell><cell></cell><cell></cell><cell>YALE</cell><cell></cell><cell></cell><cell>MNIST</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>Kmeans[21]</cell><cell>0.6118</cell><cell>0.7541</cell><cell>0.5545</cell><cell>0.7450</cell><cell>0.8715</cell><cell>0.7394</cell><cell>0.5628</cell><cell>0.5450</cell><cell>0.4213</cell></row><row><cell>Spectral[26]</cell><cell>0.6806</cell><cell>0.8324</cell><cell>0.6190</cell><cell>0.5793</cell><cell>0.7202</cell><cell>0.4600</cell><cell>0.6496</cell><cell>0.7204</cell><cell>0.5836</cell></row><row><cell>GAE[13]</cell><cell>0.6632</cell><cell>0.7420</cell><cell>0.5514</cell><cell>0.8520</cell><cell>0.8851</cell><cell>0.8122</cell><cell>0.7043</cell><cell>0.6535</cell><cell>0.5534</cell></row><row><cell>VGAE[13]</cell><cell>0.6847</cell><cell>0.7465</cell><cell>0.5627</cell><cell>0.9157</cell><cell>0.9358</cell><cell>0.8873</cell><cell>0.7163</cell><cell>0.7149</cell><cell>0.6154</cell></row><row><cell>MGAE[35]</cell><cell>0.6507</cell><cell>0.7889</cell><cell>0.6004</cell><cell>0.8203</cell><cell>0.8550</cell><cell>0.7636</cell><cell>0.5807</cell><cell>0.5820</cell><cell>0.4362</cell></row><row><cell>ARGA[27]</cell><cell>0.7271</cell><cell>0.7895</cell><cell>0.6183</cell><cell>0.9309</cell><cell>0.9394</cell><cell>0.8961</cell><cell>0.6672</cell><cell>0.6759</cell><cell>0.5552</cell></row><row><cell>ARVGA[27]</cell><cell>0.7222</cell><cell>0.7917</cell><cell>0.6240</cell><cell>0.8727</cell><cell>0.8803</cell><cell>0.7944</cell><cell>0.6328</cell><cell>0.6123</cell><cell>0.4909</cell></row><row><cell>GALA</cell><cell>0.8000</cell><cell>0.8771</cell><cell>0.7550</cell><cell>0.8530</cell><cell>0.9486</cell><cell>0.8647</cell><cell>0.7384</cell><cell>0.7506</cell><cell>0.6469</cell></row><row><cell>GALA+SCC</cell><cell>0.8229</cell><cell>0.8851</cell><cell>0.7579</cell><cell>0.9933</cell><cell>0.9860</cell><cell>0.9854</cell><cell>0.7426</cell><cell>0.7565</cell><cell>0.6675</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Experiment results on Pubmed dataset</figDesc><table><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>Kmeans[21]</cell><cell>0.5952</cell><cell>0.3152</cell><cell>0.2817</cell></row><row><cell>Spectral</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Effects of stable decoder and subspace clustering cost</figDesc><table><row><cell></cell><cell></cell><cell>COIL20</cell><cell></cell><cell></cell><cell>YALE</cell><cell></cell><cell></cell><cell>MNIST</cell><cell></cell></row><row><cell></cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell><cell>ACC</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>Unstable decoder and reconstruction cost only (Eq. 12 and Eq. 18)</cell><cell>0.5961</cell><cell>0.7986</cell><cell>0.5492</cell><cell>0.7205</cell><cell>0.9028</cell><cell>0.7530</cell><cell>0.6589</cell><cell>0.7397</cell><cell>0.5983</cell></row><row><cell>Unstable decoder and subspace clustering cost (Eq. 12 and Eq. 21)</cell><cell>0.7104</cell><cell>0.8074</cell><cell>0.6429</cell><cell>0.7810</cell><cell>0.8710</cell><cell>0.7130</cell><cell>0.6734</cell><cell>0.7211</cell><cell>0.6028</cell></row><row><cell>Stable decoder and reconstruction cost only (Eq. 16 and Eq. 18)</cell><cell>0.8000</cell><cell>0.8771</cell><cell>0.7550</cell><cell>0.8530</cell><cell>0.9486</cell><cell>0.8646</cell><cell>0.7384</cell><cell>0.7506</cell><cell>0.6469</cell></row><row><cell>Stable decoder and subspace clustering cost (Eq. 16 and Eq. 21)</cell><cell>0.8229</cell><cell>0.8851</cell><cell>0.7579</cell><cell>0.9933</cell><cell>0.9860</cell><cell>0.9854</cell><cell>0.7426</cell><cell>0.7565</cell><cell>0.6675</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Experimental results of link prediction on Citeseer</figDesc><table><row><cell>AUC</cell><cell>AP</cell></row><row><cell>GAE</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relational topic models for document networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Spectral graph theory. Number 92</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><forename type="middle">Chung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From few to many: Illumination cone models for face recognition under variable lighting and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Athinodoros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georghiades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="660" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the asymptotic spectrum of random walks on infinite families of graphs. Random Walks and Discrete Potential Theory (Cortona, 1997)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rostislav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Grigorchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sympos. Math</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="188" to="204" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gribonval</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="129" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep subspace clustering networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational graph autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Life in the network: the coming age of computational social science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Sandy</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lada</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Aral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">Laszlo</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Christakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noshir</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myron</forename><surname>Gutmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">5915</biblScope>
			<biblScope unit="page">721</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to discover social circles in ego networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Note on the normalized laplacian eigenvalues of signed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Australas. J. Combin</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deformable shape completion with graph convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00268</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can-Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-Shuang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="347" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Columbia object image library (coil-20)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sameer A Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murase</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A signal processing approach to fair surface design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Taubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference on Computer graphics and Interactive techniques</title>
		<meeting>the 22nd Annual Conference on Computer graphics and Interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">René</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mgae: Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust multiview spectral clustering via low-rank and sparse decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongkai</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2149" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overlapping community detection at scale: a nonnegative matrix factorization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the sixth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="587" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep adversarial subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1596" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
