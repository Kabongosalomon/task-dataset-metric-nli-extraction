<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
							<email>rxia@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengran</forename><surname>Zhang</surname></persName>
							<email>zhangmengran@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Ding</surname></persName>
							<email>dingzixiang@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RTHN: A RNN-Transformer Hierarchical Network for Emotion Cause Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The emotion cause extraction (ECE) task aims at discovering the potential causes behind a certain emotion expression in a document. Techniques including rule-based methods, traditional machine learning methods and deep neural networks have been proposed to solve this task. However, most of the previous work considered ECE as a set of independent clause classification problems and ignored the relations between multiple clauses in a document. In this work, we propose a joint emotion cause extraction framework, named RNN-Transformer Hierarchical Network (RTHN), to encode and classify multiple clauses synchronously. RTHN is composed of a lower word-level encoder based on RNNs to encode multiple words in each clause, and an upper clause-level encoder based on Transformer to learn the correlation between multiple clauses in a document. We furthermore propose ways to encode the relative position and global predication information into Transformer that can capture the causality between clauses and make RTHN more efficient. We finally achieve the best performance among 12 compared systems and improve the F1 score of the state-of-the-art from 72.69% to 76.77%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotion cause extraction (ECE) is a fine-grained task of emotion analysis, which aims at discovering the potential causes behind a certain emotion expression in the text. The ECE task was first proposed and defined as a word-level sequence labeling problem in . To solve the shortcomings of describing emotion cause at word/phrase level, <ref type="bibr" target="#b3">[Gui et al., 2016a]</ref> released a new corpus and re-formalized the ECE task as a clause-level classification problem. This corpus has received much attention in the following study and has become a benchmark dataset for ECE research. <ref type="figure" target="#fig_0">Figure 1</ref> gives an example to the annotation of their corpus. In this example, a document is composed of five clauses. The emotion expression "happy" is contained in Clause c 4 and the corresponding cause "the thief was caught" is contained in Clause c 3 (We call Clause c 4 and Clause c 3 emotion expression clause and emotion cause clause, respectively). The goal of ECE is to predict for each clause in a document, whether this clause contains an emotion cause, given the annotation of the emotion expression.</p><p>Rule-based methods and traditional machine learning methods have been proposed to address this problem. In recent years, deep neural networks have also been applied to this task and achieved state-of-the-art performance <ref type="bibr" target="#b0">[Cheng et al., 2017;</ref><ref type="bibr" target="#b6">Li et al., 2018;</ref><ref type="bibr">Ding et al., 2019;</ref><ref type="bibr" target="#b7">Yu et al., 2019]</ref>.</p><p>However, most of the previous work considered ECE as a set of independent clause classification problems. For example, under this framework, Example 1 will be formalized as five independent classification tasks (Clause c 1 to Clause c 5 ). Although this framework was straight-forward, it ignores the relations between multiple clauses for emotion cause inference. There are two types of relationships between clauses: 1) Correlation: two clauses with similar semantics are supposed to have similar probabilities being the emotion cause. 2) Causality: incorporating the information of the other clauses in the document can help infer the current clause in a global view. It was observed from the corpus <ref type="bibr" target="#b3">[Gui et al., 2016a</ref>] that more than 99% of the documents have only one or two causes. If one clause has a high probability being an emotion cause, the probabilities that other clauses being emotion causes should be reduced; conversely, if no high-confidence emotion cause clauses have been observed, the probability of predicting the current clause being an emotion cause should be increased.</p><p>In this work, we propose a hierarchical network architecture based on RNN and Transformer, named RNN-Transformer Hierarchical Network (RTHN), to model the relations between multiple clauses in a document and classify them synchronously in a joint framework. RTHN is composed of two layers: 1) The lower layer is a word-level en-coder consisting of multiple RNNs, each of which corresponds to one clause, in turn encodes the words in the clause and combines them to obtain the clause representation; 2) The upper layer is a clause-level encoder based on a stacked Transformer, where the clause representations are repeatedly learned and updated by incorporating the relations between multiple clauses, and finally feed to a softmax layer for synchronous classification.</p><p>We further propose ways to encode the relative position and global prediction information which have been proven to be important features in ECE, and gain further improvements. On one hand, the attention mechanism in Transformer learns the correlation between clauses. On the other hand, the encoding of global prediction the causality between clauses.</p><p>The main contributions of this work can be summarized as follows:</p><p>1. We propose a new hierarchical network architecture based on RNNs and Transformer for the ECE task. To the best of our knowledge, it is the first time that Transformer has been used to solve ECE problems. It demonstrates excellent performance in learning the correlation between multiple clauses in ECE.</p><p>2. We further encode the relative position and global predication information into the Transformer framework. It can capture the causality between clauses and achieve extra improvements.</p><p>3. The effectiveness of our model is demonstrated on the benchmark ECE corpus. We finally achieve the best performance among 12 compared systems and improve the F1 score of the state-of-the-art from 72.69% to 76.77% 1 .</p><p>2 Related Work  manually constructed a small-scale emotion cause corpus based on the Academia Sinica Balanced Chinese Corpus and first proposed the emotion cause extraction (ECE) task. In this corpus, the spans of both emotion expression and emotion cause were annotated and the ECE task was defined as a word-level sequence labeling problem. Subsequent research proposed either rule-based methods or machine learning methods to solve this problem based on manually designed rules or features <ref type="bibr" target="#b0">[Chen et al., 2010;</ref><ref type="bibr" target="#b5">Lee et al., 2013]</ref>.  constructed an emotion cause corpus based on Chinese microblog posts, and proposed a rule-based method to infer and extract the emotion cause by importing knowledge and theories from other fields such as sociology. <ref type="bibr" target="#b1">[Gao et al., 2015a]</ref> and <ref type="bibr" target="#b1">[Gao et al., 2015b]</ref> further designed a set of complex rules considering a cognitive emotion model and emotions categories to extract emotion cause on this corpus. <ref type="bibr" target="#b2">[Gui et al., 2014]</ref> also constructed a microblog emotion cause corpus based on NLPCC 2013 emotion analysis share task, and proposed a machine learning method based on SVMs and conditional random fields (CRFs) to extract emotion causes. Similar as , the ECE task in <ref type="bibr">1</ref> The source code can be obtained at https://github.com/NUSTM/RTHN these corpora was defined and evaluated as a sequence labeling problem.</p><p>There were also some individual studies that conducted ECE research on their own corpus <ref type="bibr">[Russo et al., 2011;</ref><ref type="bibr" target="#b6">Neviarouskaya and Aono, 2013;</ref><ref type="bibr">Ghazi et al., 2015;</ref><ref type="bibr" target="#b7">Song and Meng, 2015;</ref><ref type="bibr">Yada et al., 2017]</ref>. They normally regarded ECE as a sequence labeling problem and employed rule-based or traditional machine learning algorithms to solve it. <ref type="bibr" target="#b3">[Gui et al., 2016a]</ref> and <ref type="bibr" target="#b3">[Gui et al., 2016b]</ref> released a Chinese emotion cause corpus from a public SINA city news and proposed a multi-kernel based method for emotion cause extraction. Different from previous corpora, the ECE task in this corpus was defined as a clause classification problem, where the goal is to predict for each clause in a document, whether this clause is an emotion cause, given the annotation of emotion expression. It was also evaluated by the clause-level Precision, Recall and F1 score metrics. This corpus has received much attention in the following study and has become a benchmark dataset for ECE research. Several traditional machine learning methods including structure representation and multi-kernel learning has been proposed in <ref type="bibr" target="#b3">[Gui et al., 2016b;</ref>. <ref type="bibr" target="#b3">[Gui et al., 2016a]</ref> proposed a tree structurebased representation method to describe the events in emotion cause extraction on this corpus. In recent two years, deep learning techniques have also been applied to emotion cause extraction. For example, <ref type="bibr" target="#b0">[Cheng et al., 2017]</ref> used long short-term memory (LSTM),  proposed a deep memory network, and <ref type="bibr" target="#b6">[Li et al., 2018]</ref> proposed a coattention neural network, for emotion cause prediction.</p><p>Most of the above work considered the ECE task on this corpus as a set of independent clause classification tasks and ignored the relations between multiple clauses in a document. To address this, <ref type="bibr">[Ding et al., 2019]</ref> converted the task to a reordered clause classification problem. The predictions of previous clauses were used as features for predicting subsequent clauses. However, their approach depends on the clause order and can only use the predictions of the previous clauses but not the subsequent clauses. In contrast, RTHN proposed in this work is a joint emotion cause extraction framework that models and classifies multiple clauses in a document synchronously.</p><p>[Yu et al., 2019] proposed a three-level (word-phraseclause) hierarchical network based on CNNs and LSTMs. The multiple clauses are modeled with LSTMs in their approach. By contrast, in this work Transformer is used as the clause-level encoder to model the relations between multiple clauses. We will empirically prove that Transformer has shown significantly better performance as a clause-level encoder, in comparison with RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>In this paper, we represent a document containing multiple clauses as d = {c 1 , ..., c i , ..., c |d| }, where c i is the ith clause in d. Each clause c i consists of multiple words</p><formula xml:id="formula_0">c i = {w i,1 , ..., w i,t , ..., w i,|ci| }.</formula><p>In this work, we propose a RNN-Transformer Hierarchical Network (RTHN) to model such a "word-clause-document" hierarchical structure 2 . The overall architecture of RTHN is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. It contains two layers: the lower layer is a word-level encoder consisting of multiple Bi-LSTM modules, each of which corresponds to a clause (Section 3.2); the upper layer is a clause-level encoder consisting of a stacked Transformer module, where the clause representations obtained at lower layer are repeatedly updated by incorporating the relations between clauses (Section 3.3), relative position and global prediction (Section 3.4), and finally feed to a softmax layer for synchronous classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Level Encoder Based on RNNs</head><p>The lower layer is a word-level encoder consisting of multiple Bi-LSTMs. Each clause corresponds to a Bi-LSTM module, which accumulate the context information for each word of the clause. The hidden state of the t-th word in the i-th clause h i,t is obtained based on a bi-directional LSTM. A word-level attention mechanism is then adopted to get the clause representation r i by a weighted sum of hidden states of all the words in the clause. Here we omit the details of Bi-LSTM and attention for limited space.</p><p>As has mentioned in the Introduction, most of the previous studies considered ECE as a set of independent clause classification problem. This framework normally has only one encoding layer (word-level encoder). In this work, the proposed RTHN model is a 2-layer hierarchical network containing not only word-level encoders at the lower layer but also a clauselevel encoder at the upper layer. <ref type="bibr">2</ref> The document is usually very short in the benchmark corpus <ref type="bibr" target="#b3">[Gui et al., 2016a]</ref>. Most of them contain less than 20 clauses. We therefore ignore the sentence level between clause and document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Clause Level Encoder Based on Transformer</head><p>In this work, Transformer <ref type="bibr" target="#b7">[Vaswani et al., 2017]</ref> is used as the clause-level encoder at the upper layer to encode the relations between multiple clauses in a document.</p><p>The standard Transformer consists of a stack of N layers. Each layer has two sub-layers: the first is a multi-head selfattention mechanism; the second is a fully connected feedforward network.</p><p>(1) Multi-head Self-attention The unit of multi-head self-attention mechanism is the major component in the Transformer. For each clause c i , the representation r i obtained at the word-level encoder is used as the input x i by adding a positional embedding p i :</p><formula xml:id="formula_1">x i = r i + p i .<label>(1)</label></formula><p>In our setting, the Transformer contains a query vector q i , a key vector k i and a value vector v i for each clause c i in the document:</p><formula xml:id="formula_2">q i = ReLU(x i W Q ),<label>(2)</label></formula><formula xml:id="formula_3">k i = ReLU(x i W K ),<label>(3)</label></formula><formula xml:id="formula_4">v i = ReLU(r i W V ),<label>(4)</label></formula><p>where W Q , W K and W v are learnable weight matrix of query, keys and values, respectively. For each clause c i , self attention learns a set of weights β i = {β i,1 , β i,2 , ..., β i,|d| }, which measures the extent of all the input clauses [c 1 , .., c k , ..., c |d| ] answer the query q i :</p><formula xml:id="formula_5">β i,j = exp(q i · k j ) j exp(q i · k j ) .<label>(5)</label></formula><p>The output is a weighted sum of the values of all clauses:</p><formula xml:id="formula_6">z i = j β i,j v j .<label>(6)</label></formula><p>This allows that the representation of each clause can encode a global level information on all the clauses in the document, rather than rely solely on the hidden state of one clause. Moreover, the multi-head attention is employed with the number of heads as 5.</p><p>(2) Feed-Forward Network The attention sublayer is then followed by a fully connected Feed-Forward Network (FFN) sublayer:</p><formula xml:id="formula_7">e i = ReLU(z i W 1 + b 1 )W 2 + b 2 .<label>(7)</label></formula><p>Note that both of the above two sublayers use the residual connection followed by normalization layer at its output:</p><formula xml:id="formula_8">o i = Normalize(e i + x i ).<label>(8)</label></formula><p>As has mentioned, Transformer is a stack of N layers each of which includes attention and FFN sublayers. Let l denote the index of Transformer layers. The output of the previous layer will be used as the input of the next layer:</p><formula xml:id="formula_9">x (l+1) i = o (l) i .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Encoding Relative Position and Global Prediction</head><p>In this section, we propose to further encode the relative position and global prediction which have been proven to be two explicit clues in ECE.</p><p>(1) Relative Position Embedding In standard Transformer, a positional encoding was already applied to represent the of position information. But what it reflects is the absolute positional information of the word in the sentence. In our task, relative position (RP) is defined as the relative distance between the current clause and the emotion expression clause. For example, -1 is the RP of the clause left to the emotion expression clause, +2 the relative position of the second clause right to the emotion expression clause, and so on.</p><p>Relative position is more important than absolute position in ECE, because people are more inclined to explain the causes near the emotion expression. Therefore, the clauses with smaller relative position (rather than absolute position) are more likely to be an emotion cause of the given emotion expression.</p><p>In this work, we use relative position embedding (RPE) to encode such relative position information. The RPEs for all the clauses in a document are denoted by {rpe 1 , rpe 2 , ..., rpe |d| } where rpe i denotes the RPE of clause c i . Instead of Equation <ref type="formula" target="#formula_1">(1)</ref>, rpe i is concatenated to the clause representation r i , as the input of the Transformer:</p><formula xml:id="formula_10">x i = r i ⊕ rpe i .<label>(10)</label></formula><p>(2) Global Prediction Embedding As we have mentioned, there are two types of relationships between clauses: correlation and causality. In addition to using the attention mechanism in Transformer to capture the correlation between clauses, we furthermore propose to append a new global prediction sublayer to the end of each Transformer layer to introduce more causality. Global Prediction (GP) denotes the prediction labels of all the clauses in a document. As can be observed in the ECE corpus <ref type="bibr" target="#b3">[Gui et al., 2016a]</ref>, more than 99% of the documents have only one or two causes. If one clause in the document is predicted as an emotion cause with high confidence, the probability that other clauses are predicted as emotional causes should be reduced; conversely, if there are no other emotional cause clauses with high confidence in the document, the probability that the current clause is predicted to be an emotion cause should be increased. Therefore, GP is an important clue for emotion cause extraction.</p><p>Firstly, we get the prediction label of each clause l i ∈ {+1, −1} based on the output representations o i :</p><formula xml:id="formula_11">l i ← softmax(W o i + b).<label>(11)</label></formula><p>Secondly, we sort the predicted labels of different clauses according to their relative positions [. . . , -2, -1, 0, +1, +2, . . . ] and build the following global prediction vector</p><formula xml:id="formula_12">GP = [..., l i−2 , l i−1 , l i0 , l i+1 , l i+2 , ...].<label>(12)</label></formula><p>where l irp denotes the prediction label of the clause at relative position rp and the current position (i.e). Note that we mask the prediction at the current position to avoid potential interference. For example, if the rp of current clause is +1, we let l i+1 = 0. GP represents different combinations of all clause predictions and is then encoded by an embedding called Global Prediction Embedding GP E:</p><formula xml:id="formula_13">GP E = Tanh(W gpe GP + b gpe ),<label>(13)</label></formula><p>where W gpe and b gpe are learnable matrix and bias. In stacking, the average GP E of previous layers is concatenated to the output representation o <ref type="bibr">(l)</ref> i and used as the input of the next layer's input:</p><formula xml:id="formula_14">x (l+1) i = o (l) i ⊕ Ave GP E (l) ,<label>(14)</label></formula><p>where Ave GP E (l) = 1 l l GP E (l) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multiple Clause Classification</head><p>After a stack of N layers, we obtain the final clause representation o (N ) i for each clause, and employ an extra softmax function to yield the final prediction distribution</p><formula xml:id="formula_15">y i = softmax(W (N ) c o (N ) i + b (N ) c ).<label>(15)</label></formula><p>The training objective is to minimize the cross-entropy loss across all the clauses:</p><formula xml:id="formula_16">Loss = − d∈Corpus |d| i=1 y i · log(ŷ i ) + λ||θ|| 2 ,<label>(16)</label></formula><p>where y i is the ground-truth distribution of clause c i . A L2norm regulation is also adopted with λ denoting the tradeoff weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Experimental Settings</head><p>We evaluate our RTHN model on the benchmark ECE corpus <ref type="bibr" target="#b3">[Gui et al., 2016a]</ref>, which was the mostly used corpus for emotion cause extraction. The same as , we randomly divide the data with the proportion of 9:1, with 9 folds as training data and remaining 1 fold as testing data.</p><p>The following results are reported in terms of an average of 10-fold cross-validation. The performance measures are Precision (P), Recall (R), and F1 all defined at clause level. We use the word embedding provided by NLPCC. It was pre-trained on a 1.1 million Chinese Weibo corpora with the word2vec toolkit <ref type="bibr" target="#b6">[Mikolov et al., 2013]</ref>. Similar performance can be obtained by using the embedding in . The dimension of word embedding, RP embedding and GP embedding is set to be 200, 50 and 50, respectively. The hidden units of LSTM in word-level encoder is set to be 100. The dimension of the hidden states in Tranformer is 200, and the dimensions of query, key and value are 250, 250, and 200 repectively.</p><p>The maximum numbers of words in each clause and clauses in each document are set to be 75 and 45, respectively. The network is trained based on the Adam optimizer with a mini-batch size 32 and a learning rate 0.005. P R F1 RB  0.6747 0.4287 0.5243 CB <ref type="bibr">[Russo et al., 2011]</ref> 0.2672 0.7130 0.3887 RB+CB 0.5435 0.5307 0.5370 RB+CB+SVM 0.5921 0.5307 0.5597 Ngrams+SVM 0.4200 0.4375 0.4285 Word2vec+SVM 0.4301 0.4233 0.4136 Multi-Kernel <ref type="bibr" target="#b3">[Gui et al., 2016a]</ref> 0.6588 0.6927 0.6752 CNN <ref type="bibr" target="#b4">[Kim, 2014]</ref> 0.6215 0.5944 0.6076 Memnet  0.7076 0.6838 0.6955 CANN <ref type="bibr" target="#b6">[Li et al., 2018]</ref> 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Systems</head><p>We compare our model with the following 12 baseline systems:</p><p>1) RB is a rule based method ;</p><p>2) CB is common-sense based method <ref type="bibr">[Russo et al., 2011]</ref>;</p><p>3) RB+CB is a combination of RB and CB; 4) RB+CB+SVM is a SVM classifier trained on features including rules  and Chinese Emotion Cognition Lexicon ; 5) Ngrams+SVM denotes a SVM classifier that uses the unigram, bigram and trigram features. It was a baseline system in ; 6) Multi-kernel is a multi-kernel based method proposed in <ref type="bibr" target="#b3">[Gui et al., 2016a]</ref>; 7) Word2vec+SVM denotes a SVM classifier using word embeddings learned by Word2vec as features; 8) CNN is the basic convolutional neural network proposed by <ref type="bibr" target="#b4">[Kim, 2014]</ref>; 9) Memnet is convolutional multiple-slot deep memory network proposed by ; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>The past clause-level approaches regarded the ECE task as a set of independent clause classification problems. By observing the corpus, we found that the proportions of emotion cause clauses and non-emotion-cause clauses were 18.36% and 81.64%, respectively. It is a serious class-imbalance classification problem and the model tends to predict the clause as non-emotion-cause more often. This is also the reason why their Recall scores were quite low (the highest was 0.6908). By contrast, it can found in <ref type="table">Table 1</ref> that the Recall scores of the hierarchical models (HCS and RTHN) are significantly higher than pervious methods. This is because they can capture the relations of multiple clauses which help inferring the current clause. For example, if no other clauses in a document have been detected as an emotion cause, the model will increase the probability of the current clause being predicted as an emotion cause. This finally increases the Recall score. In particular, RTHN achieves a much higher Recall score (0.7699) than other methods (the improvement is more than 7% over the clause-level methods including CANN and PAE-DGL, and more than 5% over HCS), but without reducing the Precision score (only slightly lower than CANN but still higher than the other baselines).</p><p>We also plot the Precision-Recall (PR) Curves of three methods (CANN, PAE-DGL and RTHN) in <ref type="figure" target="#fig_3">Figure 3</ref>. It can be seen that the PR curve of RTHN is basically at the topright of the other curves for most cases, and the area under RTHN PR curve is also significantly larger than the others. It further proves the superiority of the overall performance of our RTHN model.</p><p>In <ref type="figure" target="#fig_4">Figure 4</ref>, We display the attention weights learned by Transformer, by using the example in <ref type="figure" target="#fig_0">Figure 1</ref> as a test document. The height of the j-th column for the i-th clause denotes the weight of clause c j in representing the clause c i : β i,j (see <ref type="table">Equation 5</ref>). <ref type="figure" target="#fig_4">Figure 4</ref> can be observed from two angles.</p><p>Firstly, for each clause, the weight at the emotion expression clause is the largest and gradually becomes smaller towards both sides. This shows that Transformer can automatically capture the relative distance information: the smaller the relative distance, the larger the weight assigned. The distribution of weights looks similar to a normal distribution centered on the emotion expression clause.</p><p>Secondly, by observing different clauses, we can find that the clause with higher probability being an emotion cause tends to have more concentrated distribution; On the contrary, clauses with smaller probabilities being an emotion cause tend to have more uniform distribution. In <ref type="figure" target="#fig_4">Figure 4</ref> the weight distribution of clause c 3 is the most concentrated, where clause c 4 is exactly the ground-truth emotion cause. The largest weight, β 3,4 (0.66) also happens to be the weight of the emotion cause clause and emotion expression clause. This phenomenon is very common across different documents in our experiments. It further confirms our model's effectiveness in capturing the relationships between multiple clauses in emotion cause inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Effectiveness of Encoding Relative Position and Global Prediction</head><p>In order to further examine the effects of encoding relative position and global prediction in RTHN, we carry out an ablation study by designing the following RTHN variants:  • RTHN-No-GPE (RTHN after removing global prediction encoding);</p><p>• RTHN-No-RPE (RTHN after removing relative position embedding);</p><p>• RTHN-APE (RTHN using absolute position embedding instead of relative position embedding).</p><p>The results are reported in <ref type="table" target="#tab_1">Table 2</ref>. We can observe that after reducing global prediction encoding, the F1 score of RTHN-No-GLE decreases more than 3% (0.7314). The removal of relative position encoding results in a greater performance degradation (RTHN-No-RPE: 0.4145). By applying the absolute position embedding, RTHN-APE still perform poorly (0.5694). All these results demonstrate that RPE and GPE are important two factors in RTHN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Why using RNN-Transformer Combination</head><p>In RTHN, RNNs and Transformer are used as the word-level and clause-level encoders respectively. To investigate the effectiveness of the RNN-Transformer combination, we further design two other combinations for hierarchical modeling:   and a 3-layer stacked Bi-LSTMs is used as the clauselevel encoder.</p><p>• TTHN (Transformer-Transformer hierarchical network). Transformer is used as both word-level and clause-level encoders.</p><p>Note that relative position and global prediction are encoded in RRHN and TTHN the same way as that in RTHN.</p><p>In <ref type="table" target="#tab_2">Table 3</ref>, we report their performance as well as the training time on a GTX-1080Ti GPU server. It can be observed that both RRHN and TTHN perform less effectively than RTHN. But what surprised us a bit is that TTHN's performance is significantly behind RTHN and RRHN. One possible reason is that we only use a layer of Transformer in the word-level encoder. Moreover, due to the advantages of parallel computing, Transformer's training time is shorter than RNN that can only perform serial operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The emotion cause extraction task was normally regarded as a set of independent clause classification problems where the relations between multiple clauses in a document were ignored. In this work, we propose a joint emotion cause extraction framework, called RNN-Transformer Hierarchical Network (RTHN), that can model and classify multiple clauses in a document synchronously. Transformer has demonstrated superior performance in capturing the correlations between multiple clauses. Moreover, we proposed ways to encode two explicit factors in ECE (i.e., relative position and global prediction) that can capture the causality between clauses and make RTHN more efficient for emotion cause extraction. The experimental results on a benchmark ECE corpus verified the effectiveness and superiority of our approach, in comparison with state-of-the-art techniques in ECE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of the emotion cause extraction task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The framework of our RTHN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>10) CANN is a co-attention neural network model with emotional context awareness [Li et al., 2018]; 11) PAE-DGL is a reordered prediction model that incorprates relative position information and dynamic global label [Ding et al., 2019]; 12) HCS is a CNN-RNN based three-level hierarchical network based clause selection [Yu et al., 2019].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The distribution of attention weights learned in Transformer for each clause of the example inFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The distribution of attention weights learned in Transformer for each clause of the example inFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>•</head><label></label><figDesc>RRHN (RNN-RNN hierarchical network). Multiple Bi-LSTMs with attention are used as word-level encoders</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The effect of global prediction and different ways of using position information.</figDesc><table><row><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Training Time (s)</cell></row><row><cell cols="3">RRHN 0.7831 0.7273 0.7534</cell><cell>732</cell></row><row><cell cols="3">TTHN 0.7123 0.6798 0.6952</cell><cell>281</cell></row><row><cell cols="3">RTHN 0.7697 0.7662 0.7677</cell><cell>360</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance of different combinations of RNN and Transformer.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>The work was supported by the Natural Science Foundation of China (No. 61672288), and the Natural Science Foundation of Jiangsu Province for Excellent Young Scholars (No. BK20160085).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Zixiang Ding, Huihui He, Mengran Zhang, and Rui Xia. From independent prediction to reordered prediction: Integrating relative position and global label information to emotion cause identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 23rd International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
	<note>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A rule-based approach to emotion cause detection for chinese micro-blogs</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics (CI-CLing)</title>
		<editor>Ghazi et al., 2015] Diman Ghazi, Diana Inkpen, and Stan Szpakowicz</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="152" to="165" />
		</imprint>
	</monogr>
	<note>Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emotion cause detection with linguistic construction in chinese weibo text</title>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing (NLPCC)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion cause extraction, a challenging task with corpus construction</title>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1593" to="1602" />
		</imprint>
	</monogr>
	<note>Empirical Methods in Natural Language Processing (EMNLP)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A text-driven rule-based system for emotion cause detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim ; Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<editor>Sophia Yat Mei Lee, Ying Chen, and Chu-Ren Huang.</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>Proceedings of the NAACL HLT 2010</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting emotion causes with a linguistic rule-based approach 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</title>
		<editor>Sophia Yat Mei Lee, Ying Chen, Chu-Ren Huang, and Shoushan Li</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="390" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A co-attention neural network model for emotion cause analysis with emotional context awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Weiyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing (IJCNLP)</meeting>
		<imprint>
			<publisher>Tommaso Caselli</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="932" to="936" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). Russo et al., 2011] Irene Russo</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shuntaro Yada, Kazushi Ikeda, Keiichiro Hoashi, and Kyo Kageura. A bootstrap method for automatic rule acquisition on emotion cause extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ester</forename><surname>Boldrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricio Martínez-Barco ; Shuangyong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA)</title>
		<meeting>the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA)</meeting>
		<imprint>
			<publisher>IEEE Access</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="9071" to="9079" />
		</imprint>
		<respStmt>
			<orgName>Song and Meng</orgName>
		</respStmt>
	</monogr>
	<note>Multiple level hierarchical network-based clause selection for emotion cause extraction</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
