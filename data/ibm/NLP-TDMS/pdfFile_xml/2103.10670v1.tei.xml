<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Image co-segmentation via Deep Metric Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Polytechnic University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiabi</forename><surname>Liu</surname></persName>
							<email>liuxiabi@bit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing Polytechnic University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Image co-segmentation via Deep Metric Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Metric learning</term>
					<term>Image Co-segmentation</term>
					<term>IS-Triplet Loss</term>
					<term>Segmentation Loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Metric Learning (DML) is helpful in computer vision tasks. In this paper, we firstly introduce DML into image co-segmentation. We propose a novel Triplet loss for Image Segmentation, called IS-Triplet loss for short, and combine it with traditional image segmentation loss. Different from the general DML task which learns the metric between pictures, we treat each pixel as a sample, and use their embedded features in high-dimensional space to form triples, then we tend to force the distance between pixels of different categories greater than of the same category by optimizing IS-Triplet loss so that the pixels from different categories are easier to be distinguished in the high-dimensional feature space. We further present an efficient triple sampling strategy to make a feasible computation of IS-Triplet loss. Finally, the IS-Triplet loss is combined with 3 traditional image segmentation losses to perform image segmentation. We apply the proposed approach to image co-segmentation and test it on the SBCoseg dataset and the Internet dataset. The experimental result shows that our approach can effectively improve the discrimination of pixels' categories in high-dimensional space and thus help traditional loss achieve better performance of image segmentation with fewer training epochs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The loss function is one of the most important factors in the applications of deep learning, including image segmentation. Currently, the cross-entropy (CE) loss <ref type="bibr" target="#b0">[1]</ref>, Dice loss <ref type="bibr" target="#b1">[2]</ref>, and Focal loss <ref type="bibr" target="#b2">[3]</ref> are the 3 most widely used loss functions in this field.</p><p>The existing image segmentation loss function mainly focuses on the distinguishability of low-dimensional semantic features of pixels, ignoring the discrimination of high-dimensional features of pixels from different categories. Theoretically, samples are more likely to be correctly distinguished in high-dimensional space than in low-dimensional space, but the similarity measurement problem in high-dimensional feature space needs to be solved.</p><p>Deep Metric Learning (DML) is concerned with learning a distance function tuned to a particular task and is useful when used in conjunction with nearest-neighbor methods and other techniques that rely on distances or similarities, e.g. face recognition <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, image retrieval <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. In recent years, it has also attracted more and more attention in image segmentation research <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. These works use the idea of Contrastive loss, and their efficiency and practicality need to be improved. In this paper, we propose a novel Triplet loss function for Image Segmentation. We present a special strategy to sample foreground pixels and background pixels to construct triples. Then we force the distance between heterogeneous pixels greater than that of similar pixels in the high-dimensional feature space so that the pixels are easier to be distinguished in the high-dimensional feature space. We call this loss as IS-Triplet and combine it with classic image segmentation loss functions to perform image segmentation. We apply our proposed loss to image cosegmentation. The experiments are conducted on SBCoseg and Internet dataset. Our main contributions are summarized as follows:</p><p>(1)We propose a novel triplet loss function for image segmentation, called IS-Triplet loss for short. We treat each pixel in the image as a sample and train their high-dimensional embedding feature vector with our IS-Triplet loss to improve the distinguishability between foreground and background pixels. To our knowledge, this is the first introduction of triplet loss into image segmentation.</p><p>(2) A new triple sampling strategy is presented under our framework, which is important for the success of DML. We not only control the upper limit of the sampling number of the triples under the premise of ensuring the effect but also keep the weights of the previous foreground points and the background points in the triples, to prevent the adverse effects caused by the imbalanced number of the two types of pixels.</p><p>(3) We combine the proposed IS-Triplet loss with traditional image segmentation loss functions to perform image co-segmentation. The experiments on SBCoseg <ref type="bibr" target="#b15">[16]</ref> and Internet dataset <ref type="bibr" target="#b16">[17]</ref> prove that our loss can lead to better performance than traditional segmentation loss and this improvement can be observed stably. Although Dice loss, CE loss, and Focal loss have different performances, they can be improved to almost the same level after applying our method. <ref type="figure">Fig. 1</ref> A fully convolutional neural network is modified to apply our metric learning method. Where BS is the batch size, W and H are the width and height of the input image: (a) Schematic diagram of the original network structure; (b) Schematic diagram of the modified network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Loss Functions in Image Segmentation</head><p>The softmax outputs of the deep network for segmentation reflect the probabilities of classifying each pixel into the foreground class. The CE loss measures the quality of image segmentation from the view of single pixels. It aims at the correct classification of each pixel in the image <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref> and ignores the integrality of the foreground or background objects. So, if most of the pixels have been classified correctly, the segmentation quality is not easy to be improved further.</p><p>More importantly, the two classes are usually highly unbalanced in image segmentation. For example, the interesting foreground objects often occupy only a small part of the image. The CE loss is unsatisfactory to deal with such class unbalance problem.</p><p>To address the foreground-background class imbalance encountered during the training of dense detectors, Lin et al. <ref type="bibr" target="#b2">[3]</ref> proposed Focal loss, which reshapes the standard CE loss such that it down-weights the loss assigned to well-classified examples. However, it still focuses on the classification of single pixels and ignores the integrity of foreground or background objects.</p><p>The Dice loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, also known as intersection over union (IoU) or Jacarrd index, computes the overlap ratio between the ground-truth object region and the predicted object region to assess the segmentation quality. Because the integrality of foreground and background objects is considered, the Dice loss usually leads to better segmentation results than the CE loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Metric Learning</head><p>Deep metric learning maps an image into a feature vector in a manifold space via deep neural networks. In this manifold space, the Euclidean distance or the cosine distance can be directly used as the distance metric between two points. Loss function also plays a key role in successful DML frameworks and a large variety of loss functions have been proposed in the literature. Contrastive loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> captures the relationship between pairwise data points, e.g. similarity or dissimilarity.</p><p>Triplet loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> was first introduced by Florian et al. <ref type="bibr" target="#b4">[5]</ref> in face recognition applications. Hoffer et al. <ref type="bibr" target="#b3">[4]</ref> applied the method of triple loss specifically to metric learning to learn the feature embedding of data. A triplet is composed of an anchor point, a similar (positive) data point, and a dissimilar (negative) data point. The purpose of triplet loss is to learn a distance metric by which the anchor point is closer to a similar point than the dissimilar one by a margin. In general, the Triplet loss outperforms the contrastive loss, because the relationship between positive and negative pairs is considered.</p><p>After the triplet loss, more methods form multiple groups with more samples, such as quadruplet loss <ref type="bibr" target="#b26">[27]</ref>, Npair loss <ref type="bibr" target="#b27">[28]</ref>, and lifted structured loss <ref type="bibr" target="#b10">[11]</ref>. However, their core idea is still based on triplet loss, the final performance has not been significantly improved compared with triplet loss <ref type="bibr" target="#b28">[29]</ref>.</p><p>One of the main challenges in DML is the large sample space. For example, the number of triples for a triple network is O(N 3 ) where N is the number of training set samples, it is usually impossible to exhaust all possibilities when training. To improve efficiency, various difficult sample mining methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> have been proposed.</p><p>In recent years, instance segmentation researchers have begun to pay attention to DML. Zhu et al. <ref type="bibr" target="#b11">[12]</ref> learned a distance metric model from the atlases to automatic segmentation of the hippocampus from MR brain images. Liu et al. <ref type="bibr" target="#b14">[15]</ref> proposed a DML enhanced neural network to promote the lesion segmentation result from the existing method. Fathi et al. <ref type="bibr" target="#b12">[13]</ref> adopted the idea of contrastive loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> to calculate the similarity between pairs of pixels so that the similarity of pixels from the same instance approaches 1 and that from different instances approaches 0. Chen et al. <ref type="bibr" target="#b13">[14]</ref> used a fully convolutional network trained by a modified triplet loss as the embedding model to tackle the problem of video object segmentation.</p><p>Most DML tasks take each image as a sample and calculate the similarity between images. In this work, our goal is to distinguish the foreground and background points of an image. We propose a metric learning loss function to learn more discriminative convolution features for image segmentation. To the best of our knowledge, this is the first time that the idea of triplet loss has been applied to the field of image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Approach</head><p>We use DML to force the pixels from different categories more distinguishable in the high-dimensional feature space, thereby improving the classification effect in the low-dimensional semantic space. First, we regard each pixel in an image as a sample, so a certain number of triples can be formed by selecting foreground points and background points. By optimizing the triplet loss, the distance between the feature embedding of similar pixels goes smaller than that of heterogeneous pixels. Second, we present a targeted triple sampling method to improve computational efficiency. Finally, we combine the proposed method with the traditional image segmentation loss function to perform image co-segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Triplet loss for Image Segmentation</head><p>Although the metric learning method based on triplet loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref> was proposed very early, its core idea is simple and practical with good performance <ref type="bibr" target="#b28">[29]</ref>. A triplet is composed of an anchor point, a similar (positive) data point, and a dissimilar (negative) data point. The purpose of triplet loss is to learn a distance metric by which the anchor point is closer to the similar point than the dissimilar one by a margin:</p><formula xml:id="formula_0">( , , ; ) = [ 2 + − 2 ] + (1) Where , ,</formula><p>denotes the anchor point, positive point, and negative point, respectively. is the embedding</p><formula xml:id="formula_1">function, 2 = ‖ ( ) − ( )‖ 2 is the Euclidean distance,</formula><p>is the violate margin that requires the distance of negative pairs to be larger than the distance of positive pairs, [·] + is the hinge function.</p><p>Metric learning methods in image analysis usually treat an image as a sample. Differently, our method regards each pixel as a sample and the high-dimensional features corresponding to each pixel are used for calculation. In this work, we only study the problem of distinguishing the foreground and the background of an image, so only two categories are considered. As shown in <ref type="figure">Figure 1</ref>, when an image is inputted into the convolutional neural network, the output of each layer of the convolutional network contains position information of pixels. The size of an image's feature vector is first compressed and then enlarged until the size is the same as the input size after a certain layer of the network. It means that each pixel is represented by a D-dimensional feature vector. We use pixels to form triples and use the corresponding D-dimensional features to form triples. Then we use the IS-Triplet loss to force the feature distances of similar pixels to decrease, and the feature distances of heterogeneous pixels to increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A New Triple sampling strategy</head><p>The sampling strategy is an important part of DML. For a data set with a data volume of n, the order of magnitude of possible triples is O(N 3 ), and most of the triples cannot provide effective information. In our applications to image segmentation, not only the number of images but also the number of pixels in each image are huge. It is neither possible nor necessary for us to optimize all possible triples. We propose an efficient triple sampling strategy to solve this problem.</p><p>Our method is to limit the total number of triples and balance the weights of the two types of pixels. In particular, as shown in <ref type="figure">Figure 2</ref>, we randomly select K foreground points from each image, use their multi-channel feature vectors to form a set F1, and repeat this operation to obtain another feature vector set F2. Note that some common pixels are allowed in F1 and F2, but their order in the two sets is generally different. Then we use the same method to form 2 feature vector sets B1 and B2 from the background points. If the number of foreground points or background points in an input picture is smaller than K, K takes the smaller value. Through the above operations, on one hand, we control the total number of triples to 2K, which effectively limits the amount of calculation; on the other hand, we can avoid imbalance problem in building triplets, even if the number of foreground pixels is different greatly from that of background pixels. <ref type="figure">Fig. 2</ref> Two sets of K pixels are randomly selected from foreground points or background points of an image Now, we can use the embedded feature vector set F1, F2, B1, B2 to build 2K triples, which results in 2 part of loss:</p><formula xml:id="formula_2">1 = [ 2 ( 1, 2) − 2 ( 1, 1) + ] + (2) 2 = [ 2 ( 1, 2) − 2 ( 1, 2) + ] +<label>(3)</label></formula><p>where ( , ) denotes the Euclidean distance between the corresponding row vector of vectors A and B, is the violate margin that requires the distance of negative pairs to be larger than the distance of positive pairs, [·] + is the hinge function.</p><p>The above Eq. 2 ensures that the embedding feature distance between any two foreground points is smaller than that between any one foreground point and any background point. Eq. 3 is used to ensure that the distance between the embedding features of any two background points is smaller than that between any one foreground point and any background point. Since the number of points in F1, F2, B1, and B2 are K, it is convenient to optimize 2K triples at once, thus ensuring that the distance between the similar pixels is shorter than the distance of the dissimilar pixels. Finally, we add loss1 and loss2 to form our IS-Triplet loss: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Application to Image Co-Segmentation</head><p>We apply the proposed IS-Triplet loss to an image co-segmentation framework <ref type="bibr" target="#b33">[34]</ref>. As shown in <ref type="figure">Figure 1</ref>, in the original framework <ref type="bibr" target="#b33">[34]</ref>, the output of the sixth deconvolution layer is a tensor with 2 channels and half the size of the input image. The Dice loss uses this 2-channel embedding to classify pixels in the semantic space. To apply our IS-Triplet loss, we made the following changes to the network structure. First, the output of the sixth deconvolution layer is modified to 64 channels and the size of the feature map is changed to be consistent with the size of the input image. This feature embedding is used in two places. One side is used for metric learning, and the other side is passed through a new convolutional layer with a kernel size of 1 to make it into 2 channels, with the same size kept, and then use a traditional loss to optimize as before. Finally, the two loss functions are combined by using</p><formula xml:id="formula_3">= + λ − (5)</formula><p>Where λ is a trade-off coefficient.</p><p>Our modified framework is shown in <ref type="figure">Figure 3</ref>. The corresponding deep network is obtained by embedding the correlation block <ref type="bibr" target="#b34">[35]</ref> into a Siamese U-net network <ref type="bibr" target="#b33">[34]</ref>. The overall architecture of the resultant network is composed of four parts. The first part is the Siamese encoders that are a pair of two feature encoder networks, each of which extracts the features from an image. The two encoders share the weights. We adopt the ResNet-50 network <ref type="bibr" target="#b35">[36]</ref> to construct our Siamese encoder. The second part is the correlation block, through which the correlation maps are calculated from the two feature maps. The third part is the Siamese decoders, which are constructed by symmetrically reversing each of the two encoders and concatenating each scale of feature maps into the corresponding layers in the reversed pathway.</p><p>The core content of this paper and the improvement to the original framework are mainly reflected in the fourth part. We have modified the original network, using the latest feature embedding layer of the neural network with the same size as the input image, and using our IS-Triplet loss for metric learning. For more detailed information about this original co-segmentation framework and correlation block, please refer to <ref type="bibr" target="#b33">[34]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resconvolution</head><p>Maxpooling Deconvolution Unpooling <ref type="figure">Fig. 3</ref> The modified image co-segmentation network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We designed 2 groups of experiments. In the first group, we combine IS-triplet loss with Dice loss and compared it with the use of only Dice loss. In the second group, the IS-triplet loss is used to combination with Dice loss, CE loss, and Focal loss to further study the effects of our approach. In the following sections, we describe our experiments in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Datasets</head><p>In the first group of experiments, we use the SBCoseg dataset <ref type="bibr" target="#b15">[16]</ref>, which includes 889 groups of images and each group consists of 18 images with a common object, leading to 16002 images in total. The whole dataset is divided into five subsets: with ECFB, with TR, with MH, with SD, and Normal (normal data). The five subsets contain 193, 251, 82, 83, and 280 image groups, respectively. Each original image is in JPG format with a pixel size of 360 ×360, and each ground-truth image is in PNG format.</p><p>In both of the two groups of experiments, we use the collection of Pascal VOC 2012 1 <ref type="bibr" target="#b36">[37]</ref> and MSRC 2 <ref type="bibr" target="#b37">[38]</ref> datasets to train our image co-segmentation network, we call it the collection dataset for short and then use the Internet <ref type="bibr" target="#b16">[17]</ref> dataset as the test set. These three data sets are widely used in the community of image co-segmentation. MSRC is composed of 591 images of 21 object groups. The ground-truth is roughly labeled, which does not align exactly with the object boundaries. VOC 2012 includes 11,540 images with ground-truth detection boxes and 2913 images with segmentation masks. Only 2913 images with segmentation masks can be considered in our problem. Note that not all of the examples in these two datasets can be used. In MSRC, some images include only stuff without obvious foregrounds, such as only the sky or grassland. In VOC 2012, the interested objects in some images have great changes in appearance and are cluttered in many other objects, so that the meaningful correlation between them is ambiguous. We exclude them from consideration. The remained 1743 images in VOC 2012 and 507 images in MSRC are used to construct our training set. From the training images, we sampled 41,329 pairs of images containing common objects to train our proposed image co-segmentation network.</p><p>The Internet <ref type="bibr" target="#b16">[17]</ref> dataset consists of 3 classes (airplane, car, and house) of thousands of downloaded Internet images. Following the compared methods, we evaluate our approach on its widely used subset, in which each class has 100 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Evaluation Metrics</head><p>We use two commonly used metrics for evaluating the effects of image co-segmentation: Precision and Jaccard index. Precision is the percentage of correctly classified pixels in both background and foreground, which can be defined as</p><formula xml:id="formula_4">Precision = | ∩ ℎ| | |</formula><p>Jaccard index (denoted by Jaccard in the following descriptions) is the overlapping rate of foreground between the segmentation result and the ground-truth mask, which can be defined as Jaccard = ∩ ℎ ∪ ℎ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Implementation Details</head><p>We conduct the experiments on a computer with RTX 2080Ti GPU and implement the image co-segmentation network with PyTorch <ref type="bibr" target="#b38">[39]</ref>. In the experiments, the learning rate is initialized to 0.001 without IS-Triplet loss or 0.0001 with IS-Triplet loss, because we find these parameters perform best in the corresponding experiments. The learning rate is decreased to 0.85 times after each epoch. The weight decay and the momentum parameters are set to be 1e-4 and 0.9, respectively. The optimization procedure ends after 30 epochs.</p><p>For the setting of hyperparameters of our approach, the value of K is set to 5000, the violation boundary m is set to 3.0, and λ is initialized to 1.0 and decreased to 0.85 times after each epoch.</p><p>In the image co-segmentation farmework, the input data are pairs of similar images, we matched the training data for the experiments in advance. Because of limited computing resources, when training on the Collection datasets, all images are resized to the resolution of 448×448 in advance. The batch size for training is set to be 3. The co-segmentation results are resized back to the original image resolution for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The First Group of Experiments</head><p>To test the effectiveness of our proposed method, we combine IS-triplet loss with Dice loss and compared it with the only use of Dice loss. We conduct the above experiments on two different datasets. The first dataset is the relatively simple SBCoseg dataset <ref type="bibr" target="#b15">[16]</ref>, both the training set and the validation set come from this dataset, and there is no duplication between them. In the second experiment, we used more complex datasets. The training set and the validation set are from the Collection dataset, and we use the Internet <ref type="bibr" target="#b16">[17]</ref> as the test set. <ref type="table" target="#tab_1">Table 1</ref> shows the results of this group of experiments. The results show that after using our proposed method, all indicators on the training set, validation set, and test set have been improved.  <ref type="figure" target="#fig_1">Figure 5</ref> shows the comparison charts of the change curve of loss, IOU, and precision on the training set and the validation set. It can be seen that after using our method, the Dice loss decreases faster, the validation IOU and Precision improve faster and better. <ref type="figure">Fig. 4</ref> The change curve of main indicators for using two methods on the Collection datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Second Group of Experiments</head><p>To further study the effects and applicability of our approach, we combine IS-Triplet loss with Dice loss, CE loss, and Focal loss. In this group of experiments, we use the Collection dataset as the training set and validation set, and Internet <ref type="bibr" target="#b16">[17]</ref> as the test set. <ref type="table">Table 2</ref> shows the results of this group of experiments. On the validation set, our metric learning method has an improved effect on all of the three loss functions. On the test set, our IS-Triplet loss improves the performance of Dice loss and CE loss but slightly decreases the performance of Focal loss.</p><p>Table2 The performance comparisons of combining our method with each of 3 losses respectively</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Serial</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Loss Validation Precision</head><p>Validation Jaccard</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test mean Precision</head><p>Test mean Jaccard  <ref type="figure" target="#fig_1">Figure 5</ref> shows a comparison chart of the change curves of loss, IOU, and precision on the training set and the validation set as the training progresses. It can be seen from the figure that after using our method, the 3 loss functions decrease faster and better, and the IOU and Precision are also improved. Furthermore, although the original performance of these three loss functions is different, they can all be improved to almost the same level after using our metric learning method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel DML loss function for image co-segmentation, called IS-Triplet loss. By optimizing the multiple triples formed by the embedding features of pixels in the high-dimensional space, we can make the pixels of different categories have better distinguishability in high-dimensional space, thereby improving the distinguishability of pixels in the semantic space. To take into account the optimization effect and calculation speed, we also especially propose a targeted triple sampling strategy, which not only controls the upper limit of the sampling number of the triples but also eliminates the adverse effects caused by the imbalance of the foreground and the background points' numbers. We apply the proposed method to the image co-segmentation, combined with dice loss, CE loss, and Focal loss. The experiment results on the SBCoseg dataset and the Internet dataset show that after using our method, the traditional loss functions decrease faster and better, and the IOU and Precision are also improved. Furthermore, although the original performance of these 3 loss functions is different, they can all be improved to almost the same level after using our metric learning method.</p><p>In the future, we try to explore the application of the proposed approach to other image segmentation problems, such as semantic segmentation, video dynamic segmentation, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1, 2) − ( 1, 1) + ] + + [ ( 1, 2) − ( 1, 2) + ] + )(4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5</head><label>5</label><figDesc>The change curve of main indicators when combining our method with each of 3 traditional loss functions on the Collection dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Performance comparison with and without using our method for Dice loss.</figDesc><table><row><cell>Method</cell><cell>Loss</cell><cell>Train&amp;Val Dataset</cell><cell>Test Dataset</cell><cell>Val P(%)</cell><cell>Val J</cell><cell>Test P(%)</cell><cell>Test J</cell></row><row><cell>Gong et al.[34]</cell><cell>Dice loss</cell><cell>SBCoseg</cell><cell>SBCoseg</cell><cell>99.0</cell><cell>0.981</cell><cell>99.1</cell><cell>0.949</cell></row><row><cell>Modified framework</cell><cell>Dice loss + IS-Triplet loss</cell><cell>SBCoseg</cell><cell>SBCoseg</cell><cell>99.4</cell><cell>0.985</cell><cell>99.2</cell><cell>0.951</cell></row><row><cell>Gong et al.[34]</cell><cell>Dice loss</cell><cell>Collection</cell><cell>Internet</cell><cell>98.7</cell><cell>0.978</cell><cell>94.5</cell><cell>0.760</cell></row><row><cell>Modified framework</cell><cell>Dice loss + IS-Triplet loss</cell><cell>Collection</cell><cell>Internet</cell><cell>99.3</cell><cell>0.987</cell><cell>95.0</cell><cell>0.801</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Pattern analysis statistical modeling and computational learning, visual object classes, 2012. 2 Microsoft Research Cambridge.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A gentle introduction to machine learning for natural language processing: How to start in 16 practical steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hladka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="55" to="76" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust liver vessel extraction using 3D U-Net with variant dice loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="153" to="162" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Book Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Focal loss for dense object detection</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Book Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
	<note>Deep metric learning using triplet network</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Book Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
	<note>Facenet: A unified embedding for face recognition and clustering</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Margin sample mining loss: A deep learning based method for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00478</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ieee Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Book Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
	<note>Learning to compare image patches via convolutional neural networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<title level="m">Ranked List Loss for Deep Metric Learning</title>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<title level="m">DISTANCE METRIC LEARNING WITH N-PAIR LOSS</title>
		<imprint>
			<publisher>PatentApplication Publication</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Book Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
	<note>Deep metric learning via lifted structured feature embedding</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Metric learning for multi-atlas based segmentation of hippocampus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation via deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Book Blazingly fast video object segmentation with pixel-wise metric learning&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
	<note>Blazingly fast video object segmentation with pixelwise metric learning</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Book An enhanced neural network based on deep metric learning for skin lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1633" to="1638" />
		</imprint>
	</monogr>
	<note>An enhanced neural network based on deep metric learning for skin lesion segmentation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new challenging image dataset with simple background for evaluating and developing co-segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">115813</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Book Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1939" to="1946" />
		</imprint>
	</monogr>
	<note>Unsupervised joint object discovery and segmentation in internet images</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Book The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
	<note>The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Book U-net: Convolutional networks for biomedical image segmentation</title>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Focal fcn: Towards small object segmentation with limited training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01506</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A fully convolutional neural network for cardiac segmentation in short-axis MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00494</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Book V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
	<note>V-net: Fully convolutional neural networks for volumetric medical image segmentation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Book Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
	<note>Dimensionality reduction by learning an invariant mapping</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Book Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1875" to="1882" />
		</imprint>
	</monogr>
	<note>Discriminative deep metric learning for face verification in the wild</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Book Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
	<note>Learning fine-grained image similarity with deep ranking</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Book Fine-grained categorization and dataset bootstrapping using deep metric learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1153" to="1162" />
		</imprint>
	</monogr>
	<note>Fine-grained categorization and dataset bootstrapping using deep metric learning with humans in the loop</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Book Beyond triplet loss: a deep quadruplet network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
	<note>Beyond triplet loss: a deep quadruplet network for person reidentification</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Book Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
	<note>Improved deep metric learning with multi-class n-pair loss objective</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08505</idno>
		<title level="m">A metric learning reality check</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Book A theoretically sound upper bound on the triplet loss for improving the efficiency of deep distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10404" to="10413" />
		</imprint>
	</monogr>
	<note>A theoretically sound upper bound on the triplet loss for improving the efficiency of deep distance metric learning</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Book Deep metric learning by online soft mining and class-aware attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5361" to="5368" />
		</imprint>
	</monogr>
	<note>Deep metric learning by online soft mining and class-aware attention</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A novel co-attention computation block for deep learning based image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">103973</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Book Deep object co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="638" to="653" />
		</imprint>
	</monogr>
	<note>Deep object co-segmentation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Book Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Deep residual learning for image recognition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust object co-segmentation using background prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1639" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Book Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Book Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<editor>&apos;, in Editor (Ed.)^</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
	<note>Pytorch: An imperative style, high-performance deep learning library</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
