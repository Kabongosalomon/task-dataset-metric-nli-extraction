<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUDO RM -RF: EFFICIENT NETWORKS FOR UNIVERSAL AUDIO SOURCE SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign Adobe Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SUDO RM -RF: EFFICIENT NETWORKS FOR UNIVERSAL AUDIO SOURCE SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Audio source separation</term>
					<term>low-cost neural net- works</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an efficient neural network for end-to-end general purpose audio source separation. Specifically, the backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRM-RF) as well as their aggregation which is performed through simple one-dimensional convolutions. In this way, we are able to obtain high quality audio source separation with limited number of floating point operations, memory requirements, number of parameters and latency. Our experiments on both speech and environmental sound separation datasets show that SuDoRM-RF performs comparably and even surpasses various state-of-the-art approaches with significantly higher computational resource requirements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The advent of the deep learning era has enabled the effective usage of neural networks towards single-channel source separation with mask-based architectures <ref type="bibr" target="#b0">[1]</ref>. Recently, end-to-end source separation in time-domain has shown state-of-the-art results in a variety of separation tasks such as: speech separation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, universal sound separation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and music source separation <ref type="bibr" target="#b5">[6]</ref>. The separation module of ConvTasNet <ref type="bibr" target="#b1">[2]</ref> and its variants <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> consist of multiple stacked layers of depth-wise separable convolutions <ref type="bibr" target="#b6">[7]</ref> which can aptly incorporate long-term temporal relationships. Building upon the effectiveness of a large temporal receptive field, a dual-path recurrent neural network (DPRNN) <ref type="bibr" target="#b2">[3]</ref> has shown remarkable performance on speech separation. Demucs <ref type="bibr" target="#b5">[6]</ref> has a refined U-Net structure <ref type="bibr" target="#b7">[8]</ref> and has shown strong performance improvement on music source separation. Specifically, it consists of several convolutional layers in each a downsampling operation is performed in order to extract high dimensional features. A two-step approach has been introduced in <ref type="bibr" target="#b8">[9]</ref> and showed that universal sound separation models could be further improved when working directly on the latent space and learning the ideal masks on a separate step.</p><p>Despite the dramatic advances in source separation performance, the computational complexity of the aforementioned methods might hinder their extensive usage across multiple devices. Specifically, many of these algorithms are not amenable to, e.g., embedded systems deployment, or other environments where computational resources are constrained. Additionally, training such systems is also an expensive computational undertaking which can amount to significant costs. <ref type="bibr">Supported</ref>  Several studies, mainly in the image domain, have introduced more efficient architectures in order to overcome the growing concern of large models with high computational requirements. Models with depth-wise separable convolutions <ref type="bibr" target="#b6">[7]</ref> have shown strong potential for several image-domain tasks <ref type="bibr" target="#b9">[10]</ref> while significantly reducing the computational requirements. Thus, several variants such as MobileNets <ref type="bibr" target="#b10">[11]</ref> have been proposed for deep learning on edgedevices. However, convolutions with a large dilation factor might inject several artifacts and thus, lightweight architectures that combine several dilation factors in each block have been proposed for image tasks <ref type="bibr" target="#b11">[12]</ref>. More recent studies propose meta-learning algorithms for optimizing architecture configurations given specific computational resource and accuracy requirements <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Despite the recent success on low-resource architectures on the image domain, little progress has been made towards proposing efficient architectures for audio tasks and especially source separation. In <ref type="bibr" target="#b14">[15]</ref> a WaveRNN is used for efficient audio synthesis in terms of floating point operations (FLOPs) and latency. Other studies have introduced audio source separation models with reduced number of trainable parameters <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> and binarized models <ref type="bibr" target="#b16">[17]</ref>. In this study, we propose a novel efficient neural network architecture for audio source separation while following a more holistic approach in terms of computational resources that we take into consideration (FLOPs, latency and total memory requirements). Our proposed model performs SUccessive DOwnsampling and Resampling of Multi-Resolution Features (SuDoRM-RF) using depth-wise convolutions. By doing so, SuDoRM-RF exploits the effectiveness of iterative temporal resampling strategies <ref type="bibr" target="#b17">[18]</ref> and avoids the need of multiple stacked dilated convolutional layers <ref type="bibr" target="#b1">[2]</ref>. We report a separation performance comparable or even better to several recent state-of-the-art models on speech and environmental sound separation tasks with significantly lower computational requirements. Our experiments suggest that SuDoRM-RF models a) could be deployed on devices with limited resources, b) be trained significantly faster and achieve good separation performance and c) scale well when increasing the number of parameters. Our code is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SUDO RM -RF NETWORK ARCHITECTURE</head><p>On par with many state-of-the-art approaches in the literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6]</ref>, SuDoRM-RF performs end-to-end audio source separation using a mask-based architecture with adaptive encoder and decoder basis. The input is the raw signal from a mixture x ∈ R T with T samples in the time-domain. First we feed the input mixture x to an encoder E in order to obtain a latent representation for the mixture vx = E (x) ∈ R C E ×L . Consequently the latent mixture representation is fed through a separation module S which estimates the corresponding masks mi ∈ R C E ×L for each one of the N sources s1, · · · , sN ∈ R T which constitute in the mixture. The estimated latent representation for each source in the latent space vi is retrieved by multiplying element-wise an estimated mask mi with the encoded mixture representation vx. Finally, the reconstruction for each source si is obtained by using a decoder D to transform the latentspace vi source estimates back into the time-domain si = D ( vi). An overview of the SuDoRM-RF architecture is displayed in <ref type="figure">Figure 1</ref>. The encoder, separator and decoder modules are described in Sections 2.1, 2.2 and 2.3, respectively. For simplicity of our notation we will describe the whole architecture assuming that the processed batch size is one. Moreover, we are going to define some useful operators of the various convolutions which are used in SuDoRM-RF. <ref type="figure">Fig. 1</ref>: SuDoRM-RF architecture for separating two sources.</p><formula xml:id="formula_0">S e p a r a t o r  U - C o n v B l o c k s ˆ 1 ˆ 2 E n c o d e r  D e c o d e r  ˆ 1 ˆ 2 ˆ 1 ˆ 2 ( 0 ) ( ) C o n v 1 D , ,    2 C o n v 1 D , 1 , 1 L a y e r N o r m C o n v 1 D , , 1  C o n v T r 1 D , ,    2 T ( 1 ) ( 2 ) T T T T r a n s p o s e S o f t m a x</formula><formula xml:id="formula_1">Definition 2.1. Conv1DC,K,S : R C in ×L in → R C×L defines a kernel W ∈ R C×C in ×K and a bias vector b ∈ R C .</formula><p>When applied on a given input x ∈ R C in ×L in it performs a one-dimensional convolution operation with stride equal to S as shown next:</p><formula xml:id="formula_2">Conv1DC,K,S (x) i,l = bi + C in j=1 K k=1 W i,j,k · x j,S·l−k ,<label>(1)</label></formula><p>where the indices i, j, k, l denotes the output channel, the input channel, the kernel sample and the temporal index, respectively. Note that without loss of generality and performing appropriate padding, the last dimension of the output representation would be L = L in/S . Definition 2.2. ConvTr1DC,K,S : R C in ×L in → R C×L defines a one-dimensional transpose convolution. Since any convolution operation could be expressed as a matrix multiplication, transposed convolution can be directly understood as the gradient calculation for a regular convolution w.r.t. its input <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b6">[7]</ref>. In essence, this operator defines G = Cin separate one-dimensional convolu-</p><formula xml:id="formula_3">Definition 2.3. DWConv1DC,K,S : R C in ×L in → R C×L defines a one-dimensional depth-wise convolution operation</formula><formula xml:id="formula_4">tions Fi = [Conv1DC G ,K,S ] i with i ∈ {1, · · · , G} where CG = C /G .</formula><p>Given an input x ∈ R C in ×L in the ith one-dimensional convolution contributes to CG = C /G output channels by considering as input only the ith row of the input as described below:</p><formula xml:id="formula_5">DWConv1DC,K,S (x) = Concat ({Fi (xi) , ∀i}) ,<label>(2)</label></formula><p>where Concat(·) performs the concatenation of all individual onedimensional convolution outputs across the channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Encoder</head><p>The encoder E architecture consists of a one-dimensional convolution with kernel size KE and stride equal to K E/2 similar to <ref type="bibr" target="#b1">[2]</ref>. Each convolved input audio-segment of KE samples is transformed to a CE -dimensional vector representation where CE is the number of output channels of the 1D-convolution. We force the output of the encoder to be strictly non-negative by applying a rectified linear unit (ReLU) activation on top of the output of the 1D-convolution. Thus, the encoded input mixture representation could be expressed as:</p><formula xml:id="formula_6">vx = E (x) = ReLU Conv1D C E ,K E ,K E /2 (x) ∈ R C E ×L ,<label>(3)</label></formula><p>where the activation ReLU (·) is applied element-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Separator</head><p>In essence, the separator S module performs the following transformations to the encoded mixture representation vx ∈ R C E ×L :</p><p>1. Projects the encoded mixture representation vx ∈ R C E ×L to a new channel space through a layer-normalization (LN) <ref type="bibr" target="#b19">[20]</ref> followed by a point-wise convolution as shown next:</p><formula xml:id="formula_7">y0 = Conv1DC,1,1 (LN (vx)) ∈ R C×L ,<label>(4)</label></formula><p>where LN (vx) denotes a layer-normalization layer in which the moments used are extracted across the temporal dimension for each channel separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Performs repetitive non-linear transformations provided by B U-convolutional blocks (U-ConvBlocks) on the intermediate representation y0.</head><p>In other words, the output of the ith U-ConvBlock would be denoted as yi ∈ R C×L and would be used as input for the (i + 1)th block. Each U-ConvBlock extracts and aggregates information from multiple resolutions which is extensively described in Section 2.2.1.</p><p>3. Aggregates the information over multiple channels by applying a regular one-dimensional convolution for each source on the transposed feature representation y T B ∈ R L×C . Effectively, for the ith source we obtain an intermediate latent representation as shown next:</p><formula xml:id="formula_8">zi = Conv1DC,C E ,1 y T B T ∈ R C E ×L<label>(5)</label></formula><p>This step has been introduced in <ref type="bibr" target="#b8">[9]</ref> and empirically shown to make the training process more stable rather than using the activations from the final block yB to estimate the masks.</p><p>4. Combines the aforementioned latent codes for all sources zi ∀i ∈ {1, · · · , N } by performing a softmax operation in order to get mask estimates mi ∈ [0, 1] C E ×L which add up to one across the dimension of the sources. Namely, the corresponding mask estimate for the ith source would be:</p><formula xml:id="formula_9">mi = vec −1 exp (vec (zi)) N j=1 exp (vec (zj)) ∈ R C E ×L ,<label>(6)</label></formula><p>where vec (·) : R K×N → R K·N and vec −1 (·) : R K·N → R K×N denotes the vectorization of an input tensor and the inverse operation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Estimates a latent representation vi ∈ R C E ×L for each source by multiplying element-wise the encoded mixture representation vx with the corresponding mask mi:</p><formula xml:id="formula_10">vi = vx mi ∈ R C E ×L ,<label>(7)</label></formula><p>where a b is the element-wise multiplication of the two tensors a and b assuming that they have the same shape.</p><formula xml:id="formula_11">( 0 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 0 ) ∈ ℝ × ∈ ( ) ℝ × ∈ ( + 1 ) ℝ × ( 3 )<label>( 2 )</label></formula><p>Channel expansion  </p><formula xml:id="formula_12">Input: y (i) ∈ R C×L Output: y (i+1) ∈ R C×L // Expand channel dimensions q ← PReLUC U LN Conv1DC U ,1,1 y (i) ; d (0) ← PReLUC U (LN (DWConv1DC U ,K U ,1 (q))); for i = 1; i++; while i &lt;= Q do // Successive depth-wise downsampling d (i) ← LN DWConv1DC U ,K U ,S U d (i−1) ; d (i) ← PReLUC U d (i) ; end u (Q) ← d (Q) ; for i = Q − 1; i−−; while i &gt;= 0 do // Upsample and add resolutions u (i) ← d (i) + IS U u (i+1) ; end o ← LN Conv1DC,1,1 PReLUC LN u (0) ;</formula><p>return PReLUC y (i) + o ;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">U-convolutional block (U-ConvBlock)</head><p>U-ConvBlock uses a block structure which resembles a depth-wise separable convolution <ref type="bibr" target="#b6">[7]</ref> with a skip connection as in ConvTas-Net <ref type="bibr" target="#b1">[2]</ref>. However, instead of performing a regular depth-wise convolution as shown in <ref type="bibr" target="#b9">[10]</ref> or a dilated depth-wise which has been successfully utilized for source separation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref> our proposed U-ConvBlock extracts information from multiple resolutions using Q successive temporal downsampling and Q upsampling operations similar to a U-Net architecture <ref type="bibr" target="#b7">[8]</ref>. More importantly the output of each block leaves the temporal resolution intact while increasing the effective receptive field of the network multiplicatively with each temporal sub-sampling operation <ref type="bibr" target="#b20">[21]</ref>. An abstract view of the ith U-ConvBlock is displayed in <ref type="figure" target="#fig_0">Figure 2</ref> while a detailed description of the operations is presented in Algorithm 1.</p><p>Definition 2.4. PReLUC : R C×L → R C×L defines a parametric rectified linear unit (PReLU) <ref type="bibr" target="#b21">[22]</ref> with C learnable parameters a ∈ R C . When applied to an input matrix y ∈ R C×L the non-linear transformation could be defined element-wise as:</p><p>PReLUC (y) i,j = max (0, yi,j) + ai · min (0, yi,j)</p><p>Definition 2.5. IM : R C×L → R C×M ·L defines a nearest neighbor temporal interpolation by a factor of M . When applied on an input matrix y ∈ R C×L this upsampling procedure could be formally expressed element-wise as: IM (u) i,j = u i, j/M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Decoder</head><p>Our decoder module D is the final step in order to transform the latent space representation vi for each source back to the time-domain. In our proposed model we follow a similar approach as in <ref type="bibr" target="#b8">[9]</ref> where each latent source representation vi if fed through a different transposed convolution decoder ConvTr1D C E ,K E ,K E /2 . The efficacy of dealing with different types of sources using multiple decoders has also been studied in <ref type="bibr" target="#b22">[23]</ref>. Ignoring the permutation problem, for the ith source we have the following reconstruction in time:</p><formula xml:id="formula_14">si = Di ( vi) = ConvTr1D C E ,K E ,K E /2 ( vi)<label>(9)</label></formula><p>3. EXPERIMENTAL SETUP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Audio source separation tasks</head><p>Speech separation: We perform speech separation experiments in accordance with the publicly available WSJ0-2mix dataset <ref type="bibr" target="#b23">[24]</ref> and other studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Speaker mixtures are generated by randomly mixing speech utterances with two active speakers at random signal to noise ratios (SNR)s between −5 and 5dB from the Wall Street Journal (WSJ0) corpus <ref type="bibr" target="#b26">[27]</ref>. Non-speech sound separation: For our non-speech sound separation experiments we follow the exact same setup as in <ref type="bibr" target="#b8">[9]</ref> and utilize audio clips from the environmental sound classification (ESC50) data collection <ref type="bibr" target="#b27">[28]</ref> which consists of a wide variety of sounds (nonspeech human sounds, animal sounds, natural soundscapes, interior sounds and urban noises). For each data sample, two audio sources are mixed with a random SNR between −2.5 and 2.5dB where each source belongs to a distinct sound category from a total of 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data preprocessing and generation</head><p>We follow the same data augmentation process which was firstly introduced in <ref type="bibr" target="#b8">[9]</ref> and it has been show beneficial in other recent studies <ref type="bibr" target="#b24">[25]</ref>. The process for generating a mixture is the following: A) random choosing two sound classes (for universal sound separation) or speakers (for speech separation) B) random cropping of 4sec segments from two sources audio files C) mixing the source segments with a random SNR (as specified in Section 3.1). For each epoch, 20, 000 new training mixtures are generated. Validation and test sets are generated once with each one containing 3, 000 mixtures. Moreover, we downsample each audio clip to 8kHz, subtract its mean and divide with the standard deviation of the mixture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and evaluation details</head><p>All models are trained for 120 epochs using a batch size equal to 4. As a loss function we use the negative permutation-invariant <ref type="bibr" target="#b28">[29]</ref> scale-invariant signal to distortion ratio (SI-SDR) <ref type="bibr" target="#b29">[30]</ref> which is defined between the clean sources s and the estimates s as:</p><formula xml:id="formula_15">L = −SI-SDR(s * , s) = −10 log 10 αs * 2 / αs * − s 2 ,<label>(10)</label></formula><p>where s * denotes the permutation of the sources that maximizes SI-SDR and α = s s * / s 2 is just a scalar. In order to evaluate the performance of our models we use the SI-SDR improvement (SI-SDRi) which is the gain that we get on SI-SDR measure using estimated signal instead of the mixture signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">SuDoRM-RF configurations</head><p>For the encoder E and decoder modules D we use a kernel size KE = 21 corresponding to 2.625ms and a number of basis equal to CE = 512. For the configuration of each U-ConvBlock we set the input number of channels equal to C = 128, the number of successive resampling operations equal to Q = 4 and the expanded number of channels equal to CU = 512. In each subsampling operation we reduce the temporal dimension by a factor of 2 and all depth-wise separable convolutions have a kernel length of KU = 5 and a stride of SU = 2. We propose 3 different models which are configured through the number B of U-ConvBlocks inside the separator module S. Namely, SuDoRM-RF 1.0x , SuDoRM-RF 0.5x , SuDoRM-RF 0.25x consist of 16, 8 and 4 blocks, respectively. During training, we use the Adam optimizer <ref type="bibr" target="#b30">[31]</ref> with an initial learning rate set to 0.001 and we decrease it by a factor of 5 every 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Literature models configurations</head><p>We compare against the best configurations of some of the latest state-of-the-art approaches for speech <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, universal <ref type="bibr" target="#b8">[9]</ref> and music <ref type="bibr" target="#b5">[6]</ref> source separation. For a fair comparison with the aforementioned models we use the authors original code, the best performing configurations for the proposed models as well as the suggested training process. For Demucs <ref type="bibr" target="#b5">[6]</ref>, 80 channels are used instead of 100 in order to be able to train it on a single graphical processing unit (GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Measuring computational resources</head><p>One of the main goals of this study is to propose a model for audio source separation which could be trained using limited computational resources and deployed easily on a mobile or edge-computing device <ref type="bibr" target="#b31">[32]</ref>. Specifically, we consider the following aspects which might cause a computational bottleneck during inference or training:</p><p>1. Number of executed floating point operations (FLOPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Number of trainable parameters.</head><p>3. Memory allocation required on the device for a single pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Time for completing each process.</head><p>We are using various sampling profilers in Python for tracing all the requirements on an Intel Xeon CPU E5-2695 v3 @ 2.30GHz CPU and a Nvidia Tesla K80 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS &amp; DISCUSSION</head><p>In <ref type="table" target="#tab_2">Table 1</ref>, we show the separation performance alongside computational requirements for some of the most recent state-of-the-art models in the literature and the proposed SuDoRM-RF configurations. It is easy to see that the proposed models can match and even outperform the separation performance of other several state-of-the-art models using orders of magnitude less computational requirements. A better visualization for understanding the Pareto efficiency of the proposed architectures is displayed in <ref type="figure" target="#fig_1">Figure 3</ref> where we show for each model its performance on non-speech sound separation vs a specific computational requirement. We do not show the same plots for speech separation on the WSJ dataset as the patterns were similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Floating point operations (FLOPs)</head><p>Different devices (CPU, GPU, mobiles, etc.) have certain limitations on their FLOPs throughput capacity. In the case of an edge device, the computational resource one might be interested in is the number of FLOPs required during inference. On the other hand, training on cloud machines might be costly if a huge number of FLOPs is needed in order to achieve high separation performance. As a result, it is extremely important to be able to train and deploy models which require a low number of computations <ref type="bibr" target="#b10">[11]</ref>. We see from the first column of <ref type="figure" target="#fig_1">Figure 3</ref> that SuDoRM-RF models scale well as we increase the number of U-ConvBlocks B from 4 → 8 → 16. Furthermore, we see that for both forward and backward passes the family of the proposed SuDoRM-RF models appear more Pareto efficient in terms of SI-SDRi performance vs Giga-FLOPs (GFLOPs) and time required compared to the other state-of-the-art models which we take into account. Specifically, the DPRNN model <ref type="bibr" target="#b2">[3]</ref> which performs sequential matrix multiplications (even with a low number of parameters) requires at least 45 times more FLOPs for a single pass compared to SuDoRM-RF 0.25x while performing worse when trained for the same number of epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Cost-efficient training</head><p>Usually one of the most detrimental factors for training deep learning models is the requirement of allocating multiple GPU devices for several days or weeks until an adequate performance is obtained on the validation set. This huge power consumption could lead to huge cloud services rental costs and carbon dioxide emissions <ref type="bibr" target="#b13">[14]</ref>.</p><p>In <ref type="figure">Figure 4</ref>, we show the validation SI-SDRi performance for the speech separation task which is obtained by each model versus the total amount of FLOPs performed. For each training epoch all models perform updates while iterating over 20, 000 audio mixtures. Notably, SuDoRM-RF models outperform all other models in terms of cost-efficient training as they obtain better separation performance while requiring significantly less amount of training FLOPs. For instance, SuDoRM-RF 1.0x obtains ≈ 16dB in terms of SI-SDRi compared to ≈ 10dB of DPRNN <ref type="bibr" target="#b2">[3]</ref> which manages to complete only 3 epochs given the same number of training FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Trainable parameters</head><p>From <ref type="table" target="#tab_2">Table 1</ref> it is easy to see that SuDoRM-RF architectures are using orders of magnitude fewer parameters compared to the Unet architectures like Demucs <ref type="bibr" target="#b5">[6]</ref> where each temporal downsampling is followed by a proportional increase to the number of channels. Moreover, the upsampling procedure inside each U-ConvBlock does not require any additional parameters. The SuDoRM-RF models seem to increase their effective receptive field with significantly fewer parameters compared to dilated convolutional architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. Notably, our largest model SuDoRM-RF 1.0x matches the relatively low number of parameters of the DPRNN <ref type="bibr" target="#b2">[3]</ref> model which is based on stacked RNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Memory requirements</head><p>In most of the studies where efficient architectures are introduced <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> authors are mainly concerned with the total number of trainable parameters of the network. The same applies to efficient architectures for source separation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16]</ref>. However, the trainable parameters is only a small portion of total amount of memory required for a single forward or backward pass. The space complexity could easily blow up by the storage of intermediate representations.</p><p>The latter could become even worse when multiple skip connections are present, gradients from multiple layers have to be stored or implementations require augmented matrices (dilated, transposed convolutions, etc.). In <ref type="figure" target="#fig_1">Figure 3</ref>, we see that SuDoRM-RF models are more pareto-efficient in terms of the memory required compared to the dilated convolutional architectures of ConvTasNet <ref type="bibr" target="#b1">[2]</ref> and Two-</p><p>Step TDCN <ref type="bibr" target="#b8">[9]</ref> where they require an increased network depth in order to increase their receptive field. Although SuDoRM-RF models do not perform downsampling in every feature extraction step as Demucs <ref type="bibr" target="#b5">[6]</ref> does, we see that the proposed models require orders of magnitude less memory especially during a backward update step as the number of parameters in Demucs is significantly higher. Finally, SuDoRM-RF models have a smaller memory footprint because the encoder E performs a temporal downsampling by a factor  <ref type="table">Table 2</ref>: SI-SDRi separation performance on WSJ0-2mix for various parameter configurations of SuDoRM-RF models. Mask Act. corresponds to the activation function before the mask estimation and Dec. specifies the number of decoders we are using before reconstructing the time-domain signals. GLN corresponds to the global layer normalization as described in <ref type="bibr" target="#b1">[2]</ref>. All the other parameters have the same values as described in Section 3.4</p><p>of div (KE , 2) = 10 compared to DPRNN <ref type="bibr" target="#b2">[3]</ref> which does not reduce the temporal resolution at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study on WSJ0-2mix</head><p>We perform a small ablation study in order to show how different parameter choices in SuDoRM-RF models affect the separation performance. In order to be directly comparable with the numbers reported by several other studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, we train our models for 200 epochs and test them using the given data splits from WSJ0-2mix dataset <ref type="bibr" target="#b23">[24]</ref>. The results are shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this study, we have introduced the SuDoRM-RF network, a novel architecture for efficient universal sound source separation. The proposed model is capable of extracting multi-resolution temporal features through successive depth-wise convolutional downsampling of intermediate representations and aggregates them using a nonparametric interpolation scheme. In this way, SuDoRM-RF models are able to significantly reduce the required number of layers in order to effectively capture long-term temporal dependencies. We show that these models can perform similarly or even better than recent state-of-the-art models while requiring significantly less computational resources in FLOPs, memory and time. In the future, we aim to use SuDoRM-RF models for real-time low-cost source separation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>U-ConvBlock architecture. Algorithm 1: U-ConvBlock forward pass</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>SI-SDRi non-speech sound separation performance on ESC50 vs computational resources with an input audio of 8000 samples for all models. (Top row) computational requirements for a single forward pass on CPU (Bottom) for a backward pass on GPU. All x-axis are shown in log-scale while the 3 connected blue stars correspond to the three SuDoRM-RF configurations from Section 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>by NSF grant #1453104. A Titan V used for this research was donated by the NVIDIA Corporation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>SI-SDRi separation performance for all models on both separation tasks (speech and non-speech) alongside their computational requirements for performing inference on CPU (I) and a backward update step on GPU (B) for one second of input audio or equivalently 8000 samples. * We assign the maximum SI-SDRi performance obtained by our runs and the reported number on the corresponding paper. Validation SI-SDRi separation performance for speechseparation vs the number of FLOPs executed during training. All models are trained using batches of 4 mixtures with 32, 000 timesamples each. Each point corresponds to a completed training epoch.</figDesc><table><row><cell cols="2">Model</cell><cell></cell><cell></cell><cell cols="3">SI-SDRi (dB) Speech separation Non-speech separation</cell><cell>Parameters (millions)</cell><cell cols="2">GFLOPs I B</cell><cell>Memory (GB) I B</cell><cell>Time (sec) I B</cell></row><row><cell cols="3">ConvTasNet [2]</cell><cell></cell><cell>15.30*</cell><cell></cell><cell>7.74</cell><cell>5.05</cell><cell>5.23</cell><cell>5.30</cell><cell>0.65</cell><cell>0.88</cell><cell>0.90 0.33</cell></row><row><cell cols="2">Demucs [6]</cell><cell></cell><cell></cell><cell>12.12</cell><cell></cell><cell>7.23</cell><cell>415.09</cell><cell>3.43</cell><cell>10.34 2.24</cell><cell>8.77</cell><cell>0.53 0.36</cell></row><row><cell cols="2">DPRNN [3]</cell><cell></cell><cell></cell><cell>18.80*</cell><cell></cell><cell>7.20</cell><cell>2.63</cell><cell cols="2">48.89 48.90 2.23</cell><cell>3.40</cell><cell>3.98 0.60</cell></row><row><cell cols="3">Two-Step TDCN [9]</cell><cell></cell><cell>16.10*</cell><cell></cell><cell>8.22</cell><cell>8.63</cell><cell>7.09</cell><cell>7.23</cell><cell>0.99</cell><cell>1.17</cell><cell>1.05 0.30</cell></row><row><cell cols="3">SuDoRM-RF 1.0x</cell><cell></cell><cell>17.02</cell><cell></cell><cell>8.35</cell><cell>2.66</cell><cell>2.52</cell><cell>2.56</cell><cell>0.61</cell><cell>0.86</cell><cell>0.67 0.38</cell></row><row><cell cols="3">SuDoRM-RF 0.5x</cell><cell></cell><cell>15.37</cell><cell></cell><cell>8.12</cell><cell>1.42</cell><cell>1.54</cell><cell>1.56</cell><cell>0.40</cell><cell>0.45</cell><cell>0.36 0.21</cell></row><row><cell cols="3">SuDoRM-RF 0.25x</cell><cell></cell><cell>13.39</cell><cell></cell><cell>7.93</cell><cell>0.79</cell><cell>1.06</cell><cell>1.07</cell><cell>0.30</cell><cell>0.25</cell><cell>0.29 0.13</cell></row><row><cell>10 12 14 16 SI-SDRi (dB)</cell><cell cols="6">Validation SI-SDRi vs Training Computation SuDoRM-RF 0.25x SuDoRM-RF 0.5x SuDoRM-RF 1.0x</cell><cell></cell><cell></cell></row><row><cell>6 8</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6 Peta FLOPs ConvTasNet 8 DPRNN</cell><cell>10</cell><cell>12 Demucs Two-Step TDCN</cell><cell></cell><cell></cell></row><row><cell cols="2">Fig. 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code: https://github.com/etzinis/sudo rm rf arXiv:2007.06833v1 [eess.AS] 14 Jul 2020</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1562" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain singlechannel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WASPAA</title>
		<meeting>WASPAA</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="175" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving universal sound separation using sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Pw</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Music source separation in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Ph. D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two-step sound source separation: Training on learned latent targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikant</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9190" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Slimmable neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Once for all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lightweight online separation of the sound source of interest through blstm-based binary masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Maldonado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Rascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivette</forename><surname>Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11241</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient source separation using bitwise neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="187" to="206" />
		</imprint>
	</monogr>
	<note>in Audio Source Separation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norimichi</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Proc. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Monaural music source separation using a resnet latent separator network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICTAI</title>
		<meeting>ICTAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1124" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wavesplit: End-toend speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11148</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The design for the wall street journal-based CSR corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop Held at</title>
		<meeting><address><addrLine>Harriman, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Esc: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speakerindependent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepx: A software accelerator for low-power deep learning inference on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Nicholas D Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petko</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Forlivesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorena</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Qendro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kawsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPSN</title>
		<meeting>IPSN</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
