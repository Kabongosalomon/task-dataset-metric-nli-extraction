<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Domain Adaptation via Structured Prediction Based Selective Pseudo-Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<email>qian.wang173@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
							<email>toby.breckon@durham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Domain Adaptation via Structured Prediction Based Selective Pseudo-Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation aims to address the problem of classifying unlabeled samples from the target domain whilst labeled samples are only available from the source domain and the data distributions are different in these two domains. As a result, classifiers trained from labeled samples in the source domain suffer from significant performance drop when directly applied to the samples from the target domain. To address this issue, different approaches have been proposed to learn domain-invariant features or domain-specific classifiers. In either case, the lack of labeled samples in the target domain can be an issue which is usually overcome by pseudo-labeling. Inaccurate pseudo-labeling, however, could result in catastrophic error accumulation during learning. In this paper, we propose a novel selective pseudo-labeling strategy based on structured prediction. The idea of structured prediction is inspired by the fact that samples in the target domain are well clustered within the deep feature space so that unsupervised clustering analysis can be used to facilitate accurate pseudo-labeling. Experimental results on four datasets (i.e. Office-Caltech, Office31, ImageCLEF-DA and Office-Home) validate our approach outperforms contemporary state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Domain adaptation problems exist in many real-world applications. Unsupervised Domain Adaptation (UDA) aims to address problems where the unlabeled test samples and the labeled training samples are from different domains (dubbed target and source domains, respectively). Models learned with labeled samples from the source domain suffer from significant performance reduction when they are directly applied to the target samples without domain adaptation. Such reduced performance is mainly due to the domain shift characterized by the difference of data distributions between the two domains. To address this issue, approaches to UDA have been proposed trying to align the source and target domains by learning a joint subspace so that samples from either domain can be projected into this common subspace. Different algorithms were employed to promote the separability of target samples in such subspaces. The use of visual Copyright Â© 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. features extracted by deep models pre-trained on the largescale ImageNet dataset <ref type="bibr" target="#b3">(Deng et al. 2009</ref>) further facilitates these feature transformation based approaches. On the other hand, deep learning models are used to learn domain invariant features in an end-to-end manner. The gradient reversal layer <ref type="bibr" target="#b5">(Ganin and Lempitsky 2015)</ref> and adversarial learning <ref type="bibr" target="#b21">(Tzeng et al. 2017</ref>) have been employed for this purpose.</p><p>In the UDA problem, both labeled samples from the source domain and unlabeled samples from the target domain are assumed to be available during model training and hence it is a transductive learning problem. The opportunity of transductive learning is when the algorithm can explore the test data (samples from the target domain in the case of UDA). One effective method is to assign pseudo-labels to the target samples so that samples from both domains can be combined and ready for supervised learning. The accuracy of pseudo-labeling plays an important role in the whole learning process. In most existing methods, the target samples are pseudo-labeled independently once the classifier is trained overlooking the structural information underlying the target domain.</p><p>In this paper, we explore such structural information via unsupervised learning (i.e. K-means) and propose a novel UDA approach based on selective pseudo-labeling and structured prediction. Specifically, our approach tries to learn a domain invariant subspace by Supervised Locality Preserving Projection (SLPP) using both labeled source data and pseudo-labeled target data. The accuracy of pseudolabeling is promoted by structured prediction and progressively selections. The contributions of this work can be summarized as follows:</p><p>-a novel iterative learning algorithm is proposed for UDA using SLPP based subspace learning and the selective pseudo-labeling strategy.</p><p>-structured prediction is employed to explore the structural information within the target domain to promote the accuracy of pseudo-labeling and domain alignment.</p><p>-thorough comparative experiments and ablation studies are conducted to demonstrate the proposed approach can achieve new state-of-the-art performance on four benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Early works on UDA problems aim to align the marginal distributions of source and target domains <ref type="bibr" target="#b8">(Gong et al. 2012;</ref><ref type="bibr" target="#b5">Ganin and Lempitsky 2015;</ref><ref type="bibr" target="#b20">Sun, Feng, and Saenko 2016)</ref>. With aligned marginal distributions, it is not guaranteed to produce good classification results as the conditional distribution of the target domain can be misaligned with that of the source domain. This is mainly due to the lack of labeled target samples. To overcome this issue, many UDA approaches have employed pseudo-labeling strategies during learning <ref type="bibr" target="#b11">(Long et al. 2013;</ref><ref type="bibr" target="#b27">Zhang, Li, and Ogunbona 2017;</ref><ref type="bibr" target="#b18">Pei et al. 2018;</ref><ref type="bibr" target="#b25">Zhang et al. 2018;</ref><ref type="bibr" target="#b24">Wang, Bu, and Breckon 2019;</ref><ref type="bibr" target="#b2">Chen et al. 2019b</ref>). Pseudo labeling the target samples allows to align the conditional distributions of source and target domains with traditional supervised learning algorithms. To give a sketch of how existing works handle the issue of lacking labeled samples in the target domain, in this section, we review related works on UDA by dividing them into three categories: approaches without pseudo-labeling, pseudo-labeling without selection and pseudo-labeling with selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approaches without Pseudo-Labeling</head><p>Approaches to UDA aiming to align the marginal distributions of source and target domains can be realized via minimising the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b12">(Long et al. 2014;</ref><ref type="bibr" target="#b20">Sun, Feng, and Saenko 2016)</ref>. The same idea has also been employed in deep learning based approaches to learning domain invariant features <ref type="bibr" target="#b13">(Long et al. 2015;</ref><ref type="bibr" target="#b20">Sun and Saenko 2016;</ref><ref type="bibr" target="#b14">Long et al. 2016;</ref><ref type="bibr" target="#b1">Chen et al. 2019a</ref>). Alternatively, the same goal can be achieved by the gradient reversal layer <ref type="bibr" target="#b5">(Ganin and Lempitsky 2015;</ref><ref type="bibr" target="#b6">Ganin et al. 2016)</ref> or generative adversarial loss <ref type="bibr" target="#b21">(Tzeng et al. 2017)</ref>. Although these models can learn domain invariant features which are also discriminative for the source domain, the separability of target samples is not guaranteed since the conditional distributions are not explicitly aligned. More recently, domain-symmetric networks were proposed to promote the alignment of joint distributions of feature and category across source and target domains <ref type="bibr" target="#b26">(Zhang et al. 2019)</ref>. In contrast to these approaches, pseudo-labeling target samples is another effective way to promote the alignment of conditional distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo-Labeling without Selection</head><p>Pseudo-labeling without selection assigns pseudo-labels to all samples in the target domain. Two strategies, i.e. hard labeling <ref type="bibr" target="#b11">(Long et al. 2013;</ref><ref type="bibr" target="#b27">Zhang, Li, and Ogunbona 2017;</ref>) and soft labeling <ref type="bibr" target="#b18">(Pei et al. 2018)</ref>, have been employed in existing works. The strategy of hard labeling assigns a pseudo-labelÅ· to each unlabeled sample without considering the confidence. It can be achieved by a classifier trained on labeled source samples <ref type="bibr" target="#b11">(Long et al. 2013;</ref><ref type="bibr" target="#b27">Zhang, Li, and Ogunbona 2017;</ref>). The pseudo-labeled target samples together with labeled source samples are used to learn an improved classification model. By iterative learning, the pseudo-labeling is expected to be progressively more accurate until convergence. The prob-lem of such hard pseudo-labeling is that mis-labeled samples by a weak classifier in the initial stage of the iterative learning can cause serious harm to the subsequent learning process. To address this issue, soft labeling was employed in <ref type="bibr" target="#b18">(Pei et al. 2018)</ref>. The strategy of soft labeling assigns the conditional probability of each class p(c|x) given a target sample x, which results in a pseudo-labeling vector y = {p(c 1 |x), p(c 2 |x), ..., p(c |C| |x)} â [0, 1] |C| , where |C| is the number of classes. The soft labelÅ· can be updated during iterative learning and the final classification results can be derived by selecting the class with the highest probability. Soft labeling is naturally suitable for neural network based approaches whose outputs are usually a vector of conditional probabilities. For instance, in the Multi-Adversarial Domain Adaptation (MADA) approach <ref type="bibr" target="#b18">(Pei et al. 2018)</ref>, the soft pseudo-label of a target sample is used to determine how much this sample should be attended to different classspecific domain discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo-Labeling with Selection</head><p>Selective pseudo-labeling is the other way to alleviate the mis-labeling issue <ref type="bibr" target="#b25">(Zhang et al. 2018;</ref><ref type="bibr" target="#b24">Wang, Bu, and Breckon 2019;</ref><ref type="bibr" target="#b2">Chen et al. 2019b</ref>). Similar to the soft labeling strategy, selective pseudo-labeling also takes into consideration the confidence in target sample labeling but in a different manner. Specifically, a subset of target samples are selected to be assigned with pseudo labels and only these pseudo-labeled target samples are combined with source samples in the next iteration of learning. The idea is that at the beginning the classifier is weak so that only a small fraction of the target samples can be correctly classified. When the classifier gets stronger after each iteration of learning, more target samples can be correctly classified hence should be pseudo-labeled and participate in the learning process.</p><p>One key factor in such algorithms is the criterion of sample selection for pseudo-labeling. An easy-to-hard strategy was employed in <ref type="bibr" target="#b2">(Chen et al. 2019b</ref>). Target samples whose similarity scores are higher than a threshold are selected for pseudo-labeling and this threshold is updated after each iteration of learning so that more unlabeled target samples can be selected. One limitation of this sample selection strategy is the risk of biasing to "easy" classes and the selected samples in the first iterations can be dominated by these "easy" classes. As a result, the learned model will be seriously biased to the "easy" classes. To address this issue, a class-wise sample selection strategy was proposed in <ref type="bibr" target="#b24">(Wang, Bu, and Breckon 2019)</ref>. Samples are selected for each class independently so that pseudo-labeled target samples will contribute to the alignment of conditional distribution for each class during learning. In this paper, we propose a novel approach to selective pseudo-labeling by exploring the structural information within the unlabeled target samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>The proposed method aims to align the conditional distributions of source and target domains. We employ Supervised Locality Preserving Projection (SLPP) <ref type="bibr" target="#b9">(He and Niyogi 2004)</ref> as an enabling technique to learn a projection matrix P which maps samples from both domains into the same latent subspace. The learned subspace is expected to have favourable properties that projections of samples from the same class will be close to each other regardless of which domain they are from. In the subspace, a classifier such as nearest neighbour is used to classify unlabeled target samples. By combining both pseudo-labeled target samples and labeled source samples, the projection matrix P can be updated. As a result, an iterative learning process is employed to improve the projection learning and pseudo-labeling alternately as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (a). Following <ref type="bibr" target="#b24">(Wang, Bu, and Breckon 2019)</ref>, we use the nearest class prototype (NCP) for pseudo-labeling. Source class prototypes are computed by averaging projections of source samples from the same class in the same space. Target samples can be pseudo-labeled by measuring the distances to these class prototypes (see <ref type="figure" target="#fig_0">Figure 1</ref>(b)). This method overlooks the intrinsic structural information in the target domain, resulting in sub-optimal pseudo-labeling results. Therefore, we explore the structural information underlying the target domain via clustering analysis (e.g., K-means). The clusters of target samples are matched with source classes via structured prediction <ref type="bibr" target="#b25">(Zhang and Saligrama 2016;</ref><ref type="bibr" target="#b22">Wang and Chen 2017)</ref> so that target samples can be labeled collectively according to which cluster they belong to (see <ref type="figure" target="#fig_0">Figure 1</ref> (c)). We calculate the distances of target samples to cluster centers as the criterion for selective pseudolabeling. The samples close to the cluster center are more likely to be selected for pseudo-labeling and participate in the projection learning in the next iteration of learning. In contrast to the existing sample selection strategies <ref type="bibr" target="#b2">(Chen et al. 2019b;</ref><ref type="bibr" target="#b24">Wang, Bu, and Breckon 2019)</ref>, the structured prediction based method tends to select samples far away from the source samples which enables a faster domain alignment. Moreover, since these two methods are intrinsically different from each other, a combination of two is expected to further benefit the learning process.</p><p>In the following subsections, we give the formulation of the problem, describe each component of the proposed method in detail and analyse the complexity of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation</head><p>Given a labeled dataset D s = {(x s i , y s i )}, i = 1, 2, ..., n s from the source domain S, x s i â R d represents the feature vector of i-th labeled sample in the source domain, d is the feature dimension and y s i â Y s denotes the corresponding label. UDA aims to classify an unlabeled data set D t = {x t i }, i = 1, 2, ..., n t from the target domain T , where x t i â R d represents the feature vector in the target domain. The target label space Y t is equal to the source label space Y s . It is assumed that both the labeled source domain data D s and the unlabeled target domain data D t are available for model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimensionality Reduction</head><p>High dimensional features contain redundant information and thus result in unnecessary computation. We apply dimensionality reduction to the high dimensional deep features as the preprocessing. Principle Component Analysis (PCA) is selected in our work since it has been successfully used in other existing UDA approaches <ref type="bibr" target="#b11">(Long et al. 2013;</ref><ref type="bibr" target="#b12">Long et al. 2014)</ref>. Feature vectors of samples from source and target domains are concatenated as a matrix X =</p><formula xml:id="formula_0">[x s 1 , ..., x s ns , x t 1 , ..., x t nt ] â R dÃn , where n = n s + n t . The centering matrix is denoted as H = I â 1 n 1, where 1 is an n Ã n matrix of ones. The objective of PCA is: max V T V=I tr(V T XHX T V)<label>(1)</label></formula><p>where tr(Â·) denotes the trace of a matrix. The problem defined in Eq.(1) is equivalent to the following eigenvalue problem:</p><formula xml:id="formula_1">XHX T v = Ïv.</formula><p>(2)</p><p>By solving the eigenvalue problem, we can have the PCA projection matrix V = [v 1 , ..., v d1 ] â R dÃd1 and the lowerdimensional features:X</p><formula xml:id="formula_2">= V T X<label>(3)</label></formula><p>whereX â R d1Ãn and d 1 â¤ d is the dimensionality of the feature space after applying PCA. Since PCA is a linear feature transformation, L2 normalization is applied to each feature vector inX asx â x/||x|| 2 . The use of L2 normalization forces samples of both source and target domains distributed on the surface of the same hyper-sphere which helps to align data from different domains <ref type="bibr" target="#b22">(Wang and Chen 2017)</ref>. Our experimental results in this study also provide empirical evidence that such sample normalization is beneficial to superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Alignment</head><p>The lower-dimensional feature spaceX learned by PCA and sample normalisation exhibit favourable properties of domain alignment. However, it is learned in an unsupervised manner and is thus not sufficiently discriminative. To promote the class-wise alignment of two domains, we use the supervised locality preserving projection <ref type="bibr" target="#b9">(He and Niyogi 2004)</ref> as an enabling technique to learn a domain invariant yet discriminative subspace Z fromX .</p><p>The objective of SLPP is to learn a projection matrix P by minimizing the following cost function :</p><formula xml:id="formula_3">min P i,j ||P Tx i â P Tx j || 2 2 M ij ,<label>(4)</label></formula><p>where P â R d1Ãd2 and d 2 â¤ d 1 is the dimensionality of the learned space;x i is the i-th column of the labeled data matrixX l â R d1Ã(ns+n t ) andX l is a collection of n s labeled source data and n t selected pseudo-labeled target data. The similarity matrix M â R (ns+n t )Ã(ns+n t ) is defined as follows:</p><formula xml:id="formula_4">M ij = 1, y i = y j , 0, otherwise.<label>(5)</label></formula><p>The idea is that samples from the same class should be projected close to each other in the subspace regardless of which domain they are originally from. Eq <ref type="formula" target="#formula_4">(5)</ref> is a simplified version of MMD matrices used in <ref type="bibr" target="#b11">(Long et al. 2013;</ref><ref type="bibr" target="#b27">Zhang, Li, and Ogunbona 2017;</ref> where the domain differentiation is reserved while we try to promote the domain invariance. Our definition of similarity matrix in Eq (5) also differs from that in the original LPP formulation where local structure of the samples is considered by defining the similarity value M ij based on the distance betweeÃ± x i andx j . Following the treatment in <ref type="bibr" target="#b9">(He and Niyogi 2004;</ref><ref type="bibr" target="#b22">Wang and Chen 2017)</ref>, the objective can be rewritten as:</p><formula xml:id="formula_5">max P tr(P TXl DX lT P) tr(P T (X l LX lT + I)P)<label>(6)</label></formula><p>where L = D â M is the laplacian matrix, D is a diagonal matrix with D ii = j M ij and the regularization term tr(P T P) is added for penalizing extreme values in the projection matrix P. The problem defined in Eq. <ref type="formula" target="#formula_5">(6)</ref> is equivalent to the following generalized eigenvalue problem:</p><formula xml:id="formula_6">X l DX lT p = Î»(X l LX lT + I)p,<label>(7)</label></formula><p>solving the generalized eigenvalue problem gives the optimal solution P = [p 1 , ..., p d2 ] where p 1 , ..., p d2 are the eigenvectors corresponding to the largest d 2 eigenvalues.</p><p>Learning the projection matrix P for domain alignment requires labeled samples from both source and target domains. To get pseudo-labels of target samples for projection learning, we describe pseudo-labeling methods via nearest class prototype and structured prediction respectively in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo-Labeling via Nearest Class Prototype (NCP)</head><p>Unlabeled target samples can be labeled in the learned subspace Z where the projections of source and target samples are computed by:</p><formula xml:id="formula_7">z s = P Txs , z t = P Txt .<label>(8)</label></formula><p>We sequentially apply centralisation (i.e. mean subtraction, z â z âz, wherez is the mean of all source and target sample projections) and L2 normalisation to z to promote the separability of different classes in the space Z. The class prototype for class y â Y is defined as the mean vector of the projected source samples whose labels are y, which can be computed by:</p><formula xml:id="formula_8">z s y = ns i=1 z s i Î´(y, y s i ) ns i=1 Î´(y, y s i ) ,<label>(9)</label></formula><p>where Î´(y, y i ) = 1 if y = y i and 0 otherwise. After applying L2 normalization to the class prototypesz s y , y = 1, ..., |Y|, where |Y| denotes the number of classes, we can derive the conditional probability of a given target sample x t belonging to class y:</p><formula xml:id="formula_9">p 1 (y|x t ) = exp (â||z t âz s y ||) |Y| y=1 exp (â||z t âz s y ||)</formula><p>.</p><p>(10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo-Labeling via Structured Prediction (SP)</head><p>Pseudo-labeling via nearest class prototype does not consider the intrinsic structure of the target samples which provides useful information for target samples classification.</p><p>To explore such structure information, we employ structured prediction for pseudo-labeling. Specifically, we use K-means to generate |Y| clusters over projection vectors z t of all target samples. The cluster centers are initialised with class prototypes calculated by Eq. (9). Subsequently, we establish a one-to-one match between a cluster from the target domain and a class from the source domain so that the sum of distances of all the matched pairs of the cluster center and the class prototype is minimised. Let A â {0, 1} |Y|Ã|Y| denote the one-to-one matching matrix where A ij = 1 indicates that the i-th target cluster is matched with the j-th source class. The optimisation problem can be formulated as follows:</p><formula xml:id="formula_10">min A |Y| i=1 |Y| j=1 A ij d(z t i ,z s j ) s.t. âi, j A ij = 1; âj, i A ij = 1,<label>(11)</label></formula><p>wherez t i denotes the i-th cluster center in the target domain. This problem can be efficiently solved by linear programming according to <ref type="bibr" target="#b25">(Zhang and Saligrama 2016;</ref><ref type="bibr" target="#b22">Wang and Chen 2017)</ref>.</p><p>Letz t y denote the cluster center corresponding to the class y, similar to Eq. (10), we can calculate the conditional probability of a given target sample x t belonging to class y:</p><formula xml:id="formula_11">p 2 (y|x t ) = exp (â||z t âz t y ||) |Y| y=1 exp (â||z t âz t y ||) .<label>(12)</label></formula><p>Iterative Learning with Selective Pseudo-Labeling (SPL)</p><p>We use an iterative learning strategy to learn the projection matrix P for domain alignment and improved pseudolabeling for target samples alternately. Although either of Algorithm 1 Unsupervised Domain Adaptation Using Selective Pseudo-Labeling Input: Labeled source data set D s = {(x s i , y s i )}, i = 1, 2, ..., n s and unlabeled target data set D t = {x t i }, i = 1, 2, ..., n t , dimensionality of PCA and SLPP subspace d 1 and d 2 , number of iteration T . Output: The projection matrix P and predicted labels {Å· t } for target samples. 1: Initialize k = 0; 2: Dimensionality reduction by Eq. (3); 3: Learn the projection P 0 using only source data D s ; 4: Assign pseudo labels for all target data using Eq. (14); 5: while k &lt; T do 6:</p><formula xml:id="formula_12">k â k + 1; 7:</formula><p>Select a subset of pseudo-labeled target data S k â D t ; 8:</p><p>Learn P k using D s and S k ; 9:</p><p>Update pseudo labels for all target data using Eq.(14). 10: end while the two pseudo-labeling methods described above is able to provide useful pseudo-labeled target samples for projection learning in the next iteration, they are intrinsically different. Pseudo-labeling via nearest class prototype tends to output high probability to the samples close to the source data, whilst structured prediction is confident in samples close to the cluster center in the target domain regardless how far they are from the source domain. We advocate to take advantage of the complementarity of these two methods via a simple combination of Eq.(10) and Eq.(12) as follows:</p><formula xml:id="formula_13">p(y|x t ) = max{p 1 (y|x t ), p 2 (y|x t )}.<label>(13)</label></formula><p>As a result, the pseudo-label of a given target sample x t can be predicted by:Å· t = arg max yâY p(y|x t ).</p><p>Now we have pseudo labels for all the target samples as well as the probability of these pseudo labels, denoted as a set of tripletsD t = {(x t i ,Å· t i , p(Å· t i |x t i ))}, i = 1, ..., n t . Instead of using all the pseudo-labeled target samples for the projection learning, we progressively select a subset S k âD t containing kn t /T target samples in the k-th iteration, where T is the number of iterations of the learning process. One straight forward strategy is to select top kn t /T samples with highest probabilities fromD t . However, this strategy has a risk of only selecting samples from specific classes while overlooking the other classes. To avoid this, we do the class-wise selection so that target samples pseudo-labeled as each class have the same opportunity to be selected. Specifically, for each class c â Y, we first pick out n c t target samples pseudo-labeled as class c from which we select top kn c t /T high-probability samples to form S k . The overall algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Complexity</head><p>We analyse the computation complexity of our learning algorithm. The complexity of PCA is O(dn 2 + d 3 ). The com-plexity of SLPP is O(2d 1 n 2 + d 3 1 ) which is repeated for T times and leads to approximately O(T (2d 1 n 2 + d 3 1 )). Since the iterative learning contributes the most to the computation cost, a small value of d 1 &lt; d can make the learning process more efficient when n is not too big.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>In this section, we describe our experiments on four commonly used domain adaptation datasets (i.e. Office+Caltech <ref type="bibr" target="#b8">(Gong et al. 2012</ref>  <ref type="formula" target="#formula_0">. 2017)</ref>). Our approach is firstly compared with state-of-the-art UDA approaches to evaluate its effectiveness. An ablation study is conducted to demonstrate the effects of different components and hyper-parameters in our approach. Finally, we investigate how different hyperparameters affect the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Office+Caltech <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setting</head><p>The algorithm is implemented in Matlab 1 . We use the deep features commonly used in existing works for a fair comparison with the state of the arts. As a result, the Decaf6 <ref type="bibr" target="#b4">(Donahue et al. 2014)</ref> features (activations of the 6th fully connected layer of a convolutional neural network trained on ImageNet, d = 4096) are used for the Office-Caltech dataset. For the other three datasets, ResNet50 <ref type="bibr" target="#b10">(He et al. 2016)</ref> has been commonly used to extract features or as the backbone of deep models in the literature, hence we   <ref type="bibr" target="#b24">(Wang, Bu, and Breckon 2019)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-Art Approaches</head><p>We compare our approach with the state of the arts including those based on deep features (extracted using deep models such as ResNet50 pre-trained on ImageNet) and deep learning models. The classification accuracy of our approaches and the comparative ones are shown in Tables 1-4 in terms of each combination of "source" â "target" domains and the average accuracy over all different combinations. In each table, the results of deep learning based models are listed on the top followed by deep feature based methods including ours. Our approach is denoted as SPL (Selective Pseudo-Labeling). We use bold and underlined fonts to indicate the best and the second best results respectively in each setting. From Tables 1-4 we can see that our proposed approach with the combination of two different pseudo-labeling methods achieves the highest average accuracy (see the last column of each table) consistently on four datasets. Specifically, our proposed SPL achieves an average accuracy of 93.0% on the Office-Caltech dataset <ref type="table" target="#tab_0">(Table 1</ref>), slightly better than MEDA ) which has an average accuracy of 92.8%. On the Office31 dataset <ref type="table" target="#tab_1">(Table 2)</ref>, SPL achieves the best or the second-best performance in five out of six tasks and the best average performance of 89.6% while the second-highest average accuracy (88.4%) was achieved by the deep learning based approaches Sym-Nets <ref type="bibr" target="#b26">(Zhang et al. 2019)</ref> and ). On the ImageCLEF-DA dataset <ref type="table" target="#tab_3">(Table 3)</ref>, the proposed SPL approach performs the best or the second-best in four out of six tasks and ranks the first with the average accuracy of 90.5% followed by the deep learning model SymNets <ref type="bibr">(Zhang et al. 2019, 89.9%)</ref> and deep feature based model MEDA <ref type="bibr">(Wang et al. 2018, 89</ref>.0%). On the Office-Home dataset <ref type="table" target="#tab_4">(Table  4</ref>, again, our approach SPL outperforms all state-of-the-art models with an average accuracy of 71.0% against 70.6% by CAPLS <ref type="bibr" target="#b24">(Wang, Bu, and Breckon 2019)</ref> and 67.6% by SymNets <ref type="bibr" target="#b26">(Zhang et al. 2019</ref>) and TADA .</p><p>In summary, our selective pseudo-labeling approach can outperform both deep learning models and traditional feature transformation approaches on four commonly used datasets for UDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We conduct an ablation study to analyse how different components of our work contribute to the final performance. To this end, we investigate different combinations of four components: pseudo-labeling (PL), sample selection (S) for pseudo-labeling, nearest class prototype (NCP) and structured prediction (SP). We report the average classification accuracy on four datasets in <ref type="table" target="#tab_5">Table 5</ref>. It can be observed that methods with pseudo-labeling outperform those without pseudo-labeling and the use of selective pseudo-labeling further improves the performance significantly on all datasets. In terms of the pseudo-labeling strategy, structured prediction (SP) outperforms nearest class prototype (NCP) consistently while the combination of these two can further improve the accuracy marginally. The dimensionality of SLPP space  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Hyper-parameters</head><p>Our approach has three hyper-parameters: the dimensionality of PCA space d 1 , the dimensionality of SLPP space d 2 and the number of iterations T . We investigate how each hyper-parameter affects the performance by setting it to a series of different values while fixing the other two. The results are shown in <ref type="figure" target="#fig_3">Figure 2</ref> in which the average accuracy over all possible source-target pairs are reported for four datasets. As we can see, the datasets with more classes (e.g., Office31 and Office-Home) tend to large values of d 1 while small values of d 1 are beneficial to the datasets with fewer classes (e.g., Office-Caltech and ImageCLEF-DA). In terms of the dimensionality of SLPP space d 2 , the performance does not change too much unless its value is less than the number of classes. The number of iterations T has nearly zero effect on the performance when it is greater than 2. To summarize, our approach is not sensitive to the hyper-parameters and performs comparably well if only d 2 is set greater than the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We propose a novel selective pseudo-labeling approach to UDA by incorporating supervised subspace learning and structured prediction based pseudo-labeling into an iterative learning framework. The proposed approach outperforms other state-of-the-art methods on four benchmark datasets. The ablation study demonstrates the effectiveness of selective pseudo-labeling and structured prediction which can also be employed to train the deep learning models for UDA in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualisation of Domain Alignment</head><p>To provide intuitive insights of domain alignment results using the proposed algorithm, we take the most challenging Office-Home dataset <ref type="bibr" target="#b21">(Venkateswara et al. 2017)</ref> as an example to visualise the data distributions before and after domain alignment. We randomly select 20 out of 65 classes from this dataset for a clear view. The t-SNE <ref type="bibr" target="#b17">(Maaten and Hinton 2008)</ref> technique is used to enable the 2D visualisation. <ref type="figure">Figure 3</ref> shows the visualisation results in the original deep feature space (left, before the domain alignment) and the learned subspace (right, after the domain alignment). The symbol "o" is used to denote samples from the source domain whilst "+" is for samples from the target domain. Different classes are indicated by different colors.</p><p>It can be seen that our domain alignment algorithm is able to promote the separability in the learned subspace (right). Both the marginal and conditional distributions are well aligned. It can also be observed from <ref type="figure">Figure 3</ref> that data distributions in the original deep feature space are already good enough due to the transferrability of deep models pre-trained on ImageNet <ref type="bibr" target="#b3">(Deng et al. 2009</ref>). This is also validated by the results <ref type="table" target="#tab_6">(Table 6</ref>) of two baseline methods, i.e., 1 Nearest Neighbor (1NN) and Support Vector Machine (SVM), based solely on the labeled source samples without any domain adaptation.</p><p>It is such favorable properties of deep features that enables the structured prediction algorithm to be able to take advantage of the cluster information and promote the domain alignment performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Process of NCP and SP</head><p>We have demonstrated the superiority of the Structured Prediction (SP) over the Nearest Class Prototype (NCP) method in the ablation study. Here we show more details of the learning process when two different methods are employed in <ref type="figure">Figure 4</ref>. In this experiment, we set the number of iterations to 10 and report the classification accuracy for each iteration. It can be seen from <ref type="figure">Figure 4</ref> that with more iterations (more pseudo-labeled target samples), the classification performance is improved gradually. The SP method can achieve much better performance than the NCP method at the beginning of the iterative learning process. It is due to the benefit of exploiting the structural information in the target domain. With more iterations when more target samples are pseudo-labeled and participate in the domain alignment, the classification accuracy can be improved gradually for both SP and NCP methods, although SP can always achieve better final performance than NCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Response to Reviewers</head><p>Reviewer1 While the results on Office-Home dataset are very good, a number of benchmarks datasets such as CIFAR, SVHN were omitted in the comparison. Can the authors explain why a more versatile collection of datasets was not used ? in that regards, what is the main limitation of the method ?</p><p>The selected four datasets in our experiments have been widely used in recent publications. We were aware of other datasets used for UDA including those mentioned by the reviewer (e.g., MNIST-SVHN, CIFAR-STL, etc.). We select the datasets containing more domains so that there are more sub-tasks (i.e., three domains lead to six sub-tasks and four domains lead to 12 sub-tasks) on which the proposed method can be thoroughly evaluated and compared against state-of-the-arts fairly. We have also conducted experiments on the CIFARâSTL and STLâCIFAR tasks using ResNet50 features and achieved 94.9% and 82.4% accuracy respectively. However, our approach is not suitable for the Digit datasets due to the limitation of excessive memory usage when the number of samples is too large. This limitation is shared by many feature transformation based methods and addressing it can be an interesting direction for our future work.</p><p>Reviewer2 I've seen the computational complexity section, but can you comment on how longs do the experiments take in practice?</p><p>Running on a laptop with Intel i5-7300 CPU and 32G RAM, it takes around 18, 83, 10, 2070 seconds for all sub-tasks of the Office-Caltech, Office31, ImageCLEF-DA and Office-Home datasets, respectively.</p><p>Reviewer3 Q1. In the second paragraph p.3, the authors stated "the structured prediction based method tends to select samples far away from the source samples". I cannot find the reason in the paper.</p><p>Thanks for pointing out this ambiguous statement in the manuscript. We would like to give an explanation here and revise the statement in the manuscript accordingly to dismiss the potential ambiguity and misleading. Most existing sample selection strategies select the target samples close to the source class prototypes (i.e. source class means) for pseudo labeling. As a result, the selected target samples are usually the outliers of the corresponding target cluster. One can imagine two point clusters (which are not completely overlapped due to the domain shift) representing the source sample cluster and the corresponding target sample cluster belonging to the same class, then the closest target point to the source cluster center must locate in the outermost shell of the target point cluster. In contrast, our structured prediction based method aims to select the target samples closest to the "target" cluster center rather than the "source" cluster center for pseudo labeling. As a result, the selected target samples are not necessarily the "closest" ones to the source samples and this is why we state "the structured prediction based method tends to select samples far away from the source samples". The pseudo-labeled target samples will participate the next iteration of supervised learning and therefore will be "pulled" to the source data of the same class. If the pseudo-labeled target samples are only the outliers or the outermost ones of the target cluster, the result of the "pulling" will only force the source and target clusters to "touch" each other. However, if the pseudolabeled target samples are close to the target cluster center, the result of the "pulling" will force the two clusters align. This explains why our structured prediction based method enables a faster (i.e. less iterations) domain alignment.</p><p>Q2. Why the pseudo-labeling approach is employed? It seems not to be a straightforward implementation of the cluster assumption. Cannot the assumption be implemented as a loss function like as, for example, a similar formulation to the semi-supervised loss, which encourages the decision boundary not to cross the dense region of unsupervised instances?</p><p>We agree that a straightforward implementation of the cluster assumption is to learn decision boundaries passing the lowdensity region of unlabeled samples. Although it works well in semi-supervised learning where the unlabeled samples come from the same distribution as the labeled samples, it is not necessarily a good solution to the domain adaptation problem. The reason is, in the domain adaptation problem, the unlabeled target samples and labeled source samples are from different domains and hence they can be distributed in separate clusters even they belong to the same class. As a result, the decision boundaries learned under the cluster assumption for UDA may not be class discriminative (they might be boundaries of domains rather than classes, see <ref type="figure" target="#fig_0">Figure 1</ref> (a) in [2]). Actually, the existing works[1][2] following the cluster assumption for UDA also employ pseudo-labeling to address this issue. As we have discussed in the section of related work, pseudolabeling is one of the enabling techniques applied in many unsupervised domain adaptation approaches towards the alignment of both marginal and conditional distributions. Our approach aims to learn a subspace where the two domains are aligned via supervised LPP which requires pseudo-labeled samples from the target domain for conditional distribution alignment. The assumption of cluster structure in the target domain is explored by our structured prediction algorithm which predicts the pseudo-labels of target samples collectively (i.e. cluster-wisely). In this sense, the cluster assumption has been used in a totally different way from learning decision boundaries passing the low-density region. In summary, the clus-ter assumption in UDA is different from that in the semi-supervised learning problem; and we explore the cluster structural information in the target domain via our structured prediction algorithm rather than the low-density separation idea for semi-supervised learning.</p><p>[1] Shu, Rui, et al. "A dirt-t approach to unsupervised domain adaptation." ICLR 2018.</p><p>[2] Kumar, Abhishek, et al. "Co-regularized alignment for unsupervised domain adaptation." NIPs 2018.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The framework of our proposed approach. (a) iterative learning process (b) pseudo-labeling via nearest class prototype (c) pseudo-labeling via structured prediction. The red and the blue colors are used for the target and source domains respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>), Office31 (Saenko et al. 2010), ImageCLEF-DA (Caputo et al. 2014) and Office-Home (Venkateswara et al</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b8">Gong et al. 2012)</ref> consists of four domains: Amazon (A, images downloaded from online merchants), Webcam (W, low-resolution images by a web camera), DSLR (D, high-resolution images by a digital SLR camera) and Caltech-256 (C). Ten common classes from all four domains are used: backpack, bike, calculator, headphone, computer-keyboard, laptop-101, computer-monitor, computer-mouse, coffee-mug, and video-projector. There are 2533 images in total with 8 to 151 images per category per domain. Office31(Saenko et al. 2010) consists of three domains: Amazon (A), Webcam (W) and DSLR (D). There are 31 common classes for all three domains containing 4,110 images in total. ImageCLEF-DA (Caputo et al. 2014) consists of four domains. We follow the existing works (Zhang et al. 2019) using three of them in our experiments: Caltech-256 (C), ImageNet ILSVRC 2012 (I), and Pascal VOC 2012 (P). There are 12 classes and 50 images for each class in each domain. Office-Home (Venkateswara et al. 2017) is another dataset recently released for evaluation of domain adaptation algorithms. It consists of four different domains: Artistic images (A), Clipart (C), Product images (P) and Real-World images (R). There are 65 object classes in each domain with a total number of 15,588 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The effect of hyper-parameters (i.e. the dimensionality of PCA space d 1 , the dimensionality of SLPP sapce d 2 and the number of iterations T ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Data distributions in the original deep feature space (left) and the learned subspace (right) for 20 classes from the Home-Office dataset. "o" and "+" denote the source and target samples respectively. Different classes are denoted by different colors. Accuracy curves of iterative learning process using nearest class prototype (NCP) and structured prediction (SP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification Accuracy (%) on Office-Caltech dataset using Decaf6 features. Each column displays the results of a pair of source â target setting.</figDesc><table><row><cell>Method</cell><cell cols="13">CâA CâW CâD AâC AâW AâD WâC WâA WâD DâC DâA DâW Average</cell></row><row><cell>DDC(Tzeng et al. 2014)</cell><cell>91.9</cell><cell>85.4</cell><cell>88.8</cell><cell>85.0</cell><cell>86.1</cell><cell>89.0</cell><cell>78.0</cell><cell>84.9</cell><cell>100.0</cell><cell>81.1</cell><cell>89.5</cell><cell>98.2</cell><cell>88.2</cell></row><row><cell>DAN(Long et al. 2015)</cell><cell>92.0</cell><cell>90.6</cell><cell>89.3</cell><cell>84.1</cell><cell>91.8</cell><cell>91.7</cell><cell>81.2</cell><cell>92.1</cell><cell>100.0</cell><cell>80.3</cell><cell>90.0</cell><cell>98.5</cell><cell>90.1</cell></row><row><cell>DCORAL(Sun and Saenko 2016)</cell><cell>92.4</cell><cell>91.1</cell><cell>91.4</cell><cell>84.7</cell><cell>-</cell><cell>-</cell><cell>79.3</cell><cell>-</cell><cell>-</cell><cell>82.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CORAL(Sun, Feng, and Saenko 2017)</cell><cell>92.0</cell><cell>80.0</cell><cell>84.7</cell><cell>83.2</cell><cell>74.6</cell><cell>84.1</cell><cell>75.5</cell><cell>81.2</cell><cell>100.0</cell><cell>76.8</cell><cell>85.5</cell><cell>99.3</cell><cell>84.7</cell></row><row><cell>SCA(Ghifary et al. 2016)</cell><cell>89.5</cell><cell>85.4</cell><cell>87.9</cell><cell>78.8</cell><cell>75.9</cell><cell>85.4</cell><cell>74.8</cell><cell>86.1</cell><cell>100.0</cell><cell>78.1</cell><cell>90.0</cell><cell>98.6</cell><cell>85.9</cell></row><row><cell>JGSA(Zhang, Li, and Ogunbona 2017)</cell><cell>91.4</cell><cell>86.8</cell><cell>93.6</cell><cell>84.9</cell><cell>81.0</cell><cell>88.5</cell><cell>85.0</cell><cell>90.7</cell><cell>100.0</cell><cell>86.2</cell><cell>92.0</cell><cell>99.7</cell><cell>90.0</cell></row><row><cell>MEDA(Wang et al. 2018)</cell><cell>93.4</cell><cell>95.6</cell><cell>91.1</cell><cell>87.4</cell><cell>88.1</cell><cell>88.1</cell><cell>93.2</cell><cell>99.4</cell><cell>99.4</cell><cell>87.5</cell><cell>93.2</cell><cell>97.6</cell><cell>92.8</cell></row><row><cell>CAPLS (Wang, Bu, and Breckon 2019)</cell><cell>90.8</cell><cell>85.4</cell><cell>95.5</cell><cell>86.1</cell><cell>87.1</cell><cell>94.9</cell><cell>88.2</cell><cell>92.3</cell><cell>100.0</cell><cell>88.8</cell><cell>93.0</cell><cell>100.0</cell><cell>91.8</cell></row><row><cell>SPL (Ours)</cell><cell>92.7</cell><cell>93.2</cell><cell>98.7</cell><cell>87.4</cell><cell>95.3</cell><cell>89.2</cell><cell>87.0</cell><cell>92.0</cell><cell>100.0</cell><cell>88.6</cell><cell>92.9</cell><cell>98.6</cell><cell>93.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="8">: Classification Accuracy (%) on Office31 dataset us-</cell></row><row><cell cols="8">ing either ResNet50 features or ResNet50 based deep mod-</cell></row><row><cell>els.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>AâW</cell><cell>DâW</cell><cell>WâD</cell><cell>AâD</cell><cell>DâA</cell><cell>WâA</cell><cell>Avg</cell></row><row><cell>RTN(Long et al. 2016)</cell><cell cols="2">84.5 96.8</cell><cell cols="5">99.4 77.5 66.2 64.8 81.6</cell></row><row><cell>MADA(Pei et al. 2018)</cell><cell cols="2">90.0 97.4</cell><cell cols="5">99.6 87.8 70.3 66.4 85.2</cell></row><row><cell>GTA (Sankaranarayanan et al. 2018)</cell><cell cols="2">89.5 97.9</cell><cell cols="5">99.8 87.7 72.8 71.4 86.5</cell></row><row><cell>iCAN(Zhang et al. 2018)</cell><cell cols="7">92.5 98.8 100.0 90.1 72.1 69.9 87.2</cell></row><row><cell>CDAN-E(Long et al. 2018)</cell><cell cols="7">94.1 98.6 100.0 92.9 71.0 69.3 87.7</cell></row><row><cell>JDDA(Chen et al. 2019a)</cell><cell cols="2">82.6 95.2</cell><cell cols="5">99.7 79.8 57.4 66.7 80.2</cell></row><row><cell>SymNets(Zhang et al. 2019)</cell><cell cols="7">90.8 98.8 100.0 93.9 74.6 72.5 88.4</cell></row><row><cell>TADA (Wang et al. 2019)</cell><cell cols="2">94.3 98.7</cell><cell cols="5">99.8 91.6 72.9 73.0 88.4</cell></row><row><cell>MEDA(Wang et al. 2018)</cell><cell cols="2">86.2 97.2</cell><cell cols="5">99.4 85.3 72.4 74.0 85.7</cell></row><row><cell>CAPLS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Home respectively. For the dimensionality of the space learned by SLPP, we set d 2 = 128 uniformly for all datasets. The number of iterations T is set to 10 in all experiments unless otherwise specified.</figDesc><table><row><cell cols="4">: Classification Accuracy (%) on ImageCLEF-DA</cell></row><row><cell cols="4">dataset using either ResNet50 features or ResNet50 based</cell></row><row><cell>deep models.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">IâP PâI IâC CâI CâP PâC Avg</cell></row><row><cell>RTN(Long et al. 2016)</cell><cell>75.6 86.8 95.3 86.9</cell><cell>72.7</cell><cell>92.2 84.9</cell></row><row><cell>MADA(Pei et al. 2018)</cell><cell>75.0 87.9 96.0 88.8</cell><cell>75.2</cell><cell>92.2 85.8</cell></row><row><cell>iCAN(Zhang et al. 2018)</cell><cell>79.5 89.7 94.7 89.9</cell><cell>78.5</cell><cell>92.0 87.4</cell></row><row><cell cols="2">CDAN-E(Long et al. 2018) 77.7 90.7 97.7 91.3</cell><cell>74.2</cell><cell>94.3 87.7</cell></row><row><cell cols="2">SymNets(Zhang et al. 2019) 80.2 93.6 97.0 93.4</cell><cell>78.7</cell><cell>96.4 89.9</cell></row><row><cell>MEDA(Wang et al. 2018)</cell><cell>79.7 92.5 95.7 92.2</cell><cell>78.5</cell><cell>95.5 89.0</cell></row><row><cell>SPL (Ours)</cell><cell>78.3 94.5 96.7 95.7</cell><cell>80.5</cell><cell>96.3 90.3</cell></row><row><cell cols="4">use ResNet50 features (d = 2048) in our experiments. Al-</cell></row><row><cell cols="4">though a small dimensionality of the PCA space d 1 is pre-</cell></row><row><cell cols="4">ferred for less computation, it can cause information loss for</cell></row><row><cell cols="4">a dataset with a large number of classes (e.g., Office-Home).</cell></row><row><cell cols="4">To trade off, we set the values of d 1 based on the number of</cell></row><row><cell cols="4">classes in the dataset which results in d 1 = 128, 512, 128</cell></row><row><cell cols="4">and 1024 for Office-Caltech, Office-31, ImageCLEF-DA</cell></row><row><cell>and Office-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Classification Accuracy (%) on Office-Home dataset using either ResNet50 features or ResNet50 based deep models.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell cols="9">AâC AâP AâR CâA CâP CâR PâA PâC PâR RâA RâC RâP Average</cell></row><row><cell></cell><cell></cell><cell cols="3">JAN(Long et al. 2017)</cell><cell></cell><cell>45.9</cell><cell>61.2</cell><cell>68.9</cell><cell>50.4</cell><cell>59.7</cell><cell>61.0</cell><cell>45.8</cell><cell>43.4</cell><cell>70.3</cell><cell>63.9</cell><cell>52.4</cell><cell>76.8</cell><cell>58.3</cell></row><row><cell></cell><cell></cell><cell cols="4">CDAN-E (Long et al. 2018)</cell><cell>50.7</cell><cell>70.6</cell><cell>76.0</cell><cell>57.6</cell><cell>70.0</cell><cell>70.0</cell><cell>57.4</cell><cell>50.9</cell><cell>77.3</cell><cell>70.9</cell><cell>56.7</cell><cell>81.6</cell><cell>65.8</cell></row><row><cell></cell><cell cols="5">SymNets (Zhang et al. 2019)</cell><cell>47.7</cell><cell>72.9</cell><cell>78.5</cell><cell>64.2</cell><cell>71.3</cell><cell>74.2</cell><cell>64.2</cell><cell>48.8</cell><cell>79.5</cell><cell>74.5</cell><cell>52.6</cell><cell>82.7</cell><cell>67.6</cell></row><row><cell></cell><cell></cell><cell cols="3">TADA (Wang et al. 2019)</cell><cell></cell><cell>53.1</cell><cell>72.3</cell><cell>77.2</cell><cell>59.1</cell><cell>71.2</cell><cell>72.1</cell><cell>59.7</cell><cell>53.1</cell><cell>78.4</cell><cell>72.4</cell><cell>60.0</cell><cell>82.9</cell><cell>67.6</cell></row><row><cell></cell><cell></cell><cell cols="3">MEDA(Wang et al. 2018)</cell><cell></cell><cell>54.6</cell><cell>75.2</cell><cell>77.0</cell><cell>56.5</cell><cell>72.8</cell><cell>72.3</cell><cell>59.0</cell><cell>51.9</cell><cell>78.2</cell><cell>67.7</cell><cell>57.2</cell><cell>81.8</cell><cell>67.0</cell></row><row><cell></cell><cell cols="5">CAPLS (Wang, Bu, and Breckon 2019)</cell><cell>56.2</cell><cell>78.3</cell><cell>80.2</cell><cell>66.0</cell><cell>75.4</cell><cell>78.4</cell><cell>66.4</cell><cell>53.2</cell><cell>81.1</cell><cell>71.6</cell><cell>56.1</cell><cell>84.3</cell><cell>70.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SPL (Ours)</cell><cell></cell><cell></cell><cell>54.5</cell><cell>77.8</cell><cell>81.9</cell><cell>65.1</cell><cell>78.0</cell><cell>81.1</cell><cell>66.0</cell><cell>53.1</cell><cell>82.8</cell><cell>69.9</cell><cell>55.3</cell><cell>86.0</cell><cell>71.0</cell></row><row><cell></cell><cell>100</cell><cell></cell><cell cols="2">The effect of</cell><cell>d 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>accuracy (%)</cell><cell>70 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average</cell><cell>50 60</cell><cell></cell><cell></cell><cell cols="3">Office-Caltech Office31 ImageCLEF-DA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Office-Home</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell cols="2">128 256 512</cell><cell></cell></row><row><cell></cell><cell cols="5">The dimensionality of PCA space</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of ablation study.</figDesc><table><row><cell>Method PL S NCP SP</cell><cell cols="4">Office-Caltech Office31 ImageCLEF-DA Office-Home</cell></row><row><cell></cell><cell>81.8</cell><cell>82.0</cell><cell>86.2</cell><cell>63.9</cell></row><row><cell></cell><cell>90.3</cell><cell>87.5</cell><cell>89.5</cell><cell>68.0</cell></row><row><cell></cell><cell>90.7</cell><cell>87.6</cell><cell>89.4</cell><cell>68.1</cell></row><row><cell></cell><cell>85.5</cell><cell>83.7</cell><cell>86.9</cell><cell>66.2</cell></row><row><cell></cell><cell>91.9</cell><cell>88.0</cell><cell>90.0</cell><cell>68.9</cell></row><row><cell></cell><cell>92.0</cell><cell>88.0</cell><cell>90.0</cell><cell>69.0</cell></row><row><cell></cell><cell>90.8</cell><cell>87.8</cell><cell>89.0</cell><cell>70.8</cell></row><row><cell></cell><cell>93.0</cell><cell>89.5</cell><cell>90.2</cell><cell>71.0</cell></row><row><cell></cell><cell>93.0</cell><cell>89.6</cell><cell>90.3</cell><cell>71.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results of 1NN and SVM without domain adaptation.</figDesc><table><row><cell cols="5">Method Office-Caltech Office31 ImageCLEF-DA Office-Home</cell></row><row><cell>1NN</cell><cell>83.8</cell><cell>79.8</cell><cell>80.0</cell><cell>54.5</cell></row><row><cell>SVM</cell><cell>83.9</cell><cell>81.0</cell><cell>85.9</cell><cell>61.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available: https://github.com/hellowangqian/domainadaptation-capls</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Due to the page limit, we present more experimental results and discussion in this supplementary material.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imageclef 2014: Overview and analysis of the results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="192" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint domain alignment and discriminative feature learning for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
	<note>Ganin and Lempitsky</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ghifary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Long</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1647" to="1657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
	<note>Maaten and Hinton</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
	<note>AAAI 2016</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Correlation alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tzeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zero-shot visual recognition via bidirectional latent embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="356" to="383" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
	<note>AAAI Conference on Artificial Intelligence (AAAI)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unifying unsupervised domain adaptation and zero-shot visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno>Zhang et al. 2018</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Saligrama</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
	<note>European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain-symmetric networks for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5031" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint geometrical and statistical alignment for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5150" to="5158" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
